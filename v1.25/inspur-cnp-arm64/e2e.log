I0129 02:56:36.819922      22 e2e.go:116] Starting e2e run "28d95024-e2ae-4f1a-8336-248a37063b71" on Ginkgo node 1
Jan 29 02:56:36.843: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1674960996 - will randomize all specs

Will run 362 of 7067 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
{"msg":"Test Suite starting","completed":0,"skipped":0,"failed":0}
Jan 29 02:56:37.021: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
Jan 29 02:56:37.023: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jan 29 02:56:37.061: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jan 29 02:56:37.123: INFO: 49 / 49 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jan 29 02:56:37.123: INFO: expected 2 pod replicas in namespace 'kube-system', 2 are Running and Ready.
Jan 29 02:56:37.123: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jan 29 02:56:37.135: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Jan 29 02:56:37.135: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'cke-admission-daemonset' (0 seconds elapsed)
Jan 29 02:56:37.135: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'coredns' (0 seconds elapsed)
Jan 29 02:56:37.135: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'kube-multus-ds' (0 seconds elapsed)
Jan 29 02:56:37.135: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'node-problem-detector' (0 seconds elapsed)
Jan 29 02:56:37.135: INFO: e2e test version: v1.25.4
Jan 29 02:56:37.137: INFO: kube-apiserver version: v1.25.4-1
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
Jan 29 02:56:37.137: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
Jan 29 02:56:37.145: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.125 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    Jan 29 02:56:37.021: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    Jan 29 02:56:37.023: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    Jan 29 02:56:37.061: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Jan 29 02:56:37.123: INFO: 49 / 49 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Jan 29 02:56:37.123: INFO: expected 2 pod replicas in namespace 'kube-system', 2 are Running and Ready.
    Jan 29 02:56:37.123: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Jan 29 02:56:37.135: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
    Jan 29 02:56:37.135: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'cke-admission-daemonset' (0 seconds elapsed)
    Jan 29 02:56:37.135: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'coredns' (0 seconds elapsed)
    Jan 29 02:56:37.135: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'kube-multus-ds' (0 seconds elapsed)
    Jan 29 02:56:37.135: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'node-problem-detector' (0 seconds elapsed)
    Jan 29 02:56:37.135: INFO: e2e test version: v1.25.4
    Jan 29 02:56:37.137: INFO: kube-apiserver version: v1.25.4-1
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    Jan 29 02:56:37.137: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    Jan 29 02:56:37.145: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 02:56:37.186
Jan 29 02:56:37.186: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename deployment 01/29/23 02:56:37.187
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 02:56:37.219
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 02:56:37.225
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 01/29/23 02:56:37.237
Jan 29 02:56:37.237: INFO: Creating simple deployment test-deployment-pd5gl
Jan 29 02:56:37.261: INFO: new replicaset for deployment "test-deployment-pd5gl" is yet to be created
STEP: Getting /status 01/29/23 02:56:39.286
Jan 29 02:56:39.296: INFO: Deployment test-deployment-pd5gl has Conditions: [{Available True 2023-01-29 02:56:39 +0000 UTC 2023-01-29 02:56:39 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-01-29 02:56:39 +0000 UTC 2023-01-29 02:56:37 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-pd5gl-777898ffcc" has successfully progressed.}]
STEP: updating Deployment Status 01/29/23 02:56:39.297
Jan 29 02:56:39.309: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 29, 2, 56, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 2, 56, 39, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 29, 2, 56, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 2, 56, 37, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-pd5gl-777898ffcc\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 01/29/23 02:56:39.309
Jan 29 02:56:39.312: INFO: Observed &Deployment event: ADDED
Jan 29 02:56:39.312: INFO: Observed Deployment test-deployment-pd5gl in namespace deployment-6111 with annotations: map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-29 02:56:37 +0000 UTC 2023-01-29 02:56:37 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-pd5gl-777898ffcc"}
Jan 29 02:56:39.312: INFO: Observed &Deployment event: MODIFIED
Jan 29 02:56:39.312: INFO: Observed Deployment test-deployment-pd5gl in namespace deployment-6111 with annotations: map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-29 02:56:37 +0000 UTC 2023-01-29 02:56:37 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-pd5gl-777898ffcc"}
Jan 29 02:56:39.312: INFO: Observed Deployment test-deployment-pd5gl in namespace deployment-6111 with annotations: map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-29 02:56:37 +0000 UTC 2023-01-29 02:56:37 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 29 02:56:39.313: INFO: Observed &Deployment event: MODIFIED
Jan 29 02:56:39.313: INFO: Observed Deployment test-deployment-pd5gl in namespace deployment-6111 with annotations: map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-29 02:56:37 +0000 UTC 2023-01-29 02:56:37 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 29 02:56:39.313: INFO: Observed Deployment test-deployment-pd5gl in namespace deployment-6111 with annotations: map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-29 02:56:37 +0000 UTC 2023-01-29 02:56:37 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-pd5gl-777898ffcc" is progressing.}
Jan 29 02:56:39.313: INFO: Observed &Deployment event: MODIFIED
Jan 29 02:56:39.313: INFO: Observed Deployment test-deployment-pd5gl in namespace deployment-6111 with annotations: map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-29 02:56:39 +0000 UTC 2023-01-29 02:56:39 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 29 02:56:39.313: INFO: Observed Deployment test-deployment-pd5gl in namespace deployment-6111 with annotations: map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-29 02:56:39 +0000 UTC 2023-01-29 02:56:37 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-pd5gl-777898ffcc" has successfully progressed.}
Jan 29 02:56:39.313: INFO: Observed &Deployment event: MODIFIED
Jan 29 02:56:39.313: INFO: Observed Deployment test-deployment-pd5gl in namespace deployment-6111 with annotations: map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-29 02:56:39 +0000 UTC 2023-01-29 02:56:39 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 29 02:56:39.313: INFO: Observed Deployment test-deployment-pd5gl in namespace deployment-6111 with annotations: map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-29 02:56:39 +0000 UTC 2023-01-29 02:56:37 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-pd5gl-777898ffcc" has successfully progressed.}
Jan 29 02:56:39.313: INFO: Found Deployment test-deployment-pd5gl in namespace deployment-6111 with labels: map[e2e:testing name:httpd] annotations: map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 29 02:56:39.313: INFO: Deployment test-deployment-pd5gl has an updated status
STEP: patching the Statefulset Status 01/29/23 02:56:39.313
Jan 29 02:56:39.313: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan 29 02:56:39.321: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 01/29/23 02:56:39.321
Jan 29 02:56:39.325: INFO: Observed &Deployment event: ADDED
Jan 29 02:56:39.325: INFO: Observed deployment test-deployment-pd5gl in namespace deployment-6111 with annotations: map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-29 02:56:37 +0000 UTC 2023-01-29 02:56:37 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-pd5gl-777898ffcc"}
Jan 29 02:56:39.325: INFO: Observed &Deployment event: MODIFIED
Jan 29 02:56:39.325: INFO: Observed deployment test-deployment-pd5gl in namespace deployment-6111 with annotations: map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-29 02:56:37 +0000 UTC 2023-01-29 02:56:37 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-pd5gl-777898ffcc"}
Jan 29 02:56:39.325: INFO: Observed deployment test-deployment-pd5gl in namespace deployment-6111 with annotations: map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-29 02:56:37 +0000 UTC 2023-01-29 02:56:37 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 29 02:56:39.325: INFO: Observed &Deployment event: MODIFIED
Jan 29 02:56:39.325: INFO: Observed deployment test-deployment-pd5gl in namespace deployment-6111 with annotations: map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-29 02:56:37 +0000 UTC 2023-01-29 02:56:37 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 29 02:56:39.325: INFO: Observed deployment test-deployment-pd5gl in namespace deployment-6111 with annotations: map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-29 02:56:37 +0000 UTC 2023-01-29 02:56:37 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-pd5gl-777898ffcc" is progressing.}
Jan 29 02:56:39.326: INFO: Observed &Deployment event: MODIFIED
Jan 29 02:56:39.326: INFO: Observed deployment test-deployment-pd5gl in namespace deployment-6111 with annotations: map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-29 02:56:39 +0000 UTC 2023-01-29 02:56:39 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 29 02:56:39.326: INFO: Observed deployment test-deployment-pd5gl in namespace deployment-6111 with annotations: map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-29 02:56:39 +0000 UTC 2023-01-29 02:56:37 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-pd5gl-777898ffcc" has successfully progressed.}
Jan 29 02:56:39.326: INFO: Observed &Deployment event: MODIFIED
Jan 29 02:56:39.326: INFO: Observed deployment test-deployment-pd5gl in namespace deployment-6111 with annotations: map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-29 02:56:39 +0000 UTC 2023-01-29 02:56:39 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 29 02:56:39.326: INFO: Observed deployment test-deployment-pd5gl in namespace deployment-6111 with annotations: map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-29 02:56:39 +0000 UTC 2023-01-29 02:56:37 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-pd5gl-777898ffcc" has successfully progressed.}
Jan 29 02:56:39.326: INFO: Observed deployment test-deployment-pd5gl in namespace deployment-6111 with annotations: map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 29 02:56:39.326: INFO: Observed &Deployment event: MODIFIED
Jan 29 02:56:39.326: INFO: Found deployment test-deployment-pd5gl in namespace deployment-6111 with labels: map[e2e:testing name:httpd] annotations: map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Jan 29 02:56:39.326: INFO: Deployment test-deployment-pd5gl has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 29 02:56:39.332: INFO: Deployment "test-deployment-pd5gl":
&Deployment{ObjectMeta:{test-deployment-pd5gl  deployment-6111  eb8adc5e-96ac-4efb-902d-ae273035a7b3 5938681 1 2023-01-29 02:56:37 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/revision:1] [] [] []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0x4002cfb168 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-pd5gl-777898ffcc",LastUpdateTime:2023-01-29 02:56:39 +0000 UTC,LastTransitionTime:2023-01-29 02:56:39 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 29 02:56:39.342: INFO: New ReplicaSet "test-deployment-pd5gl-777898ffcc" of Deployment "test-deployment-pd5gl":
&ReplicaSet{ObjectMeta:{test-deployment-pd5gl-777898ffcc  deployment-6111  17891097-a147-4800-a20b-fb2cd3664ee4 5938676 1 2023-01-29 02:56:37 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-pd5gl eb8adc5e-96ac-4efb-902d-ae273035a7b3 0x400226d707 0x400226d708}] [] []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 777898ffcc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0x400226d778 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 29 02:56:39.355: INFO: Pod "test-deployment-pd5gl-777898ffcc-xl4hr" is available:
&Pod{ObjectMeta:{test-deployment-pd5gl-777898ffcc-xl4hr test-deployment-pd5gl-777898ffcc- deployment-6111  dfc8ac1f-9881-437f-bb02-e723beab5025 5938675 0 2023-01-29 02:56:37 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "100.101.49.103"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "100.101.49.103"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet test-deployment-pd5gl-777898ffcc 17891097-a147-4800-a20b-fb2cd3664ee4 0x4002a42a27 0x4002a42a28}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8rpzt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8rpzt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 02:56:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 02:56:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 02:56:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 02:56:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.245,PodIP:100.101.49.103,StartTime:2023-01-29 02:56:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-29 02:56:38 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry-jinan-lab.inspurcloud.cn/library/cke/httpd-arm64:2.4.38-2,ImageID:docker-pullable://registry-jinan-lab.inspurcloud.cn/library/cke/httpd-arm64@sha256:a4f0279e65f8d239ea154d08f0572c5419f43e4ed748bee8a1a1b762d0d36893,ContainerID:docker://aa11839c7d8199a18a078935571ea6b955b7fa26b89c74a9de1535aa70a7d97e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.101.49.103,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan 29 02:56:39.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6111" for this suite. 01/29/23 02:56:39.368
{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","completed":1,"skipped":5,"failed":0}
------------------------------
• [2.195 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 02:56:37.186
    Jan 29 02:56:37.186: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename deployment 01/29/23 02:56:37.187
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 02:56:37.219
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 02:56:37.225
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 01/29/23 02:56:37.237
    Jan 29 02:56:37.237: INFO: Creating simple deployment test-deployment-pd5gl
    Jan 29 02:56:37.261: INFO: new replicaset for deployment "test-deployment-pd5gl" is yet to be created
    STEP: Getting /status 01/29/23 02:56:39.286
    Jan 29 02:56:39.296: INFO: Deployment test-deployment-pd5gl has Conditions: [{Available True 2023-01-29 02:56:39 +0000 UTC 2023-01-29 02:56:39 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-01-29 02:56:39 +0000 UTC 2023-01-29 02:56:37 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-pd5gl-777898ffcc" has successfully progressed.}]
    STEP: updating Deployment Status 01/29/23 02:56:39.297
    Jan 29 02:56:39.309: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 29, 2, 56, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 2, 56, 39, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 29, 2, 56, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 2, 56, 37, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-pd5gl-777898ffcc\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 01/29/23 02:56:39.309
    Jan 29 02:56:39.312: INFO: Observed &Deployment event: ADDED
    Jan 29 02:56:39.312: INFO: Observed Deployment test-deployment-pd5gl in namespace deployment-6111 with annotations: map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-29 02:56:37 +0000 UTC 2023-01-29 02:56:37 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-pd5gl-777898ffcc"}
    Jan 29 02:56:39.312: INFO: Observed &Deployment event: MODIFIED
    Jan 29 02:56:39.312: INFO: Observed Deployment test-deployment-pd5gl in namespace deployment-6111 with annotations: map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-29 02:56:37 +0000 UTC 2023-01-29 02:56:37 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-pd5gl-777898ffcc"}
    Jan 29 02:56:39.312: INFO: Observed Deployment test-deployment-pd5gl in namespace deployment-6111 with annotations: map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-29 02:56:37 +0000 UTC 2023-01-29 02:56:37 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan 29 02:56:39.313: INFO: Observed &Deployment event: MODIFIED
    Jan 29 02:56:39.313: INFO: Observed Deployment test-deployment-pd5gl in namespace deployment-6111 with annotations: map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-29 02:56:37 +0000 UTC 2023-01-29 02:56:37 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan 29 02:56:39.313: INFO: Observed Deployment test-deployment-pd5gl in namespace deployment-6111 with annotations: map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-29 02:56:37 +0000 UTC 2023-01-29 02:56:37 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-pd5gl-777898ffcc" is progressing.}
    Jan 29 02:56:39.313: INFO: Observed &Deployment event: MODIFIED
    Jan 29 02:56:39.313: INFO: Observed Deployment test-deployment-pd5gl in namespace deployment-6111 with annotations: map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-29 02:56:39 +0000 UTC 2023-01-29 02:56:39 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan 29 02:56:39.313: INFO: Observed Deployment test-deployment-pd5gl in namespace deployment-6111 with annotations: map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-29 02:56:39 +0000 UTC 2023-01-29 02:56:37 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-pd5gl-777898ffcc" has successfully progressed.}
    Jan 29 02:56:39.313: INFO: Observed &Deployment event: MODIFIED
    Jan 29 02:56:39.313: INFO: Observed Deployment test-deployment-pd5gl in namespace deployment-6111 with annotations: map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-29 02:56:39 +0000 UTC 2023-01-29 02:56:39 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan 29 02:56:39.313: INFO: Observed Deployment test-deployment-pd5gl in namespace deployment-6111 with annotations: map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-29 02:56:39 +0000 UTC 2023-01-29 02:56:37 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-pd5gl-777898ffcc" has successfully progressed.}
    Jan 29 02:56:39.313: INFO: Found Deployment test-deployment-pd5gl in namespace deployment-6111 with labels: map[e2e:testing name:httpd] annotations: map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan 29 02:56:39.313: INFO: Deployment test-deployment-pd5gl has an updated status
    STEP: patching the Statefulset Status 01/29/23 02:56:39.313
    Jan 29 02:56:39.313: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jan 29 02:56:39.321: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 01/29/23 02:56:39.321
    Jan 29 02:56:39.325: INFO: Observed &Deployment event: ADDED
    Jan 29 02:56:39.325: INFO: Observed deployment test-deployment-pd5gl in namespace deployment-6111 with annotations: map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-29 02:56:37 +0000 UTC 2023-01-29 02:56:37 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-pd5gl-777898ffcc"}
    Jan 29 02:56:39.325: INFO: Observed &Deployment event: MODIFIED
    Jan 29 02:56:39.325: INFO: Observed deployment test-deployment-pd5gl in namespace deployment-6111 with annotations: map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-29 02:56:37 +0000 UTC 2023-01-29 02:56:37 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-pd5gl-777898ffcc"}
    Jan 29 02:56:39.325: INFO: Observed deployment test-deployment-pd5gl in namespace deployment-6111 with annotations: map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-29 02:56:37 +0000 UTC 2023-01-29 02:56:37 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan 29 02:56:39.325: INFO: Observed &Deployment event: MODIFIED
    Jan 29 02:56:39.325: INFO: Observed deployment test-deployment-pd5gl in namespace deployment-6111 with annotations: map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-29 02:56:37 +0000 UTC 2023-01-29 02:56:37 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan 29 02:56:39.325: INFO: Observed deployment test-deployment-pd5gl in namespace deployment-6111 with annotations: map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-29 02:56:37 +0000 UTC 2023-01-29 02:56:37 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-pd5gl-777898ffcc" is progressing.}
    Jan 29 02:56:39.326: INFO: Observed &Deployment event: MODIFIED
    Jan 29 02:56:39.326: INFO: Observed deployment test-deployment-pd5gl in namespace deployment-6111 with annotations: map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-29 02:56:39 +0000 UTC 2023-01-29 02:56:39 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan 29 02:56:39.326: INFO: Observed deployment test-deployment-pd5gl in namespace deployment-6111 with annotations: map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-29 02:56:39 +0000 UTC 2023-01-29 02:56:37 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-pd5gl-777898ffcc" has successfully progressed.}
    Jan 29 02:56:39.326: INFO: Observed &Deployment event: MODIFIED
    Jan 29 02:56:39.326: INFO: Observed deployment test-deployment-pd5gl in namespace deployment-6111 with annotations: map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-29 02:56:39 +0000 UTC 2023-01-29 02:56:39 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan 29 02:56:39.326: INFO: Observed deployment test-deployment-pd5gl in namespace deployment-6111 with annotations: map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-29 02:56:39 +0000 UTC 2023-01-29 02:56:37 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-pd5gl-777898ffcc" has successfully progressed.}
    Jan 29 02:56:39.326: INFO: Observed deployment test-deployment-pd5gl in namespace deployment-6111 with annotations: map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan 29 02:56:39.326: INFO: Observed &Deployment event: MODIFIED
    Jan 29 02:56:39.326: INFO: Found deployment test-deployment-pd5gl in namespace deployment-6111 with labels: map[e2e:testing name:httpd] annotations: map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Jan 29 02:56:39.326: INFO: Deployment test-deployment-pd5gl has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 29 02:56:39.332: INFO: Deployment "test-deployment-pd5gl":
    &Deployment{ObjectMeta:{test-deployment-pd5gl  deployment-6111  eb8adc5e-96ac-4efb-902d-ae273035a7b3 5938681 1 2023-01-29 02:56:37 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/revision:1] [] [] []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0x4002cfb168 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-pd5gl-777898ffcc",LastUpdateTime:2023-01-29 02:56:39 +0000 UTC,LastTransitionTime:2023-01-29 02:56:39 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan 29 02:56:39.342: INFO: New ReplicaSet "test-deployment-pd5gl-777898ffcc" of Deployment "test-deployment-pd5gl":
    &ReplicaSet{ObjectMeta:{test-deployment-pd5gl-777898ffcc  deployment-6111  17891097-a147-4800-a20b-fb2cd3664ee4 5938676 1 2023-01-29 02:56:37 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-pd5gl eb8adc5e-96ac-4efb-902d-ae273035a7b3 0x400226d707 0x400226d708}] [] []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 777898ffcc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0x400226d778 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 29 02:56:39.355: INFO: Pod "test-deployment-pd5gl-777898ffcc-xl4hr" is available:
    &Pod{ObjectMeta:{test-deployment-pd5gl-777898ffcc-xl4hr test-deployment-pd5gl-777898ffcc- deployment-6111  dfc8ac1f-9881-437f-bb02-e723beab5025 5938675 0 2023-01-29 02:56:37 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "",
        "ips": [
            "100.101.49.103"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "",
        "ips": [
            "100.101.49.103"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet test-deployment-pd5gl-777898ffcc 17891097-a147-4800-a20b-fb2cd3664ee4 0x4002a42a27 0x4002a42a28}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8rpzt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8rpzt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 02:56:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 02:56:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 02:56:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 02:56:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.245,PodIP:100.101.49.103,StartTime:2023-01-29 02:56:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-29 02:56:38 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry-jinan-lab.inspurcloud.cn/library/cke/httpd-arm64:2.4.38-2,ImageID:docker-pullable://registry-jinan-lab.inspurcloud.cn/library/cke/httpd-arm64@sha256:a4f0279e65f8d239ea154d08f0572c5419f43e4ed748bee8a1a1b762d0d36893,ContainerID:docker://aa11839c7d8199a18a078935571ea6b955b7fa26b89c74a9de1535aa70a7d97e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.101.49.103,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan 29 02:56:39.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-6111" for this suite. 01/29/23 02:56:39.368
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 02:56:39.383
Jan 29 02:56:39.383: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename daemonsets 01/29/23 02:56:39.385
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 02:56:39.412
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 02:56:39.417
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
STEP: Creating simple DaemonSet "daemon-set" 01/29/23 02:56:39.472
STEP: Check that daemon pods launch on every node of the cluster. 01/29/23 02:56:39.485
Jan 29 02:56:39.500: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 29 02:56:39.500: INFO: Node master1 is running 0 daemon pod, expected 1
Jan 29 02:56:40.522: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 29 02:56:40.522: INFO: Node master1 is running 0 daemon pod, expected 1
Jan 29 02:56:41.520: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Jan 29 02:56:41.520: INFO: Node master1 is running 0 daemon pod, expected 1
Jan 29 02:56:42.520: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Jan 29 02:56:42.520: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
STEP: listing all DeamonSets 01/29/23 02:56:42.526
STEP: DeleteCollection of the DaemonSets 01/29/23 02:56:42.533
STEP: Verify that ReplicaSets have been deleted 01/29/23 02:56:42.545
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
Jan 29 02:56:42.564: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"5938776"},"items":null}

Jan 29 02:56:42.572: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"5938776"},"items":[{"metadata":{"name":"daemon-set-4pgn9","generateName":"daemon-set-","namespace":"daemonsets-2103","uid":"aba63c4e-908f-4e24-9556-9e0bebafae53","resourceVersion":"5938774","creationTimestamp":"2023-01-29T02:56:39Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"\",\n    \"ips\": [\n        \"100.101.161.176\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"\",\n    \"ips\": [\n        \"100.101.161.176\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"81a2e92d-7b76-45ef-9741-c65f856b37d1","controller":true,"blockOwnerDeletion":true}]},"spec":{"volumes":[{"name":"kube-api-access-5zkvt","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-5zkvt","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"master1","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["master1"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priorityClassName":"priority-class-apps","priority":10000000,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-29T02:56:39Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-29T02:56:41Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-29T02:56:41Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-29T02:56:39Z"}],"hostIP":"192.168.122.241","podIP":"100.101.161.176","podIPs":[{"ip":"100.101.161.176"}],"startTime":"2023-01-29T02:56:39Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-29T02:56:41Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"dominicyin/httpd:2.4.38-2","imageID":"docker-pullable://dominicyin/httpd@sha256:a4f0279e65f8d239ea154d08f0572c5419f43e4ed748bee8a1a1b762d0d36893","containerID":"docker://c64a7e375ac42bc5b2fba83bc677cc009f0b0d7afaa7b02406f2c7087c85b9f3","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-6qs75","generateName":"daemon-set-","namespace":"daemonsets-2103","uid":"7bfcdf52-d458-43d0-bb37-15152e5baa03","resourceVersion":"5938754","creationTimestamp":"2023-01-29T02:56:39Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"\",\n    \"ips\": [\n        \"100.101.49.106\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"\",\n    \"ips\": [\n        \"100.101.49.106\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"81a2e92d-7b76-45ef-9741-c65f856b37d1","controller":true,"blockOwnerDeletion":true}]},"spec":{"volumes":[{"name":"kube-api-access-l5ctv","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-l5ctv","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"slave2","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["slave2"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priorityClassName":"priority-class-apps","priority":10000000,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-29T02:56:39Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-29T02:56:41Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-29T02:56:41Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-29T02:56:39Z"}],"hostIP":"192.168.122.245","podIP":"100.101.49.106","podIPs":[{"ip":"100.101.49.106"}],"startTime":"2023-01-29T02:56:39Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-29T02:56:40Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry-jinan-lab.inspurcloud.cn/library/cke/httpd-arm64:2.4.38-2","imageID":"docker-pullable://registry-jinan-lab.inspurcloud.cn/library/cke/httpd-arm64@sha256:a4f0279e65f8d239ea154d08f0572c5419f43e4ed748bee8a1a1b762d0d36893","containerID":"docker://960cc1e63793db082c24793c41a7585794590915515ce2936d6a9329ae16a228","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-kbthf","generateName":"daemon-set-","namespace":"daemonsets-2103","uid":"3489e0a6-dd9e-438d-add2-574c2d78e058","resourceVersion":"5938757","creationTimestamp":"2023-01-29T02:56:39Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"\",\n    \"ips\": [\n        \"100.101.32.85\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"\",\n    \"ips\": [\n        \"100.101.32.85\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"81a2e92d-7b76-45ef-9741-c65f856b37d1","controller":true,"blockOwnerDeletion":true}]},"spec":{"volumes":[{"name":"kube-api-access-ngzgb","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-ngzgb","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"master3","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["master3"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priorityClassName":"priority-class-apps","priority":10000000,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-29T02:56:39Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-29T02:56:41Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-29T02:56:41Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-29T02:56:39Z"}],"hostIP":"192.168.122.243","podIP":"100.101.32.85","podIPs":[{"ip":"100.101.32.85"}],"startTime":"2023-01-29T02:56:39Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-29T02:56:40Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry-jinan-lab.inspurcloud.cn/library/cke/httpd-arm64:2.4.38-2","imageID":"docker-pullable://registry-jinan-lab.inspurcloud.cn/library/cke/httpd-arm64@sha256:a4f0279e65f8d239ea154d08f0572c5419f43e4ed748bee8a1a1b762d0d36893","containerID":"docker://177b3d3e22a4dc8c066a5e137254dc11ee4971e26b51e30fb5e2b9968e434e6a","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-q2zdk","generateName":"daemon-set-","namespace":"daemonsets-2103","uid":"5c4d769c-a0d2-47f1-bc8e-85883bc55ece","resourceVersion":"5938759","creationTimestamp":"2023-01-29T02:56:39Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"\",\n    \"ips\": [\n        \"100.101.51.248\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"\",\n    \"ips\": [\n        \"100.101.51.248\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"81a2e92d-7b76-45ef-9741-c65f856b37d1","controller":true,"blockOwnerDeletion":true}]},"spec":{"volumes":[{"name":"kube-api-access-xddd2","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-xddd2","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"slave1","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["slave1"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priorityClassName":"priority-class-apps","priority":10000000,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-29T02:56:39Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-29T02:56:41Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-29T02:56:41Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-29T02:56:39Z"}],"hostIP":"192.168.122.244","podIP":"100.101.51.248","podIPs":[{"ip":"100.101.51.248"}],"startTime":"2023-01-29T02:56:39Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-29T02:56:40Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry-jinan-lab.inspurcloud.cn/library/cke/httpd-arm64:2.4.38-2","imageID":"docker-pullable://registry-jinan-lab.inspurcloud.cn/library/cke/httpd-arm64@sha256:a4f0279e65f8d239ea154d08f0572c5419f43e4ed748bee8a1a1b762d0d36893","containerID":"docker://7925e1b95910b6f3619ad182587412acc0bf0904505a6177abde995da29eb8fe","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-swglp","generateName":"daemon-set-","namespace":"daemonsets-2103","uid":"7d1ac42f-f0b9-409a-b03d-4deb0aeeb36a","resourceVersion":"5938764","creationTimestamp":"2023-01-29T02:56:39Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"\",\n    \"ips\": [\n        \"100.101.208.195\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"\",\n    \"ips\": [\n        \"100.101.208.195\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"81a2e92d-7b76-45ef-9741-c65f856b37d1","controller":true,"blockOwnerDeletion":true}]},"spec":{"volumes":[{"name":"kube-api-access-kngmw","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-kngmw","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"master2","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["master2"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priorityClassName":"priority-class-apps","priority":10000000,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-29T02:56:39Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-29T02:56:41Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-29T02:56:41Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-29T02:56:39Z"}],"hostIP":"192.168.122.242","podIP":"100.101.208.195","podIPs":[{"ip":"100.101.208.195"}],"startTime":"2023-01-29T02:56:39Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-29T02:56:41Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry-jinan-lab.inspurcloud.cn/library/cke/httpd-arm64:2.4.38-2","imageID":"docker-pullable://registry-jinan-lab.inspurcloud.cn/library/cke/httpd-arm64@sha256:a4f0279e65f8d239ea154d08f0572c5419f43e4ed748bee8a1a1b762d0d36893","containerID":"docker://6a5fbeceab530cc15043a35775f00f0391db8258847d7828519ce1ee7e1605f4","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan 29 02:56:42.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2103" for this suite. 01/29/23 02:56:42.655
{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","completed":2,"skipped":30,"failed":0}
------------------------------
• [3.284 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 02:56:39.383
    Jan 29 02:56:39.383: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename daemonsets 01/29/23 02:56:39.385
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 02:56:39.412
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 02:56:39.417
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:822
    STEP: Creating simple DaemonSet "daemon-set" 01/29/23 02:56:39.472
    STEP: Check that daemon pods launch on every node of the cluster. 01/29/23 02:56:39.485
    Jan 29 02:56:39.500: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 29 02:56:39.500: INFO: Node master1 is running 0 daemon pod, expected 1
    Jan 29 02:56:40.522: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 29 02:56:40.522: INFO: Node master1 is running 0 daemon pod, expected 1
    Jan 29 02:56:41.520: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Jan 29 02:56:41.520: INFO: Node master1 is running 0 daemon pod, expected 1
    Jan 29 02:56:42.520: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
    Jan 29 02:56:42.520: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
    STEP: listing all DeamonSets 01/29/23 02:56:42.526
    STEP: DeleteCollection of the DaemonSets 01/29/23 02:56:42.533
    STEP: Verify that ReplicaSets have been deleted 01/29/23 02:56:42.545
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    Jan 29 02:56:42.564: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"5938776"},"items":null}

    Jan 29 02:56:42.572: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"5938776"},"items":[{"metadata":{"name":"daemon-set-4pgn9","generateName":"daemon-set-","namespace":"daemonsets-2103","uid":"aba63c4e-908f-4e24-9556-9e0bebafae53","resourceVersion":"5938774","creationTimestamp":"2023-01-29T02:56:39Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"\",\n    \"ips\": [\n        \"100.101.161.176\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"\",\n    \"ips\": [\n        \"100.101.161.176\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"81a2e92d-7b76-45ef-9741-c65f856b37d1","controller":true,"blockOwnerDeletion":true}]},"spec":{"volumes":[{"name":"kube-api-access-5zkvt","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-5zkvt","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"master1","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["master1"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priorityClassName":"priority-class-apps","priority":10000000,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-29T02:56:39Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-29T02:56:41Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-29T02:56:41Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-29T02:56:39Z"}],"hostIP":"192.168.122.241","podIP":"100.101.161.176","podIPs":[{"ip":"100.101.161.176"}],"startTime":"2023-01-29T02:56:39Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-29T02:56:41Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"dominicyin/httpd:2.4.38-2","imageID":"docker-pullable://dominicyin/httpd@sha256:a4f0279e65f8d239ea154d08f0572c5419f43e4ed748bee8a1a1b762d0d36893","containerID":"docker://c64a7e375ac42bc5b2fba83bc677cc009f0b0d7afaa7b02406f2c7087c85b9f3","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-6qs75","generateName":"daemon-set-","namespace":"daemonsets-2103","uid":"7bfcdf52-d458-43d0-bb37-15152e5baa03","resourceVersion":"5938754","creationTimestamp":"2023-01-29T02:56:39Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"\",\n    \"ips\": [\n        \"100.101.49.106\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"\",\n    \"ips\": [\n        \"100.101.49.106\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"81a2e92d-7b76-45ef-9741-c65f856b37d1","controller":true,"blockOwnerDeletion":true}]},"spec":{"volumes":[{"name":"kube-api-access-l5ctv","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-l5ctv","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"slave2","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["slave2"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priorityClassName":"priority-class-apps","priority":10000000,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-29T02:56:39Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-29T02:56:41Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-29T02:56:41Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-29T02:56:39Z"}],"hostIP":"192.168.122.245","podIP":"100.101.49.106","podIPs":[{"ip":"100.101.49.106"}],"startTime":"2023-01-29T02:56:39Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-29T02:56:40Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry-jinan-lab.inspurcloud.cn/library/cke/httpd-arm64:2.4.38-2","imageID":"docker-pullable://registry-jinan-lab.inspurcloud.cn/library/cke/httpd-arm64@sha256:a4f0279e65f8d239ea154d08f0572c5419f43e4ed748bee8a1a1b762d0d36893","containerID":"docker://960cc1e63793db082c24793c41a7585794590915515ce2936d6a9329ae16a228","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-kbthf","generateName":"daemon-set-","namespace":"daemonsets-2103","uid":"3489e0a6-dd9e-438d-add2-574c2d78e058","resourceVersion":"5938757","creationTimestamp":"2023-01-29T02:56:39Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"\",\n    \"ips\": [\n        \"100.101.32.85\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"\",\n    \"ips\": [\n        \"100.101.32.85\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"81a2e92d-7b76-45ef-9741-c65f856b37d1","controller":true,"blockOwnerDeletion":true}]},"spec":{"volumes":[{"name":"kube-api-access-ngzgb","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-ngzgb","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"master3","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["master3"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priorityClassName":"priority-class-apps","priority":10000000,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-29T02:56:39Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-29T02:56:41Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-29T02:56:41Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-29T02:56:39Z"}],"hostIP":"192.168.122.243","podIP":"100.101.32.85","podIPs":[{"ip":"100.101.32.85"}],"startTime":"2023-01-29T02:56:39Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-29T02:56:40Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry-jinan-lab.inspurcloud.cn/library/cke/httpd-arm64:2.4.38-2","imageID":"docker-pullable://registry-jinan-lab.inspurcloud.cn/library/cke/httpd-arm64@sha256:a4f0279e65f8d239ea154d08f0572c5419f43e4ed748bee8a1a1b762d0d36893","containerID":"docker://177b3d3e22a4dc8c066a5e137254dc11ee4971e26b51e30fb5e2b9968e434e6a","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-q2zdk","generateName":"daemon-set-","namespace":"daemonsets-2103","uid":"5c4d769c-a0d2-47f1-bc8e-85883bc55ece","resourceVersion":"5938759","creationTimestamp":"2023-01-29T02:56:39Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"\",\n    \"ips\": [\n        \"100.101.51.248\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"\",\n    \"ips\": [\n        \"100.101.51.248\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"81a2e92d-7b76-45ef-9741-c65f856b37d1","controller":true,"blockOwnerDeletion":true}]},"spec":{"volumes":[{"name":"kube-api-access-xddd2","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-xddd2","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"slave1","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["slave1"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priorityClassName":"priority-class-apps","priority":10000000,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-29T02:56:39Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-29T02:56:41Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-29T02:56:41Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-29T02:56:39Z"}],"hostIP":"192.168.122.244","podIP":"100.101.51.248","podIPs":[{"ip":"100.101.51.248"}],"startTime":"2023-01-29T02:56:39Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-29T02:56:40Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry-jinan-lab.inspurcloud.cn/library/cke/httpd-arm64:2.4.38-2","imageID":"docker-pullable://registry-jinan-lab.inspurcloud.cn/library/cke/httpd-arm64@sha256:a4f0279e65f8d239ea154d08f0572c5419f43e4ed748bee8a1a1b762d0d36893","containerID":"docker://7925e1b95910b6f3619ad182587412acc0bf0904505a6177abde995da29eb8fe","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-swglp","generateName":"daemon-set-","namespace":"daemonsets-2103","uid":"7d1ac42f-f0b9-409a-b03d-4deb0aeeb36a","resourceVersion":"5938764","creationTimestamp":"2023-01-29T02:56:39Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"\",\n    \"ips\": [\n        \"100.101.208.195\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"\",\n    \"ips\": [\n        \"100.101.208.195\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"81a2e92d-7b76-45ef-9741-c65f856b37d1","controller":true,"blockOwnerDeletion":true}]},"spec":{"volumes":[{"name":"kube-api-access-kngmw","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-kngmw","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"master2","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["master2"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priorityClassName":"priority-class-apps","priority":10000000,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-29T02:56:39Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-29T02:56:41Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-29T02:56:41Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-29T02:56:39Z"}],"hostIP":"192.168.122.242","podIP":"100.101.208.195","podIPs":[{"ip":"100.101.208.195"}],"startTime":"2023-01-29T02:56:39Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-29T02:56:41Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry-jinan-lab.inspurcloud.cn/library/cke/httpd-arm64:2.4.38-2","imageID":"docker-pullable://registry-jinan-lab.inspurcloud.cn/library/cke/httpd-arm64@sha256:a4f0279e65f8d239ea154d08f0572c5419f43e4ed748bee8a1a1b762d0d36893","containerID":"docker://6a5fbeceab530cc15043a35775f00f0391db8258847d7828519ce1ee7e1605f4","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan 29 02:56:42.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-2103" for this suite. 01/29/23 02:56:42.655
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 02:56:42.668
Jan 29 02:56:42.668: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename kubelet-test 01/29/23 02:56:42.669
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 02:56:42.698
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 02:56:42.704
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 01/29/23 02:56:42.727
Jan 29 02:56:42.727: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases7b0324ae-9a4b-4677-868b-fcf5abf94321" in namespace "kubelet-test-1717" to be "completed"
Jan 29 02:56:42.733: INFO: Pod "agnhost-host-aliases7b0324ae-9a4b-4677-868b-fcf5abf94321": Phase="Pending", Reason="", readiness=false. Elapsed: 5.73218ms
Jan 29 02:56:44.739: INFO: Pod "agnhost-host-aliases7b0324ae-9a4b-4677-868b-fcf5abf94321": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012066003s
Jan 29 02:56:46.741: INFO: Pod "agnhost-host-aliases7b0324ae-9a4b-4677-868b-fcf5abf94321": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013575371s
Jan 29 02:56:46.741: INFO: Pod "agnhost-host-aliases7b0324ae-9a4b-4677-868b-fcf5abf94321" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jan 29 02:56:46.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1717" for this suite. 01/29/23 02:56:46.786
{"msg":"PASSED [sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]","completed":3,"skipped":41,"failed":0}
------------------------------
• [4.129 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 02:56:42.668
    Jan 29 02:56:42.668: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename kubelet-test 01/29/23 02:56:42.669
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 02:56:42.698
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 02:56:42.704
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 01/29/23 02:56:42.727
    Jan 29 02:56:42.727: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases7b0324ae-9a4b-4677-868b-fcf5abf94321" in namespace "kubelet-test-1717" to be "completed"
    Jan 29 02:56:42.733: INFO: Pod "agnhost-host-aliases7b0324ae-9a4b-4677-868b-fcf5abf94321": Phase="Pending", Reason="", readiness=false. Elapsed: 5.73218ms
    Jan 29 02:56:44.739: INFO: Pod "agnhost-host-aliases7b0324ae-9a4b-4677-868b-fcf5abf94321": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012066003s
    Jan 29 02:56:46.741: INFO: Pod "agnhost-host-aliases7b0324ae-9a4b-4677-868b-fcf5abf94321": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013575371s
    Jan 29 02:56:46.741: INFO: Pod "agnhost-host-aliases7b0324ae-9a4b-4677-868b-fcf5abf94321" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jan 29 02:56:46.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-1717" for this suite. 01/29/23 02:56:46.786
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 02:56:46.799
Jan 29 02:56:46.800: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename emptydir 01/29/23 02:56:46.801
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 02:56:46.842
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 02:56:46.848
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
STEP: Creating a pod to test emptydir 0644 on tmpfs 01/29/23 02:56:46.854
Jan 29 02:56:46.873: INFO: Waiting up to 5m0s for pod "pod-cd82fb5b-bb3e-4b2f-bf66-da915ae960a9" in namespace "emptydir-7443" to be "Succeeded or Failed"
Jan 29 02:56:46.886: INFO: Pod "pod-cd82fb5b-bb3e-4b2f-bf66-da915ae960a9": Phase="Pending", Reason="", readiness=false. Elapsed: 13.346253ms
Jan 29 02:56:48.893: INFO: Pod "pod-cd82fb5b-bb3e-4b2f-bf66-da915ae960a9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020420581s
Jan 29 02:56:50.895: INFO: Pod "pod-cd82fb5b-bb3e-4b2f-bf66-da915ae960a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021854129s
STEP: Saw pod success 01/29/23 02:56:50.895
Jan 29 02:56:50.895: INFO: Pod "pod-cd82fb5b-bb3e-4b2f-bf66-da915ae960a9" satisfied condition "Succeeded or Failed"
Jan 29 02:56:50.902: INFO: Trying to get logs from node slave2 pod pod-cd82fb5b-bb3e-4b2f-bf66-da915ae960a9 container test-container: <nil>
STEP: delete the pod 01/29/23 02:56:50.918
Jan 29 02:56:51.011: INFO: Waiting for pod pod-cd82fb5b-bb3e-4b2f-bf66-da915ae960a9 to disappear
Jan 29 02:56:51.017: INFO: Pod pod-cd82fb5b-bb3e-4b2f-bf66-da915ae960a9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 29 02:56:51.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7443" for this suite. 01/29/23 02:56:51.027
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":4,"skipped":71,"failed":0}
------------------------------
• [4.242 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 02:56:46.799
    Jan 29 02:56:46.800: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename emptydir 01/29/23 02:56:46.801
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 02:56:46.842
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 02:56:46.848
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:96
    STEP: Creating a pod to test emptydir 0644 on tmpfs 01/29/23 02:56:46.854
    Jan 29 02:56:46.873: INFO: Waiting up to 5m0s for pod "pod-cd82fb5b-bb3e-4b2f-bf66-da915ae960a9" in namespace "emptydir-7443" to be "Succeeded or Failed"
    Jan 29 02:56:46.886: INFO: Pod "pod-cd82fb5b-bb3e-4b2f-bf66-da915ae960a9": Phase="Pending", Reason="", readiness=false. Elapsed: 13.346253ms
    Jan 29 02:56:48.893: INFO: Pod "pod-cd82fb5b-bb3e-4b2f-bf66-da915ae960a9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020420581s
    Jan 29 02:56:50.895: INFO: Pod "pod-cd82fb5b-bb3e-4b2f-bf66-da915ae960a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021854129s
    STEP: Saw pod success 01/29/23 02:56:50.895
    Jan 29 02:56:50.895: INFO: Pod "pod-cd82fb5b-bb3e-4b2f-bf66-da915ae960a9" satisfied condition "Succeeded or Failed"
    Jan 29 02:56:50.902: INFO: Trying to get logs from node slave2 pod pod-cd82fb5b-bb3e-4b2f-bf66-da915ae960a9 container test-container: <nil>
    STEP: delete the pod 01/29/23 02:56:50.918
    Jan 29 02:56:51.011: INFO: Waiting for pod pod-cd82fb5b-bb3e-4b2f-bf66-da915ae960a9 to disappear
    Jan 29 02:56:51.017: INFO: Pod pod-cd82fb5b-bb3e-4b2f-bf66-da915ae960a9 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 29 02:56:51.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-7443" for this suite. 01/29/23 02:56:51.027
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 02:56:51.045
Jan 29 02:56:51.045: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename emptydir 01/29/23 02:56:51.046
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 02:56:51.078
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 02:56:51.083
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
STEP: Creating a pod to test emptydir 0644 on node default medium 01/29/23 02:56:51.092
Jan 29 02:56:51.112: INFO: Waiting up to 5m0s for pod "pod-e5515304-87a0-4ac2-b9dd-8293a3c7baed" in namespace "emptydir-7770" to be "Succeeded or Failed"
Jan 29 02:56:51.118: INFO: Pod "pod-e5515304-87a0-4ac2-b9dd-8293a3c7baed": Phase="Pending", Reason="", readiness=false. Elapsed: 6.080882ms
Jan 29 02:56:53.126: INFO: Pod "pod-e5515304-87a0-4ac2-b9dd-8293a3c7baed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013635653s
Jan 29 02:56:55.126: INFO: Pod "pod-e5515304-87a0-4ac2-b9dd-8293a3c7baed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014275896s
STEP: Saw pod success 01/29/23 02:56:55.126
Jan 29 02:56:55.127: INFO: Pod "pod-e5515304-87a0-4ac2-b9dd-8293a3c7baed" satisfied condition "Succeeded or Failed"
Jan 29 02:56:55.133: INFO: Trying to get logs from node slave2 pod pod-e5515304-87a0-4ac2-b9dd-8293a3c7baed container test-container: <nil>
STEP: delete the pod 01/29/23 02:56:55.15
Jan 29 02:56:55.241: INFO: Waiting for pod pod-e5515304-87a0-4ac2-b9dd-8293a3c7baed to disappear
Jan 29 02:56:55.247: INFO: Pod pod-e5515304-87a0-4ac2-b9dd-8293a3c7baed no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 29 02:56:55.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7770" for this suite. 01/29/23 02:56:55.257
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":5,"skipped":124,"failed":0}
------------------------------
• [4.225 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 02:56:51.045
    Jan 29 02:56:51.045: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename emptydir 01/29/23 02:56:51.046
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 02:56:51.078
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 02:56:51.083
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:166
    STEP: Creating a pod to test emptydir 0644 on node default medium 01/29/23 02:56:51.092
    Jan 29 02:56:51.112: INFO: Waiting up to 5m0s for pod "pod-e5515304-87a0-4ac2-b9dd-8293a3c7baed" in namespace "emptydir-7770" to be "Succeeded or Failed"
    Jan 29 02:56:51.118: INFO: Pod "pod-e5515304-87a0-4ac2-b9dd-8293a3c7baed": Phase="Pending", Reason="", readiness=false. Elapsed: 6.080882ms
    Jan 29 02:56:53.126: INFO: Pod "pod-e5515304-87a0-4ac2-b9dd-8293a3c7baed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013635653s
    Jan 29 02:56:55.126: INFO: Pod "pod-e5515304-87a0-4ac2-b9dd-8293a3c7baed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014275896s
    STEP: Saw pod success 01/29/23 02:56:55.126
    Jan 29 02:56:55.127: INFO: Pod "pod-e5515304-87a0-4ac2-b9dd-8293a3c7baed" satisfied condition "Succeeded or Failed"
    Jan 29 02:56:55.133: INFO: Trying to get logs from node slave2 pod pod-e5515304-87a0-4ac2-b9dd-8293a3c7baed container test-container: <nil>
    STEP: delete the pod 01/29/23 02:56:55.15
    Jan 29 02:56:55.241: INFO: Waiting for pod pod-e5515304-87a0-4ac2-b9dd-8293a3c7baed to disappear
    Jan 29 02:56:55.247: INFO: Pod pod-e5515304-87a0-4ac2-b9dd-8293a3c7baed no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 29 02:56:55.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-7770" for this suite. 01/29/23 02:56:55.257
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 02:56:55.272
Jan 29 02:56:55.272: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename statefulset 01/29/23 02:56:55.273
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 02:56:55.307
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 02:56:55.313
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-7977 01/29/23 02:56:55.319
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
STEP: Looking for a node to schedule stateful set and pod 01/29/23 02:56:55.327
STEP: Creating pod with conflicting port in namespace statefulset-7977 01/29/23 02:56:55.34
STEP: Waiting until pod test-pod will start running in namespace statefulset-7977 01/29/23 02:56:55.359
Jan 29 02:56:55.359: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-7977" to be "running"
Jan 29 02:56:55.365: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.231963ms
Jan 29 02:56:57.371: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.012440905s
Jan 29 02:56:57.371: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-7977 01/29/23 02:56:57.371
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-7977 01/29/23 02:56:57.386
Jan 29 02:56:57.427: INFO: Observed stateful pod in namespace: statefulset-7977, name: ss-0, uid: ab733a42-b554-4f7f-92bc-16fb3060a0eb, status phase: Pending. Waiting for statefulset controller to delete.
Jan 29 02:56:57.446: INFO: Observed stateful pod in namespace: statefulset-7977, name: ss-0, uid: ab733a42-b554-4f7f-92bc-16fb3060a0eb, status phase: Failed. Waiting for statefulset controller to delete.
Jan 29 02:56:57.485: INFO: Observed stateful pod in namespace: statefulset-7977, name: ss-0, uid: ab733a42-b554-4f7f-92bc-16fb3060a0eb, status phase: Failed. Waiting for statefulset controller to delete.
Jan 29 02:56:57.550: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-7977
STEP: Removing pod with conflicting port in namespace statefulset-7977 01/29/23 02:56:57.551
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-7977 and will be in running state 01/29/23 02:56:57.622
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 29 02:56:59.637: INFO: Deleting all statefulset in ns statefulset-7977
Jan 29 02:56:59.645: INFO: Scaling statefulset ss to 0
Jan 29 02:57:09.678: INFO: Waiting for statefulset status.replicas updated to 0
Jan 29 02:57:09.687: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan 29 02:57:09.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7977" for this suite. 01/29/23 02:57:09.724
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","completed":6,"skipped":145,"failed":0}
------------------------------
• [SLOW TEST] [14.464 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:737

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 02:56:55.272
    Jan 29 02:56:55.272: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename statefulset 01/29/23 02:56:55.273
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 02:56:55.307
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 02:56:55.313
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-7977 01/29/23 02:56:55.319
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:737
    STEP: Looking for a node to schedule stateful set and pod 01/29/23 02:56:55.327
    STEP: Creating pod with conflicting port in namespace statefulset-7977 01/29/23 02:56:55.34
    STEP: Waiting until pod test-pod will start running in namespace statefulset-7977 01/29/23 02:56:55.359
    Jan 29 02:56:55.359: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-7977" to be "running"
    Jan 29 02:56:55.365: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.231963ms
    Jan 29 02:56:57.371: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.012440905s
    Jan 29 02:56:57.371: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-7977 01/29/23 02:56:57.371
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-7977 01/29/23 02:56:57.386
    Jan 29 02:56:57.427: INFO: Observed stateful pod in namespace: statefulset-7977, name: ss-0, uid: ab733a42-b554-4f7f-92bc-16fb3060a0eb, status phase: Pending. Waiting for statefulset controller to delete.
    Jan 29 02:56:57.446: INFO: Observed stateful pod in namespace: statefulset-7977, name: ss-0, uid: ab733a42-b554-4f7f-92bc-16fb3060a0eb, status phase: Failed. Waiting for statefulset controller to delete.
    Jan 29 02:56:57.485: INFO: Observed stateful pod in namespace: statefulset-7977, name: ss-0, uid: ab733a42-b554-4f7f-92bc-16fb3060a0eb, status phase: Failed. Waiting for statefulset controller to delete.
    Jan 29 02:56:57.550: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-7977
    STEP: Removing pod with conflicting port in namespace statefulset-7977 01/29/23 02:56:57.551
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-7977 and will be in running state 01/29/23 02:56:57.622
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan 29 02:56:59.637: INFO: Deleting all statefulset in ns statefulset-7977
    Jan 29 02:56:59.645: INFO: Scaling statefulset ss to 0
    Jan 29 02:57:09.678: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 29 02:57:09.687: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan 29 02:57:09.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-7977" for this suite. 01/29/23 02:57:09.724
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 02:57:09.737
Jan 29 02:57:09.737: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename statefulset 01/29/23 02:57:09.739
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 02:57:09.792
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 02:57:09.8
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-1287 01/29/23 02:57:09.806
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
STEP: Creating a new StatefulSet 01/29/23 02:57:09.814
Jan 29 02:57:09.842: INFO: Found 0 stateful pods, waiting for 3
Jan 29 02:57:19.851: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 29 02:57:19.851: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 29 02:57:19.851: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jan 29 02:57:19.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=statefulset-1287 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 29 02:57:20.181: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 29 02:57:20.181: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 29 02:57:20.181: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 01/29/23 02:57:30.213
Jan 29 02:57:30.240: INFO: Updating stateful set ss2
STEP: Creating a new revision 01/29/23 02:57:30.24
STEP: Updating Pods in reverse ordinal order 01/29/23 02:57:40.271
Jan 29 02:57:40.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=statefulset-1287 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 29 02:57:40.529: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 29 02:57:40.529: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 29 02:57:40.529: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 29 02:57:50.570: INFO: Waiting for StatefulSet statefulset-1287/ss2 to complete update
STEP: Rolling back to a previous revision 01/29/23 02:58:00.588
Jan 29 02:58:00.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=statefulset-1287 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 29 02:58:00.839: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 29 02:58:00.839: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 29 02:58:00.839: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 29 02:58:10.895: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 01/29/23 02:58:20.935
Jan 29 02:58:20.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=statefulset-1287 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 29 02:58:21.182: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 29 02:58:21.182: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 29 02:58:21.182: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 29 02:58:31.233: INFO: Waiting for StatefulSet statefulset-1287/ss2 to complete update
Jan 29 02:58:31.233: INFO: Waiting for Pod statefulset-1287/ss2-0 to have revision ss2-6557876d87 update revision ss2-5d8c6ff87d
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 29 02:58:41.247: INFO: Deleting all statefulset in ns statefulset-1287
Jan 29 02:58:41.254: INFO: Scaling statefulset ss2 to 0
Jan 29 02:58:51.287: INFO: Waiting for statefulset status.replicas updated to 0
Jan 29 02:58:51.293: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan 29 02:58:51.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1287" for this suite. 01/29/23 02:58:51.333
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","completed":7,"skipped":155,"failed":0}
------------------------------
• [SLOW TEST] [101.616 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:304

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 02:57:09.737
    Jan 29 02:57:09.737: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename statefulset 01/29/23 02:57:09.739
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 02:57:09.792
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 02:57:09.8
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-1287 01/29/23 02:57:09.806
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:304
    STEP: Creating a new StatefulSet 01/29/23 02:57:09.814
    Jan 29 02:57:09.842: INFO: Found 0 stateful pods, waiting for 3
    Jan 29 02:57:19.851: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 29 02:57:19.851: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 29 02:57:19.851: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Jan 29 02:57:19.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=statefulset-1287 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 29 02:57:20.181: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 29 02:57:20.181: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 29 02:57:20.181: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 01/29/23 02:57:30.213
    Jan 29 02:57:30.240: INFO: Updating stateful set ss2
    STEP: Creating a new revision 01/29/23 02:57:30.24
    STEP: Updating Pods in reverse ordinal order 01/29/23 02:57:40.271
    Jan 29 02:57:40.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=statefulset-1287 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 29 02:57:40.529: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 29 02:57:40.529: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 29 02:57:40.529: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 29 02:57:50.570: INFO: Waiting for StatefulSet statefulset-1287/ss2 to complete update
    STEP: Rolling back to a previous revision 01/29/23 02:58:00.588
    Jan 29 02:58:00.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=statefulset-1287 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 29 02:58:00.839: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 29 02:58:00.839: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 29 02:58:00.839: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 29 02:58:10.895: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 01/29/23 02:58:20.935
    Jan 29 02:58:20.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=statefulset-1287 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 29 02:58:21.182: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 29 02:58:21.182: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 29 02:58:21.182: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 29 02:58:31.233: INFO: Waiting for StatefulSet statefulset-1287/ss2 to complete update
    Jan 29 02:58:31.233: INFO: Waiting for Pod statefulset-1287/ss2-0 to have revision ss2-6557876d87 update revision ss2-5d8c6ff87d
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan 29 02:58:41.247: INFO: Deleting all statefulset in ns statefulset-1287
    Jan 29 02:58:41.254: INFO: Scaling statefulset ss2 to 0
    Jan 29 02:58:51.287: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 29 02:58:51.293: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan 29 02:58:51.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-1287" for this suite. 01/29/23 02:58:51.333
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 02:58:51.356
Jan 29 02:58:51.356: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename kubectl 01/29/23 02:58:51.357
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 02:58:51.41
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 02:58:51.416
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
STEP: validating api versions 01/29/23 02:58:51.421
Jan 29 02:58:51.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-7600 api-versions'
Jan 29 02:58:51.519: INFO: stderr: ""
Jan 29 02:58:51.519: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\ncertificates.k8s.io/v1\ncie.inspur.com/v1\ncie.inspur.com/v1alpha1\ncke.inspur.com/v1alpha1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nk8s.cni.cncf.io/v1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 29 02:58:51.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7600" for this suite. 01/29/23 02:58:51.528
{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","completed":8,"skipped":169,"failed":0}
------------------------------
• [0.185 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:816
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 02:58:51.356
    Jan 29 02:58:51.356: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename kubectl 01/29/23 02:58:51.357
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 02:58:51.41
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 02:58:51.416
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:822
    STEP: validating api versions 01/29/23 02:58:51.421
    Jan 29 02:58:51.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-7600 api-versions'
    Jan 29 02:58:51.519: INFO: stderr: ""
    Jan 29 02:58:51.519: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\ncertificates.k8s.io/v1\ncie.inspur.com/v1\ncie.inspur.com/v1alpha1\ncke.inspur.com/v1alpha1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nk8s.cni.cncf.io/v1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 29 02:58:51.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-7600" for this suite. 01/29/23 02:58:51.528
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 02:58:51.543
Jan 29 02:58:51.543: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename secrets 01/29/23 02:58:51.544
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 02:58:51.572
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 02:58:51.577
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
STEP: Creating secret with name secret-test-41b125c0-8fe6-4360-81a4-d07a6eb37977 01/29/23 02:58:51.582
STEP: Creating a pod to test consume secrets 01/29/23 02:58:51.588
Jan 29 02:58:51.615: INFO: Waiting up to 5m0s for pod "pod-secrets-50b7138f-9a0d-4940-9793-6d3cf43b8c23" in namespace "secrets-6609" to be "Succeeded or Failed"
Jan 29 02:58:51.623: INFO: Pod "pod-secrets-50b7138f-9a0d-4940-9793-6d3cf43b8c23": Phase="Pending", Reason="", readiness=false. Elapsed: 7.823315ms
Jan 29 02:58:53.631: INFO: Pod "pod-secrets-50b7138f-9a0d-4940-9793-6d3cf43b8c23": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016378248s
Jan 29 02:58:55.631: INFO: Pod "pod-secrets-50b7138f-9a0d-4940-9793-6d3cf43b8c23": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015659017s
STEP: Saw pod success 01/29/23 02:58:55.631
Jan 29 02:58:55.631: INFO: Pod "pod-secrets-50b7138f-9a0d-4940-9793-6d3cf43b8c23" satisfied condition "Succeeded or Failed"
Jan 29 02:58:55.637: INFO: Trying to get logs from node slave2 pod pod-secrets-50b7138f-9a0d-4940-9793-6d3cf43b8c23 container secret-volume-test: <nil>
STEP: delete the pod 01/29/23 02:58:55.669
Jan 29 02:58:55.734: INFO: Waiting for pod pod-secrets-50b7138f-9a0d-4940-9793-6d3cf43b8c23 to disappear
Jan 29 02:58:55.740: INFO: Pod pod-secrets-50b7138f-9a0d-4940-9793-6d3cf43b8c23 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 29 02:58:55.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6609" for this suite. 01/29/23 02:58:55.75
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","completed":9,"skipped":196,"failed":0}
------------------------------
• [4.220 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 02:58:51.543
    Jan 29 02:58:51.543: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename secrets 01/29/23 02:58:51.544
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 02:58:51.572
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 02:58:51.577
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:46
    STEP: Creating secret with name secret-test-41b125c0-8fe6-4360-81a4-d07a6eb37977 01/29/23 02:58:51.582
    STEP: Creating a pod to test consume secrets 01/29/23 02:58:51.588
    Jan 29 02:58:51.615: INFO: Waiting up to 5m0s for pod "pod-secrets-50b7138f-9a0d-4940-9793-6d3cf43b8c23" in namespace "secrets-6609" to be "Succeeded or Failed"
    Jan 29 02:58:51.623: INFO: Pod "pod-secrets-50b7138f-9a0d-4940-9793-6d3cf43b8c23": Phase="Pending", Reason="", readiness=false. Elapsed: 7.823315ms
    Jan 29 02:58:53.631: INFO: Pod "pod-secrets-50b7138f-9a0d-4940-9793-6d3cf43b8c23": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016378248s
    Jan 29 02:58:55.631: INFO: Pod "pod-secrets-50b7138f-9a0d-4940-9793-6d3cf43b8c23": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015659017s
    STEP: Saw pod success 01/29/23 02:58:55.631
    Jan 29 02:58:55.631: INFO: Pod "pod-secrets-50b7138f-9a0d-4940-9793-6d3cf43b8c23" satisfied condition "Succeeded or Failed"
    Jan 29 02:58:55.637: INFO: Trying to get logs from node slave2 pod pod-secrets-50b7138f-9a0d-4940-9793-6d3cf43b8c23 container secret-volume-test: <nil>
    STEP: delete the pod 01/29/23 02:58:55.669
    Jan 29 02:58:55.734: INFO: Waiting for pod pod-secrets-50b7138f-9a0d-4940-9793-6d3cf43b8c23 to disappear
    Jan 29 02:58:55.740: INFO: Pod pod-secrets-50b7138f-9a0d-4940-9793-6d3cf43b8c23 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 29 02:58:55.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-6609" for this suite. 01/29/23 02:58:55.75
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 02:58:55.764
Jan 29 02:58:55.765: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename subpath 01/29/23 02:58:55.766
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 02:58:55.793
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 02:58:55.799
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/29/23 02:58:55.805
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-sskx 01/29/23 02:58:55.82
STEP: Creating a pod to test atomic-volume-subpath 01/29/23 02:58:55.82
Jan 29 02:58:55.839: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-sskx" in namespace "subpath-3560" to be "Succeeded or Failed"
Jan 29 02:58:55.847: INFO: Pod "pod-subpath-test-secret-sskx": Phase="Pending", Reason="", readiness=false. Elapsed: 7.364112ms
Jan 29 02:58:57.853: INFO: Pod "pod-subpath-test-secret-sskx": Phase="Running", Reason="", readiness=true. Elapsed: 2.013118845s
Jan 29 02:58:59.855: INFO: Pod "pod-subpath-test-secret-sskx": Phase="Running", Reason="", readiness=true. Elapsed: 4.015783378s
Jan 29 02:59:01.855: INFO: Pod "pod-subpath-test-secret-sskx": Phase="Running", Reason="", readiness=true. Elapsed: 6.015097667s
Jan 29 02:59:03.856: INFO: Pod "pod-subpath-test-secret-sskx": Phase="Running", Reason="", readiness=true. Elapsed: 8.016727612s
Jan 29 02:59:05.853: INFO: Pod "pod-subpath-test-secret-sskx": Phase="Running", Reason="", readiness=true. Elapsed: 10.013769985s
Jan 29 02:59:07.859: INFO: Pod "pod-subpath-test-secret-sskx": Phase="Running", Reason="", readiness=true. Elapsed: 12.019447538s
Jan 29 02:59:09.855: INFO: Pod "pod-subpath-test-secret-sskx": Phase="Running", Reason="", readiness=true. Elapsed: 14.015490444s
Jan 29 02:59:11.854: INFO: Pod "pod-subpath-test-secret-sskx": Phase="Running", Reason="", readiness=true. Elapsed: 16.014504014s
Jan 29 02:59:13.854: INFO: Pod "pod-subpath-test-secret-sskx": Phase="Running", Reason="", readiness=true. Elapsed: 18.014653422s
Jan 29 02:59:15.855: INFO: Pod "pod-subpath-test-secret-sskx": Phase="Running", Reason="", readiness=true. Elapsed: 20.015653736s
Jan 29 02:59:17.854: INFO: Pod "pod-subpath-test-secret-sskx": Phase="Running", Reason="", readiness=false. Elapsed: 22.014407755s
Jan 29 02:59:19.855: INFO: Pod "pod-subpath-test-secret-sskx": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.015817812s
STEP: Saw pod success 01/29/23 02:59:19.855
Jan 29 02:59:19.856: INFO: Pod "pod-subpath-test-secret-sskx" satisfied condition "Succeeded or Failed"
Jan 29 02:59:19.861: INFO: Trying to get logs from node slave2 pod pod-subpath-test-secret-sskx container test-container-subpath-secret-sskx: <nil>
STEP: delete the pod 01/29/23 02:59:19.877
Jan 29 02:59:19.965: INFO: Waiting for pod pod-subpath-test-secret-sskx to disappear
Jan 29 02:59:19.972: INFO: Pod pod-subpath-test-secret-sskx no longer exists
STEP: Deleting pod pod-subpath-test-secret-sskx 01/29/23 02:59:19.972
Jan 29 02:59:19.972: INFO: Deleting pod "pod-subpath-test-secret-sskx" in namespace "subpath-3560"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jan 29 02:59:19.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3560" for this suite. 01/29/23 02:59:19.987
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]","completed":10,"skipped":205,"failed":0}
------------------------------
• [SLOW TEST] [24.233 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 02:58:55.764
    Jan 29 02:58:55.765: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename subpath 01/29/23 02:58:55.766
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 02:58:55.793
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 02:58:55.799
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/29/23 02:58:55.805
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-sskx 01/29/23 02:58:55.82
    STEP: Creating a pod to test atomic-volume-subpath 01/29/23 02:58:55.82
    Jan 29 02:58:55.839: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-sskx" in namespace "subpath-3560" to be "Succeeded or Failed"
    Jan 29 02:58:55.847: INFO: Pod "pod-subpath-test-secret-sskx": Phase="Pending", Reason="", readiness=false. Elapsed: 7.364112ms
    Jan 29 02:58:57.853: INFO: Pod "pod-subpath-test-secret-sskx": Phase="Running", Reason="", readiness=true. Elapsed: 2.013118845s
    Jan 29 02:58:59.855: INFO: Pod "pod-subpath-test-secret-sskx": Phase="Running", Reason="", readiness=true. Elapsed: 4.015783378s
    Jan 29 02:59:01.855: INFO: Pod "pod-subpath-test-secret-sskx": Phase="Running", Reason="", readiness=true. Elapsed: 6.015097667s
    Jan 29 02:59:03.856: INFO: Pod "pod-subpath-test-secret-sskx": Phase="Running", Reason="", readiness=true. Elapsed: 8.016727612s
    Jan 29 02:59:05.853: INFO: Pod "pod-subpath-test-secret-sskx": Phase="Running", Reason="", readiness=true. Elapsed: 10.013769985s
    Jan 29 02:59:07.859: INFO: Pod "pod-subpath-test-secret-sskx": Phase="Running", Reason="", readiness=true. Elapsed: 12.019447538s
    Jan 29 02:59:09.855: INFO: Pod "pod-subpath-test-secret-sskx": Phase="Running", Reason="", readiness=true. Elapsed: 14.015490444s
    Jan 29 02:59:11.854: INFO: Pod "pod-subpath-test-secret-sskx": Phase="Running", Reason="", readiness=true. Elapsed: 16.014504014s
    Jan 29 02:59:13.854: INFO: Pod "pod-subpath-test-secret-sskx": Phase="Running", Reason="", readiness=true. Elapsed: 18.014653422s
    Jan 29 02:59:15.855: INFO: Pod "pod-subpath-test-secret-sskx": Phase="Running", Reason="", readiness=true. Elapsed: 20.015653736s
    Jan 29 02:59:17.854: INFO: Pod "pod-subpath-test-secret-sskx": Phase="Running", Reason="", readiness=false. Elapsed: 22.014407755s
    Jan 29 02:59:19.855: INFO: Pod "pod-subpath-test-secret-sskx": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.015817812s
    STEP: Saw pod success 01/29/23 02:59:19.855
    Jan 29 02:59:19.856: INFO: Pod "pod-subpath-test-secret-sskx" satisfied condition "Succeeded or Failed"
    Jan 29 02:59:19.861: INFO: Trying to get logs from node slave2 pod pod-subpath-test-secret-sskx container test-container-subpath-secret-sskx: <nil>
    STEP: delete the pod 01/29/23 02:59:19.877
    Jan 29 02:59:19.965: INFO: Waiting for pod pod-subpath-test-secret-sskx to disappear
    Jan 29 02:59:19.972: INFO: Pod pod-subpath-test-secret-sskx no longer exists
    STEP: Deleting pod pod-subpath-test-secret-sskx 01/29/23 02:59:19.972
    Jan 29 02:59:19.972: INFO: Deleting pod "pod-subpath-test-secret-sskx" in namespace "subpath-3560"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jan 29 02:59:19.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-3560" for this suite. 01/29/23 02:59:19.987
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 02:59:19.998
Jan 29 02:59:19.998: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename pods 01/29/23 02:59:19.999
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 02:59:20.028
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 02:59:20.034
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
Jan 29 02:59:20.055: INFO: Waiting up to 5m0s for pod "server-envvars-5d619167-102f-44d2-9f80-4eb72275e3dc" in namespace "pods-2817" to be "running and ready"
Jan 29 02:59:20.063: INFO: Pod "server-envvars-5d619167-102f-44d2-9f80-4eb72275e3dc": Phase="Pending", Reason="", readiness=false. Elapsed: 7.392372ms
Jan 29 02:59:20.063: INFO: The phase of Pod server-envvars-5d619167-102f-44d2-9f80-4eb72275e3dc is Pending, waiting for it to be Running (with Ready = true)
Jan 29 02:59:22.069: INFO: Pod "server-envvars-5d619167-102f-44d2-9f80-4eb72275e3dc": Phase="Running", Reason="", readiness=true. Elapsed: 2.013912365s
Jan 29 02:59:22.069: INFO: The phase of Pod server-envvars-5d619167-102f-44d2-9f80-4eb72275e3dc is Running (Ready = true)
Jan 29 02:59:22.069: INFO: Pod "server-envvars-5d619167-102f-44d2-9f80-4eb72275e3dc" satisfied condition "running and ready"
Jan 29 02:59:22.109: INFO: Waiting up to 5m0s for pod "client-envvars-affc93a8-fd34-4333-a0fc-ff43e0ae9e5d" in namespace "pods-2817" to be "Succeeded or Failed"
Jan 29 02:59:22.115: INFO: Pod "client-envvars-affc93a8-fd34-4333-a0fc-ff43e0ae9e5d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.102762ms
Jan 29 02:59:24.124: INFO: Pod "client-envvars-affc93a8-fd34-4333-a0fc-ff43e0ae9e5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014266907s
Jan 29 02:59:26.124: INFO: Pod "client-envvars-affc93a8-fd34-4333-a0fc-ff43e0ae9e5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015051119s
STEP: Saw pod success 01/29/23 02:59:26.124
Jan 29 02:59:26.125: INFO: Pod "client-envvars-affc93a8-fd34-4333-a0fc-ff43e0ae9e5d" satisfied condition "Succeeded or Failed"
Jan 29 02:59:26.131: INFO: Trying to get logs from node slave2 pod client-envvars-affc93a8-fd34-4333-a0fc-ff43e0ae9e5d container env3cont: <nil>
STEP: delete the pod 01/29/23 02:59:26.146
Jan 29 02:59:26.232: INFO: Waiting for pod client-envvars-affc93a8-fd34-4333-a0fc-ff43e0ae9e5d to disappear
Jan 29 02:59:26.237: INFO: Pod client-envvars-affc93a8-fd34-4333-a0fc-ff43e0ae9e5d no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 29 02:59:26.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2817" for this suite. 01/29/23 02:59:26.247
{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","completed":11,"skipped":210,"failed":0}
------------------------------
• [SLOW TEST] [6.259 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 02:59:19.998
    Jan 29 02:59:19.998: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename pods 01/29/23 02:59:19.999
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 02:59:20.028
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 02:59:20.034
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:443
    Jan 29 02:59:20.055: INFO: Waiting up to 5m0s for pod "server-envvars-5d619167-102f-44d2-9f80-4eb72275e3dc" in namespace "pods-2817" to be "running and ready"
    Jan 29 02:59:20.063: INFO: Pod "server-envvars-5d619167-102f-44d2-9f80-4eb72275e3dc": Phase="Pending", Reason="", readiness=false. Elapsed: 7.392372ms
    Jan 29 02:59:20.063: INFO: The phase of Pod server-envvars-5d619167-102f-44d2-9f80-4eb72275e3dc is Pending, waiting for it to be Running (with Ready = true)
    Jan 29 02:59:22.069: INFO: Pod "server-envvars-5d619167-102f-44d2-9f80-4eb72275e3dc": Phase="Running", Reason="", readiness=true. Elapsed: 2.013912365s
    Jan 29 02:59:22.069: INFO: The phase of Pod server-envvars-5d619167-102f-44d2-9f80-4eb72275e3dc is Running (Ready = true)
    Jan 29 02:59:22.069: INFO: Pod "server-envvars-5d619167-102f-44d2-9f80-4eb72275e3dc" satisfied condition "running and ready"
    Jan 29 02:59:22.109: INFO: Waiting up to 5m0s for pod "client-envvars-affc93a8-fd34-4333-a0fc-ff43e0ae9e5d" in namespace "pods-2817" to be "Succeeded or Failed"
    Jan 29 02:59:22.115: INFO: Pod "client-envvars-affc93a8-fd34-4333-a0fc-ff43e0ae9e5d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.102762ms
    Jan 29 02:59:24.124: INFO: Pod "client-envvars-affc93a8-fd34-4333-a0fc-ff43e0ae9e5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014266907s
    Jan 29 02:59:26.124: INFO: Pod "client-envvars-affc93a8-fd34-4333-a0fc-ff43e0ae9e5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015051119s
    STEP: Saw pod success 01/29/23 02:59:26.124
    Jan 29 02:59:26.125: INFO: Pod "client-envvars-affc93a8-fd34-4333-a0fc-ff43e0ae9e5d" satisfied condition "Succeeded or Failed"
    Jan 29 02:59:26.131: INFO: Trying to get logs from node slave2 pod client-envvars-affc93a8-fd34-4333-a0fc-ff43e0ae9e5d container env3cont: <nil>
    STEP: delete the pod 01/29/23 02:59:26.146
    Jan 29 02:59:26.232: INFO: Waiting for pod client-envvars-affc93a8-fd34-4333-a0fc-ff43e0ae9e5d to disappear
    Jan 29 02:59:26.237: INFO: Pod client-envvars-affc93a8-fd34-4333-a0fc-ff43e0ae9e5d no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 29 02:59:26.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-2817" for this suite. 01/29/23 02:59:26.247
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 02:59:26.257
Jan 29 02:59:26.257: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename kubectl 01/29/23 02:59:26.259
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 02:59:26.289
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 02:59:26.296
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
STEP: creating a replication controller 01/29/23 02:59:26.301
Jan 29 02:59:26.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8511 create -f -'
Jan 29 02:59:28.046: INFO: stderr: ""
Jan 29 02:59:28.046: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/29/23 02:59:28.046
Jan 29 02:59:28.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8511 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 29 02:59:28.173: INFO: stderr: ""
Jan 29 02:59:28.173: INFO: stdout: "update-demo-nautilus-hk4bb update-demo-nautilus-ltfrd "
Jan 29 02:59:28.173: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8511 get pods update-demo-nautilus-hk4bb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 29 02:59:28.287: INFO: stderr: ""
Jan 29 02:59:28.287: INFO: stdout: ""
Jan 29 02:59:28.287: INFO: update-demo-nautilus-hk4bb is created but not running
Jan 29 02:59:33.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8511 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 29 02:59:33.400: INFO: stderr: ""
Jan 29 02:59:33.400: INFO: stdout: "update-demo-nautilus-hk4bb update-demo-nautilus-ltfrd "
Jan 29 02:59:33.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8511 get pods update-demo-nautilus-hk4bb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 29 02:59:33.511: INFO: stderr: ""
Jan 29 02:59:33.511: INFO: stdout: "true"
Jan 29 02:59:33.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8511 get pods update-demo-nautilus-hk4bb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 29 02:59:33.619: INFO: stderr: ""
Jan 29 02:59:33.619: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan 29 02:59:33.619: INFO: validating pod update-demo-nautilus-hk4bb
Jan 29 02:59:33.626: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 29 02:59:33.626: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 29 02:59:33.626: INFO: update-demo-nautilus-hk4bb is verified up and running
Jan 29 02:59:33.626: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8511 get pods update-demo-nautilus-ltfrd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 29 02:59:33.733: INFO: stderr: ""
Jan 29 02:59:33.733: INFO: stdout: "true"
Jan 29 02:59:33.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8511 get pods update-demo-nautilus-ltfrd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 29 02:59:33.839: INFO: stderr: ""
Jan 29 02:59:33.839: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan 29 02:59:33.839: INFO: validating pod update-demo-nautilus-ltfrd
Jan 29 02:59:33.847: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 29 02:59:33.847: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 29 02:59:33.847: INFO: update-demo-nautilus-ltfrd is verified up and running
STEP: using delete to clean up resources 01/29/23 02:59:33.847
Jan 29 02:59:33.848: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8511 delete --grace-period=0 --force -f -'
Jan 29 02:59:33.963: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 29 02:59:33.963: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan 29 02:59:33.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8511 get rc,svc -l name=update-demo --no-headers'
Jan 29 02:59:34.121: INFO: stderr: "No resources found in kubectl-8511 namespace.\n"
Jan 29 02:59:34.121: INFO: stdout: ""
Jan 29 02:59:34.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8511 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 29 02:59:34.265: INFO: stderr: ""
Jan 29 02:59:34.266: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 29 02:59:34.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8511" for this suite. 01/29/23 02:59:34.282
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","completed":12,"skipped":211,"failed":0}
------------------------------
• [SLOW TEST] [8.038 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:337

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 02:59:26.257
    Jan 29 02:59:26.257: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename kubectl 01/29/23 02:59:26.259
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 02:59:26.289
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 02:59:26.296
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:337
    STEP: creating a replication controller 01/29/23 02:59:26.301
    Jan 29 02:59:26.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8511 create -f -'
    Jan 29 02:59:28.046: INFO: stderr: ""
    Jan 29 02:59:28.046: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/29/23 02:59:28.046
    Jan 29 02:59:28.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8511 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 29 02:59:28.173: INFO: stderr: ""
    Jan 29 02:59:28.173: INFO: stdout: "update-demo-nautilus-hk4bb update-demo-nautilus-ltfrd "
    Jan 29 02:59:28.173: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8511 get pods update-demo-nautilus-hk4bb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 29 02:59:28.287: INFO: stderr: ""
    Jan 29 02:59:28.287: INFO: stdout: ""
    Jan 29 02:59:28.287: INFO: update-demo-nautilus-hk4bb is created but not running
    Jan 29 02:59:33.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8511 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 29 02:59:33.400: INFO: stderr: ""
    Jan 29 02:59:33.400: INFO: stdout: "update-demo-nautilus-hk4bb update-demo-nautilus-ltfrd "
    Jan 29 02:59:33.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8511 get pods update-demo-nautilus-hk4bb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 29 02:59:33.511: INFO: stderr: ""
    Jan 29 02:59:33.511: INFO: stdout: "true"
    Jan 29 02:59:33.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8511 get pods update-demo-nautilus-hk4bb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 29 02:59:33.619: INFO: stderr: ""
    Jan 29 02:59:33.619: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan 29 02:59:33.619: INFO: validating pod update-demo-nautilus-hk4bb
    Jan 29 02:59:33.626: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 29 02:59:33.626: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 29 02:59:33.626: INFO: update-demo-nautilus-hk4bb is verified up and running
    Jan 29 02:59:33.626: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8511 get pods update-demo-nautilus-ltfrd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 29 02:59:33.733: INFO: stderr: ""
    Jan 29 02:59:33.733: INFO: stdout: "true"
    Jan 29 02:59:33.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8511 get pods update-demo-nautilus-ltfrd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 29 02:59:33.839: INFO: stderr: ""
    Jan 29 02:59:33.839: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan 29 02:59:33.839: INFO: validating pod update-demo-nautilus-ltfrd
    Jan 29 02:59:33.847: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 29 02:59:33.847: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 29 02:59:33.847: INFO: update-demo-nautilus-ltfrd is verified up and running
    STEP: using delete to clean up resources 01/29/23 02:59:33.847
    Jan 29 02:59:33.848: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8511 delete --grace-period=0 --force -f -'
    Jan 29 02:59:33.963: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 29 02:59:33.963: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Jan 29 02:59:33.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8511 get rc,svc -l name=update-demo --no-headers'
    Jan 29 02:59:34.121: INFO: stderr: "No resources found in kubectl-8511 namespace.\n"
    Jan 29 02:59:34.121: INFO: stdout: ""
    Jan 29 02:59:34.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8511 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jan 29 02:59:34.265: INFO: stderr: ""
    Jan 29 02:59:34.266: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 29 02:59:34.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-8511" for this suite. 01/29/23 02:59:34.282
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 02:59:34.296
Jan 29 02:59:34.296: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename downward-api 01/29/23 02:59:34.297
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 02:59:34.362
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 02:59:34.388
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
STEP: Creating a pod to test downward API volume plugin 01/29/23 02:59:34.411
Jan 29 02:59:34.465: INFO: Waiting up to 5m0s for pod "downwardapi-volume-16905f14-97ff-42c7-87c6-a7c6d1d15d2f" in namespace "downward-api-4342" to be "Succeeded or Failed"
Jan 29 02:59:34.474: INFO: Pod "downwardapi-volume-16905f14-97ff-42c7-87c6-a7c6d1d15d2f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.985703ms
Jan 29 02:59:36.481: INFO: Pod "downwardapi-volume-16905f14-97ff-42c7-87c6-a7c6d1d15d2f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016420922s
Jan 29 02:59:38.481: INFO: Pod "downwardapi-volume-16905f14-97ff-42c7-87c6-a7c6d1d15d2f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016131068s
STEP: Saw pod success 01/29/23 02:59:38.481
Jan 29 02:59:38.481: INFO: Pod "downwardapi-volume-16905f14-97ff-42c7-87c6-a7c6d1d15d2f" satisfied condition "Succeeded or Failed"
Jan 29 02:59:38.487: INFO: Trying to get logs from node slave2 pod downwardapi-volume-16905f14-97ff-42c7-87c6-a7c6d1d15d2f container client-container: <nil>
STEP: delete the pod 01/29/23 02:59:38.502
Jan 29 02:59:38.594: INFO: Waiting for pod downwardapi-volume-16905f14-97ff-42c7-87c6-a7c6d1d15d2f to disappear
Jan 29 02:59:38.602: INFO: Pod downwardapi-volume-16905f14-97ff-42c7-87c6-a7c6d1d15d2f no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 29 02:59:38.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4342" for this suite. 01/29/23 02:59:38.612
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","completed":13,"skipped":219,"failed":0}
------------------------------
• [4.328 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 02:59:34.296
    Jan 29 02:59:34.296: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename downward-api 01/29/23 02:59:34.297
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 02:59:34.362
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 02:59:34.388
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:192
    STEP: Creating a pod to test downward API volume plugin 01/29/23 02:59:34.411
    Jan 29 02:59:34.465: INFO: Waiting up to 5m0s for pod "downwardapi-volume-16905f14-97ff-42c7-87c6-a7c6d1d15d2f" in namespace "downward-api-4342" to be "Succeeded or Failed"
    Jan 29 02:59:34.474: INFO: Pod "downwardapi-volume-16905f14-97ff-42c7-87c6-a7c6d1d15d2f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.985703ms
    Jan 29 02:59:36.481: INFO: Pod "downwardapi-volume-16905f14-97ff-42c7-87c6-a7c6d1d15d2f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016420922s
    Jan 29 02:59:38.481: INFO: Pod "downwardapi-volume-16905f14-97ff-42c7-87c6-a7c6d1d15d2f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016131068s
    STEP: Saw pod success 01/29/23 02:59:38.481
    Jan 29 02:59:38.481: INFO: Pod "downwardapi-volume-16905f14-97ff-42c7-87c6-a7c6d1d15d2f" satisfied condition "Succeeded or Failed"
    Jan 29 02:59:38.487: INFO: Trying to get logs from node slave2 pod downwardapi-volume-16905f14-97ff-42c7-87c6-a7c6d1d15d2f container client-container: <nil>
    STEP: delete the pod 01/29/23 02:59:38.502
    Jan 29 02:59:38.594: INFO: Waiting for pod downwardapi-volume-16905f14-97ff-42c7-87c6-a7c6d1d15d2f to disappear
    Jan 29 02:59:38.602: INFO: Pod downwardapi-volume-16905f14-97ff-42c7-87c6-a7c6d1d15d2f no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 29 02:59:38.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-4342" for this suite. 01/29/23 02:59:38.612
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 02:59:38.626
Jan 29 02:59:38.626: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename container-probe 01/29/23 02:59:38.627
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 02:59:38.654
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 02:59:38.659
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
STEP: Creating pod liveness-b4ac8c30-0d5d-4ca3-94bb-8ebb9a5e84c8 in namespace container-probe-2898 01/29/23 02:59:38.665
Jan 29 02:59:38.682: INFO: Waiting up to 5m0s for pod "liveness-b4ac8c30-0d5d-4ca3-94bb-8ebb9a5e84c8" in namespace "container-probe-2898" to be "not pending"
Jan 29 02:59:38.688: INFO: Pod "liveness-b4ac8c30-0d5d-4ca3-94bb-8ebb9a5e84c8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.053243ms
Jan 29 02:59:40.695: INFO: Pod "liveness-b4ac8c30-0d5d-4ca3-94bb-8ebb9a5e84c8": Phase="Running", Reason="", readiness=true. Elapsed: 2.012446635s
Jan 29 02:59:40.695: INFO: Pod "liveness-b4ac8c30-0d5d-4ca3-94bb-8ebb9a5e84c8" satisfied condition "not pending"
Jan 29 02:59:40.695: INFO: Started pod liveness-b4ac8c30-0d5d-4ca3-94bb-8ebb9a5e84c8 in namespace container-probe-2898
STEP: checking the pod's current state and verifying that restartCount is present 01/29/23 02:59:40.695
Jan 29 02:59:40.701: INFO: Initial restart count of pod liveness-b4ac8c30-0d5d-4ca3-94bb-8ebb9a5e84c8 is 0
Jan 29 03:00:00.781: INFO: Restart count of pod container-probe-2898/liveness-b4ac8c30-0d5d-4ca3-94bb-8ebb9a5e84c8 is now 1 (20.079931411s elapsed)
STEP: deleting the pod 01/29/23 03:00:00.781
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan 29 03:00:00.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2898" for this suite. 01/29/23 03:00:00.899
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":14,"skipped":233,"failed":0}
------------------------------
• [SLOW TEST] [22.284 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 02:59:38.626
    Jan 29 02:59:38.626: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename container-probe 01/29/23 02:59:38.627
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 02:59:38.654
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 02:59:38.659
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:165
    STEP: Creating pod liveness-b4ac8c30-0d5d-4ca3-94bb-8ebb9a5e84c8 in namespace container-probe-2898 01/29/23 02:59:38.665
    Jan 29 02:59:38.682: INFO: Waiting up to 5m0s for pod "liveness-b4ac8c30-0d5d-4ca3-94bb-8ebb9a5e84c8" in namespace "container-probe-2898" to be "not pending"
    Jan 29 02:59:38.688: INFO: Pod "liveness-b4ac8c30-0d5d-4ca3-94bb-8ebb9a5e84c8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.053243ms
    Jan 29 02:59:40.695: INFO: Pod "liveness-b4ac8c30-0d5d-4ca3-94bb-8ebb9a5e84c8": Phase="Running", Reason="", readiness=true. Elapsed: 2.012446635s
    Jan 29 02:59:40.695: INFO: Pod "liveness-b4ac8c30-0d5d-4ca3-94bb-8ebb9a5e84c8" satisfied condition "not pending"
    Jan 29 02:59:40.695: INFO: Started pod liveness-b4ac8c30-0d5d-4ca3-94bb-8ebb9a5e84c8 in namespace container-probe-2898
    STEP: checking the pod's current state and verifying that restartCount is present 01/29/23 02:59:40.695
    Jan 29 02:59:40.701: INFO: Initial restart count of pod liveness-b4ac8c30-0d5d-4ca3-94bb-8ebb9a5e84c8 is 0
    Jan 29 03:00:00.781: INFO: Restart count of pod container-probe-2898/liveness-b4ac8c30-0d5d-4ca3-94bb-8ebb9a5e84c8 is now 1 (20.079931411s elapsed)
    STEP: deleting the pod 01/29/23 03:00:00.781
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan 29 03:00:00.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-2898" for this suite. 01/29/23 03:00:00.899
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:00:00.911
Jan 29 03:00:00.911: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename replication-controller 01/29/23 03:00:00.912
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:00:00.945
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:00:00.951
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
STEP: Given a Pod with a 'name' label pod-adoption is created 01/29/23 03:00:00.957
Jan 29 03:00:00.975: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-366" to be "running and ready"
Jan 29 03:00:00.981: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 5.957021ms
Jan 29 03:00:00.981: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Jan 29 03:00:02.993: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017703031s
Jan 29 03:00:02.993: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Jan 29 03:00:04.991: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 4.015473562s
Jan 29 03:00:04.991: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Jan 29 03:00:04.991: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 01/29/23 03:00:04.997
STEP: Then the orphan pod is adopted 01/29/23 03:00:05.005
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jan 29 03:00:06.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-366" for this suite. 01/29/23 03:00:06.028
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","completed":15,"skipped":245,"failed":0}
------------------------------
• [SLOW TEST] [5.128 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:00:00.911
    Jan 29 03:00:00.911: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename replication-controller 01/29/23 03:00:00.912
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:00:00.945
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:00:00.951
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:91
    STEP: Given a Pod with a 'name' label pod-adoption is created 01/29/23 03:00:00.957
    Jan 29 03:00:00.975: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-366" to be "running and ready"
    Jan 29 03:00:00.981: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 5.957021ms
    Jan 29 03:00:00.981: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Jan 29 03:00:02.993: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017703031s
    Jan 29 03:00:02.993: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Jan 29 03:00:04.991: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 4.015473562s
    Jan 29 03:00:04.991: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Jan 29 03:00:04.991: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 01/29/23 03:00:04.997
    STEP: Then the orphan pod is adopted 01/29/23 03:00:05.005
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jan 29 03:00:06.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-366" for this suite. 01/29/23 03:00:06.028
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:00:06.041
Jan 29 03:00:06.041: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename webhook 01/29/23 03:00:06.043
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:00:06.07
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:00:06.075
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/29/23 03:00:06.108
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/29/23 03:00:07.253
STEP: Deploying the webhook pod 01/29/23 03:00:07.266
STEP: Wait for the deployment to be ready 01/29/23 03:00:07.286
Jan 29 03:00:07.298: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/29/23 03:00:09.319
STEP: Verifying the service has paired with the endpoint 01/29/23 03:00:09.342
Jan 29 03:00:10.343: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
Jan 29 03:00:10.352: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4502-crds.webhook.example.com via the AdmissionRegistration API 01/29/23 03:00:10.948
STEP: Creating a custom resource that should be mutated by the webhook 01/29/23 03:00:10.989
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 29 03:00:13.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5445" for this suite. 01/29/23 03:00:13.651
STEP: Destroying namespace "webhook-5445-markers" for this suite. 01/29/23 03:00:13.667
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","completed":16,"skipped":278,"failed":0}
------------------------------
• [SLOW TEST] [7.753 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:00:06.041
    Jan 29 03:00:06.041: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename webhook 01/29/23 03:00:06.043
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:00:06.07
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:00:06.075
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/29/23 03:00:06.108
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/29/23 03:00:07.253
    STEP: Deploying the webhook pod 01/29/23 03:00:07.266
    STEP: Wait for the deployment to be ready 01/29/23 03:00:07.286
    Jan 29 03:00:07.298: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/29/23 03:00:09.319
    STEP: Verifying the service has paired with the endpoint 01/29/23 03:00:09.342
    Jan 29 03:00:10.343: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:340
    Jan 29 03:00:10.352: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4502-crds.webhook.example.com via the AdmissionRegistration API 01/29/23 03:00:10.948
    STEP: Creating a custom resource that should be mutated by the webhook 01/29/23 03:00:10.989
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 29 03:00:13.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-5445" for this suite. 01/29/23 03:00:13.651
    STEP: Destroying namespace "webhook-5445-markers" for this suite. 01/29/23 03:00:13.667
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:00:13.798
Jan 29 03:00:13.799: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename replicaset 01/29/23 03:00:13.8
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:00:13.86
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:00:13.87
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 01/29/23 03:00:13.902
STEP: Verify that the required pods have come up. 01/29/23 03:00:13.93
Jan 29 03:00:13.938: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 29 03:00:18.950: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/29/23 03:00:18.95
STEP: Getting /status 01/29/23 03:00:18.95
Jan 29 03:00:18.959: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 01/29/23 03:00:18.96
Jan 29 03:00:18.978: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 01/29/23 03:00:18.978
Jan 29 03:00:18.982: INFO: Observed &ReplicaSet event: ADDED
Jan 29 03:00:18.982: INFO: Observed &ReplicaSet event: MODIFIED
Jan 29 03:00:18.982: INFO: Observed &ReplicaSet event: MODIFIED
Jan 29 03:00:18.982: INFO: Observed &ReplicaSet event: MODIFIED
Jan 29 03:00:18.982: INFO: Found replicaset test-rs in namespace replicaset-2843 with labels: map[name:sample-pod pod:httpd] annotations: map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 29 03:00:18.982: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 01/29/23 03:00:18.982
Jan 29 03:00:18.983: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan 29 03:00:18.993: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 01/29/23 03:00:18.993
Jan 29 03:00:18.997: INFO: Observed &ReplicaSet event: ADDED
Jan 29 03:00:18.997: INFO: Observed &ReplicaSet event: MODIFIED
Jan 29 03:00:18.998: INFO: Observed &ReplicaSet event: MODIFIED
Jan 29 03:00:18.998: INFO: Observed &ReplicaSet event: MODIFIED
Jan 29 03:00:18.998: INFO: Observed replicaset test-rs in namespace replicaset-2843 with annotations: map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 29 03:00:18.998: INFO: Observed &ReplicaSet event: MODIFIED
Jan 29 03:00:18.998: INFO: Found replicaset test-rs in namespace replicaset-2843 with labels: map[name:sample-pod pod:httpd] annotations: map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Jan 29 03:00:18.998: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jan 29 03:00:18.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2843" for this suite. 01/29/23 03:00:19.01
{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","completed":17,"skipped":293,"failed":0}
------------------------------
• [SLOW TEST] [5.222 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:00:13.798
    Jan 29 03:00:13.799: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename replicaset 01/29/23 03:00:13.8
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:00:13.86
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:00:13.87
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 01/29/23 03:00:13.902
    STEP: Verify that the required pods have come up. 01/29/23 03:00:13.93
    Jan 29 03:00:13.938: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan 29 03:00:18.950: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/29/23 03:00:18.95
    STEP: Getting /status 01/29/23 03:00:18.95
    Jan 29 03:00:18.959: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 01/29/23 03:00:18.96
    Jan 29 03:00:18.978: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 01/29/23 03:00:18.978
    Jan 29 03:00:18.982: INFO: Observed &ReplicaSet event: ADDED
    Jan 29 03:00:18.982: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 29 03:00:18.982: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 29 03:00:18.982: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 29 03:00:18.982: INFO: Found replicaset test-rs in namespace replicaset-2843 with labels: map[name:sample-pod pod:httpd] annotations: map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan 29 03:00:18.982: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 01/29/23 03:00:18.982
    Jan 29 03:00:18.983: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jan 29 03:00:18.993: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 01/29/23 03:00:18.993
    Jan 29 03:00:18.997: INFO: Observed &ReplicaSet event: ADDED
    Jan 29 03:00:18.997: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 29 03:00:18.998: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 29 03:00:18.998: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 29 03:00:18.998: INFO: Observed replicaset test-rs in namespace replicaset-2843 with annotations: map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan 29 03:00:18.998: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 29 03:00:18.998: INFO: Found replicaset test-rs in namespace replicaset-2843 with labels: map[name:sample-pod pod:httpd] annotations: map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Jan 29 03:00:18.998: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jan 29 03:00:18.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-2843" for this suite. 01/29/23 03:00:19.01
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:00:19.024
Jan 29 03:00:19.024: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename downward-api 01/29/23 03:00:19.037
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:00:19.069
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:00:19.074
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
STEP: Creating a pod to test downward api env vars 01/29/23 03:00:19.08
Jan 29 03:00:19.099: INFO: Waiting up to 5m0s for pod "downward-api-f5d1adcc-1d2d-4a1b-8e95-def7ab9834df" in namespace "downward-api-2771" to be "Succeeded or Failed"
Jan 29 03:00:19.105: INFO: Pod "downward-api-f5d1adcc-1d2d-4a1b-8e95-def7ab9834df": Phase="Pending", Reason="", readiness=false. Elapsed: 6.035703ms
Jan 29 03:00:21.113: INFO: Pod "downward-api-f5d1adcc-1d2d-4a1b-8e95-def7ab9834df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014249907s
Jan 29 03:00:23.116: INFO: Pod "downward-api-f5d1adcc-1d2d-4a1b-8e95-def7ab9834df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017262055s
STEP: Saw pod success 01/29/23 03:00:23.117
Jan 29 03:00:23.117: INFO: Pod "downward-api-f5d1adcc-1d2d-4a1b-8e95-def7ab9834df" satisfied condition "Succeeded or Failed"
Jan 29 03:00:23.124: INFO: Trying to get logs from node slave2 pod downward-api-f5d1adcc-1d2d-4a1b-8e95-def7ab9834df container dapi-container: <nil>
STEP: delete the pod 01/29/23 03:00:23.145
Jan 29 03:00:23.240: INFO: Waiting for pod downward-api-f5d1adcc-1d2d-4a1b-8e95-def7ab9834df to disappear
Jan 29 03:00:23.245: INFO: Pod downward-api-f5d1adcc-1d2d-4a1b-8e95-def7ab9834df no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jan 29 03:00:23.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2771" for this suite. 01/29/23 03:00:23.256
{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","completed":18,"skipped":327,"failed":0}
------------------------------
• [4.243 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:00:19.024
    Jan 29 03:00:19.024: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename downward-api 01/29/23 03:00:19.037
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:00:19.069
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:00:19.074
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:216
    STEP: Creating a pod to test downward api env vars 01/29/23 03:00:19.08
    Jan 29 03:00:19.099: INFO: Waiting up to 5m0s for pod "downward-api-f5d1adcc-1d2d-4a1b-8e95-def7ab9834df" in namespace "downward-api-2771" to be "Succeeded or Failed"
    Jan 29 03:00:19.105: INFO: Pod "downward-api-f5d1adcc-1d2d-4a1b-8e95-def7ab9834df": Phase="Pending", Reason="", readiness=false. Elapsed: 6.035703ms
    Jan 29 03:00:21.113: INFO: Pod "downward-api-f5d1adcc-1d2d-4a1b-8e95-def7ab9834df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014249907s
    Jan 29 03:00:23.116: INFO: Pod "downward-api-f5d1adcc-1d2d-4a1b-8e95-def7ab9834df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017262055s
    STEP: Saw pod success 01/29/23 03:00:23.117
    Jan 29 03:00:23.117: INFO: Pod "downward-api-f5d1adcc-1d2d-4a1b-8e95-def7ab9834df" satisfied condition "Succeeded or Failed"
    Jan 29 03:00:23.124: INFO: Trying to get logs from node slave2 pod downward-api-f5d1adcc-1d2d-4a1b-8e95-def7ab9834df container dapi-container: <nil>
    STEP: delete the pod 01/29/23 03:00:23.145
    Jan 29 03:00:23.240: INFO: Waiting for pod downward-api-f5d1adcc-1d2d-4a1b-8e95-def7ab9834df to disappear
    Jan 29 03:00:23.245: INFO: Pod downward-api-f5d1adcc-1d2d-4a1b-8e95-def7ab9834df no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jan 29 03:00:23.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-2771" for this suite. 01/29/23 03:00:23.256
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:00:23.268
Jan 29 03:00:23.268: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename configmap 01/29/23 03:00:23.269
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:00:23.298
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:00:23.305
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
STEP: Creating configMap with name configmap-test-volume-map-5153e2bc-f990-46ab-941b-f50d1d376710 01/29/23 03:00:23.311
STEP: Creating a pod to test consume configMaps 01/29/23 03:00:23.318
Jan 29 03:00:23.335: INFO: Waiting up to 5m0s for pod "pod-configmaps-dee09a76-c6b6-436b-96eb-21efa007d769" in namespace "configmap-1572" to be "Succeeded or Failed"
Jan 29 03:00:23.342: INFO: Pod "pod-configmaps-dee09a76-c6b6-436b-96eb-21efa007d769": Phase="Pending", Reason="", readiness=false. Elapsed: 6.992329ms
Jan 29 03:00:25.349: INFO: Pod "pod-configmaps-dee09a76-c6b6-436b-96eb-21efa007d769": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014020985s
Jan 29 03:00:27.350: INFO: Pod "pod-configmaps-dee09a76-c6b6-436b-96eb-21efa007d769": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014610817s
STEP: Saw pod success 01/29/23 03:00:27.35
Jan 29 03:00:27.350: INFO: Pod "pod-configmaps-dee09a76-c6b6-436b-96eb-21efa007d769" satisfied condition "Succeeded or Failed"
Jan 29 03:00:27.357: INFO: Trying to get logs from node slave2 pod pod-configmaps-dee09a76-c6b6-436b-96eb-21efa007d769 container agnhost-container: <nil>
STEP: delete the pod 01/29/23 03:00:27.374
Jan 29 03:00:27.469: INFO: Waiting for pod pod-configmaps-dee09a76-c6b6-436b-96eb-21efa007d769 to disappear
Jan 29 03:00:27.474: INFO: Pod pod-configmaps-dee09a76-c6b6-436b-96eb-21efa007d769 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 29 03:00:27.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1572" for this suite. 01/29/23 03:00:27.485
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":19,"skipped":327,"failed":0}
------------------------------
• [4.242 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:00:23.268
    Jan 29 03:00:23.268: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename configmap 01/29/23 03:00:23.269
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:00:23.298
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:00:23.305
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:98
    STEP: Creating configMap with name configmap-test-volume-map-5153e2bc-f990-46ab-941b-f50d1d376710 01/29/23 03:00:23.311
    STEP: Creating a pod to test consume configMaps 01/29/23 03:00:23.318
    Jan 29 03:00:23.335: INFO: Waiting up to 5m0s for pod "pod-configmaps-dee09a76-c6b6-436b-96eb-21efa007d769" in namespace "configmap-1572" to be "Succeeded or Failed"
    Jan 29 03:00:23.342: INFO: Pod "pod-configmaps-dee09a76-c6b6-436b-96eb-21efa007d769": Phase="Pending", Reason="", readiness=false. Elapsed: 6.992329ms
    Jan 29 03:00:25.349: INFO: Pod "pod-configmaps-dee09a76-c6b6-436b-96eb-21efa007d769": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014020985s
    Jan 29 03:00:27.350: INFO: Pod "pod-configmaps-dee09a76-c6b6-436b-96eb-21efa007d769": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014610817s
    STEP: Saw pod success 01/29/23 03:00:27.35
    Jan 29 03:00:27.350: INFO: Pod "pod-configmaps-dee09a76-c6b6-436b-96eb-21efa007d769" satisfied condition "Succeeded or Failed"
    Jan 29 03:00:27.357: INFO: Trying to get logs from node slave2 pod pod-configmaps-dee09a76-c6b6-436b-96eb-21efa007d769 container agnhost-container: <nil>
    STEP: delete the pod 01/29/23 03:00:27.374
    Jan 29 03:00:27.469: INFO: Waiting for pod pod-configmaps-dee09a76-c6b6-436b-96eb-21efa007d769 to disappear
    Jan 29 03:00:27.474: INFO: Pod pod-configmaps-dee09a76-c6b6-436b-96eb-21efa007d769 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 29 03:00:27.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-1572" for this suite. 01/29/23 03:00:27.485
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:00:27.51
Jan 29 03:00:27.511: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename var-expansion 01/29/23 03:00:27.512
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:00:27.541
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:00:27.546
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
Jan 29 03:00:27.567: INFO: Waiting up to 2m0s for pod "var-expansion-2d26d474-7453-47eb-9a68-3ffad8ee571c" in namespace "var-expansion-5588" to be "container 0 failed with reason CreateContainerConfigError"
Jan 29 03:00:27.573: INFO: Pod "var-expansion-2d26d474-7453-47eb-9a68-3ffad8ee571c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.648707ms
Jan 29 03:00:29.580: INFO: Pod "var-expansion-2d26d474-7453-47eb-9a68-3ffad8ee571c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013400261s
Jan 29 03:00:29.580: INFO: Pod "var-expansion-2d26d474-7453-47eb-9a68-3ffad8ee571c" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Jan 29 03:00:29.580: INFO: Deleting pod "var-expansion-2d26d474-7453-47eb-9a68-3ffad8ee571c" in namespace "var-expansion-5588"
Jan 29 03:00:29.617: INFO: Wait up to 5m0s for pod "var-expansion-2d26d474-7453-47eb-9a68-3ffad8ee571c" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan 29 03:00:33.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5588" for this suite. 01/29/23 03:00:33.643
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","completed":20,"skipped":327,"failed":0}
------------------------------
• [SLOW TEST] [6.144 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:00:27.51
    Jan 29 03:00:27.511: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename var-expansion 01/29/23 03:00:27.512
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:00:27.541
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:00:27.546
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:151
    Jan 29 03:00:27.567: INFO: Waiting up to 2m0s for pod "var-expansion-2d26d474-7453-47eb-9a68-3ffad8ee571c" in namespace "var-expansion-5588" to be "container 0 failed with reason CreateContainerConfigError"
    Jan 29 03:00:27.573: INFO: Pod "var-expansion-2d26d474-7453-47eb-9a68-3ffad8ee571c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.648707ms
    Jan 29 03:00:29.580: INFO: Pod "var-expansion-2d26d474-7453-47eb-9a68-3ffad8ee571c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013400261s
    Jan 29 03:00:29.580: INFO: Pod "var-expansion-2d26d474-7453-47eb-9a68-3ffad8ee571c" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Jan 29 03:00:29.580: INFO: Deleting pod "var-expansion-2d26d474-7453-47eb-9a68-3ffad8ee571c" in namespace "var-expansion-5588"
    Jan 29 03:00:29.617: INFO: Wait up to 5m0s for pod "var-expansion-2d26d474-7453-47eb-9a68-3ffad8ee571c" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan 29 03:00:33.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-5588" for this suite. 01/29/23 03:00:33.643
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:00:33.656
Jan 29 03:00:33.656: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename container-runtime 01/29/23 03:00:33.657
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:00:33.69
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:00:33.696
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
STEP: create the container 01/29/23 03:00:33.701
STEP: wait for the container to reach Succeeded 01/29/23 03:00:33.721
STEP: get the container status 01/29/23 03:00:37.766
STEP: the container should be terminated 01/29/23 03:00:37.773
STEP: the termination message should be set 01/29/23 03:00:37.773
Jan 29 03:00:37.773: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 01/29/23 03:00:37.773
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jan 29 03:00:37.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9884" for this suite. 01/29/23 03:00:37.889
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":21,"skipped":339,"failed":0}
------------------------------
• [4.244 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:00:33.656
    Jan 29 03:00:33.656: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename container-runtime 01/29/23 03:00:33.657
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:00:33.69
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:00:33.696
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247
    STEP: create the container 01/29/23 03:00:33.701
    STEP: wait for the container to reach Succeeded 01/29/23 03:00:33.721
    STEP: get the container status 01/29/23 03:00:37.766
    STEP: the container should be terminated 01/29/23 03:00:37.773
    STEP: the termination message should be set 01/29/23 03:00:37.773
    Jan 29 03:00:37.773: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 01/29/23 03:00:37.773
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jan 29 03:00:37.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-9884" for this suite. 01/29/23 03:00:37.889
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:00:37.903
Jan 29 03:00:37.903: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename replicaset 01/29/23 03:00:37.904
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:00:37.935
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:00:37.941
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Jan 29 03:00:37.959: INFO: Creating ReplicaSet my-hostname-basic-017722fd-ac3a-424d-b3fd-22b2adb2b20f
Jan 29 03:00:37.983: INFO: Pod name my-hostname-basic-017722fd-ac3a-424d-b3fd-22b2adb2b20f: Found 0 pods out of 1
Jan 29 03:00:42.992: INFO: Pod name my-hostname-basic-017722fd-ac3a-424d-b3fd-22b2adb2b20f: Found 1 pods out of 1
Jan 29 03:00:42.992: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-017722fd-ac3a-424d-b3fd-22b2adb2b20f" is running
Jan 29 03:00:42.992: INFO: Waiting up to 5m0s for pod "my-hostname-basic-017722fd-ac3a-424d-b3fd-22b2adb2b20f-g2857" in namespace "replicaset-3922" to be "running"
Jan 29 03:00:42.998: INFO: Pod "my-hostname-basic-017722fd-ac3a-424d-b3fd-22b2adb2b20f-g2857": Phase="Running", Reason="", readiness=true. Elapsed: 6.243763ms
Jan 29 03:00:42.998: INFO: Pod "my-hostname-basic-017722fd-ac3a-424d-b3fd-22b2adb2b20f-g2857" satisfied condition "running"
Jan 29 03:00:42.998: INFO: Pod "my-hostname-basic-017722fd-ac3a-424d-b3fd-22b2adb2b20f-g2857" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-29 03:00:38 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-29 03:00:39 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-29 03:00:39 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-29 03:00:38 +0000 UTC Reason: Message:}])
Jan 29 03:00:42.999: INFO: Trying to dial the pod
Jan 29 03:00:48.020: INFO: Controller my-hostname-basic-017722fd-ac3a-424d-b3fd-22b2adb2b20f: Got expected result from replica 1 [my-hostname-basic-017722fd-ac3a-424d-b3fd-22b2adb2b20f-g2857]: "my-hostname-basic-017722fd-ac3a-424d-b3fd-22b2adb2b20f-g2857", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jan 29 03:00:48.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3922" for this suite. 01/29/23 03:00:48.031
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","completed":22,"skipped":369,"failed":0}
------------------------------
• [SLOW TEST] [10.142 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:00:37.903
    Jan 29 03:00:37.903: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename replicaset 01/29/23 03:00:37.904
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:00:37.935
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:00:37.941
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Jan 29 03:00:37.959: INFO: Creating ReplicaSet my-hostname-basic-017722fd-ac3a-424d-b3fd-22b2adb2b20f
    Jan 29 03:00:37.983: INFO: Pod name my-hostname-basic-017722fd-ac3a-424d-b3fd-22b2adb2b20f: Found 0 pods out of 1
    Jan 29 03:00:42.992: INFO: Pod name my-hostname-basic-017722fd-ac3a-424d-b3fd-22b2adb2b20f: Found 1 pods out of 1
    Jan 29 03:00:42.992: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-017722fd-ac3a-424d-b3fd-22b2adb2b20f" is running
    Jan 29 03:00:42.992: INFO: Waiting up to 5m0s for pod "my-hostname-basic-017722fd-ac3a-424d-b3fd-22b2adb2b20f-g2857" in namespace "replicaset-3922" to be "running"
    Jan 29 03:00:42.998: INFO: Pod "my-hostname-basic-017722fd-ac3a-424d-b3fd-22b2adb2b20f-g2857": Phase="Running", Reason="", readiness=true. Elapsed: 6.243763ms
    Jan 29 03:00:42.998: INFO: Pod "my-hostname-basic-017722fd-ac3a-424d-b3fd-22b2adb2b20f-g2857" satisfied condition "running"
    Jan 29 03:00:42.998: INFO: Pod "my-hostname-basic-017722fd-ac3a-424d-b3fd-22b2adb2b20f-g2857" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-29 03:00:38 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-29 03:00:39 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-29 03:00:39 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-29 03:00:38 +0000 UTC Reason: Message:}])
    Jan 29 03:00:42.999: INFO: Trying to dial the pod
    Jan 29 03:00:48.020: INFO: Controller my-hostname-basic-017722fd-ac3a-424d-b3fd-22b2adb2b20f: Got expected result from replica 1 [my-hostname-basic-017722fd-ac3a-424d-b3fd-22b2adb2b20f-g2857]: "my-hostname-basic-017722fd-ac3a-424d-b3fd-22b2adb2b20f-g2857", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jan 29 03:00:48.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-3922" for this suite. 01/29/23 03:00:48.031
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:00:48.047
Jan 29 03:00:48.047: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename podtemplate 01/29/23 03:00:48.048
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:00:48.089
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:00:48.096
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 01/29/23 03:00:48.102
Jan 29 03:00:48.113: INFO: created test-podtemplate-1
Jan 29 03:00:48.126: INFO: created test-podtemplate-2
Jan 29 03:00:48.135: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 01/29/23 03:00:48.135
STEP: delete collection of pod templates 01/29/23 03:00:48.142
Jan 29 03:00:48.143: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 01/29/23 03:00:48.183
Jan 29 03:00:48.183: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Jan 29 03:00:48.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-2909" for this suite. 01/29/23 03:00:48.204
{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","completed":23,"skipped":385,"failed":0}
------------------------------
• [0.172 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:00:48.047
    Jan 29 03:00:48.047: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename podtemplate 01/29/23 03:00:48.048
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:00:48.089
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:00:48.096
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 01/29/23 03:00:48.102
    Jan 29 03:00:48.113: INFO: created test-podtemplate-1
    Jan 29 03:00:48.126: INFO: created test-podtemplate-2
    Jan 29 03:00:48.135: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 01/29/23 03:00:48.135
    STEP: delete collection of pod templates 01/29/23 03:00:48.142
    Jan 29 03:00:48.143: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 01/29/23 03:00:48.183
    Jan 29 03:00:48.183: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Jan 29 03:00:48.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-2909" for this suite. 01/29/23 03:00:48.204
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:00:48.22
Jan 29 03:00:48.220: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename downward-api 01/29/23 03:00:48.222
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:00:48.264
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:00:48.271
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
STEP: Creating the pod 01/29/23 03:00:48.278
Jan 29 03:00:48.310: INFO: Waiting up to 5m0s for pod "annotationupdate95d2a3eb-36b2-49ff-ba48-7effeeb18712" in namespace "downward-api-2202" to be "running and ready"
Jan 29 03:00:48.318: INFO: Pod "annotationupdate95d2a3eb-36b2-49ff-ba48-7effeeb18712": Phase="Pending", Reason="", readiness=false. Elapsed: 7.09191ms
Jan 29 03:00:48.318: INFO: The phase of Pod annotationupdate95d2a3eb-36b2-49ff-ba48-7effeeb18712 is Pending, waiting for it to be Running (with Ready = true)
Jan 29 03:00:50.326: INFO: Pod "annotationupdate95d2a3eb-36b2-49ff-ba48-7effeeb18712": Phase="Running", Reason="", readiness=true. Elapsed: 2.015629536s
Jan 29 03:00:50.326: INFO: The phase of Pod annotationupdate95d2a3eb-36b2-49ff-ba48-7effeeb18712 is Running (Ready = true)
Jan 29 03:00:50.326: INFO: Pod "annotationupdate95d2a3eb-36b2-49ff-ba48-7effeeb18712" satisfied condition "running and ready"
Jan 29 03:00:50.873: INFO: Successfully updated pod "annotationupdate95d2a3eb-36b2-49ff-ba48-7effeeb18712"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 29 03:00:52.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2202" for this suite. 01/29/23 03:00:52.918
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","completed":24,"skipped":399,"failed":0}
------------------------------
• [4.718 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:00:48.22
    Jan 29 03:00:48.220: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename downward-api 01/29/23 03:00:48.222
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:00:48.264
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:00:48.271
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:161
    STEP: Creating the pod 01/29/23 03:00:48.278
    Jan 29 03:00:48.310: INFO: Waiting up to 5m0s for pod "annotationupdate95d2a3eb-36b2-49ff-ba48-7effeeb18712" in namespace "downward-api-2202" to be "running and ready"
    Jan 29 03:00:48.318: INFO: Pod "annotationupdate95d2a3eb-36b2-49ff-ba48-7effeeb18712": Phase="Pending", Reason="", readiness=false. Elapsed: 7.09191ms
    Jan 29 03:00:48.318: INFO: The phase of Pod annotationupdate95d2a3eb-36b2-49ff-ba48-7effeeb18712 is Pending, waiting for it to be Running (with Ready = true)
    Jan 29 03:00:50.326: INFO: Pod "annotationupdate95d2a3eb-36b2-49ff-ba48-7effeeb18712": Phase="Running", Reason="", readiness=true. Elapsed: 2.015629536s
    Jan 29 03:00:50.326: INFO: The phase of Pod annotationupdate95d2a3eb-36b2-49ff-ba48-7effeeb18712 is Running (Ready = true)
    Jan 29 03:00:50.326: INFO: Pod "annotationupdate95d2a3eb-36b2-49ff-ba48-7effeeb18712" satisfied condition "running and ready"
    Jan 29 03:00:50.873: INFO: Successfully updated pod "annotationupdate95d2a3eb-36b2-49ff-ba48-7effeeb18712"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 29 03:00:52.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-2202" for this suite. 01/29/23 03:00:52.918
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:00:52.939
Jan 29 03:00:52.939: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename dns 01/29/23 03:00:52.941
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:00:52.985
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:00:52.992
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 01/29/23 03:00:52.997
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 01/29/23 03:00:52.997
STEP: creating a pod to probe DNS 01/29/23 03:00:52.997
STEP: submitting the pod to kubernetes 01/29/23 03:00:52.997
Jan 29 03:00:53.018: INFO: Waiting up to 15m0s for pod "dns-test-0b9a2ca1-a826-4a53-990e-6000efc46baa" in namespace "dns-6230" to be "running"
Jan 29 03:00:53.024: INFO: Pod "dns-test-0b9a2ca1-a826-4a53-990e-6000efc46baa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.225783ms
Jan 29 03:00:55.031: INFO: Pod "dns-test-0b9a2ca1-a826-4a53-990e-6000efc46baa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013392921s
Jan 29 03:00:57.033: INFO: Pod "dns-test-0b9a2ca1-a826-4a53-990e-6000efc46baa": Phase="Running", Reason="", readiness=true. Elapsed: 4.014628376s
Jan 29 03:00:57.033: INFO: Pod "dns-test-0b9a2ca1-a826-4a53-990e-6000efc46baa" satisfied condition "running"
STEP: retrieving the pod 01/29/23 03:00:57.033
STEP: looking for the results for each expected name from probers 01/29/23 03:00:57.039
Jan 29 03:00:57.064: INFO: DNS probes using dns-6230/dns-test-0b9a2ca1-a826-4a53-990e-6000efc46baa succeeded

STEP: deleting the pod 01/29/23 03:00:57.064
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan 29 03:00:57.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6230" for this suite. 01/29/23 03:00:57.163
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","completed":25,"skipped":409,"failed":0}
------------------------------
• [4.236 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:00:52.939
    Jan 29 03:00:52.939: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename dns 01/29/23 03:00:52.941
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:00:52.985
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:00:52.992
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     01/29/23 03:00:52.997
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     01/29/23 03:00:52.997
    STEP: creating a pod to probe DNS 01/29/23 03:00:52.997
    STEP: submitting the pod to kubernetes 01/29/23 03:00:52.997
    Jan 29 03:00:53.018: INFO: Waiting up to 15m0s for pod "dns-test-0b9a2ca1-a826-4a53-990e-6000efc46baa" in namespace "dns-6230" to be "running"
    Jan 29 03:00:53.024: INFO: Pod "dns-test-0b9a2ca1-a826-4a53-990e-6000efc46baa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.225783ms
    Jan 29 03:00:55.031: INFO: Pod "dns-test-0b9a2ca1-a826-4a53-990e-6000efc46baa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013392921s
    Jan 29 03:00:57.033: INFO: Pod "dns-test-0b9a2ca1-a826-4a53-990e-6000efc46baa": Phase="Running", Reason="", readiness=true. Elapsed: 4.014628376s
    Jan 29 03:00:57.033: INFO: Pod "dns-test-0b9a2ca1-a826-4a53-990e-6000efc46baa" satisfied condition "running"
    STEP: retrieving the pod 01/29/23 03:00:57.033
    STEP: looking for the results for each expected name from probers 01/29/23 03:00:57.039
    Jan 29 03:00:57.064: INFO: DNS probes using dns-6230/dns-test-0b9a2ca1-a826-4a53-990e-6000efc46baa succeeded

    STEP: deleting the pod 01/29/23 03:00:57.064
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan 29 03:00:57.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-6230" for this suite. 01/29/23 03:00:57.163
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:00:57.177
Jan 29 03:00:57.177: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename configmap 01/29/23 03:00:57.178
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:00:57.209
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:00:57.215
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 29 03:00:57.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1069" for this suite. 01/29/23 03:00:57.284
{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","completed":26,"skipped":430,"failed":0}
------------------------------
• [0.120 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:00:57.177
    Jan 29 03:00:57.177: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename configmap 01/29/23 03:00:57.178
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:00:57.209
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:00:57.215
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:503
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 29 03:00:57.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-1069" for this suite. 01/29/23 03:00:57.284
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:00:57.297
Jan 29 03:00:57.297: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename cronjob 01/29/23 03:00:57.299
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:00:57.331
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:00:57.337
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 01/29/23 03:00:57.343
STEP: creating 01/29/23 03:00:57.343
STEP: getting 01/29/23 03:00:57.356
STEP: listing 01/29/23 03:00:57.362
STEP: watching 01/29/23 03:00:57.368
Jan 29 03:00:57.368: INFO: starting watch
STEP: cluster-wide listing 01/29/23 03:00:57.37
STEP: cluster-wide watching 01/29/23 03:00:57.377
Jan 29 03:00:57.377: INFO: starting watch
STEP: patching 01/29/23 03:00:57.379
STEP: updating 01/29/23 03:00:57.387
Jan 29 03:00:57.400: INFO: waiting for watch events with expected annotations
Jan 29 03:00:57.400: INFO: saw patched and updated annotations
STEP: patching /status 01/29/23 03:00:57.401
STEP: updating /status 01/29/23 03:00:57.409
STEP: get /status 01/29/23 03:00:57.421
STEP: deleting 01/29/23 03:00:57.427
STEP: deleting a collection 01/29/23 03:00:57.456
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jan 29 03:00:57.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-9240" for this suite. 01/29/23 03:00:57.485
{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","completed":27,"skipped":443,"failed":0}
------------------------------
• [0.204 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:00:57.297
    Jan 29 03:00:57.297: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename cronjob 01/29/23 03:00:57.299
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:00:57.331
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:00:57.337
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 01/29/23 03:00:57.343
    STEP: creating 01/29/23 03:00:57.343
    STEP: getting 01/29/23 03:00:57.356
    STEP: listing 01/29/23 03:00:57.362
    STEP: watching 01/29/23 03:00:57.368
    Jan 29 03:00:57.368: INFO: starting watch
    STEP: cluster-wide listing 01/29/23 03:00:57.37
    STEP: cluster-wide watching 01/29/23 03:00:57.377
    Jan 29 03:00:57.377: INFO: starting watch
    STEP: patching 01/29/23 03:00:57.379
    STEP: updating 01/29/23 03:00:57.387
    Jan 29 03:00:57.400: INFO: waiting for watch events with expected annotations
    Jan 29 03:00:57.400: INFO: saw patched and updated annotations
    STEP: patching /status 01/29/23 03:00:57.401
    STEP: updating /status 01/29/23 03:00:57.409
    STEP: get /status 01/29/23 03:00:57.421
    STEP: deleting 01/29/23 03:00:57.427
    STEP: deleting a collection 01/29/23 03:00:57.456
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jan 29 03:00:57.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-9240" for this suite. 01/29/23 03:00:57.485
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:00:57.502
Jan 29 03:00:57.502: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename disruption 01/29/23 03:00:57.503
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:00:57.541
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:00:57.547
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
STEP: Waiting for the pdb to be processed 01/29/23 03:00:57.561
STEP: Updating PodDisruptionBudget status 01/29/23 03:00:59.576
STEP: Waiting for all pods to be running 01/29/23 03:00:59.594
Jan 29 03:00:59.605: INFO: running pods: 0 < 1
STEP: locating a running pod 01/29/23 03:01:01.613
STEP: Waiting for the pdb to be processed 01/29/23 03:01:01.632
STEP: Patching PodDisruptionBudget status 01/29/23 03:01:01.645
STEP: Waiting for the pdb to be processed 01/29/23 03:01:01.661
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jan 29 03:01:01.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-7608" for this suite. 01/29/23 03:01:01.676
{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","completed":28,"skipped":444,"failed":0}
------------------------------
• [4.186 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:00:57.502
    Jan 29 03:00:57.502: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename disruption 01/29/23 03:00:57.503
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:00:57.541
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:00:57.547
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:163
    STEP: Waiting for the pdb to be processed 01/29/23 03:00:57.561
    STEP: Updating PodDisruptionBudget status 01/29/23 03:00:59.576
    STEP: Waiting for all pods to be running 01/29/23 03:00:59.594
    Jan 29 03:00:59.605: INFO: running pods: 0 < 1
    STEP: locating a running pod 01/29/23 03:01:01.613
    STEP: Waiting for the pdb to be processed 01/29/23 03:01:01.632
    STEP: Patching PodDisruptionBudget status 01/29/23 03:01:01.645
    STEP: Waiting for the pdb to be processed 01/29/23 03:01:01.661
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jan 29 03:01:01.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-7608" for this suite. 01/29/23 03:01:01.676
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:01:01.692
Jan 29 03:01:01.692: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename csistoragecapacity 01/29/23 03:01:01.693
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:01:01.725
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:01:01.73
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 01/29/23 03:01:01.736
STEP: getting /apis/storage.k8s.io 01/29/23 03:01:01.74
STEP: getting /apis/storage.k8s.io/v1 01/29/23 03:01:01.742
STEP: creating 01/29/23 03:01:01.744
STEP: watching 01/29/23 03:01:01.77
Jan 29 03:01:01.770: INFO: starting watch
STEP: getting 01/29/23 03:01:01.782
STEP: listing in namespace 01/29/23 03:01:01.79
STEP: listing across namespaces 01/29/23 03:01:01.799
STEP: patching 01/29/23 03:01:01.807
STEP: updating 01/29/23 03:01:01.817
Jan 29 03:01:01.826: INFO: waiting for watch events with expected annotations in namespace
Jan 29 03:01:01.826: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 01/29/23 03:01:01.827
STEP: deleting a collection 01/29/23 03:01:01.852
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:187
Jan 29 03:01:01.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "csistoragecapacity-1288" for this suite. 01/29/23 03:01:01.889
{"msg":"PASSED [sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]","completed":29,"skipped":498,"failed":0}
------------------------------
• [0.213 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:01:01.692
    Jan 29 03:01:01.692: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename csistoragecapacity 01/29/23 03:01:01.693
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:01:01.725
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:01:01.73
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 01/29/23 03:01:01.736
    STEP: getting /apis/storage.k8s.io 01/29/23 03:01:01.74
    STEP: getting /apis/storage.k8s.io/v1 01/29/23 03:01:01.742
    STEP: creating 01/29/23 03:01:01.744
    STEP: watching 01/29/23 03:01:01.77
    Jan 29 03:01:01.770: INFO: starting watch
    STEP: getting 01/29/23 03:01:01.782
    STEP: listing in namespace 01/29/23 03:01:01.79
    STEP: listing across namespaces 01/29/23 03:01:01.799
    STEP: patching 01/29/23 03:01:01.807
    STEP: updating 01/29/23 03:01:01.817
    Jan 29 03:01:01.826: INFO: waiting for watch events with expected annotations in namespace
    Jan 29 03:01:01.826: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 01/29/23 03:01:01.827
    STEP: deleting a collection 01/29/23 03:01:01.852
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:187
    Jan 29 03:01:01.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "csistoragecapacity-1288" for this suite. 01/29/23 03:01:01.889
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:01:01.906
Jan 29 03:01:01.906: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename projected 01/29/23 03:01:01.908
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:01:01.94
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:01:01.947
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
STEP: Creating projection with secret that has name projected-secret-test-map-f2cd7c55-472c-4aa4-b85d-dc3e40964d54 01/29/23 03:01:01.954
STEP: Creating a pod to test consume secrets 01/29/23 03:01:01.963
Jan 29 03:01:01.984: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f3bdd021-e22f-44af-b1d2-5f4cdfa35cab" in namespace "projected-2788" to be "Succeeded or Failed"
Jan 29 03:01:01.991: INFO: Pod "pod-projected-secrets-f3bdd021-e22f-44af-b1d2-5f4cdfa35cab": Phase="Pending", Reason="", readiness=false. Elapsed: 6.399465ms
Jan 29 03:01:03.998: INFO: Pod "pod-projected-secrets-f3bdd021-e22f-44af-b1d2-5f4cdfa35cab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014066385s
Jan 29 03:01:05.999: INFO: Pod "pod-projected-secrets-f3bdd021-e22f-44af-b1d2-5f4cdfa35cab": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014166933s
Jan 29 03:01:07.998: INFO: Pod "pod-projected-secrets-f3bdd021-e22f-44af-b1d2-5f4cdfa35cab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013682037s
STEP: Saw pod success 01/29/23 03:01:07.998
Jan 29 03:01:07.998: INFO: Pod "pod-projected-secrets-f3bdd021-e22f-44af-b1d2-5f4cdfa35cab" satisfied condition "Succeeded or Failed"
Jan 29 03:01:08.004: INFO: Trying to get logs from node slave2 pod pod-projected-secrets-f3bdd021-e22f-44af-b1d2-5f4cdfa35cab container projected-secret-volume-test: <nil>
STEP: delete the pod 01/29/23 03:01:08.02
Jan 29 03:01:08.082: INFO: Waiting for pod pod-projected-secrets-f3bdd021-e22f-44af-b1d2-5f4cdfa35cab to disappear
Jan 29 03:01:08.088: INFO: Pod pod-projected-secrets-f3bdd021-e22f-44af-b1d2-5f4cdfa35cab no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan 29 03:01:08.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2788" for this suite. 01/29/23 03:01:08.097
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":30,"skipped":501,"failed":0}
------------------------------
• [SLOW TEST] [6.203 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:01:01.906
    Jan 29 03:01:01.906: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename projected 01/29/23 03:01:01.908
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:01:01.94
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:01:01.947
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:77
    STEP: Creating projection with secret that has name projected-secret-test-map-f2cd7c55-472c-4aa4-b85d-dc3e40964d54 01/29/23 03:01:01.954
    STEP: Creating a pod to test consume secrets 01/29/23 03:01:01.963
    Jan 29 03:01:01.984: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f3bdd021-e22f-44af-b1d2-5f4cdfa35cab" in namespace "projected-2788" to be "Succeeded or Failed"
    Jan 29 03:01:01.991: INFO: Pod "pod-projected-secrets-f3bdd021-e22f-44af-b1d2-5f4cdfa35cab": Phase="Pending", Reason="", readiness=false. Elapsed: 6.399465ms
    Jan 29 03:01:03.998: INFO: Pod "pod-projected-secrets-f3bdd021-e22f-44af-b1d2-5f4cdfa35cab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014066385s
    Jan 29 03:01:05.999: INFO: Pod "pod-projected-secrets-f3bdd021-e22f-44af-b1d2-5f4cdfa35cab": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014166933s
    Jan 29 03:01:07.998: INFO: Pod "pod-projected-secrets-f3bdd021-e22f-44af-b1d2-5f4cdfa35cab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013682037s
    STEP: Saw pod success 01/29/23 03:01:07.998
    Jan 29 03:01:07.998: INFO: Pod "pod-projected-secrets-f3bdd021-e22f-44af-b1d2-5f4cdfa35cab" satisfied condition "Succeeded or Failed"
    Jan 29 03:01:08.004: INFO: Trying to get logs from node slave2 pod pod-projected-secrets-f3bdd021-e22f-44af-b1d2-5f4cdfa35cab container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/29/23 03:01:08.02
    Jan 29 03:01:08.082: INFO: Waiting for pod pod-projected-secrets-f3bdd021-e22f-44af-b1d2-5f4cdfa35cab to disappear
    Jan 29 03:01:08.088: INFO: Pod pod-projected-secrets-f3bdd021-e22f-44af-b1d2-5f4cdfa35cab no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan 29 03:01:08.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2788" for this suite. 01/29/23 03:01:08.097
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:01:08.11
Jan 29 03:01:08.110: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename downward-api 01/29/23 03:01:08.111
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:01:08.141
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:01:08.147
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
STEP: Creating a pod to test downward api env vars 01/29/23 03:01:08.152
Jan 29 03:01:08.170: INFO: Waiting up to 5m0s for pod "downward-api-a28942cf-4bbd-48eb-8de0-aae04e3021ae" in namespace "downward-api-4462" to be "Succeeded or Failed"
Jan 29 03:01:08.176: INFO: Pod "downward-api-a28942cf-4bbd-48eb-8de0-aae04e3021ae": Phase="Pending", Reason="", readiness=false. Elapsed: 5.62196ms
Jan 29 03:01:10.184: INFO: Pod "downward-api-a28942cf-4bbd-48eb-8de0-aae04e3021ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013949045s
Jan 29 03:01:12.184: INFO: Pod "downward-api-a28942cf-4bbd-48eb-8de0-aae04e3021ae": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014035833s
Jan 29 03:01:14.184: INFO: Pod "downward-api-a28942cf-4bbd-48eb-8de0-aae04e3021ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01398242s
STEP: Saw pod success 01/29/23 03:01:14.184
Jan 29 03:01:14.184: INFO: Pod "downward-api-a28942cf-4bbd-48eb-8de0-aae04e3021ae" satisfied condition "Succeeded or Failed"
Jan 29 03:01:14.190: INFO: Trying to get logs from node slave2 pod downward-api-a28942cf-4bbd-48eb-8de0-aae04e3021ae container dapi-container: <nil>
STEP: delete the pod 01/29/23 03:01:14.206
Jan 29 03:01:14.302: INFO: Waiting for pod downward-api-a28942cf-4bbd-48eb-8de0-aae04e3021ae to disappear
Jan 29 03:01:14.309: INFO: Pod downward-api-a28942cf-4bbd-48eb-8de0-aae04e3021ae no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jan 29 03:01:14.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4462" for this suite. 01/29/23 03:01:14.319
{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","completed":31,"skipped":503,"failed":0}
------------------------------
• [SLOW TEST] [6.220 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:01:08.11
    Jan 29 03:01:08.110: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename downward-api 01/29/23 03:01:08.111
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:01:08.141
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:01:08.147
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:43
    STEP: Creating a pod to test downward api env vars 01/29/23 03:01:08.152
    Jan 29 03:01:08.170: INFO: Waiting up to 5m0s for pod "downward-api-a28942cf-4bbd-48eb-8de0-aae04e3021ae" in namespace "downward-api-4462" to be "Succeeded or Failed"
    Jan 29 03:01:08.176: INFO: Pod "downward-api-a28942cf-4bbd-48eb-8de0-aae04e3021ae": Phase="Pending", Reason="", readiness=false. Elapsed: 5.62196ms
    Jan 29 03:01:10.184: INFO: Pod "downward-api-a28942cf-4bbd-48eb-8de0-aae04e3021ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013949045s
    Jan 29 03:01:12.184: INFO: Pod "downward-api-a28942cf-4bbd-48eb-8de0-aae04e3021ae": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014035833s
    Jan 29 03:01:14.184: INFO: Pod "downward-api-a28942cf-4bbd-48eb-8de0-aae04e3021ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01398242s
    STEP: Saw pod success 01/29/23 03:01:14.184
    Jan 29 03:01:14.184: INFO: Pod "downward-api-a28942cf-4bbd-48eb-8de0-aae04e3021ae" satisfied condition "Succeeded or Failed"
    Jan 29 03:01:14.190: INFO: Trying to get logs from node slave2 pod downward-api-a28942cf-4bbd-48eb-8de0-aae04e3021ae container dapi-container: <nil>
    STEP: delete the pod 01/29/23 03:01:14.206
    Jan 29 03:01:14.302: INFO: Waiting for pod downward-api-a28942cf-4bbd-48eb-8de0-aae04e3021ae to disappear
    Jan 29 03:01:14.309: INFO: Pod downward-api-a28942cf-4bbd-48eb-8de0-aae04e3021ae no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jan 29 03:01:14.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-4462" for this suite. 01/29/23 03:01:14.319
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:01:14.332
Jan 29 03:01:14.332: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename init-container 01/29/23 03:01:14.333
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:01:14.363
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:01:14.369
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
STEP: creating the pod 01/29/23 03:01:14.374
Jan 29 03:01:14.375: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Jan 29 03:01:18.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9252" for this suite. 01/29/23 03:01:18.771
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","completed":32,"skipped":531,"failed":0}
------------------------------
• [4.452 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:01:14.332
    Jan 29 03:01:14.332: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename init-container 01/29/23 03:01:14.333
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:01:14.363
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:01:14.369
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:254
    STEP: creating the pod 01/29/23 03:01:14.374
    Jan 29 03:01:14.375: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan 29 03:01:18.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-9252" for this suite. 01/29/23 03:01:18.771
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:01:18.785
Jan 29 03:01:18.786: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename services 01/29/23 03:01:18.787
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:01:18.816
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:01:18.822
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204
STEP: creating service in namespace services-3064 01/29/23 03:01:18.827
STEP: creating service affinity-nodeport in namespace services-3064 01/29/23 03:01:18.827
STEP: creating replication controller affinity-nodeport in namespace services-3064 01/29/23 03:01:18.863
I0129 03:01:18.882582      22 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-3064, replica count: 3
I0129 03:01:21.933461      22 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 29 03:01:21.952: INFO: Creating new exec pod
Jan 29 03:01:21.969: INFO: Waiting up to 5m0s for pod "execpod-affinityr47pr" in namespace "services-3064" to be "running"
Jan 29 03:01:21.975: INFO: Pod "execpod-affinityr47pr": Phase="Pending", Reason="", readiness=false. Elapsed: 5.764561ms
Jan 29 03:01:23.983: INFO: Pod "execpod-affinityr47pr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013176408s
Jan 29 03:01:25.985: INFO: Pod "execpod-affinityr47pr": Phase="Running", Reason="", readiness=true. Elapsed: 4.015010436s
Jan 29 03:01:25.985: INFO: Pod "execpod-affinityr47pr" satisfied condition "running"
Jan 29 03:01:26.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-3064 exec execpod-affinityr47pr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Jan 29 03:01:27.241: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Jan 29 03:01:27.241: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 29 03:01:27.241: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-3064 exec execpod-affinityr47pr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.105.91.100 80'
Jan 29 03:01:27.471: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.105.91.100 80\nConnection to 100.105.91.100 80 port [tcp/http] succeeded!\n"
Jan 29 03:01:27.471: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 29 03:01:27.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-3064 exec execpod-affinityr47pr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.122.245 32690'
Jan 29 03:01:27.703: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.122.245 32690\nConnection to 192.168.122.245 32690 port [tcp/*] succeeded!\n"
Jan 29 03:01:27.703: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 29 03:01:27.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-3064 exec execpod-affinityr47pr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.122.244 32690'
Jan 29 03:01:27.949: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.122.244 32690\nConnection to 192.168.122.244 32690 port [tcp/*] succeeded!\n"
Jan 29 03:01:27.949: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 29 03:01:27.950: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-3064 exec execpod-affinityr47pr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.122.241:32690/ ; done'
Jan 29 03:01:28.309: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:32690/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:32690/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:32690/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:32690/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:32690/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:32690/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:32690/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:32690/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:32690/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:32690/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:32690/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:32690/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:32690/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:32690/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:32690/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:32690/\n"
Jan 29 03:01:28.309: INFO: stdout: "\naffinity-nodeport-x76tf\naffinity-nodeport-x76tf\naffinity-nodeport-x76tf\naffinity-nodeport-x76tf\naffinity-nodeport-x76tf\naffinity-nodeport-x76tf\naffinity-nodeport-x76tf\naffinity-nodeport-x76tf\naffinity-nodeport-x76tf\naffinity-nodeport-x76tf\naffinity-nodeport-x76tf\naffinity-nodeport-x76tf\naffinity-nodeport-x76tf\naffinity-nodeport-x76tf\naffinity-nodeport-x76tf\naffinity-nodeport-x76tf"
Jan 29 03:01:28.309: INFO: Received response from host: affinity-nodeport-x76tf
Jan 29 03:01:28.309: INFO: Received response from host: affinity-nodeport-x76tf
Jan 29 03:01:28.309: INFO: Received response from host: affinity-nodeport-x76tf
Jan 29 03:01:28.309: INFO: Received response from host: affinity-nodeport-x76tf
Jan 29 03:01:28.309: INFO: Received response from host: affinity-nodeport-x76tf
Jan 29 03:01:28.309: INFO: Received response from host: affinity-nodeport-x76tf
Jan 29 03:01:28.309: INFO: Received response from host: affinity-nodeport-x76tf
Jan 29 03:01:28.309: INFO: Received response from host: affinity-nodeport-x76tf
Jan 29 03:01:28.309: INFO: Received response from host: affinity-nodeport-x76tf
Jan 29 03:01:28.309: INFO: Received response from host: affinity-nodeport-x76tf
Jan 29 03:01:28.309: INFO: Received response from host: affinity-nodeport-x76tf
Jan 29 03:01:28.309: INFO: Received response from host: affinity-nodeport-x76tf
Jan 29 03:01:28.309: INFO: Received response from host: affinity-nodeport-x76tf
Jan 29 03:01:28.309: INFO: Received response from host: affinity-nodeport-x76tf
Jan 29 03:01:28.309: INFO: Received response from host: affinity-nodeport-x76tf
Jan 29 03:01:28.309: INFO: Received response from host: affinity-nodeport-x76tf
Jan 29 03:01:28.309: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-3064, will wait for the garbage collector to delete the pods 01/29/23 03:01:28.416
Jan 29 03:01:28.502: INFO: Deleting ReplicationController affinity-nodeport took: 26.392444ms
Jan 29 03:01:28.704: INFO: Terminating ReplicationController affinity-nodeport pods took: 201.111745ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 29 03:01:31.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3064" for this suite. 01/29/23 03:01:31.692
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","completed":33,"skipped":541,"failed":0}
------------------------------
• [SLOW TEST] [12.937 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:01:18.785
    Jan 29 03:01:18.786: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename services 01/29/23 03:01:18.787
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:01:18.816
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:01:18.822
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2204
    STEP: creating service in namespace services-3064 01/29/23 03:01:18.827
    STEP: creating service affinity-nodeport in namespace services-3064 01/29/23 03:01:18.827
    STEP: creating replication controller affinity-nodeport in namespace services-3064 01/29/23 03:01:18.863
    I0129 03:01:18.882582      22 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-3064, replica count: 3
    I0129 03:01:21.933461      22 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 29 03:01:21.952: INFO: Creating new exec pod
    Jan 29 03:01:21.969: INFO: Waiting up to 5m0s for pod "execpod-affinityr47pr" in namespace "services-3064" to be "running"
    Jan 29 03:01:21.975: INFO: Pod "execpod-affinityr47pr": Phase="Pending", Reason="", readiness=false. Elapsed: 5.764561ms
    Jan 29 03:01:23.983: INFO: Pod "execpod-affinityr47pr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013176408s
    Jan 29 03:01:25.985: INFO: Pod "execpod-affinityr47pr": Phase="Running", Reason="", readiness=true. Elapsed: 4.015010436s
    Jan 29 03:01:25.985: INFO: Pod "execpod-affinityr47pr" satisfied condition "running"
    Jan 29 03:01:26.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-3064 exec execpod-affinityr47pr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
    Jan 29 03:01:27.241: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Jan 29 03:01:27.241: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 29 03:01:27.241: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-3064 exec execpod-affinityr47pr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.105.91.100 80'
    Jan 29 03:01:27.471: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.105.91.100 80\nConnection to 100.105.91.100 80 port [tcp/http] succeeded!\n"
    Jan 29 03:01:27.471: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 29 03:01:27.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-3064 exec execpod-affinityr47pr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.122.245 32690'
    Jan 29 03:01:27.703: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.122.245 32690\nConnection to 192.168.122.245 32690 port [tcp/*] succeeded!\n"
    Jan 29 03:01:27.703: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 29 03:01:27.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-3064 exec execpod-affinityr47pr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.122.244 32690'
    Jan 29 03:01:27.949: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.122.244 32690\nConnection to 192.168.122.244 32690 port [tcp/*] succeeded!\n"
    Jan 29 03:01:27.949: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 29 03:01:27.950: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-3064 exec execpod-affinityr47pr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.122.241:32690/ ; done'
    Jan 29 03:01:28.309: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:32690/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:32690/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:32690/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:32690/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:32690/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:32690/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:32690/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:32690/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:32690/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:32690/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:32690/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:32690/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:32690/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:32690/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:32690/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:32690/\n"
    Jan 29 03:01:28.309: INFO: stdout: "\naffinity-nodeport-x76tf\naffinity-nodeport-x76tf\naffinity-nodeport-x76tf\naffinity-nodeport-x76tf\naffinity-nodeport-x76tf\naffinity-nodeport-x76tf\naffinity-nodeport-x76tf\naffinity-nodeport-x76tf\naffinity-nodeport-x76tf\naffinity-nodeport-x76tf\naffinity-nodeport-x76tf\naffinity-nodeport-x76tf\naffinity-nodeport-x76tf\naffinity-nodeport-x76tf\naffinity-nodeport-x76tf\naffinity-nodeport-x76tf"
    Jan 29 03:01:28.309: INFO: Received response from host: affinity-nodeport-x76tf
    Jan 29 03:01:28.309: INFO: Received response from host: affinity-nodeport-x76tf
    Jan 29 03:01:28.309: INFO: Received response from host: affinity-nodeport-x76tf
    Jan 29 03:01:28.309: INFO: Received response from host: affinity-nodeport-x76tf
    Jan 29 03:01:28.309: INFO: Received response from host: affinity-nodeport-x76tf
    Jan 29 03:01:28.309: INFO: Received response from host: affinity-nodeport-x76tf
    Jan 29 03:01:28.309: INFO: Received response from host: affinity-nodeport-x76tf
    Jan 29 03:01:28.309: INFO: Received response from host: affinity-nodeport-x76tf
    Jan 29 03:01:28.309: INFO: Received response from host: affinity-nodeport-x76tf
    Jan 29 03:01:28.309: INFO: Received response from host: affinity-nodeport-x76tf
    Jan 29 03:01:28.309: INFO: Received response from host: affinity-nodeport-x76tf
    Jan 29 03:01:28.309: INFO: Received response from host: affinity-nodeport-x76tf
    Jan 29 03:01:28.309: INFO: Received response from host: affinity-nodeport-x76tf
    Jan 29 03:01:28.309: INFO: Received response from host: affinity-nodeport-x76tf
    Jan 29 03:01:28.309: INFO: Received response from host: affinity-nodeport-x76tf
    Jan 29 03:01:28.309: INFO: Received response from host: affinity-nodeport-x76tf
    Jan 29 03:01:28.309: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-3064, will wait for the garbage collector to delete the pods 01/29/23 03:01:28.416
    Jan 29 03:01:28.502: INFO: Deleting ReplicationController affinity-nodeport took: 26.392444ms
    Jan 29 03:01:28.704: INFO: Terminating ReplicationController affinity-nodeport pods took: 201.111745ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 29 03:01:31.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3064" for this suite. 01/29/23 03:01:31.692
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:01:31.723
Jan 29 03:01:31.723: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename replication-controller 01/29/23 03:01:31.725
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:01:31.774
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:01:31.78
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
Jan 29 03:01:31.786: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 01/29/23 03:01:32.81
STEP: Checking rc "condition-test" has the desired failure condition set 01/29/23 03:01:32.819
STEP: Scaling down rc "condition-test" to satisfy pod quota 01/29/23 03:01:33.835
Jan 29 03:01:33.852: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 01/29/23 03:01:33.852
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jan 29 03:01:33.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5860" for this suite. 01/29/23 03:01:33.869
{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","completed":34,"skipped":546,"failed":0}
------------------------------
• [2.159 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:01:31.723
    Jan 29 03:01:31.723: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename replication-controller 01/29/23 03:01:31.725
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:01:31.774
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:01:31.78
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:82
    Jan 29 03:01:31.786: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 01/29/23 03:01:32.81
    STEP: Checking rc "condition-test" has the desired failure condition set 01/29/23 03:01:32.819
    STEP: Scaling down rc "condition-test" to satisfy pod quota 01/29/23 03:01:33.835
    Jan 29 03:01:33.852: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 01/29/23 03:01:33.852
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jan 29 03:01:33.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-5860" for this suite. 01/29/23 03:01:33.869
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:01:33.883
Jan 29 03:01:33.883: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename pods 01/29/23 03:01:33.884
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:01:33.928
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:01:33.934
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
Jan 29 03:01:33.939: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: creating the pod 01/29/23 03:01:33.941
STEP: submitting the pod to kubernetes 01/29/23 03:01:33.952
Jan 29 03:01:33.976: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-ad295e60-f02e-459e-89eb-16ec8b51ea54" in namespace "pods-9298" to be "running and ready"
Jan 29 03:01:33.985: INFO: Pod "pod-exec-websocket-ad295e60-f02e-459e-89eb-16ec8b51ea54": Phase="Pending", Reason="", readiness=false. Elapsed: 8.741201ms
Jan 29 03:01:33.985: INFO: The phase of Pod pod-exec-websocket-ad295e60-f02e-459e-89eb-16ec8b51ea54 is Pending, waiting for it to be Running (with Ready = true)
Jan 29 03:01:35.994: INFO: Pod "pod-exec-websocket-ad295e60-f02e-459e-89eb-16ec8b51ea54": Phase="Running", Reason="", readiness=true. Elapsed: 2.017811439s
Jan 29 03:01:35.994: INFO: The phase of Pod pod-exec-websocket-ad295e60-f02e-459e-89eb-16ec8b51ea54 is Running (Ready = true)
Jan 29 03:01:35.994: INFO: Pod "pod-exec-websocket-ad295e60-f02e-459e-89eb-16ec8b51ea54" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 29 03:01:36.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9298" for this suite. 01/29/23 03:01:36.141
{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","completed":35,"skipped":555,"failed":0}
------------------------------
• [2.274 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:01:33.883
    Jan 29 03:01:33.883: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename pods 01/29/23 03:01:33.884
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:01:33.928
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:01:33.934
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:535
    Jan 29 03:01:33.939: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: creating the pod 01/29/23 03:01:33.941
    STEP: submitting the pod to kubernetes 01/29/23 03:01:33.952
    Jan 29 03:01:33.976: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-ad295e60-f02e-459e-89eb-16ec8b51ea54" in namespace "pods-9298" to be "running and ready"
    Jan 29 03:01:33.985: INFO: Pod "pod-exec-websocket-ad295e60-f02e-459e-89eb-16ec8b51ea54": Phase="Pending", Reason="", readiness=false. Elapsed: 8.741201ms
    Jan 29 03:01:33.985: INFO: The phase of Pod pod-exec-websocket-ad295e60-f02e-459e-89eb-16ec8b51ea54 is Pending, waiting for it to be Running (with Ready = true)
    Jan 29 03:01:35.994: INFO: Pod "pod-exec-websocket-ad295e60-f02e-459e-89eb-16ec8b51ea54": Phase="Running", Reason="", readiness=true. Elapsed: 2.017811439s
    Jan 29 03:01:35.994: INFO: The phase of Pod pod-exec-websocket-ad295e60-f02e-459e-89eb-16ec8b51ea54 is Running (Ready = true)
    Jan 29 03:01:35.994: INFO: Pod "pod-exec-websocket-ad295e60-f02e-459e-89eb-16ec8b51ea54" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 29 03:01:36.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-9298" for this suite. 01/29/23 03:01:36.141
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:01:36.157
Jan 29 03:01:36.157: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename var-expansion 01/29/23 03:01:36.159
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:01:36.21
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:01:36.217
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
STEP: creating the pod 01/29/23 03:01:36.223
STEP: waiting for pod running 01/29/23 03:01:36.244
Jan 29 03:01:36.244: INFO: Waiting up to 2m0s for pod "var-expansion-130ae76f-a931-4166-a21f-b1fc6ddf45fe" in namespace "var-expansion-6422" to be "running"
Jan 29 03:01:36.250: INFO: Pod "var-expansion-130ae76f-a931-4166-a21f-b1fc6ddf45fe": Phase="Pending", Reason="", readiness=false. Elapsed: 6.087962ms
Jan 29 03:01:38.259: INFO: Pod "var-expansion-130ae76f-a931-4166-a21f-b1fc6ddf45fe": Phase="Running", Reason="", readiness=true. Elapsed: 2.015189601s
Jan 29 03:01:38.259: INFO: Pod "var-expansion-130ae76f-a931-4166-a21f-b1fc6ddf45fe" satisfied condition "running"
STEP: creating a file in subpath 01/29/23 03:01:38.259
Jan 29 03:01:38.269: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-6422 PodName:var-expansion-130ae76f-a931-4166-a21f-b1fc6ddf45fe ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 29 03:01:38.269: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
Jan 29 03:01:38.270: INFO: ExecWithOptions: Clientset creation
Jan 29 03:01:38.271: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/var-expansion-6422/pods/var-expansion-130ae76f-a931-4166-a21f-b1fc6ddf45fe/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 01/29/23 03:01:38.414
Jan 29 03:01:38.423: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-6422 PodName:var-expansion-130ae76f-a931-4166-a21f-b1fc6ddf45fe ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 29 03:01:38.423: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
Jan 29 03:01:38.424: INFO: ExecWithOptions: Clientset creation
Jan 29 03:01:38.424: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/var-expansion-6422/pods/var-expansion-130ae76f-a931-4166-a21f-b1fc6ddf45fe/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 01/29/23 03:01:38.546
Jan 29 03:01:39.060: INFO: Successfully updated pod "var-expansion-130ae76f-a931-4166-a21f-b1fc6ddf45fe"
STEP: waiting for annotated pod running 01/29/23 03:01:39.061
Jan 29 03:01:39.061: INFO: Waiting up to 2m0s for pod "var-expansion-130ae76f-a931-4166-a21f-b1fc6ddf45fe" in namespace "var-expansion-6422" to be "running"
Jan 29 03:01:39.066: INFO: Pod "var-expansion-130ae76f-a931-4166-a21f-b1fc6ddf45fe": Phase="Running", Reason="", readiness=true. Elapsed: 5.359057ms
Jan 29 03:01:39.066: INFO: Pod "var-expansion-130ae76f-a931-4166-a21f-b1fc6ddf45fe" satisfied condition "running"
STEP: deleting the pod gracefully 01/29/23 03:01:39.066
Jan 29 03:01:39.066: INFO: Deleting pod "var-expansion-130ae76f-a931-4166-a21f-b1fc6ddf45fe" in namespace "var-expansion-6422"
Jan 29 03:01:39.104: INFO: Wait up to 5m0s for pod "var-expansion-130ae76f-a931-4166-a21f-b1fc6ddf45fe" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan 29 03:02:13.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6422" for this suite. 01/29/23 03:02:13.128
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","completed":36,"skipped":561,"failed":0}
------------------------------
• [SLOW TEST] [36.982 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:01:36.157
    Jan 29 03:01:36.157: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename var-expansion 01/29/23 03:01:36.159
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:01:36.21
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:01:36.217
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:296
    STEP: creating the pod 01/29/23 03:01:36.223
    STEP: waiting for pod running 01/29/23 03:01:36.244
    Jan 29 03:01:36.244: INFO: Waiting up to 2m0s for pod "var-expansion-130ae76f-a931-4166-a21f-b1fc6ddf45fe" in namespace "var-expansion-6422" to be "running"
    Jan 29 03:01:36.250: INFO: Pod "var-expansion-130ae76f-a931-4166-a21f-b1fc6ddf45fe": Phase="Pending", Reason="", readiness=false. Elapsed: 6.087962ms
    Jan 29 03:01:38.259: INFO: Pod "var-expansion-130ae76f-a931-4166-a21f-b1fc6ddf45fe": Phase="Running", Reason="", readiness=true. Elapsed: 2.015189601s
    Jan 29 03:01:38.259: INFO: Pod "var-expansion-130ae76f-a931-4166-a21f-b1fc6ddf45fe" satisfied condition "running"
    STEP: creating a file in subpath 01/29/23 03:01:38.259
    Jan 29 03:01:38.269: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-6422 PodName:var-expansion-130ae76f-a931-4166-a21f-b1fc6ddf45fe ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 29 03:01:38.269: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    Jan 29 03:01:38.270: INFO: ExecWithOptions: Clientset creation
    Jan 29 03:01:38.271: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/var-expansion-6422/pods/var-expansion-130ae76f-a931-4166-a21f-b1fc6ddf45fe/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 01/29/23 03:01:38.414
    Jan 29 03:01:38.423: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-6422 PodName:var-expansion-130ae76f-a931-4166-a21f-b1fc6ddf45fe ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 29 03:01:38.423: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    Jan 29 03:01:38.424: INFO: ExecWithOptions: Clientset creation
    Jan 29 03:01:38.424: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/var-expansion-6422/pods/var-expansion-130ae76f-a931-4166-a21f-b1fc6ddf45fe/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 01/29/23 03:01:38.546
    Jan 29 03:01:39.060: INFO: Successfully updated pod "var-expansion-130ae76f-a931-4166-a21f-b1fc6ddf45fe"
    STEP: waiting for annotated pod running 01/29/23 03:01:39.061
    Jan 29 03:01:39.061: INFO: Waiting up to 2m0s for pod "var-expansion-130ae76f-a931-4166-a21f-b1fc6ddf45fe" in namespace "var-expansion-6422" to be "running"
    Jan 29 03:01:39.066: INFO: Pod "var-expansion-130ae76f-a931-4166-a21f-b1fc6ddf45fe": Phase="Running", Reason="", readiness=true. Elapsed: 5.359057ms
    Jan 29 03:01:39.066: INFO: Pod "var-expansion-130ae76f-a931-4166-a21f-b1fc6ddf45fe" satisfied condition "running"
    STEP: deleting the pod gracefully 01/29/23 03:01:39.066
    Jan 29 03:01:39.066: INFO: Deleting pod "var-expansion-130ae76f-a931-4166-a21f-b1fc6ddf45fe" in namespace "var-expansion-6422"
    Jan 29 03:01:39.104: INFO: Wait up to 5m0s for pod "var-expansion-130ae76f-a931-4166-a21f-b1fc6ddf45fe" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan 29 03:02:13.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-6422" for this suite. 01/29/23 03:02:13.128
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:02:13.141
Jan 29 03:02:13.141: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename services 01/29/23 03:02:13.142
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:02:13.172
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:02:13.177
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
STEP: creating a service externalname-service with the type=ExternalName in namespace services-8472 01/29/23 03:02:13.183
STEP: changing the ExternalName service to type=NodePort 01/29/23 03:02:13.19
STEP: creating replication controller externalname-service in namespace services-8472 01/29/23 03:02:13.221
I0129 03:02:13.233829      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-8472, replica count: 2
I0129 03:02:16.285395      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 29 03:02:16.285: INFO: Creating new exec pod
Jan 29 03:02:16.301: INFO: Waiting up to 5m0s for pod "execpod2qdbv" in namespace "services-8472" to be "running"
Jan 29 03:02:16.307: INFO: Pod "execpod2qdbv": Phase="Pending", Reason="", readiness=false. Elapsed: 5.69676ms
Jan 29 03:02:18.314: INFO: Pod "execpod2qdbv": Phase="Running", Reason="", readiness=true. Elapsed: 2.012227401s
Jan 29 03:02:18.314: INFO: Pod "execpod2qdbv" satisfied condition "running"
Jan 29 03:02:19.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-8472 exec execpod2qdbv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jan 29 03:02:19.569: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 29 03:02:19.569: INFO: stdout: ""
Jan 29 03:02:20.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-8472 exec execpod2qdbv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jan 29 03:02:20.820: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 29 03:02:20.820: INFO: stdout: "externalname-service-764f8"
Jan 29 03:02:20.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-8472 exec execpod2qdbv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.105.6.242 80'
Jan 29 03:02:21.101: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.105.6.242 80\nConnection to 100.105.6.242 80 port [tcp/http] succeeded!\n"
Jan 29 03:02:21.101: INFO: stdout: ""
Jan 29 03:02:22.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-8472 exec execpod2qdbv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.105.6.242 80'
Jan 29 03:02:22.339: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.105.6.242 80\nConnection to 100.105.6.242 80 port [tcp/http] succeeded!\n"
Jan 29 03:02:22.340: INFO: stdout: "externalname-service-764f8"
Jan 29 03:02:22.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-8472 exec execpod2qdbv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.122.244 30995'
Jan 29 03:02:22.586: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.122.244 30995\nConnection to 192.168.122.244 30995 port [tcp/*] succeeded!\n"
Jan 29 03:02:22.586: INFO: stdout: ""
Jan 29 03:02:23.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-8472 exec execpod2qdbv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.122.244 30995'
Jan 29 03:02:23.820: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.122.244 30995\nConnection to 192.168.122.244 30995 port [tcp/*] succeeded!\n"
Jan 29 03:02:23.820: INFO: stdout: ""
Jan 29 03:02:24.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-8472 exec execpod2qdbv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.122.244 30995'
Jan 29 03:02:24.842: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.122.244 30995\nConnection to 192.168.122.244 30995 port [tcp/*] succeeded!\n"
Jan 29 03:02:24.842: INFO: stdout: ""
Jan 29 03:02:25.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-8472 exec execpod2qdbv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.122.244 30995'
Jan 29 03:02:25.836: INFO: stderr: "+ nc -v -t -w 2 192.168.122.244 30995\n+ echo hostName\nConnection to 192.168.122.244 30995 port [tcp/*] succeeded!\n"
Jan 29 03:02:25.836: INFO: stdout: "externalname-service-h4r2t"
Jan 29 03:02:25.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-8472 exec execpod2qdbv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.122.241 30995'
Jan 29 03:02:26.083: INFO: stderr: "+ + ncecho -v hostName -t\n -w 2 192.168.122.241 30995\nConnection to 192.168.122.241 30995 port [tcp/*] succeeded!\n"
Jan 29 03:02:26.083: INFO: stdout: "externalname-service-764f8"
Jan 29 03:02:26.083: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 29 03:02:26.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8472" for this suite. 01/29/23 03:02:26.159
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","completed":37,"skipped":567,"failed":0}
------------------------------
• [SLOW TEST] [13.035 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:02:13.141
    Jan 29 03:02:13.141: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename services 01/29/23 03:02:13.142
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:02:13.172
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:02:13.177
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1443
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-8472 01/29/23 03:02:13.183
    STEP: changing the ExternalName service to type=NodePort 01/29/23 03:02:13.19
    STEP: creating replication controller externalname-service in namespace services-8472 01/29/23 03:02:13.221
    I0129 03:02:13.233829      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-8472, replica count: 2
    I0129 03:02:16.285395      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 29 03:02:16.285: INFO: Creating new exec pod
    Jan 29 03:02:16.301: INFO: Waiting up to 5m0s for pod "execpod2qdbv" in namespace "services-8472" to be "running"
    Jan 29 03:02:16.307: INFO: Pod "execpod2qdbv": Phase="Pending", Reason="", readiness=false. Elapsed: 5.69676ms
    Jan 29 03:02:18.314: INFO: Pod "execpod2qdbv": Phase="Running", Reason="", readiness=true. Elapsed: 2.012227401s
    Jan 29 03:02:18.314: INFO: Pod "execpod2qdbv" satisfied condition "running"
    Jan 29 03:02:19.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-8472 exec execpod2qdbv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Jan 29 03:02:19.569: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jan 29 03:02:19.569: INFO: stdout: ""
    Jan 29 03:02:20.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-8472 exec execpod2qdbv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Jan 29 03:02:20.820: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jan 29 03:02:20.820: INFO: stdout: "externalname-service-764f8"
    Jan 29 03:02:20.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-8472 exec execpod2qdbv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.105.6.242 80'
    Jan 29 03:02:21.101: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.105.6.242 80\nConnection to 100.105.6.242 80 port [tcp/http] succeeded!\n"
    Jan 29 03:02:21.101: INFO: stdout: ""
    Jan 29 03:02:22.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-8472 exec execpod2qdbv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.105.6.242 80'
    Jan 29 03:02:22.339: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.105.6.242 80\nConnection to 100.105.6.242 80 port [tcp/http] succeeded!\n"
    Jan 29 03:02:22.340: INFO: stdout: "externalname-service-764f8"
    Jan 29 03:02:22.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-8472 exec execpod2qdbv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.122.244 30995'
    Jan 29 03:02:22.586: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.122.244 30995\nConnection to 192.168.122.244 30995 port [tcp/*] succeeded!\n"
    Jan 29 03:02:22.586: INFO: stdout: ""
    Jan 29 03:02:23.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-8472 exec execpod2qdbv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.122.244 30995'
    Jan 29 03:02:23.820: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.122.244 30995\nConnection to 192.168.122.244 30995 port [tcp/*] succeeded!\n"
    Jan 29 03:02:23.820: INFO: stdout: ""
    Jan 29 03:02:24.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-8472 exec execpod2qdbv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.122.244 30995'
    Jan 29 03:02:24.842: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.122.244 30995\nConnection to 192.168.122.244 30995 port [tcp/*] succeeded!\n"
    Jan 29 03:02:24.842: INFO: stdout: ""
    Jan 29 03:02:25.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-8472 exec execpod2qdbv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.122.244 30995'
    Jan 29 03:02:25.836: INFO: stderr: "+ nc -v -t -w 2 192.168.122.244 30995\n+ echo hostName\nConnection to 192.168.122.244 30995 port [tcp/*] succeeded!\n"
    Jan 29 03:02:25.836: INFO: stdout: "externalname-service-h4r2t"
    Jan 29 03:02:25.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-8472 exec execpod2qdbv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.122.241 30995'
    Jan 29 03:02:26.083: INFO: stderr: "+ + ncecho -v hostName -t\n -w 2 192.168.122.241 30995\nConnection to 192.168.122.241 30995 port [tcp/*] succeeded!\n"
    Jan 29 03:02:26.083: INFO: stdout: "externalname-service-764f8"
    Jan 29 03:02:26.083: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 29 03:02:26.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-8472" for this suite. 01/29/23 03:02:26.159
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:02:26.177
Jan 29 03:02:26.177: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename statefulset 01/29/23 03:02:26.178
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:02:26.231
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:02:26.238
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-1060 01/29/23 03:02:26.273
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
STEP: Creating statefulset ss in namespace statefulset-1060 01/29/23 03:02:26.292
Jan 29 03:02:26.312: INFO: Found 0 stateful pods, waiting for 1
Jan 29 03:02:36.319: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 01/29/23 03:02:36.338
STEP: Getting /status 01/29/23 03:02:36.348
Jan 29 03:02:36.358: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 01/29/23 03:02:36.358
Jan 29 03:02:36.379: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 01/29/23 03:02:36.379
Jan 29 03:02:36.383: INFO: Observed &StatefulSet event: ADDED
Jan 29 03:02:36.383: INFO: Found Statefulset ss in namespace statefulset-1060 with labels: map[e2e:testing] annotations: map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 29 03:02:36.383: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 01/29/23 03:02:36.383
Jan 29 03:02:36.383: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan 29 03:02:36.395: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 01/29/23 03:02:36.395
Jan 29 03:02:36.401: INFO: Observed &StatefulSet event: ADDED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 29 03:02:36.402: INFO: Deleting all statefulset in ns statefulset-1060
Jan 29 03:02:36.411: INFO: Scaling statefulset ss to 0
Jan 29 03:02:46.458: INFO: Waiting for statefulset status.replicas updated to 0
Jan 29 03:02:46.466: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan 29 03:02:46.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1060" for this suite. 01/29/23 03:02:46.509
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","completed":38,"skipped":575,"failed":0}
------------------------------
• [SLOW TEST] [20.347 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:975

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:02:26.177
    Jan 29 03:02:26.177: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename statefulset 01/29/23 03:02:26.178
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:02:26.231
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:02:26.238
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-1060 01/29/23 03:02:26.273
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:975
    STEP: Creating statefulset ss in namespace statefulset-1060 01/29/23 03:02:26.292
    Jan 29 03:02:26.312: INFO: Found 0 stateful pods, waiting for 1
    Jan 29 03:02:36.319: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 01/29/23 03:02:36.338
    STEP: Getting /status 01/29/23 03:02:36.348
    Jan 29 03:02:36.358: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 01/29/23 03:02:36.358
    Jan 29 03:02:36.379: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 01/29/23 03:02:36.379
    Jan 29 03:02:36.383: INFO: Observed &StatefulSet event: ADDED
    Jan 29 03:02:36.383: INFO: Found Statefulset ss in namespace statefulset-1060 with labels: map[e2e:testing] annotations: map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan 29 03:02:36.383: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 01/29/23 03:02:36.383
    Jan 29 03:02:36.383: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jan 29 03:02:36.395: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 01/29/23 03:02:36.395
    Jan 29 03:02:36.401: INFO: Observed &StatefulSet event: ADDED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan 29 03:02:36.402: INFO: Deleting all statefulset in ns statefulset-1060
    Jan 29 03:02:36.411: INFO: Scaling statefulset ss to 0
    Jan 29 03:02:46.458: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 29 03:02:46.466: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan 29 03:02:46.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-1060" for this suite. 01/29/23 03:02:46.509
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:02:46.528
Jan 29 03:02:46.528: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename daemonsets 01/29/23 03:02:46.529
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:02:46.557
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:02:46.563
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
Jan 29 03:02:46.651: INFO: Create a RollingUpdate DaemonSet
Jan 29 03:02:46.670: INFO: Check that daemon pods launch on every node of the cluster
Jan 29 03:02:46.697: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 29 03:02:46.697: INFO: Node master1 is running 0 daemon pod, expected 1
Jan 29 03:02:47.717: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 29 03:02:47.717: INFO: Node master1 is running 0 daemon pod, expected 1
Jan 29 03:02:48.720: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 29 03:02:48.720: INFO: Node master3 is running 0 daemon pod, expected 1
Jan 29 03:02:49.714: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Jan 29 03:02:49.714: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
Jan 29 03:02:49.714: INFO: Update the DaemonSet to trigger a rollout
Jan 29 03:02:49.727: INFO: Updating DaemonSet daemon-set
Jan 29 03:02:52.763: INFO: Roll back the DaemonSet before rollout is complete
Jan 29 03:02:52.781: INFO: Updating DaemonSet daemon-set
Jan 29 03:02:52.781: INFO: Make sure DaemonSet rollback is complete
Jan 29 03:02:52.792: INFO: Wrong image for pod: daemon-set-g4f8t. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
Jan 29 03:02:52.792: INFO: Pod daemon-set-g4f8t is not available
Jan 29 03:02:55.813: INFO: Pod daemon-set-ssvz4 is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 01/29/23 03:02:55.839
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6077, will wait for the garbage collector to delete the pods 01/29/23 03:02:55.839
Jan 29 03:02:55.906: INFO: Deleting DaemonSet.extensions daemon-set took: 9.330445ms
Jan 29 03:02:56.006: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.21186ms
Jan 29 03:02:58.314: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 29 03:02:58.314: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 29 03:02:58.320: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"5942660"},"items":null}

Jan 29 03:02:58.326: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"5942660"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan 29 03:02:58.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6077" for this suite. 01/29/23 03:02:58.384
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","completed":39,"skipped":608,"failed":0}
------------------------------
• [SLOW TEST] [11.868 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:02:46.528
    Jan 29 03:02:46.528: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename daemonsets 01/29/23 03:02:46.529
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:02:46.557
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:02:46.563
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:431
    Jan 29 03:02:46.651: INFO: Create a RollingUpdate DaemonSet
    Jan 29 03:02:46.670: INFO: Check that daemon pods launch on every node of the cluster
    Jan 29 03:02:46.697: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 29 03:02:46.697: INFO: Node master1 is running 0 daemon pod, expected 1
    Jan 29 03:02:47.717: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 29 03:02:47.717: INFO: Node master1 is running 0 daemon pod, expected 1
    Jan 29 03:02:48.720: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 29 03:02:48.720: INFO: Node master3 is running 0 daemon pod, expected 1
    Jan 29 03:02:49.714: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
    Jan 29 03:02:49.714: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
    Jan 29 03:02:49.714: INFO: Update the DaemonSet to trigger a rollout
    Jan 29 03:02:49.727: INFO: Updating DaemonSet daemon-set
    Jan 29 03:02:52.763: INFO: Roll back the DaemonSet before rollout is complete
    Jan 29 03:02:52.781: INFO: Updating DaemonSet daemon-set
    Jan 29 03:02:52.781: INFO: Make sure DaemonSet rollback is complete
    Jan 29 03:02:52.792: INFO: Wrong image for pod: daemon-set-g4f8t. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
    Jan 29 03:02:52.792: INFO: Pod daemon-set-g4f8t is not available
    Jan 29 03:02:55.813: INFO: Pod daemon-set-ssvz4 is not available
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 01/29/23 03:02:55.839
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6077, will wait for the garbage collector to delete the pods 01/29/23 03:02:55.839
    Jan 29 03:02:55.906: INFO: Deleting DaemonSet.extensions daemon-set took: 9.330445ms
    Jan 29 03:02:56.006: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.21186ms
    Jan 29 03:02:58.314: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 29 03:02:58.314: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 29 03:02:58.320: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"5942660"},"items":null}

    Jan 29 03:02:58.326: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"5942660"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan 29 03:02:58.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-6077" for this suite. 01/29/23 03:02:58.384
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:02:58.397
Jan 29 03:02:58.397: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename configmap 01/29/23 03:02:58.398
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:02:58.429
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:02:58.435
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
STEP: Creating configMap that has name configmap-test-emptyKey-120a4be2-5806-4644-a3cb-3c53d46ea417 01/29/23 03:02:58.441
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Jan 29 03:02:58.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5079" for this suite. 01/29/23 03:02:58.454
{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","completed":40,"skipped":608,"failed":0}
------------------------------
• [0.071 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:02:58.397
    Jan 29 03:02:58.397: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename configmap 01/29/23 03:02:58.398
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:02:58.429
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:02:58.435
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:137
    STEP: Creating configMap that has name configmap-test-emptyKey-120a4be2-5806-4644-a3cb-3c53d46ea417 01/29/23 03:02:58.441
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 29 03:02:58.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-5079" for this suite. 01/29/23 03:02:58.454
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:02:58.468
Jan 29 03:02:58.468: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename proxy 01/29/23 03:02:58.469
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:02:58.506
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:02:58.511
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 01/29/23 03:02:58.533
STEP: creating replication controller proxy-service-hgvdg in namespace proxy-7408 01/29/23 03:02:58.533
I0129 03:02:58.544897      22 runners.go:193] Created replication controller with name: proxy-service-hgvdg, namespace: proxy-7408, replica count: 1
I0129 03:02:59.595802      22 runners.go:193] proxy-service-hgvdg Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0129 03:03:00.596013      22 runners.go:193] proxy-service-hgvdg Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0129 03:03:01.596286      22 runners.go:193] proxy-service-hgvdg Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 29 03:03:01.604: INFO: setup took 3.087770556s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 01/29/23 03:03:01.604
Jan 29 03:03:01.615: INFO: (0) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 9.221524ms)
Jan 29 03:03:01.615: INFO: (0) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">test<... (200; 10.233371ms)
Jan 29 03:03:01.615: INFO: (0) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 10.078251ms)
Jan 29 03:03:01.615: INFO: (0) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 9.91897ms)
Jan 29 03:03:01.622: INFO: (0) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 16.772558ms)
Jan 29 03:03:01.622: INFO: (0) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname1/proxy/: foo (200; 16.737417ms)
Jan 29 03:03:01.622: INFO: (0) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/rewriteme">test</a> (200; 16.625477ms)
Jan 29 03:03:01.622: INFO: (0) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">... (200; 16.220194ms)
Jan 29 03:03:01.630: INFO: (0) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname1/proxy/: foo (200; 23.719806ms)
Jan 29 03:03:01.630: INFO: (0) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname2/proxy/: bar (200; 23.912467ms)
Jan 29 03:03:01.630: INFO: (0) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:462/proxy/: tls qux (200; 23.849427ms)
Jan 29 03:03:01.630: INFO: (0) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname2/proxy/: tls qux (200; 24.240429ms)
Jan 29 03:03:01.630: INFO: (0) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:460/proxy/: tls baz (200; 25.205176ms)
Jan 29 03:03:01.630: INFO: (0) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname1/proxy/: tls baz (200; 23.076802ms)
Jan 29 03:03:01.630: INFO: (0) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname2/proxy/: bar (200; 23.675326ms)
Jan 29 03:03:01.631: INFO: (0) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/tlsrewritem... (200; 24.873394ms)
Jan 29 03:03:01.639: INFO: (1) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:460/proxy/: tls baz (200; 7.940736ms)
Jan 29 03:03:01.640: INFO: (1) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 8.152757ms)
Jan 29 03:03:01.640: INFO: (1) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">test<... (200; 8.348479ms)
Jan 29 03:03:01.640: INFO: (1) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 8.47996ms)
Jan 29 03:03:01.640: INFO: (1) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/rewriteme">test</a> (200; 8.344498ms)
Jan 29 03:03:01.641: INFO: (1) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 9.250765ms)
Jan 29 03:03:01.641: INFO: (1) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">... (200; 9.138324ms)
Jan 29 03:03:01.641: INFO: (1) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/tlsrewritem... (200; 9.552447ms)
Jan 29 03:03:01.641: INFO: (1) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 9.794468ms)
Jan 29 03:03:01.641: INFO: (1) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:462/proxy/: tls qux (200; 9.611747ms)
Jan 29 03:03:01.646: INFO: (1) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname2/proxy/: tls qux (200; 14.587662ms)
Jan 29 03:03:01.647: INFO: (1) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname2/proxy/: bar (200; 15.481448ms)
Jan 29 03:03:01.649: INFO: (1) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname1/proxy/: foo (200; 16.743377ms)
Jan 29 03:03:01.649: INFO: (1) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname2/proxy/: bar (200; 16.876977ms)
Jan 29 03:03:01.649: INFO: (1) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname1/proxy/: tls baz (200; 17.09792ms)
Jan 29 03:03:01.649: INFO: (1) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname1/proxy/: foo (200; 17.21546ms)
Jan 29 03:03:01.658: INFO: (2) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/rewriteme">test</a> (200; 8.739981ms)
Jan 29 03:03:01.658: INFO: (2) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:462/proxy/: tls qux (200; 8.814441ms)
Jan 29 03:03:01.659: INFO: (2) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/tlsrewritem... (200; 10.067051ms)
Jan 29 03:03:01.659: INFO: (2) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">test<... (200; 10.02861ms)
Jan 29 03:03:01.659: INFO: (2) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname2/proxy/: bar (200; 10.375952ms)
Jan 29 03:03:01.659: INFO: (2) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">... (200; 10.139031ms)
Jan 29 03:03:01.659: INFO: (2) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 10.276172ms)
Jan 29 03:03:01.659: INFO: (2) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:460/proxy/: tls baz (200; 10.07255ms)
Jan 29 03:03:01.659: INFO: (2) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 10.13021ms)
Jan 29 03:03:01.659: INFO: (2) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 10.490733ms)
Jan 29 03:03:01.660: INFO: (2) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 10.558174ms)
Jan 29 03:03:01.664: INFO: (2) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname1/proxy/: foo (200; 14.514161ms)
Jan 29 03:03:01.664: INFO: (2) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname2/proxy/: bar (200; 15.413067ms)
Jan 29 03:03:01.665: INFO: (2) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname2/proxy/: tls qux (200; 15.642529ms)
Jan 29 03:03:01.665: INFO: (2) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname1/proxy/: tls baz (200; 15.694129ms)
Jan 29 03:03:01.665: INFO: (2) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname1/proxy/: foo (200; 15.522108ms)
Jan 29 03:03:01.671: INFO: (3) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/rewriteme">test</a> (200; 5.996341ms)
Jan 29 03:03:01.671: INFO: (3) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:460/proxy/: tls baz (200; 6.572986ms)
Jan 29 03:03:01.672: INFO: (3) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">test<... (200; 6.949488ms)
Jan 29 03:03:01.672: INFO: (3) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 6.852348ms)
Jan 29 03:03:01.672: INFO: (3) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">... (200; 7.14843ms)
Jan 29 03:03:01.672: INFO: (3) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/tlsrewritem... (200; 7.323611ms)
Jan 29 03:03:01.672: INFO: (3) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:462/proxy/: tls qux (200; 7.301131ms)
Jan 29 03:03:01.672: INFO: (3) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 6.999749ms)
Jan 29 03:03:01.672: INFO: (3) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 7.10943ms)
Jan 29 03:03:01.673: INFO: (3) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 7.731894ms)
Jan 29 03:03:01.678: INFO: (3) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname2/proxy/: bar (200; 13.615135ms)
Jan 29 03:03:01.680: INFO: (3) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname1/proxy/: tls baz (200; 14.723102ms)
Jan 29 03:03:01.680: INFO: (3) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname2/proxy/: tls qux (200; 15.323567ms)
Jan 29 03:03:01.680: INFO: (3) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname1/proxy/: foo (200; 15.442308ms)
Jan 29 03:03:01.680: INFO: (3) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname1/proxy/: foo (200; 15.226206ms)
Jan 29 03:03:01.681: INFO: (3) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname2/proxy/: bar (200; 16.056472ms)
Jan 29 03:03:01.690: INFO: (4) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:462/proxy/: tls qux (200; 8.62358ms)
Jan 29 03:03:01.690: INFO: (4) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 9.052463ms)
Jan 29 03:03:01.690: INFO: (4) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 9.078924ms)
Jan 29 03:03:01.691: INFO: (4) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 10.216932ms)
Jan 29 03:03:01.691: INFO: (4) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">test<... (200; 9.459026ms)
Jan 29 03:03:01.691: INFO: (4) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">... (200; 9.721128ms)
Jan 29 03:03:01.691: INFO: (4) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:460/proxy/: tls baz (200; 9.446086ms)
Jan 29 03:03:01.691: INFO: (4) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 9.410425ms)
Jan 29 03:03:01.691: INFO: (4) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/rewriteme">test</a> (200; 9.925549ms)
Jan 29 03:03:01.692: INFO: (4) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/tlsrewritem... (200; 9.881209ms)
Jan 29 03:03:01.693: INFO: (4) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname1/proxy/: tls baz (200; 11.738602ms)
Jan 29 03:03:01.700: INFO: (4) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname1/proxy/: foo (200; 18.174787ms)
Jan 29 03:03:01.700: INFO: (4) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname2/proxy/: bar (200; 18.158867ms)
Jan 29 03:03:01.700: INFO: (4) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname1/proxy/: foo (200; 18.430229ms)
Jan 29 03:03:01.700: INFO: (4) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname2/proxy/: bar (200; 17.939286ms)
Jan 29 03:03:01.700: INFO: (4) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname2/proxy/: tls qux (200; 18.350568ms)
Jan 29 03:03:01.710: INFO: (5) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 9.575147ms)
Jan 29 03:03:01.710: INFO: (5) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/rewriteme">test</a> (200; 9.610847ms)
Jan 29 03:03:01.710: INFO: (5) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:460/proxy/: tls baz (200; 9.776988ms)
Jan 29 03:03:01.710: INFO: (5) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:462/proxy/: tls qux (200; 10.065151ms)
Jan 29 03:03:01.710: INFO: (5) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">test<... (200; 10.01257ms)
Jan 29 03:03:01.711: INFO: (5) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">... (200; 10.863336ms)
Jan 29 03:03:01.711: INFO: (5) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 10.826995ms)
Jan 29 03:03:01.711: INFO: (5) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 10.836395ms)
Jan 29 03:03:01.711: INFO: (5) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/tlsrewritem... (200; 10.982956ms)
Jan 29 03:03:01.713: INFO: (5) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 12.82501ms)
Jan 29 03:03:01.714: INFO: (5) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname2/proxy/: tls qux (200; 13.669456ms)
Jan 29 03:03:01.717: INFO: (5) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname1/proxy/: foo (200; 16.883798ms)
Jan 29 03:03:01.718: INFO: (5) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname2/proxy/: bar (200; 17.560062ms)
Jan 29 03:03:01.718: INFO: (5) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname2/proxy/: bar (200; 17.788984ms)
Jan 29 03:03:01.718: INFO: (5) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname1/proxy/: tls baz (200; 17.676464ms)
Jan 29 03:03:01.718: INFO: (5) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname1/proxy/: foo (200; 17.667904ms)
Jan 29 03:03:01.725: INFO: (6) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 7.337352ms)
Jan 29 03:03:01.727: INFO: (6) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 8.615081ms)
Jan 29 03:03:01.727: INFO: (6) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:462/proxy/: tls qux (200; 8.989443ms)
Jan 29 03:03:01.727: INFO: (6) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:460/proxy/: tls baz (200; 8.699781ms)
Jan 29 03:03:01.727: INFO: (6) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 9.080423ms)
Jan 29 03:03:01.727: INFO: (6) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/rewriteme">test</a> (200; 9.153364ms)
Jan 29 03:03:01.727: INFO: (6) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/tlsrewritem... (200; 9.303105ms)
Jan 29 03:03:01.727: INFO: (6) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">test<... (200; 8.5863ms)
Jan 29 03:03:01.727: INFO: (6) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">... (200; 8.63324ms)
Jan 29 03:03:01.727: INFO: (6) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 8.930122ms)
Jan 29 03:03:01.731: INFO: (6) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname1/proxy/: foo (200; 12.655769ms)
Jan 29 03:03:01.733: INFO: (6) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname2/proxy/: bar (200; 15.125746ms)
Jan 29 03:03:01.734: INFO: (6) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname2/proxy/: bar (200; 16.297234ms)
Jan 29 03:03:01.734: INFO: (6) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname1/proxy/: foo (200; 16.257233ms)
Jan 29 03:03:01.735: INFO: (6) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname2/proxy/: tls qux (200; 16.751097ms)
Jan 29 03:03:01.735: INFO: (6) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname1/proxy/: tls baz (200; 16.566795ms)
Jan 29 03:03:01.741: INFO: (7) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/tlsrewritem... (200; 5.71432ms)
Jan 29 03:03:01.742: INFO: (7) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 7.028149ms)
Jan 29 03:03:01.742: INFO: (7) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 6.207523ms)
Jan 29 03:03:01.743: INFO: (7) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">... (200; 7.705014ms)
Jan 29 03:03:01.743: INFO: (7) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 7.13193ms)
Jan 29 03:03:01.744: INFO: (7) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">test<... (200; 7.075809ms)
Jan 29 03:03:01.744: INFO: (7) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:460/proxy/: tls baz (200; 8.225257ms)
Jan 29 03:03:01.745: INFO: (7) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 7.821455ms)
Jan 29 03:03:01.746: INFO: (7) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/rewriteme">test</a> (200; 9.179804ms)
Jan 29 03:03:01.747: INFO: (7) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname2/proxy/: bar (200; 11.809263ms)
Jan 29 03:03:01.747: INFO: (7) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname1/proxy/: foo (200; 12.071364ms)
Jan 29 03:03:01.747: INFO: (7) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:462/proxy/: tls qux (200; 9.888989ms)
Jan 29 03:03:01.747: INFO: (7) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname1/proxy/: foo (200; 11.780003ms)
Jan 29 03:03:01.748: INFO: (7) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname1/proxy/: tls baz (200; 11.47948ms)
Jan 29 03:03:01.749: INFO: (7) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname2/proxy/: tls qux (200; 12.025244ms)
Jan 29 03:03:01.750: INFO: (7) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname2/proxy/: bar (200; 13.822056ms)
Jan 29 03:03:01.758: INFO: (8) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/rewriteme">test</a> (200; 8.096216ms)
Jan 29 03:03:01.758: INFO: (8) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 8.490479ms)
Jan 29 03:03:01.759: INFO: (8) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 8.55298ms)
Jan 29 03:03:01.759: INFO: (8) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 9.478026ms)
Jan 29 03:03:01.761: INFO: (8) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 10.667655ms)
Jan 29 03:03:01.761: INFO: (8) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">... (200; 10.854496ms)
Jan 29 03:03:01.761: INFO: (8) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname2/proxy/: bar (200; 10.996337ms)
Jan 29 03:03:01.762: INFO: (8) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">test<... (200; 11.44014ms)
Jan 29 03:03:01.762: INFO: (8) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:460/proxy/: tls baz (200; 11.486881ms)
Jan 29 03:03:01.763: INFO: (8) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname1/proxy/: foo (200; 12.756669ms)
Jan 29 03:03:01.763: INFO: (8) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:462/proxy/: tls qux (200; 12.627408ms)
Jan 29 03:03:01.763: INFO: (8) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/tlsrewritem... (200; 12.461367ms)
Jan 29 03:03:01.764: INFO: (8) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname1/proxy/: foo (200; 14.075638ms)
Jan 29 03:03:01.765: INFO: (8) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname2/proxy/: bar (200; 14.786903ms)
Jan 29 03:03:01.766: INFO: (8) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname1/proxy/: tls baz (200; 16.169693ms)
Jan 29 03:03:01.767: INFO: (8) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname2/proxy/: tls qux (200; 16.864358ms)
Jan 29 03:03:01.774: INFO: (9) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">test<... (200; 7.067929ms)
Jan 29 03:03:01.774: INFO: (9) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 7.272111ms)
Jan 29 03:03:01.774: INFO: (9) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 7.229411ms)
Jan 29 03:03:01.774: INFO: (9) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:462/proxy/: tls qux (200; 7.380672ms)
Jan 29 03:03:01.776: INFO: (9) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/rewriteme">test</a> (200; 9.035563ms)
Jan 29 03:03:01.777: INFO: (9) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">... (200; 8.817321ms)
Jan 29 03:03:01.777: INFO: (9) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:460/proxy/: tls baz (200; 9.346446ms)
Jan 29 03:03:01.777: INFO: (9) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 9.723988ms)
Jan 29 03:03:01.777: INFO: (9) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/tlsrewritem... (200; 9.693708ms)
Jan 29 03:03:01.778: INFO: (9) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname2/proxy/: bar (200; 10.465633ms)
Jan 29 03:03:01.778: INFO: (9) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 10.602694ms)
Jan 29 03:03:01.781: INFO: (9) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname2/proxy/: bar (200; 13.423014ms)
Jan 29 03:03:01.781: INFO: (9) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname1/proxy/: tls baz (200; 13.764256ms)
Jan 29 03:03:01.781: INFO: (9) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname2/proxy/: tls qux (200; 14.022518ms)
Jan 29 03:03:01.783: INFO: (9) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname1/proxy/: foo (200; 15.636949ms)
Jan 29 03:03:01.783: INFO: (9) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname1/proxy/: foo (200; 15.478708ms)
Jan 29 03:03:01.790: INFO: (10) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/rewriteme">test</a> (200; 6.499705ms)
Jan 29 03:03:01.791: INFO: (10) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 7.446492ms)
Jan 29 03:03:01.793: INFO: (10) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 8.013296ms)
Jan 29 03:03:01.793: INFO: (10) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:462/proxy/: tls qux (200; 8.535039ms)
Jan 29 03:03:01.793: INFO: (10) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 8.709281ms)
Jan 29 03:03:01.793: INFO: (10) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:460/proxy/: tls baz (200; 7.536813ms)
Jan 29 03:03:01.798: INFO: (10) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">... (200; 12.958191ms)
Jan 29 03:03:01.798: INFO: (10) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">test<... (200; 12.656628ms)
Jan 29 03:03:01.798: INFO: (10) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/tlsrewritem... (200; 13.662316ms)
Jan 29 03:03:01.798: INFO: (10) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 12.83007ms)
Jan 29 03:03:01.799: INFO: (10) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname2/proxy/: tls qux (200; 15.82277ms)
Jan 29 03:03:01.803: INFO: (10) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname1/proxy/: foo (200; 18.182507ms)
Jan 29 03:03:01.804: INFO: (10) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname2/proxy/: bar (200; 19.051073ms)
Jan 29 03:03:01.804: INFO: (10) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname1/proxy/: tls baz (200; 19.92082ms)
Jan 29 03:03:01.804: INFO: (10) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname2/proxy/: bar (200; 19.404176ms)
Jan 29 03:03:01.804: INFO: (10) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname1/proxy/: foo (200; 20.210681ms)
Jan 29 03:03:01.812: INFO: (11) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">test<... (200; 7.637134ms)
Jan 29 03:03:01.815: INFO: (11) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 9.743588ms)
Jan 29 03:03:01.815: INFO: (11) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/tlsrewritem... (200; 10.315512ms)
Jan 29 03:03:01.817: INFO: (11) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/rewriteme">test</a> (200; 12.215606ms)
Jan 29 03:03:01.817: INFO: (11) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">... (200; 11.896443ms)
Jan 29 03:03:01.818: INFO: (11) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:460/proxy/: tls baz (200; 13.037991ms)
Jan 29 03:03:01.820: INFO: (11) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:462/proxy/: tls qux (200; 15.598629ms)
Jan 29 03:03:01.820: INFO: (11) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname1/proxy/: tls baz (200; 15.956891ms)
Jan 29 03:03:01.821: INFO: (11) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 16.719097ms)
Jan 29 03:03:01.821: INFO: (11) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 16.760857ms)
Jan 29 03:03:01.821: INFO: (11) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 16.601716ms)
Jan 29 03:03:01.825: INFO: (11) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname1/proxy/: foo (200; 20.723345ms)
Jan 29 03:03:01.826: INFO: (11) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname1/proxy/: foo (200; 21.311089ms)
Jan 29 03:03:01.826: INFO: (11) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname2/proxy/: bar (200; 21.274908ms)
Jan 29 03:03:01.826: INFO: (11) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname2/proxy/: bar (200; 21.166588ms)
Jan 29 03:03:01.826: INFO: (11) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname2/proxy/: tls qux (200; 21.818572ms)
Jan 29 03:03:01.837: INFO: (12) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/tlsrewritem... (200; 10.107231ms)
Jan 29 03:03:01.837: INFO: (12) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 9.274725ms)
Jan 29 03:03:01.837: INFO: (12) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">test<... (200; 9.464126ms)
Jan 29 03:03:01.837: INFO: (12) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">... (200; 9.847949ms)
Jan 29 03:03:01.838: INFO: (12) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 10.869336ms)
Jan 29 03:03:01.838: INFO: (12) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/rewriteme">test</a> (200; 10.317012ms)
Jan 29 03:03:01.838: INFO: (12) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 10.02327ms)
Jan 29 03:03:01.840: INFO: (12) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 10.350393ms)
Jan 29 03:03:01.840: INFO: (12) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:462/proxy/: tls qux (200; 10.997317ms)
Jan 29 03:03:01.840: INFO: (12) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:460/proxy/: tls baz (200; 12.738569ms)
Jan 29 03:03:01.843: INFO: (12) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname2/proxy/: tls qux (200; 15.128745ms)
Jan 29 03:03:01.843: INFO: (12) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname2/proxy/: bar (200; 16.655516ms)
Jan 29 03:03:01.844: INFO: (12) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname1/proxy/: foo (200; 16.157653ms)
Jan 29 03:03:01.844: INFO: (12) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname2/proxy/: bar (200; 14.781163ms)
Jan 29 03:03:01.844: INFO: (12) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname1/proxy/: tls baz (200; 16.450455ms)
Jan 29 03:03:01.844: INFO: (12) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname1/proxy/: foo (200; 15.391547ms)
Jan 29 03:03:01.854: INFO: (13) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/rewriteme">test</a> (200; 9.148824ms)
Jan 29 03:03:01.856: INFO: (13) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">test<... (200; 11.638341ms)
Jan 29 03:03:01.857: INFO: (13) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/tlsrewritem... (200; 11.959063ms)
Jan 29 03:03:01.858: INFO: (13) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 13.779396ms)
Jan 29 03:03:01.863: INFO: (13) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 17.875505ms)
Jan 29 03:03:01.863: INFO: (13) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 18.318448ms)
Jan 29 03:03:01.863: INFO: (13) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:460/proxy/: tls baz (200; 18.191607ms)
Jan 29 03:03:01.863: INFO: (13) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">... (200; 18.300408ms)
Jan 29 03:03:01.863: INFO: (13) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 18.234528ms)
Jan 29 03:03:01.863: INFO: (13) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:462/proxy/: tls qux (200; 18.715071ms)
Jan 29 03:03:01.863: INFO: (13) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname1/proxy/: foo (200; 18.51303ms)
Jan 29 03:03:01.864: INFO: (13) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname1/proxy/: tls baz (200; 19.086853ms)
Jan 29 03:03:01.864: INFO: (13) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname2/proxy/: bar (200; 18.523889ms)
Jan 29 03:03:01.864: INFO: (13) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname2/proxy/: tls qux (200; 19.081054ms)
Jan 29 03:03:01.864: INFO: (13) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname2/proxy/: bar (200; 18.864772ms)
Jan 29 03:03:01.864: INFO: (13) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname1/proxy/: foo (200; 18.919292ms)
Jan 29 03:03:01.872: INFO: (14) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 8.354619ms)
Jan 29 03:03:01.875: INFO: (14) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/rewriteme">test</a> (200; 10.202491ms)
Jan 29 03:03:01.875: INFO: (14) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:462/proxy/: tls qux (200; 9.842749ms)
Jan 29 03:03:01.875: INFO: (14) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/tlsrewritem... (200; 9.037303ms)
Jan 29 03:03:01.875: INFO: (14) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 10.122931ms)
Jan 29 03:03:01.875: INFO: (14) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 9.306045ms)
Jan 29 03:03:01.875: INFO: (14) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 8.031517ms)
Jan 29 03:03:01.876: INFO: (14) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">test<... (200; 8.730741ms)
Jan 29 03:03:01.876: INFO: (14) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">... (200; 9.239024ms)
Jan 29 03:03:01.877: INFO: (14) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:460/proxy/: tls baz (200; 10.305272ms)
Jan 29 03:03:01.877: INFO: (14) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname1/proxy/: foo (200; 11.629222ms)
Jan 29 03:03:01.882: INFO: (14) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname2/proxy/: tls qux (200; 17.724104ms)
Jan 29 03:03:01.883: INFO: (14) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname2/proxy/: bar (200; 16.712677ms)
Jan 29 03:03:01.883: INFO: (14) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname1/proxy/: foo (200; 19.145614ms)
Jan 29 03:03:01.883: INFO: (14) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname2/proxy/: bar (200; 17.374821ms)
Jan 29 03:03:01.883: INFO: (14) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname1/proxy/: tls baz (200; 18.875892ms)
Jan 29 03:03:01.893: INFO: (15) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 9.918049ms)
Jan 29 03:03:01.894: INFO: (15) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/tlsrewritem... (200; 9.756648ms)
Jan 29 03:03:01.894: INFO: (15) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 10.278271ms)
Jan 29 03:03:01.894: INFO: (15) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/rewriteme">test</a> (200; 10.489773ms)
Jan 29 03:03:01.895: INFO: (15) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:462/proxy/: tls qux (200; 10.915456ms)
Jan 29 03:03:01.895: INFO: (15) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:460/proxy/: tls baz (200; 10.714595ms)
Jan 29 03:03:01.895: INFO: (15) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">... (200; 10.898537ms)
Jan 29 03:03:01.895: INFO: (15) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">test<... (200; 10.902336ms)
Jan 29 03:03:01.895: INFO: (15) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 11.572701ms)
Jan 29 03:03:01.895: INFO: (15) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 11.233918ms)
Jan 29 03:03:01.896: INFO: (15) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname2/proxy/: bar (200; 12.180625ms)
Jan 29 03:03:01.899: INFO: (15) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname1/proxy/: foo (200; 15.253287ms)
Jan 29 03:03:01.899: INFO: (15) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname2/proxy/: tls qux (200; 15.301927ms)
Jan 29 03:03:01.899: INFO: (15) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname1/proxy/: tls baz (200; 15.412688ms)
Jan 29 03:03:01.899: INFO: (15) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname2/proxy/: bar (200; 15.581749ms)
Jan 29 03:03:01.901: INFO: (15) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname1/proxy/: foo (200; 16.990099ms)
Jan 29 03:03:01.908: INFO: (16) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">test<... (200; 6.726847ms)
Jan 29 03:03:01.909: INFO: (16) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 7.585413ms)
Jan 29 03:03:01.910: INFO: (16) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/rewriteme">test</a> (200; 8.860382ms)
Jan 29 03:03:01.910: INFO: (16) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:462/proxy/: tls qux (200; 8.475399ms)
Jan 29 03:03:01.911: INFO: (16) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 8.842142ms)
Jan 29 03:03:01.911: INFO: (16) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/tlsrewritem... (200; 8.670741ms)
Jan 29 03:03:01.912: INFO: (16) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">... (200; 8.58036ms)
Jan 29 03:03:01.912: INFO: (16) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 8.211997ms)
Jan 29 03:03:01.913: INFO: (16) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 8.694021ms)
Jan 29 03:03:01.913: INFO: (16) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:460/proxy/: tls baz (200; 9.394565ms)
Jan 29 03:03:01.913: INFO: (16) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname1/proxy/: tls baz (200; 11.927724ms)
Jan 29 03:03:01.914: INFO: (16) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname2/proxy/: tls qux (200; 12.539327ms)
Jan 29 03:03:01.919: INFO: (16) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname1/proxy/: foo (200; 16.574215ms)
Jan 29 03:03:01.919: INFO: (16) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname1/proxy/: foo (200; 17.345522ms)
Jan 29 03:03:01.919: INFO: (16) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname2/proxy/: bar (200; 15.930171ms)
Jan 29 03:03:01.919: INFO: (16) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname2/proxy/: bar (200; 17.19214ms)
Jan 29 03:03:01.928: INFO: (17) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/tlsrewritem... (200; 8.861282ms)
Jan 29 03:03:01.930: INFO: (17) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 10.370492ms)
Jan 29 03:03:01.931: INFO: (17) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/rewriteme">test</a> (200; 11.581741ms)
Jan 29 03:03:01.932: INFO: (17) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:460/proxy/: tls baz (200; 11.957463ms)
Jan 29 03:03:01.932: INFO: (17) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">... (200; 12.071325ms)
Jan 29 03:03:01.932: INFO: (17) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 12.178005ms)
Jan 29 03:03:01.932: INFO: (17) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">test<... (200; 12.363867ms)
Jan 29 03:03:01.932: INFO: (17) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 12.416567ms)
Jan 29 03:03:01.932: INFO: (17) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 12.385006ms)
Jan 29 03:03:01.933: INFO: (17) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:462/proxy/: tls qux (200; 13.011751ms)
Jan 29 03:03:01.933: INFO: (17) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname1/proxy/: tls baz (200; 13.074311ms)
Jan 29 03:03:01.934: INFO: (17) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname1/proxy/: foo (200; 14.480481ms)
Jan 29 03:03:01.936: INFO: (17) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname1/proxy/: foo (200; 15.944691ms)
Jan 29 03:03:01.936: INFO: (17) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname2/proxy/: tls qux (200; 16.151933ms)
Jan 29 03:03:01.936: INFO: (17) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname2/proxy/: bar (200; 16.195593ms)
Jan 29 03:03:01.936: INFO: (17) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname2/proxy/: bar (200; 16.142633ms)
Jan 29 03:03:01.944: INFO: (18) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:460/proxy/: tls baz (200; 7.278311ms)
Jan 29 03:03:01.947: INFO: (18) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 9.536966ms)
Jan 29 03:03:01.947: INFO: (18) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 9.933689ms)
Jan 29 03:03:01.947: INFO: (18) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">... (200; 10.08701ms)
Jan 29 03:03:01.948: INFO: (18) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 10.695275ms)
Jan 29 03:03:01.949: INFO: (18) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">test<... (200; 12.083544ms)
Jan 29 03:03:01.950: INFO: (18) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname2/proxy/: bar (200; 14.162239ms)
Jan 29 03:03:01.951: INFO: (18) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 13.548335ms)
Jan 29 03:03:01.951: INFO: (18) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/tlsrewritem... (200; 13.447194ms)
Jan 29 03:03:01.951: INFO: (18) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname2/proxy/: bar (200; 15.041165ms)
Jan 29 03:03:01.952: INFO: (18) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:462/proxy/: tls qux (200; 13.994058ms)
Jan 29 03:03:01.952: INFO: (18) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/rewriteme">test</a> (200; 15.062445ms)
Jan 29 03:03:01.953: INFO: (18) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname1/proxy/: tls baz (200; 15.267067ms)
Jan 29 03:03:01.953: INFO: (18) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname1/proxy/: foo (200; 14.917924ms)
Jan 29 03:03:01.953: INFO: (18) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname1/proxy/: foo (200; 15.457988ms)
Jan 29 03:03:01.954: INFO: (18) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname2/proxy/: tls qux (200; 16.390014ms)
Jan 29 03:03:01.963: INFO: (19) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 9.422486ms)
Jan 29 03:03:01.964: INFO: (19) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 10.218491ms)
Jan 29 03:03:01.964: INFO: (19) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">... (200; 9.720708ms)
Jan 29 03:03:01.965: INFO: (19) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">test<... (200; 11.021057ms)
Jan 29 03:03:01.965: INFO: (19) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:462/proxy/: tls qux (200; 10.943057ms)
Jan 29 03:03:01.966: INFO: (19) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 11.44464ms)
Jan 29 03:03:01.968: INFO: (19) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/rewriteme">test</a> (200; 14.042619ms)
Jan 29 03:03:01.968: INFO: (19) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:460/proxy/: tls baz (200; 14.437981ms)
Jan 29 03:03:01.968: INFO: (19) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname1/proxy/: foo (200; 14.23606ms)
Jan 29 03:03:01.968: INFO: (19) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 14.052978ms)
Jan 29 03:03:01.968: INFO: (19) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/tlsrewritem... (200; 13.879937ms)
Jan 29 03:03:01.971: INFO: (19) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname1/proxy/: tls baz (200; 17.25428ms)
Jan 29 03:03:01.972: INFO: (19) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname2/proxy/: bar (200; 17.576523ms)
Jan 29 03:03:01.972: INFO: (19) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname1/proxy/: foo (200; 17.792644ms)
Jan 29 03:03:01.972: INFO: (19) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname2/proxy/: tls qux (200; 17.947506ms)
Jan 29 03:03:01.972: INFO: (19) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname2/proxy/: bar (200; 17.833105ms)
STEP: deleting ReplicationController proxy-service-hgvdg in namespace proxy-7408, will wait for the garbage collector to delete the pods 01/29/23 03:03:01.972
Jan 29 03:03:02.042: INFO: Deleting ReplicationController proxy-service-hgvdg took: 13.193092ms
Jan 29 03:03:02.143: INFO: Terminating ReplicationController proxy-service-hgvdg pods took: 101.011926ms
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Jan 29 03:03:04.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-7408" for this suite. 01/29/23 03:03:04.156
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","completed":41,"skipped":609,"failed":0}
------------------------------
• [SLOW TEST] [5.702 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:02:58.468
    Jan 29 03:02:58.468: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename proxy 01/29/23 03:02:58.469
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:02:58.506
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:02:58.511
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 01/29/23 03:02:58.533
    STEP: creating replication controller proxy-service-hgvdg in namespace proxy-7408 01/29/23 03:02:58.533
    I0129 03:02:58.544897      22 runners.go:193] Created replication controller with name: proxy-service-hgvdg, namespace: proxy-7408, replica count: 1
    I0129 03:02:59.595802      22 runners.go:193] proxy-service-hgvdg Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0129 03:03:00.596013      22 runners.go:193] proxy-service-hgvdg Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
    I0129 03:03:01.596286      22 runners.go:193] proxy-service-hgvdg Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 29 03:03:01.604: INFO: setup took 3.087770556s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 01/29/23 03:03:01.604
    Jan 29 03:03:01.615: INFO: (0) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 9.221524ms)
    Jan 29 03:03:01.615: INFO: (0) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">test<... (200; 10.233371ms)
    Jan 29 03:03:01.615: INFO: (0) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 10.078251ms)
    Jan 29 03:03:01.615: INFO: (0) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 9.91897ms)
    Jan 29 03:03:01.622: INFO: (0) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 16.772558ms)
    Jan 29 03:03:01.622: INFO: (0) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname1/proxy/: foo (200; 16.737417ms)
    Jan 29 03:03:01.622: INFO: (0) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/rewriteme">test</a> (200; 16.625477ms)
    Jan 29 03:03:01.622: INFO: (0) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">... (200; 16.220194ms)
    Jan 29 03:03:01.630: INFO: (0) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname1/proxy/: foo (200; 23.719806ms)
    Jan 29 03:03:01.630: INFO: (0) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname2/proxy/: bar (200; 23.912467ms)
    Jan 29 03:03:01.630: INFO: (0) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:462/proxy/: tls qux (200; 23.849427ms)
    Jan 29 03:03:01.630: INFO: (0) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname2/proxy/: tls qux (200; 24.240429ms)
    Jan 29 03:03:01.630: INFO: (0) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:460/proxy/: tls baz (200; 25.205176ms)
    Jan 29 03:03:01.630: INFO: (0) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname1/proxy/: tls baz (200; 23.076802ms)
    Jan 29 03:03:01.630: INFO: (0) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname2/proxy/: bar (200; 23.675326ms)
    Jan 29 03:03:01.631: INFO: (0) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/tlsrewritem... (200; 24.873394ms)
    Jan 29 03:03:01.639: INFO: (1) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:460/proxy/: tls baz (200; 7.940736ms)
    Jan 29 03:03:01.640: INFO: (1) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 8.152757ms)
    Jan 29 03:03:01.640: INFO: (1) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">test<... (200; 8.348479ms)
    Jan 29 03:03:01.640: INFO: (1) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 8.47996ms)
    Jan 29 03:03:01.640: INFO: (1) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/rewriteme">test</a> (200; 8.344498ms)
    Jan 29 03:03:01.641: INFO: (1) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 9.250765ms)
    Jan 29 03:03:01.641: INFO: (1) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">... (200; 9.138324ms)
    Jan 29 03:03:01.641: INFO: (1) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/tlsrewritem... (200; 9.552447ms)
    Jan 29 03:03:01.641: INFO: (1) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 9.794468ms)
    Jan 29 03:03:01.641: INFO: (1) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:462/proxy/: tls qux (200; 9.611747ms)
    Jan 29 03:03:01.646: INFO: (1) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname2/proxy/: tls qux (200; 14.587662ms)
    Jan 29 03:03:01.647: INFO: (1) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname2/proxy/: bar (200; 15.481448ms)
    Jan 29 03:03:01.649: INFO: (1) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname1/proxy/: foo (200; 16.743377ms)
    Jan 29 03:03:01.649: INFO: (1) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname2/proxy/: bar (200; 16.876977ms)
    Jan 29 03:03:01.649: INFO: (1) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname1/proxy/: tls baz (200; 17.09792ms)
    Jan 29 03:03:01.649: INFO: (1) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname1/proxy/: foo (200; 17.21546ms)
    Jan 29 03:03:01.658: INFO: (2) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/rewriteme">test</a> (200; 8.739981ms)
    Jan 29 03:03:01.658: INFO: (2) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:462/proxy/: tls qux (200; 8.814441ms)
    Jan 29 03:03:01.659: INFO: (2) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/tlsrewritem... (200; 10.067051ms)
    Jan 29 03:03:01.659: INFO: (2) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">test<... (200; 10.02861ms)
    Jan 29 03:03:01.659: INFO: (2) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname2/proxy/: bar (200; 10.375952ms)
    Jan 29 03:03:01.659: INFO: (2) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">... (200; 10.139031ms)
    Jan 29 03:03:01.659: INFO: (2) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 10.276172ms)
    Jan 29 03:03:01.659: INFO: (2) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:460/proxy/: tls baz (200; 10.07255ms)
    Jan 29 03:03:01.659: INFO: (2) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 10.13021ms)
    Jan 29 03:03:01.659: INFO: (2) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 10.490733ms)
    Jan 29 03:03:01.660: INFO: (2) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 10.558174ms)
    Jan 29 03:03:01.664: INFO: (2) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname1/proxy/: foo (200; 14.514161ms)
    Jan 29 03:03:01.664: INFO: (2) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname2/proxy/: bar (200; 15.413067ms)
    Jan 29 03:03:01.665: INFO: (2) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname2/proxy/: tls qux (200; 15.642529ms)
    Jan 29 03:03:01.665: INFO: (2) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname1/proxy/: tls baz (200; 15.694129ms)
    Jan 29 03:03:01.665: INFO: (2) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname1/proxy/: foo (200; 15.522108ms)
    Jan 29 03:03:01.671: INFO: (3) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/rewriteme">test</a> (200; 5.996341ms)
    Jan 29 03:03:01.671: INFO: (3) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:460/proxy/: tls baz (200; 6.572986ms)
    Jan 29 03:03:01.672: INFO: (3) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">test<... (200; 6.949488ms)
    Jan 29 03:03:01.672: INFO: (3) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 6.852348ms)
    Jan 29 03:03:01.672: INFO: (3) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">... (200; 7.14843ms)
    Jan 29 03:03:01.672: INFO: (3) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/tlsrewritem... (200; 7.323611ms)
    Jan 29 03:03:01.672: INFO: (3) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:462/proxy/: tls qux (200; 7.301131ms)
    Jan 29 03:03:01.672: INFO: (3) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 6.999749ms)
    Jan 29 03:03:01.672: INFO: (3) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 7.10943ms)
    Jan 29 03:03:01.673: INFO: (3) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 7.731894ms)
    Jan 29 03:03:01.678: INFO: (3) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname2/proxy/: bar (200; 13.615135ms)
    Jan 29 03:03:01.680: INFO: (3) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname1/proxy/: tls baz (200; 14.723102ms)
    Jan 29 03:03:01.680: INFO: (3) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname2/proxy/: tls qux (200; 15.323567ms)
    Jan 29 03:03:01.680: INFO: (3) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname1/proxy/: foo (200; 15.442308ms)
    Jan 29 03:03:01.680: INFO: (3) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname1/proxy/: foo (200; 15.226206ms)
    Jan 29 03:03:01.681: INFO: (3) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname2/proxy/: bar (200; 16.056472ms)
    Jan 29 03:03:01.690: INFO: (4) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:462/proxy/: tls qux (200; 8.62358ms)
    Jan 29 03:03:01.690: INFO: (4) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 9.052463ms)
    Jan 29 03:03:01.690: INFO: (4) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 9.078924ms)
    Jan 29 03:03:01.691: INFO: (4) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 10.216932ms)
    Jan 29 03:03:01.691: INFO: (4) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">test<... (200; 9.459026ms)
    Jan 29 03:03:01.691: INFO: (4) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">... (200; 9.721128ms)
    Jan 29 03:03:01.691: INFO: (4) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:460/proxy/: tls baz (200; 9.446086ms)
    Jan 29 03:03:01.691: INFO: (4) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 9.410425ms)
    Jan 29 03:03:01.691: INFO: (4) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/rewriteme">test</a> (200; 9.925549ms)
    Jan 29 03:03:01.692: INFO: (4) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/tlsrewritem... (200; 9.881209ms)
    Jan 29 03:03:01.693: INFO: (4) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname1/proxy/: tls baz (200; 11.738602ms)
    Jan 29 03:03:01.700: INFO: (4) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname1/proxy/: foo (200; 18.174787ms)
    Jan 29 03:03:01.700: INFO: (4) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname2/proxy/: bar (200; 18.158867ms)
    Jan 29 03:03:01.700: INFO: (4) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname1/proxy/: foo (200; 18.430229ms)
    Jan 29 03:03:01.700: INFO: (4) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname2/proxy/: bar (200; 17.939286ms)
    Jan 29 03:03:01.700: INFO: (4) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname2/proxy/: tls qux (200; 18.350568ms)
    Jan 29 03:03:01.710: INFO: (5) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 9.575147ms)
    Jan 29 03:03:01.710: INFO: (5) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/rewriteme">test</a> (200; 9.610847ms)
    Jan 29 03:03:01.710: INFO: (5) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:460/proxy/: tls baz (200; 9.776988ms)
    Jan 29 03:03:01.710: INFO: (5) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:462/proxy/: tls qux (200; 10.065151ms)
    Jan 29 03:03:01.710: INFO: (5) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">test<... (200; 10.01257ms)
    Jan 29 03:03:01.711: INFO: (5) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">... (200; 10.863336ms)
    Jan 29 03:03:01.711: INFO: (5) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 10.826995ms)
    Jan 29 03:03:01.711: INFO: (5) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 10.836395ms)
    Jan 29 03:03:01.711: INFO: (5) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/tlsrewritem... (200; 10.982956ms)
    Jan 29 03:03:01.713: INFO: (5) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 12.82501ms)
    Jan 29 03:03:01.714: INFO: (5) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname2/proxy/: tls qux (200; 13.669456ms)
    Jan 29 03:03:01.717: INFO: (5) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname1/proxy/: foo (200; 16.883798ms)
    Jan 29 03:03:01.718: INFO: (5) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname2/proxy/: bar (200; 17.560062ms)
    Jan 29 03:03:01.718: INFO: (5) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname2/proxy/: bar (200; 17.788984ms)
    Jan 29 03:03:01.718: INFO: (5) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname1/proxy/: tls baz (200; 17.676464ms)
    Jan 29 03:03:01.718: INFO: (5) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname1/proxy/: foo (200; 17.667904ms)
    Jan 29 03:03:01.725: INFO: (6) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 7.337352ms)
    Jan 29 03:03:01.727: INFO: (6) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 8.615081ms)
    Jan 29 03:03:01.727: INFO: (6) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:462/proxy/: tls qux (200; 8.989443ms)
    Jan 29 03:03:01.727: INFO: (6) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:460/proxy/: tls baz (200; 8.699781ms)
    Jan 29 03:03:01.727: INFO: (6) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 9.080423ms)
    Jan 29 03:03:01.727: INFO: (6) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/rewriteme">test</a> (200; 9.153364ms)
    Jan 29 03:03:01.727: INFO: (6) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/tlsrewritem... (200; 9.303105ms)
    Jan 29 03:03:01.727: INFO: (6) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">test<... (200; 8.5863ms)
    Jan 29 03:03:01.727: INFO: (6) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">... (200; 8.63324ms)
    Jan 29 03:03:01.727: INFO: (6) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 8.930122ms)
    Jan 29 03:03:01.731: INFO: (6) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname1/proxy/: foo (200; 12.655769ms)
    Jan 29 03:03:01.733: INFO: (6) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname2/proxy/: bar (200; 15.125746ms)
    Jan 29 03:03:01.734: INFO: (6) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname2/proxy/: bar (200; 16.297234ms)
    Jan 29 03:03:01.734: INFO: (6) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname1/proxy/: foo (200; 16.257233ms)
    Jan 29 03:03:01.735: INFO: (6) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname2/proxy/: tls qux (200; 16.751097ms)
    Jan 29 03:03:01.735: INFO: (6) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname1/proxy/: tls baz (200; 16.566795ms)
    Jan 29 03:03:01.741: INFO: (7) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/tlsrewritem... (200; 5.71432ms)
    Jan 29 03:03:01.742: INFO: (7) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 7.028149ms)
    Jan 29 03:03:01.742: INFO: (7) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 6.207523ms)
    Jan 29 03:03:01.743: INFO: (7) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">... (200; 7.705014ms)
    Jan 29 03:03:01.743: INFO: (7) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 7.13193ms)
    Jan 29 03:03:01.744: INFO: (7) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">test<... (200; 7.075809ms)
    Jan 29 03:03:01.744: INFO: (7) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:460/proxy/: tls baz (200; 8.225257ms)
    Jan 29 03:03:01.745: INFO: (7) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 7.821455ms)
    Jan 29 03:03:01.746: INFO: (7) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/rewriteme">test</a> (200; 9.179804ms)
    Jan 29 03:03:01.747: INFO: (7) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname2/proxy/: bar (200; 11.809263ms)
    Jan 29 03:03:01.747: INFO: (7) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname1/proxy/: foo (200; 12.071364ms)
    Jan 29 03:03:01.747: INFO: (7) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:462/proxy/: tls qux (200; 9.888989ms)
    Jan 29 03:03:01.747: INFO: (7) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname1/proxy/: foo (200; 11.780003ms)
    Jan 29 03:03:01.748: INFO: (7) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname1/proxy/: tls baz (200; 11.47948ms)
    Jan 29 03:03:01.749: INFO: (7) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname2/proxy/: tls qux (200; 12.025244ms)
    Jan 29 03:03:01.750: INFO: (7) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname2/proxy/: bar (200; 13.822056ms)
    Jan 29 03:03:01.758: INFO: (8) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/rewriteme">test</a> (200; 8.096216ms)
    Jan 29 03:03:01.758: INFO: (8) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 8.490479ms)
    Jan 29 03:03:01.759: INFO: (8) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 8.55298ms)
    Jan 29 03:03:01.759: INFO: (8) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 9.478026ms)
    Jan 29 03:03:01.761: INFO: (8) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 10.667655ms)
    Jan 29 03:03:01.761: INFO: (8) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">... (200; 10.854496ms)
    Jan 29 03:03:01.761: INFO: (8) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname2/proxy/: bar (200; 10.996337ms)
    Jan 29 03:03:01.762: INFO: (8) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">test<... (200; 11.44014ms)
    Jan 29 03:03:01.762: INFO: (8) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:460/proxy/: tls baz (200; 11.486881ms)
    Jan 29 03:03:01.763: INFO: (8) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname1/proxy/: foo (200; 12.756669ms)
    Jan 29 03:03:01.763: INFO: (8) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:462/proxy/: tls qux (200; 12.627408ms)
    Jan 29 03:03:01.763: INFO: (8) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/tlsrewritem... (200; 12.461367ms)
    Jan 29 03:03:01.764: INFO: (8) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname1/proxy/: foo (200; 14.075638ms)
    Jan 29 03:03:01.765: INFO: (8) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname2/proxy/: bar (200; 14.786903ms)
    Jan 29 03:03:01.766: INFO: (8) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname1/proxy/: tls baz (200; 16.169693ms)
    Jan 29 03:03:01.767: INFO: (8) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname2/proxy/: tls qux (200; 16.864358ms)
    Jan 29 03:03:01.774: INFO: (9) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">test<... (200; 7.067929ms)
    Jan 29 03:03:01.774: INFO: (9) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 7.272111ms)
    Jan 29 03:03:01.774: INFO: (9) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 7.229411ms)
    Jan 29 03:03:01.774: INFO: (9) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:462/proxy/: tls qux (200; 7.380672ms)
    Jan 29 03:03:01.776: INFO: (9) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/rewriteme">test</a> (200; 9.035563ms)
    Jan 29 03:03:01.777: INFO: (9) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">... (200; 8.817321ms)
    Jan 29 03:03:01.777: INFO: (9) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:460/proxy/: tls baz (200; 9.346446ms)
    Jan 29 03:03:01.777: INFO: (9) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 9.723988ms)
    Jan 29 03:03:01.777: INFO: (9) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/tlsrewritem... (200; 9.693708ms)
    Jan 29 03:03:01.778: INFO: (9) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname2/proxy/: bar (200; 10.465633ms)
    Jan 29 03:03:01.778: INFO: (9) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 10.602694ms)
    Jan 29 03:03:01.781: INFO: (9) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname2/proxy/: bar (200; 13.423014ms)
    Jan 29 03:03:01.781: INFO: (9) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname1/proxy/: tls baz (200; 13.764256ms)
    Jan 29 03:03:01.781: INFO: (9) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname2/proxy/: tls qux (200; 14.022518ms)
    Jan 29 03:03:01.783: INFO: (9) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname1/proxy/: foo (200; 15.636949ms)
    Jan 29 03:03:01.783: INFO: (9) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname1/proxy/: foo (200; 15.478708ms)
    Jan 29 03:03:01.790: INFO: (10) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/rewriteme">test</a> (200; 6.499705ms)
    Jan 29 03:03:01.791: INFO: (10) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 7.446492ms)
    Jan 29 03:03:01.793: INFO: (10) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 8.013296ms)
    Jan 29 03:03:01.793: INFO: (10) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:462/proxy/: tls qux (200; 8.535039ms)
    Jan 29 03:03:01.793: INFO: (10) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 8.709281ms)
    Jan 29 03:03:01.793: INFO: (10) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:460/proxy/: tls baz (200; 7.536813ms)
    Jan 29 03:03:01.798: INFO: (10) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">... (200; 12.958191ms)
    Jan 29 03:03:01.798: INFO: (10) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">test<... (200; 12.656628ms)
    Jan 29 03:03:01.798: INFO: (10) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/tlsrewritem... (200; 13.662316ms)
    Jan 29 03:03:01.798: INFO: (10) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 12.83007ms)
    Jan 29 03:03:01.799: INFO: (10) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname2/proxy/: tls qux (200; 15.82277ms)
    Jan 29 03:03:01.803: INFO: (10) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname1/proxy/: foo (200; 18.182507ms)
    Jan 29 03:03:01.804: INFO: (10) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname2/proxy/: bar (200; 19.051073ms)
    Jan 29 03:03:01.804: INFO: (10) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname1/proxy/: tls baz (200; 19.92082ms)
    Jan 29 03:03:01.804: INFO: (10) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname2/proxy/: bar (200; 19.404176ms)
    Jan 29 03:03:01.804: INFO: (10) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname1/proxy/: foo (200; 20.210681ms)
    Jan 29 03:03:01.812: INFO: (11) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">test<... (200; 7.637134ms)
    Jan 29 03:03:01.815: INFO: (11) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 9.743588ms)
    Jan 29 03:03:01.815: INFO: (11) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/tlsrewritem... (200; 10.315512ms)
    Jan 29 03:03:01.817: INFO: (11) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/rewriteme">test</a> (200; 12.215606ms)
    Jan 29 03:03:01.817: INFO: (11) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">... (200; 11.896443ms)
    Jan 29 03:03:01.818: INFO: (11) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:460/proxy/: tls baz (200; 13.037991ms)
    Jan 29 03:03:01.820: INFO: (11) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:462/proxy/: tls qux (200; 15.598629ms)
    Jan 29 03:03:01.820: INFO: (11) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname1/proxy/: tls baz (200; 15.956891ms)
    Jan 29 03:03:01.821: INFO: (11) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 16.719097ms)
    Jan 29 03:03:01.821: INFO: (11) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 16.760857ms)
    Jan 29 03:03:01.821: INFO: (11) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 16.601716ms)
    Jan 29 03:03:01.825: INFO: (11) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname1/proxy/: foo (200; 20.723345ms)
    Jan 29 03:03:01.826: INFO: (11) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname1/proxy/: foo (200; 21.311089ms)
    Jan 29 03:03:01.826: INFO: (11) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname2/proxy/: bar (200; 21.274908ms)
    Jan 29 03:03:01.826: INFO: (11) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname2/proxy/: bar (200; 21.166588ms)
    Jan 29 03:03:01.826: INFO: (11) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname2/proxy/: tls qux (200; 21.818572ms)
    Jan 29 03:03:01.837: INFO: (12) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/tlsrewritem... (200; 10.107231ms)
    Jan 29 03:03:01.837: INFO: (12) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 9.274725ms)
    Jan 29 03:03:01.837: INFO: (12) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">test<... (200; 9.464126ms)
    Jan 29 03:03:01.837: INFO: (12) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">... (200; 9.847949ms)
    Jan 29 03:03:01.838: INFO: (12) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 10.869336ms)
    Jan 29 03:03:01.838: INFO: (12) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/rewriteme">test</a> (200; 10.317012ms)
    Jan 29 03:03:01.838: INFO: (12) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 10.02327ms)
    Jan 29 03:03:01.840: INFO: (12) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 10.350393ms)
    Jan 29 03:03:01.840: INFO: (12) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:462/proxy/: tls qux (200; 10.997317ms)
    Jan 29 03:03:01.840: INFO: (12) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:460/proxy/: tls baz (200; 12.738569ms)
    Jan 29 03:03:01.843: INFO: (12) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname2/proxy/: tls qux (200; 15.128745ms)
    Jan 29 03:03:01.843: INFO: (12) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname2/proxy/: bar (200; 16.655516ms)
    Jan 29 03:03:01.844: INFO: (12) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname1/proxy/: foo (200; 16.157653ms)
    Jan 29 03:03:01.844: INFO: (12) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname2/proxy/: bar (200; 14.781163ms)
    Jan 29 03:03:01.844: INFO: (12) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname1/proxy/: tls baz (200; 16.450455ms)
    Jan 29 03:03:01.844: INFO: (12) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname1/proxy/: foo (200; 15.391547ms)
    Jan 29 03:03:01.854: INFO: (13) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/rewriteme">test</a> (200; 9.148824ms)
    Jan 29 03:03:01.856: INFO: (13) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">test<... (200; 11.638341ms)
    Jan 29 03:03:01.857: INFO: (13) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/tlsrewritem... (200; 11.959063ms)
    Jan 29 03:03:01.858: INFO: (13) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 13.779396ms)
    Jan 29 03:03:01.863: INFO: (13) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 17.875505ms)
    Jan 29 03:03:01.863: INFO: (13) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 18.318448ms)
    Jan 29 03:03:01.863: INFO: (13) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:460/proxy/: tls baz (200; 18.191607ms)
    Jan 29 03:03:01.863: INFO: (13) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">... (200; 18.300408ms)
    Jan 29 03:03:01.863: INFO: (13) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 18.234528ms)
    Jan 29 03:03:01.863: INFO: (13) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:462/proxy/: tls qux (200; 18.715071ms)
    Jan 29 03:03:01.863: INFO: (13) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname1/proxy/: foo (200; 18.51303ms)
    Jan 29 03:03:01.864: INFO: (13) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname1/proxy/: tls baz (200; 19.086853ms)
    Jan 29 03:03:01.864: INFO: (13) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname2/proxy/: bar (200; 18.523889ms)
    Jan 29 03:03:01.864: INFO: (13) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname2/proxy/: tls qux (200; 19.081054ms)
    Jan 29 03:03:01.864: INFO: (13) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname2/proxy/: bar (200; 18.864772ms)
    Jan 29 03:03:01.864: INFO: (13) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname1/proxy/: foo (200; 18.919292ms)
    Jan 29 03:03:01.872: INFO: (14) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 8.354619ms)
    Jan 29 03:03:01.875: INFO: (14) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/rewriteme">test</a> (200; 10.202491ms)
    Jan 29 03:03:01.875: INFO: (14) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:462/proxy/: tls qux (200; 9.842749ms)
    Jan 29 03:03:01.875: INFO: (14) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/tlsrewritem... (200; 9.037303ms)
    Jan 29 03:03:01.875: INFO: (14) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 10.122931ms)
    Jan 29 03:03:01.875: INFO: (14) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 9.306045ms)
    Jan 29 03:03:01.875: INFO: (14) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 8.031517ms)
    Jan 29 03:03:01.876: INFO: (14) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">test<... (200; 8.730741ms)
    Jan 29 03:03:01.876: INFO: (14) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">... (200; 9.239024ms)
    Jan 29 03:03:01.877: INFO: (14) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:460/proxy/: tls baz (200; 10.305272ms)
    Jan 29 03:03:01.877: INFO: (14) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname1/proxy/: foo (200; 11.629222ms)
    Jan 29 03:03:01.882: INFO: (14) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname2/proxy/: tls qux (200; 17.724104ms)
    Jan 29 03:03:01.883: INFO: (14) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname2/proxy/: bar (200; 16.712677ms)
    Jan 29 03:03:01.883: INFO: (14) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname1/proxy/: foo (200; 19.145614ms)
    Jan 29 03:03:01.883: INFO: (14) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname2/proxy/: bar (200; 17.374821ms)
    Jan 29 03:03:01.883: INFO: (14) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname1/proxy/: tls baz (200; 18.875892ms)
    Jan 29 03:03:01.893: INFO: (15) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 9.918049ms)
    Jan 29 03:03:01.894: INFO: (15) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/tlsrewritem... (200; 9.756648ms)
    Jan 29 03:03:01.894: INFO: (15) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 10.278271ms)
    Jan 29 03:03:01.894: INFO: (15) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/rewriteme">test</a> (200; 10.489773ms)
    Jan 29 03:03:01.895: INFO: (15) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:462/proxy/: tls qux (200; 10.915456ms)
    Jan 29 03:03:01.895: INFO: (15) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:460/proxy/: tls baz (200; 10.714595ms)
    Jan 29 03:03:01.895: INFO: (15) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">... (200; 10.898537ms)
    Jan 29 03:03:01.895: INFO: (15) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">test<... (200; 10.902336ms)
    Jan 29 03:03:01.895: INFO: (15) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 11.572701ms)
    Jan 29 03:03:01.895: INFO: (15) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 11.233918ms)
    Jan 29 03:03:01.896: INFO: (15) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname2/proxy/: bar (200; 12.180625ms)
    Jan 29 03:03:01.899: INFO: (15) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname1/proxy/: foo (200; 15.253287ms)
    Jan 29 03:03:01.899: INFO: (15) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname2/proxy/: tls qux (200; 15.301927ms)
    Jan 29 03:03:01.899: INFO: (15) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname1/proxy/: tls baz (200; 15.412688ms)
    Jan 29 03:03:01.899: INFO: (15) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname2/proxy/: bar (200; 15.581749ms)
    Jan 29 03:03:01.901: INFO: (15) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname1/proxy/: foo (200; 16.990099ms)
    Jan 29 03:03:01.908: INFO: (16) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">test<... (200; 6.726847ms)
    Jan 29 03:03:01.909: INFO: (16) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 7.585413ms)
    Jan 29 03:03:01.910: INFO: (16) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/rewriteme">test</a> (200; 8.860382ms)
    Jan 29 03:03:01.910: INFO: (16) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:462/proxy/: tls qux (200; 8.475399ms)
    Jan 29 03:03:01.911: INFO: (16) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 8.842142ms)
    Jan 29 03:03:01.911: INFO: (16) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/tlsrewritem... (200; 8.670741ms)
    Jan 29 03:03:01.912: INFO: (16) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">... (200; 8.58036ms)
    Jan 29 03:03:01.912: INFO: (16) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 8.211997ms)
    Jan 29 03:03:01.913: INFO: (16) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 8.694021ms)
    Jan 29 03:03:01.913: INFO: (16) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:460/proxy/: tls baz (200; 9.394565ms)
    Jan 29 03:03:01.913: INFO: (16) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname1/proxy/: tls baz (200; 11.927724ms)
    Jan 29 03:03:01.914: INFO: (16) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname2/proxy/: tls qux (200; 12.539327ms)
    Jan 29 03:03:01.919: INFO: (16) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname1/proxy/: foo (200; 16.574215ms)
    Jan 29 03:03:01.919: INFO: (16) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname1/proxy/: foo (200; 17.345522ms)
    Jan 29 03:03:01.919: INFO: (16) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname2/proxy/: bar (200; 15.930171ms)
    Jan 29 03:03:01.919: INFO: (16) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname2/proxy/: bar (200; 17.19214ms)
    Jan 29 03:03:01.928: INFO: (17) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/tlsrewritem... (200; 8.861282ms)
    Jan 29 03:03:01.930: INFO: (17) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 10.370492ms)
    Jan 29 03:03:01.931: INFO: (17) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/rewriteme">test</a> (200; 11.581741ms)
    Jan 29 03:03:01.932: INFO: (17) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:460/proxy/: tls baz (200; 11.957463ms)
    Jan 29 03:03:01.932: INFO: (17) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">... (200; 12.071325ms)
    Jan 29 03:03:01.932: INFO: (17) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 12.178005ms)
    Jan 29 03:03:01.932: INFO: (17) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">test<... (200; 12.363867ms)
    Jan 29 03:03:01.932: INFO: (17) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 12.416567ms)
    Jan 29 03:03:01.932: INFO: (17) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 12.385006ms)
    Jan 29 03:03:01.933: INFO: (17) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:462/proxy/: tls qux (200; 13.011751ms)
    Jan 29 03:03:01.933: INFO: (17) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname1/proxy/: tls baz (200; 13.074311ms)
    Jan 29 03:03:01.934: INFO: (17) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname1/proxy/: foo (200; 14.480481ms)
    Jan 29 03:03:01.936: INFO: (17) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname1/proxy/: foo (200; 15.944691ms)
    Jan 29 03:03:01.936: INFO: (17) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname2/proxy/: tls qux (200; 16.151933ms)
    Jan 29 03:03:01.936: INFO: (17) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname2/proxy/: bar (200; 16.195593ms)
    Jan 29 03:03:01.936: INFO: (17) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname2/proxy/: bar (200; 16.142633ms)
    Jan 29 03:03:01.944: INFO: (18) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:460/proxy/: tls baz (200; 7.278311ms)
    Jan 29 03:03:01.947: INFO: (18) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 9.536966ms)
    Jan 29 03:03:01.947: INFO: (18) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 9.933689ms)
    Jan 29 03:03:01.947: INFO: (18) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">... (200; 10.08701ms)
    Jan 29 03:03:01.948: INFO: (18) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 10.695275ms)
    Jan 29 03:03:01.949: INFO: (18) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">test<... (200; 12.083544ms)
    Jan 29 03:03:01.950: INFO: (18) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname2/proxy/: bar (200; 14.162239ms)
    Jan 29 03:03:01.951: INFO: (18) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 13.548335ms)
    Jan 29 03:03:01.951: INFO: (18) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/tlsrewritem... (200; 13.447194ms)
    Jan 29 03:03:01.951: INFO: (18) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname2/proxy/: bar (200; 15.041165ms)
    Jan 29 03:03:01.952: INFO: (18) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:462/proxy/: tls qux (200; 13.994058ms)
    Jan 29 03:03:01.952: INFO: (18) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/rewriteme">test</a> (200; 15.062445ms)
    Jan 29 03:03:01.953: INFO: (18) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname1/proxy/: tls baz (200; 15.267067ms)
    Jan 29 03:03:01.953: INFO: (18) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname1/proxy/: foo (200; 14.917924ms)
    Jan 29 03:03:01.953: INFO: (18) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname1/proxy/: foo (200; 15.457988ms)
    Jan 29 03:03:01.954: INFO: (18) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname2/proxy/: tls qux (200; 16.390014ms)
    Jan 29 03:03:01.963: INFO: (19) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 9.422486ms)
    Jan 29 03:03:01.964: INFO: (19) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 10.218491ms)
    Jan 29 03:03:01.964: INFO: (19) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">... (200; 9.720708ms)
    Jan 29 03:03:01.965: INFO: (19) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk:1080/proxy/rewriteme">test<... (200; 11.021057ms)
    Jan 29 03:03:01.965: INFO: (19) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:462/proxy/: tls qux (200; 10.943057ms)
    Jan 29 03:03:01.966: INFO: (19) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:160/proxy/: foo (200; 11.44464ms)
    Jan 29 03:03:01.968: INFO: (19) /api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/proxy-service-hgvdg-mx7vk/proxy/rewriteme">test</a> (200; 14.042619ms)
    Jan 29 03:03:01.968: INFO: (19) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:460/proxy/: tls baz (200; 14.437981ms)
    Jan 29 03:03:01.968: INFO: (19) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname1/proxy/: foo (200; 14.23606ms)
    Jan 29 03:03:01.968: INFO: (19) /api/v1/namespaces/proxy-7408/pods/http:proxy-service-hgvdg-mx7vk:162/proxy/: bar (200; 14.052978ms)
    Jan 29 03:03:01.968: INFO: (19) /api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/: <a href="/api/v1/namespaces/proxy-7408/pods/https:proxy-service-hgvdg-mx7vk:443/proxy/tlsrewritem... (200; 13.879937ms)
    Jan 29 03:03:01.971: INFO: (19) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname1/proxy/: tls baz (200; 17.25428ms)
    Jan 29 03:03:01.972: INFO: (19) /api/v1/namespaces/proxy-7408/services/http:proxy-service-hgvdg:portname2/proxy/: bar (200; 17.576523ms)
    Jan 29 03:03:01.972: INFO: (19) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname1/proxy/: foo (200; 17.792644ms)
    Jan 29 03:03:01.972: INFO: (19) /api/v1/namespaces/proxy-7408/services/https:proxy-service-hgvdg:tlsportname2/proxy/: tls qux (200; 17.947506ms)
    Jan 29 03:03:01.972: INFO: (19) /api/v1/namespaces/proxy-7408/services/proxy-service-hgvdg:portname2/proxy/: bar (200; 17.833105ms)
    STEP: deleting ReplicationController proxy-service-hgvdg in namespace proxy-7408, will wait for the garbage collector to delete the pods 01/29/23 03:03:01.972
    Jan 29 03:03:02.042: INFO: Deleting ReplicationController proxy-service-hgvdg took: 13.193092ms
    Jan 29 03:03:02.143: INFO: Terminating ReplicationController proxy-service-hgvdg pods took: 101.011926ms
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Jan 29 03:03:04.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-7408" for this suite. 01/29/23 03:03:04.156
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:03:04.171
Jan 29 03:03:04.171: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename container-probe 01/29/23 03:03:04.172
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:03:04.198
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:03:04.205
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
STEP: Creating pod test-webserver-85e68574-ecc6-4c04-8868-aaff90dbb127 in namespace container-probe-6487 01/29/23 03:03:04.211
Jan 29 03:03:04.229: INFO: Waiting up to 5m0s for pod "test-webserver-85e68574-ecc6-4c04-8868-aaff90dbb127" in namespace "container-probe-6487" to be "not pending"
Jan 29 03:03:04.236: INFO: Pod "test-webserver-85e68574-ecc6-4c04-8868-aaff90dbb127": Phase="Pending", Reason="", readiness=false. Elapsed: 6.564426ms
Jan 29 03:03:06.243: INFO: Pod "test-webserver-85e68574-ecc6-4c04-8868-aaff90dbb127": Phase="Running", Reason="", readiness=true. Elapsed: 2.01357733s
Jan 29 03:03:06.243: INFO: Pod "test-webserver-85e68574-ecc6-4c04-8868-aaff90dbb127" satisfied condition "not pending"
Jan 29 03:03:06.243: INFO: Started pod test-webserver-85e68574-ecc6-4c04-8868-aaff90dbb127 in namespace container-probe-6487
STEP: checking the pod's current state and verifying that restartCount is present 01/29/23 03:03:06.243
Jan 29 03:03:06.249: INFO: Initial restart count of pod test-webserver-85e68574-ecc6-4c04-8868-aaff90dbb127 is 0
STEP: deleting the pod 01/29/23 03:07:07.229
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan 29 03:07:07.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6487" for this suite. 01/29/23 03:07:07.342
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":42,"skipped":611,"failed":0}
------------------------------
• [SLOW TEST] [243.183 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:03:04.171
    Jan 29 03:03:04.171: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename container-probe 01/29/23 03:03:04.172
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:03:04.198
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:03:04.205
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:211
    STEP: Creating pod test-webserver-85e68574-ecc6-4c04-8868-aaff90dbb127 in namespace container-probe-6487 01/29/23 03:03:04.211
    Jan 29 03:03:04.229: INFO: Waiting up to 5m0s for pod "test-webserver-85e68574-ecc6-4c04-8868-aaff90dbb127" in namespace "container-probe-6487" to be "not pending"
    Jan 29 03:03:04.236: INFO: Pod "test-webserver-85e68574-ecc6-4c04-8868-aaff90dbb127": Phase="Pending", Reason="", readiness=false. Elapsed: 6.564426ms
    Jan 29 03:03:06.243: INFO: Pod "test-webserver-85e68574-ecc6-4c04-8868-aaff90dbb127": Phase="Running", Reason="", readiness=true. Elapsed: 2.01357733s
    Jan 29 03:03:06.243: INFO: Pod "test-webserver-85e68574-ecc6-4c04-8868-aaff90dbb127" satisfied condition "not pending"
    Jan 29 03:03:06.243: INFO: Started pod test-webserver-85e68574-ecc6-4c04-8868-aaff90dbb127 in namespace container-probe-6487
    STEP: checking the pod's current state and verifying that restartCount is present 01/29/23 03:03:06.243
    Jan 29 03:03:06.249: INFO: Initial restart count of pod test-webserver-85e68574-ecc6-4c04-8868-aaff90dbb127 is 0
    STEP: deleting the pod 01/29/23 03:07:07.229
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan 29 03:07:07.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-6487" for this suite. 01/29/23 03:07:07.342
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:07:07.357
Jan 29 03:07:07.357: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename security-context 01/29/23 03:07:07.358
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:07:07.406
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:07:07.411
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/29/23 03:07:07.417
Jan 29 03:07:07.435: INFO: Waiting up to 5m0s for pod "security-context-6a007a24-ba2d-4739-80ff-8468e2d3a63e" in namespace "security-context-3539" to be "Succeeded or Failed"
Jan 29 03:07:07.443: INFO: Pod "security-context-6a007a24-ba2d-4739-80ff-8468e2d3a63e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.130737ms
Jan 29 03:07:09.451: INFO: Pod "security-context-6a007a24-ba2d-4739-80ff-8468e2d3a63e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016120847s
Jan 29 03:07:11.452: INFO: Pod "security-context-6a007a24-ba2d-4739-80ff-8468e2d3a63e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016788485s
STEP: Saw pod success 01/29/23 03:07:11.452
Jan 29 03:07:11.452: INFO: Pod "security-context-6a007a24-ba2d-4739-80ff-8468e2d3a63e" satisfied condition "Succeeded or Failed"
Jan 29 03:07:11.458: INFO: Trying to get logs from node slave2 pod security-context-6a007a24-ba2d-4739-80ff-8468e2d3a63e container test-container: <nil>
STEP: delete the pod 01/29/23 03:07:11.488
Jan 29 03:07:11.579: INFO: Waiting for pod security-context-6a007a24-ba2d-4739-80ff-8468e2d3a63e to disappear
Jan 29 03:07:11.586: INFO: Pod security-context-6a007a24-ba2d-4739-80ff-8468e2d3a63e no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jan 29 03:07:11.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-3539" for this suite. 01/29/23 03:07:11.595
{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":43,"skipped":653,"failed":0}
------------------------------
• [4.249 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:07:07.357
    Jan 29 03:07:07.357: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename security-context 01/29/23 03:07:07.358
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:07:07.406
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:07:07.411
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:132
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/29/23 03:07:07.417
    Jan 29 03:07:07.435: INFO: Waiting up to 5m0s for pod "security-context-6a007a24-ba2d-4739-80ff-8468e2d3a63e" in namespace "security-context-3539" to be "Succeeded or Failed"
    Jan 29 03:07:07.443: INFO: Pod "security-context-6a007a24-ba2d-4739-80ff-8468e2d3a63e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.130737ms
    Jan 29 03:07:09.451: INFO: Pod "security-context-6a007a24-ba2d-4739-80ff-8468e2d3a63e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016120847s
    Jan 29 03:07:11.452: INFO: Pod "security-context-6a007a24-ba2d-4739-80ff-8468e2d3a63e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016788485s
    STEP: Saw pod success 01/29/23 03:07:11.452
    Jan 29 03:07:11.452: INFO: Pod "security-context-6a007a24-ba2d-4739-80ff-8468e2d3a63e" satisfied condition "Succeeded or Failed"
    Jan 29 03:07:11.458: INFO: Trying to get logs from node slave2 pod security-context-6a007a24-ba2d-4739-80ff-8468e2d3a63e container test-container: <nil>
    STEP: delete the pod 01/29/23 03:07:11.488
    Jan 29 03:07:11.579: INFO: Waiting for pod security-context-6a007a24-ba2d-4739-80ff-8468e2d3a63e to disappear
    Jan 29 03:07:11.586: INFO: Pod security-context-6a007a24-ba2d-4739-80ff-8468e2d3a63e no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jan 29 03:07:11.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-3539" for this suite. 01/29/23 03:07:11.595
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:07:11.608
Jan 29 03:07:11.608: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename gc 01/29/23 03:07:11.61
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:07:11.637
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:07:11.643
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 01/29/23 03:07:11.659
STEP: delete the rc 01/29/23 03:07:16.774
STEP: wait for the rc to be deleted 01/29/23 03:07:16.962
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 01/29/23 03:07:22.006
STEP: Gathering metrics 01/29/23 03:07:52.028
Jan 29 03:07:52.081: INFO: Waiting up to 5m0s for pod "kube-controller-manager-master3" in namespace "kube-system" to be "running and ready"
Jan 29 03:07:52.087: INFO: Pod "kube-controller-manager-master3": Phase="Running", Reason="", readiness=true. Elapsed: 5.860501ms
Jan 29 03:07:52.087: INFO: The phase of Pod kube-controller-manager-master3 is Running (Ready = true)
Jan 29 03:07:52.087: INFO: Pod "kube-controller-manager-master3" satisfied condition "running and ready"
E0129 03:07:53.254898      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:07:53.254898      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:07:54.344000      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:07:54.344000      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:07:55.434540      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:07:55.434540      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:07:56.512562      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:07:56.512562      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:07:57.630192      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:07:57.630192      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:07:58.710261      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:07:58.710261      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:00.047535      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:00.047535      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:01.131550      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:01.131550      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:02.222315      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:02.222315      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:03.306661      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:03.306661      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:04.899685      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:04.899685      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:05.982881      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:05.982881      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:07.080193      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:07.080193      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:08.162508      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:08.162508      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:09.246111      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:09.246111      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:10.330832      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:10.330832      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:11.416015      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:11.416015      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:12.916279      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:12.916279      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:14.009075      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:14.009075      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:15.128648      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:15.128648      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:16.220354      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:16.220354      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:16.396355      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:16.396355      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:17.482069      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:17.482069      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:18.566685      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:18.566685      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:19.651893      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:19.651893      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:20.734522      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:20.734522      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:21.850529      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:21.850529      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:22.951101      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:22.951101      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:24.035206      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:24.035206      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:25.130827      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:25.130827      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:26.232714      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:26.232714      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:27.313791      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:27.313791      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:28.387052      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:28.387052      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:29.474879      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:29.474879      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:30.555918      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:30.555918      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:31.636517      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:31.636517      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:32.717168      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:32.717168      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:33.822242      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:33.822242      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:34.962818      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:34.962818      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:36.048361      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:36.048361      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:37.152508      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:37.152508      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:38.250849      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:38.250849      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:40.386968      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:40.386968      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:41.473091      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:41.473091      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:42.559670      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:42.559670      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:43.642768      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:43.642768      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:44.721014      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:44.721014      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:45.814245      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:45.814245      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:47.995007      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:47.995007      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:49.080443      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:49.080443      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:50.164636      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:50.164636      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:51.257369      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:51.257369      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:51.917699      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:51.917699      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:53.013124      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:53.013124      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:54.118737      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:54.118737      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:55.203086      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:55.203086      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:56.284793      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:56.284793      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:57.370690      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:57.370690      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:58.450055      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:58.450055      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:59.528499      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:08:59.528499      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:09:00.610619      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:09:00.610619      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:09:01.717417      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:09:01.717417      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:09:02.798135      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:09:02.798135      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:09:03.392299      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:09:03.392299      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:09:05.950641      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:09:05.950641      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:09:07.097312      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:09:07.097312      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:09:08.177583      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:09:08.177583      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:09:09.261107      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:09:09.261107      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:09:10.345875      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:09:10.345875      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:09:11.436025      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:09:11.436025      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:09:12.518734      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:09:12.518734      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:09:13.617378      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:09:13.617378      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:09:14.702808      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:09:14.702808      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:09:14.780885      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:09:14.780885      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:09:15.867747      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:09:15.867747      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:09:16.967305      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:09:16.967305      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:09:18.056806      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:09:18.056806      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:09:19.149344      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:09:19.149344      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:09:21.343869      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:09:21.343869      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:09:22.434635      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:09:22.434635      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:09:23.518540      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:09:23.518540      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:09:45.440957      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:09:45.440957      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:09:46.531171      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:09:46.531171      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
Jan 29 03:09:46.531: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
Jan 29 03:09:46.531: INFO: Deleting pod "simpletest.rc-22gzf" in namespace "gc-7865"
Jan 29 03:09:46.645: INFO: Deleting pod "simpletest.rc-2b5rb" in namespace "gc-7865"
Jan 29 03:09:46.708: INFO: Deleting pod "simpletest.rc-2bpcd" in namespace "gc-7865"
Jan 29 03:09:46.797: INFO: Deleting pod "simpletest.rc-2jdd5" in namespace "gc-7865"
Jan 29 03:09:46.888: INFO: Deleting pod "simpletest.rc-2vnz4" in namespace "gc-7865"
Jan 29 03:09:46.996: INFO: Deleting pod "simpletest.rc-42j4q" in namespace "gc-7865"
Jan 29 03:09:47.145: INFO: Deleting pod "simpletest.rc-4hz6c" in namespace "gc-7865"
Jan 29 03:09:47.237: INFO: Deleting pod "simpletest.rc-4jjfk" in namespace "gc-7865"
Jan 29 03:09:47.345: INFO: Deleting pod "simpletest.rc-4s6ln" in namespace "gc-7865"
Jan 29 03:09:47.512: INFO: Deleting pod "simpletest.rc-52fg6" in namespace "gc-7865"
Jan 29 03:09:47.678: INFO: Deleting pod "simpletest.rc-5fcr4" in namespace "gc-7865"
Jan 29 03:09:47.940: INFO: Deleting pod "simpletest.rc-5hwzf" in namespace "gc-7865"
Jan 29 03:09:48.034: INFO: Deleting pod "simpletest.rc-5nkbj" in namespace "gc-7865"
Jan 29 03:09:48.178: INFO: Deleting pod "simpletest.rc-6n59b" in namespace "gc-7865"
Jan 29 03:09:48.324: INFO: Deleting pod "simpletest.rc-6nvcv" in namespace "gc-7865"
Jan 29 03:09:48.448: INFO: Deleting pod "simpletest.rc-7kj4j" in namespace "gc-7865"
Jan 29 03:09:48.633: INFO: Deleting pod "simpletest.rc-7qhwr" in namespace "gc-7865"
Jan 29 03:09:48.770: INFO: Deleting pod "simpletest.rc-7w9cw" in namespace "gc-7865"
Jan 29 03:09:48.853: INFO: Deleting pod "simpletest.rc-84mv2" in namespace "gc-7865"
Jan 29 03:09:49.075: INFO: Deleting pod "simpletest.rc-89hwr" in namespace "gc-7865"
Jan 29 03:09:49.283: INFO: Deleting pod "simpletest.rc-8fgbt" in namespace "gc-7865"
Jan 29 03:09:49.480: INFO: Deleting pod "simpletest.rc-8h6nz" in namespace "gc-7865"
Jan 29 03:09:49.627: INFO: Deleting pod "simpletest.rc-8hnbp" in namespace "gc-7865"
Jan 29 03:09:49.742: INFO: Deleting pod "simpletest.rc-8qjw7" in namespace "gc-7865"
Jan 29 03:09:49.872: INFO: Deleting pod "simpletest.rc-8zf8b" in namespace "gc-7865"
Jan 29 03:09:50.034: INFO: Deleting pod "simpletest.rc-9v6rn" in namespace "gc-7865"
Jan 29 03:09:50.193: INFO: Deleting pod "simpletest.rc-9wrdm" in namespace "gc-7865"
Jan 29 03:09:50.350: INFO: Deleting pod "simpletest.rc-b7hsn" in namespace "gc-7865"
Jan 29 03:09:50.528: INFO: Deleting pod "simpletest.rc-bzjv4" in namespace "gc-7865"
Jan 29 03:09:50.652: INFO: Deleting pod "simpletest.rc-cdnlh" in namespace "gc-7865"
Jan 29 03:09:50.825: INFO: Deleting pod "simpletest.rc-cftpn" in namespace "gc-7865"
Jan 29 03:09:51.101: INFO: Deleting pod "simpletest.rc-cnr8q" in namespace "gc-7865"
Jan 29 03:09:51.224: INFO: Deleting pod "simpletest.rc-dbks6" in namespace "gc-7865"
Jan 29 03:09:51.371: INFO: Deleting pod "simpletest.rc-ddr2b" in namespace "gc-7865"
Jan 29 03:09:51.556: INFO: Deleting pod "simpletest.rc-dthlg" in namespace "gc-7865"
Jan 29 03:09:51.738: INFO: Deleting pod "simpletest.rc-dxx7q" in namespace "gc-7865"
Jan 29 03:09:51.890: INFO: Deleting pod "simpletest.rc-dzqpp" in namespace "gc-7865"
Jan 29 03:09:52.054: INFO: Deleting pod "simpletest.rc-f2zmc" in namespace "gc-7865"
Jan 29 03:09:52.200: INFO: Deleting pod "simpletest.rc-fqzc8" in namespace "gc-7865"
Jan 29 03:09:52.352: INFO: Deleting pod "simpletest.rc-ftcws" in namespace "gc-7865"
Jan 29 03:09:52.636: INFO: Deleting pod "simpletest.rc-fx8r2" in namespace "gc-7865"
Jan 29 03:09:52.754: INFO: Deleting pod "simpletest.rc-g56z9" in namespace "gc-7865"
Jan 29 03:09:52.847: INFO: Deleting pod "simpletest.rc-g65dt" in namespace "gc-7865"
Jan 29 03:09:52.996: INFO: Deleting pod "simpletest.rc-gxsms" in namespace "gc-7865"
Jan 29 03:09:53.114: INFO: Deleting pod "simpletest.rc-h8d5n" in namespace "gc-7865"
Jan 29 03:09:53.276: INFO: Deleting pod "simpletest.rc-hlcwv" in namespace "gc-7865"
Jan 29 03:09:53.391: INFO: Deleting pod "simpletest.rc-hn2kk" in namespace "gc-7865"
Jan 29 03:09:53.560: INFO: Deleting pod "simpletest.rc-hnqt6" in namespace "gc-7865"
Jan 29 03:09:53.742: INFO: Deleting pod "simpletest.rc-hq9bf" in namespace "gc-7865"
Jan 29 03:09:53.845: INFO: Deleting pod "simpletest.rc-j5x8j" in namespace "gc-7865"
Jan 29 03:09:53.986: INFO: Deleting pod "simpletest.rc-jfvbg" in namespace "gc-7865"
Jan 29 03:09:54.168: INFO: Deleting pod "simpletest.rc-jqbrd" in namespace "gc-7865"
Jan 29 03:09:54.306: INFO: Deleting pod "simpletest.rc-jqxjd" in namespace "gc-7865"
Jan 29 03:09:54.418: INFO: Deleting pod "simpletest.rc-k8g46" in namespace "gc-7865"
Jan 29 03:09:54.545: INFO: Deleting pod "simpletest.rc-knvkx" in namespace "gc-7865"
Jan 29 03:09:54.634: INFO: Deleting pod "simpletest.rc-kwx9h" in namespace "gc-7865"
Jan 29 03:09:54.813: INFO: Deleting pod "simpletest.rc-l7tn7" in namespace "gc-7865"
Jan 29 03:09:54.898: INFO: Deleting pod "simpletest.rc-ln6b4" in namespace "gc-7865"
Jan 29 03:09:55.072: INFO: Deleting pod "simpletest.rc-lx982" in namespace "gc-7865"
Jan 29 03:09:55.179: INFO: Deleting pod "simpletest.rc-m6pxf" in namespace "gc-7865"
Jan 29 03:09:55.357: INFO: Deleting pod "simpletest.rc-mpdt7" in namespace "gc-7865"
Jan 29 03:09:55.448: INFO: Deleting pod "simpletest.rc-mql9g" in namespace "gc-7865"
Jan 29 03:09:55.668: INFO: Deleting pod "simpletest.rc-mskzl" in namespace "gc-7865"
Jan 29 03:09:55.807: INFO: Deleting pod "simpletest.rc-nbc7f" in namespace "gc-7865"
Jan 29 03:09:55.923: INFO: Deleting pod "simpletest.rc-nbdqh" in namespace "gc-7865"
Jan 29 03:09:56.062: INFO: Deleting pod "simpletest.rc-nmlbv" in namespace "gc-7865"
Jan 29 03:09:56.207: INFO: Deleting pod "simpletest.rc-ntzzb" in namespace "gc-7865"
Jan 29 03:09:56.410: INFO: Deleting pod "simpletest.rc-nvngc" in namespace "gc-7865"
Jan 29 03:09:56.658: INFO: Deleting pod "simpletest.rc-p8lch" in namespace "gc-7865"
Jan 29 03:09:56.766: INFO: Deleting pod "simpletest.rc-pjlrs" in namespace "gc-7865"
Jan 29 03:09:57.048: INFO: Deleting pod "simpletest.rc-prq8k" in namespace "gc-7865"
Jan 29 03:09:57.261: INFO: Deleting pod "simpletest.rc-pvdnj" in namespace "gc-7865"
Jan 29 03:09:57.418: INFO: Deleting pod "simpletest.rc-pxfbp" in namespace "gc-7865"
Jan 29 03:09:57.585: INFO: Deleting pod "simpletest.rc-pzxlc" in namespace "gc-7865"
Jan 29 03:09:57.698: INFO: Deleting pod "simpletest.rc-q8jpm" in namespace "gc-7865"
Jan 29 03:09:57.806: INFO: Deleting pod "simpletest.rc-qzk54" in namespace "gc-7865"
Jan 29 03:09:57.962: INFO: Deleting pod "simpletest.rc-r5ngc" in namespace "gc-7865"
Jan 29 03:09:58.085: INFO: Deleting pod "simpletest.rc-rgqck" in namespace "gc-7865"
Jan 29 03:09:58.245: INFO: Deleting pod "simpletest.rc-rhq2c" in namespace "gc-7865"
Jan 29 03:09:58.396: INFO: Deleting pod "simpletest.rc-rtrzw" in namespace "gc-7865"
Jan 29 03:09:58.569: INFO: Deleting pod "simpletest.rc-sc9zx" in namespace "gc-7865"
Jan 29 03:09:58.717: INFO: Deleting pod "simpletest.rc-scgvj" in namespace "gc-7865"
Jan 29 03:09:58.840: INFO: Deleting pod "simpletest.rc-sjqzj" in namespace "gc-7865"
Jan 29 03:09:58.938: INFO: Deleting pod "simpletest.rc-sngx2" in namespace "gc-7865"
Jan 29 03:09:59.091: INFO: Deleting pod "simpletest.rc-ss7k2" in namespace "gc-7865"
Jan 29 03:09:59.182: INFO: Deleting pod "simpletest.rc-swzxk" in namespace "gc-7865"
Jan 29 03:09:59.278: INFO: Deleting pod "simpletest.rc-sz8xz" in namespace "gc-7865"
Jan 29 03:09:59.458: INFO: Deleting pod "simpletest.rc-tdrgn" in namespace "gc-7865"
Jan 29 03:09:59.559: INFO: Deleting pod "simpletest.rc-v9x5v" in namespace "gc-7865"
Jan 29 03:09:59.812: INFO: Deleting pod "simpletest.rc-vtcbd" in namespace "gc-7865"
Jan 29 03:09:59.992: INFO: Deleting pod "simpletest.rc-vz9pr" in namespace "gc-7865"
Jan 29 03:10:00.212: INFO: Deleting pod "simpletest.rc-w66k4" in namespace "gc-7865"
Jan 29 03:10:00.317: INFO: Deleting pod "simpletest.rc-w6m4g" in namespace "gc-7865"
Jan 29 03:10:00.411: INFO: Deleting pod "simpletest.rc-wmhr6" in namespace "gc-7865"
Jan 29 03:10:00.554: INFO: Deleting pod "simpletest.rc-xsmp4" in namespace "gc-7865"
Jan 29 03:10:00.692: INFO: Deleting pod "simpletest.rc-xvq9j" in namespace "gc-7865"
Jan 29 03:10:00.772: INFO: Deleting pod "simpletest.rc-z567w" in namespace "gc-7865"
Jan 29 03:10:00.895: INFO: Deleting pod "simpletest.rc-z6k76" in namespace "gc-7865"
Jan 29 03:10:01.011: INFO: Deleting pod "simpletest.rc-z8vm8" in namespace "gc-7865"
Jan 29 03:10:01.121: INFO: Deleting pod "simpletest.rc-zj5fw" in namespace "gc-7865"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan 29 03:10:01.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7865" for this suite. 01/29/23 03:10:01.274
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","completed":44,"skipped":682,"failed":0}
------------------------------
• [SLOW TEST] [169.703 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:07:11.608
    Jan 29 03:07:11.608: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename gc 01/29/23 03:07:11.61
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:07:11.637
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:07:11.643
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 01/29/23 03:07:11.659
    STEP: delete the rc 01/29/23 03:07:16.774
    STEP: wait for the rc to be deleted 01/29/23 03:07:16.962
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 01/29/23 03:07:22.006
    STEP: Gathering metrics 01/29/23 03:07:52.028
    Jan 29 03:07:52.081: INFO: Waiting up to 5m0s for pod "kube-controller-manager-master3" in namespace "kube-system" to be "running and ready"
    Jan 29 03:07:52.087: INFO: Pod "kube-controller-manager-master3": Phase="Running", Reason="", readiness=true. Elapsed: 5.860501ms
    Jan 29 03:07:52.087: INFO: The phase of Pod kube-controller-manager-master3 is Running (Ready = true)
    Jan 29 03:07:52.087: INFO: Pod "kube-controller-manager-master3" satisfied condition "running and ready"
    E0129 03:07:53.254898      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:07:54.344000      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:07:55.434540      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:07:56.512562      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:07:57.630192      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:07:58.710261      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:08:00.047535      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:08:01.131550      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:08:02.222315      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:08:03.306661      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:08:04.899685      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:08:05.982881      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:08:07.080193      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:08:08.162508      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:08:09.246111      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:08:10.330832      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:08:11.416015      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:08:12.916279      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:08:14.009075      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:08:15.128648      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:08:16.220354      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:08:16.396355      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:08:17.482069      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:08:18.566685      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:08:19.651893      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:08:20.734522      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:08:21.850529      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:08:22.951101      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:08:24.035206      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:08:25.130827      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:08:26.232714      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:08:27.313791      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:08:28.387052      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:08:29.474879      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:08:30.555918      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:08:31.636517      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:08:32.717168      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:08:33.822242      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:08:34.962818      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:08:36.048361      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:08:37.152508      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:08:38.250849      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:08:40.386968      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:08:41.473091      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:08:42.559670      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:08:43.642768      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:08:44.721014      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:08:45.814245      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:08:47.995007      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:08:49.080443      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:08:50.164636      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:08:51.257369      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:08:51.917699      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:08:53.013124      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:08:54.118737      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:08:55.203086      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:08:56.284793      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:08:57.370690      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:08:58.450055      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:08:59.528499      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:09:00.610619      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:09:01.717417      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:09:02.798135      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:09:03.392299      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:09:05.950641      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:09:07.097312      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:09:08.177583      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:09:09.261107      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:09:10.345875      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:09:11.436025      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:09:12.518734      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:09:13.617378      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:09:14.702808      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:09:14.780885      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:09:15.867747      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:09:16.967305      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:09:18.056806      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:09:19.149344      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:09:21.343869      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:09:22.434635      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:09:23.518540      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:09:45.440957      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:09:46.531171      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    Jan 29 03:09:46.531: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
    Jan 29 03:09:46.531: INFO: Deleting pod "simpletest.rc-22gzf" in namespace "gc-7865"
    Jan 29 03:09:46.645: INFO: Deleting pod "simpletest.rc-2b5rb" in namespace "gc-7865"
    Jan 29 03:09:46.708: INFO: Deleting pod "simpletest.rc-2bpcd" in namespace "gc-7865"
    Jan 29 03:09:46.797: INFO: Deleting pod "simpletest.rc-2jdd5" in namespace "gc-7865"
    Jan 29 03:09:46.888: INFO: Deleting pod "simpletest.rc-2vnz4" in namespace "gc-7865"
    Jan 29 03:09:46.996: INFO: Deleting pod "simpletest.rc-42j4q" in namespace "gc-7865"
    Jan 29 03:09:47.145: INFO: Deleting pod "simpletest.rc-4hz6c" in namespace "gc-7865"
    Jan 29 03:09:47.237: INFO: Deleting pod "simpletest.rc-4jjfk" in namespace "gc-7865"
    Jan 29 03:09:47.345: INFO: Deleting pod "simpletest.rc-4s6ln" in namespace "gc-7865"
    Jan 29 03:09:47.512: INFO: Deleting pod "simpletest.rc-52fg6" in namespace "gc-7865"
    Jan 29 03:09:47.678: INFO: Deleting pod "simpletest.rc-5fcr4" in namespace "gc-7865"
    Jan 29 03:09:47.940: INFO: Deleting pod "simpletest.rc-5hwzf" in namespace "gc-7865"
    Jan 29 03:09:48.034: INFO: Deleting pod "simpletest.rc-5nkbj" in namespace "gc-7865"
    Jan 29 03:09:48.178: INFO: Deleting pod "simpletest.rc-6n59b" in namespace "gc-7865"
    Jan 29 03:09:48.324: INFO: Deleting pod "simpletest.rc-6nvcv" in namespace "gc-7865"
    Jan 29 03:09:48.448: INFO: Deleting pod "simpletest.rc-7kj4j" in namespace "gc-7865"
    Jan 29 03:09:48.633: INFO: Deleting pod "simpletest.rc-7qhwr" in namespace "gc-7865"
    Jan 29 03:09:48.770: INFO: Deleting pod "simpletest.rc-7w9cw" in namespace "gc-7865"
    Jan 29 03:09:48.853: INFO: Deleting pod "simpletest.rc-84mv2" in namespace "gc-7865"
    Jan 29 03:09:49.075: INFO: Deleting pod "simpletest.rc-89hwr" in namespace "gc-7865"
    Jan 29 03:09:49.283: INFO: Deleting pod "simpletest.rc-8fgbt" in namespace "gc-7865"
    Jan 29 03:09:49.480: INFO: Deleting pod "simpletest.rc-8h6nz" in namespace "gc-7865"
    Jan 29 03:09:49.627: INFO: Deleting pod "simpletest.rc-8hnbp" in namespace "gc-7865"
    Jan 29 03:09:49.742: INFO: Deleting pod "simpletest.rc-8qjw7" in namespace "gc-7865"
    Jan 29 03:09:49.872: INFO: Deleting pod "simpletest.rc-8zf8b" in namespace "gc-7865"
    Jan 29 03:09:50.034: INFO: Deleting pod "simpletest.rc-9v6rn" in namespace "gc-7865"
    Jan 29 03:09:50.193: INFO: Deleting pod "simpletest.rc-9wrdm" in namespace "gc-7865"
    Jan 29 03:09:50.350: INFO: Deleting pod "simpletest.rc-b7hsn" in namespace "gc-7865"
    Jan 29 03:09:50.528: INFO: Deleting pod "simpletest.rc-bzjv4" in namespace "gc-7865"
    Jan 29 03:09:50.652: INFO: Deleting pod "simpletest.rc-cdnlh" in namespace "gc-7865"
    Jan 29 03:09:50.825: INFO: Deleting pod "simpletest.rc-cftpn" in namespace "gc-7865"
    Jan 29 03:09:51.101: INFO: Deleting pod "simpletest.rc-cnr8q" in namespace "gc-7865"
    Jan 29 03:09:51.224: INFO: Deleting pod "simpletest.rc-dbks6" in namespace "gc-7865"
    Jan 29 03:09:51.371: INFO: Deleting pod "simpletest.rc-ddr2b" in namespace "gc-7865"
    Jan 29 03:09:51.556: INFO: Deleting pod "simpletest.rc-dthlg" in namespace "gc-7865"
    Jan 29 03:09:51.738: INFO: Deleting pod "simpletest.rc-dxx7q" in namespace "gc-7865"
    Jan 29 03:09:51.890: INFO: Deleting pod "simpletest.rc-dzqpp" in namespace "gc-7865"
    Jan 29 03:09:52.054: INFO: Deleting pod "simpletest.rc-f2zmc" in namespace "gc-7865"
    Jan 29 03:09:52.200: INFO: Deleting pod "simpletest.rc-fqzc8" in namespace "gc-7865"
    Jan 29 03:09:52.352: INFO: Deleting pod "simpletest.rc-ftcws" in namespace "gc-7865"
    Jan 29 03:09:52.636: INFO: Deleting pod "simpletest.rc-fx8r2" in namespace "gc-7865"
    Jan 29 03:09:52.754: INFO: Deleting pod "simpletest.rc-g56z9" in namespace "gc-7865"
    Jan 29 03:09:52.847: INFO: Deleting pod "simpletest.rc-g65dt" in namespace "gc-7865"
    Jan 29 03:09:52.996: INFO: Deleting pod "simpletest.rc-gxsms" in namespace "gc-7865"
    Jan 29 03:09:53.114: INFO: Deleting pod "simpletest.rc-h8d5n" in namespace "gc-7865"
    Jan 29 03:09:53.276: INFO: Deleting pod "simpletest.rc-hlcwv" in namespace "gc-7865"
    Jan 29 03:09:53.391: INFO: Deleting pod "simpletest.rc-hn2kk" in namespace "gc-7865"
    Jan 29 03:09:53.560: INFO: Deleting pod "simpletest.rc-hnqt6" in namespace "gc-7865"
    Jan 29 03:09:53.742: INFO: Deleting pod "simpletest.rc-hq9bf" in namespace "gc-7865"
    Jan 29 03:09:53.845: INFO: Deleting pod "simpletest.rc-j5x8j" in namespace "gc-7865"
    Jan 29 03:09:53.986: INFO: Deleting pod "simpletest.rc-jfvbg" in namespace "gc-7865"
    Jan 29 03:09:54.168: INFO: Deleting pod "simpletest.rc-jqbrd" in namespace "gc-7865"
    Jan 29 03:09:54.306: INFO: Deleting pod "simpletest.rc-jqxjd" in namespace "gc-7865"
    Jan 29 03:09:54.418: INFO: Deleting pod "simpletest.rc-k8g46" in namespace "gc-7865"
    Jan 29 03:09:54.545: INFO: Deleting pod "simpletest.rc-knvkx" in namespace "gc-7865"
    Jan 29 03:09:54.634: INFO: Deleting pod "simpletest.rc-kwx9h" in namespace "gc-7865"
    Jan 29 03:09:54.813: INFO: Deleting pod "simpletest.rc-l7tn7" in namespace "gc-7865"
    Jan 29 03:09:54.898: INFO: Deleting pod "simpletest.rc-ln6b4" in namespace "gc-7865"
    Jan 29 03:09:55.072: INFO: Deleting pod "simpletest.rc-lx982" in namespace "gc-7865"
    Jan 29 03:09:55.179: INFO: Deleting pod "simpletest.rc-m6pxf" in namespace "gc-7865"
    Jan 29 03:09:55.357: INFO: Deleting pod "simpletest.rc-mpdt7" in namespace "gc-7865"
    Jan 29 03:09:55.448: INFO: Deleting pod "simpletest.rc-mql9g" in namespace "gc-7865"
    Jan 29 03:09:55.668: INFO: Deleting pod "simpletest.rc-mskzl" in namespace "gc-7865"
    Jan 29 03:09:55.807: INFO: Deleting pod "simpletest.rc-nbc7f" in namespace "gc-7865"
    Jan 29 03:09:55.923: INFO: Deleting pod "simpletest.rc-nbdqh" in namespace "gc-7865"
    Jan 29 03:09:56.062: INFO: Deleting pod "simpletest.rc-nmlbv" in namespace "gc-7865"
    Jan 29 03:09:56.207: INFO: Deleting pod "simpletest.rc-ntzzb" in namespace "gc-7865"
    Jan 29 03:09:56.410: INFO: Deleting pod "simpletest.rc-nvngc" in namespace "gc-7865"
    Jan 29 03:09:56.658: INFO: Deleting pod "simpletest.rc-p8lch" in namespace "gc-7865"
    Jan 29 03:09:56.766: INFO: Deleting pod "simpletest.rc-pjlrs" in namespace "gc-7865"
    Jan 29 03:09:57.048: INFO: Deleting pod "simpletest.rc-prq8k" in namespace "gc-7865"
    Jan 29 03:09:57.261: INFO: Deleting pod "simpletest.rc-pvdnj" in namespace "gc-7865"
    Jan 29 03:09:57.418: INFO: Deleting pod "simpletest.rc-pxfbp" in namespace "gc-7865"
    Jan 29 03:09:57.585: INFO: Deleting pod "simpletest.rc-pzxlc" in namespace "gc-7865"
    Jan 29 03:09:57.698: INFO: Deleting pod "simpletest.rc-q8jpm" in namespace "gc-7865"
    Jan 29 03:09:57.806: INFO: Deleting pod "simpletest.rc-qzk54" in namespace "gc-7865"
    Jan 29 03:09:57.962: INFO: Deleting pod "simpletest.rc-r5ngc" in namespace "gc-7865"
    Jan 29 03:09:58.085: INFO: Deleting pod "simpletest.rc-rgqck" in namespace "gc-7865"
    Jan 29 03:09:58.245: INFO: Deleting pod "simpletest.rc-rhq2c" in namespace "gc-7865"
    Jan 29 03:09:58.396: INFO: Deleting pod "simpletest.rc-rtrzw" in namespace "gc-7865"
    Jan 29 03:09:58.569: INFO: Deleting pod "simpletest.rc-sc9zx" in namespace "gc-7865"
    Jan 29 03:09:58.717: INFO: Deleting pod "simpletest.rc-scgvj" in namespace "gc-7865"
    Jan 29 03:09:58.840: INFO: Deleting pod "simpletest.rc-sjqzj" in namespace "gc-7865"
    Jan 29 03:09:58.938: INFO: Deleting pod "simpletest.rc-sngx2" in namespace "gc-7865"
    Jan 29 03:09:59.091: INFO: Deleting pod "simpletest.rc-ss7k2" in namespace "gc-7865"
    Jan 29 03:09:59.182: INFO: Deleting pod "simpletest.rc-swzxk" in namespace "gc-7865"
    Jan 29 03:09:59.278: INFO: Deleting pod "simpletest.rc-sz8xz" in namespace "gc-7865"
    Jan 29 03:09:59.458: INFO: Deleting pod "simpletest.rc-tdrgn" in namespace "gc-7865"
    Jan 29 03:09:59.559: INFO: Deleting pod "simpletest.rc-v9x5v" in namespace "gc-7865"
    Jan 29 03:09:59.812: INFO: Deleting pod "simpletest.rc-vtcbd" in namespace "gc-7865"
    Jan 29 03:09:59.992: INFO: Deleting pod "simpletest.rc-vz9pr" in namespace "gc-7865"
    Jan 29 03:10:00.212: INFO: Deleting pod "simpletest.rc-w66k4" in namespace "gc-7865"
    Jan 29 03:10:00.317: INFO: Deleting pod "simpletest.rc-w6m4g" in namespace "gc-7865"
    Jan 29 03:10:00.411: INFO: Deleting pod "simpletest.rc-wmhr6" in namespace "gc-7865"
    Jan 29 03:10:00.554: INFO: Deleting pod "simpletest.rc-xsmp4" in namespace "gc-7865"
    Jan 29 03:10:00.692: INFO: Deleting pod "simpletest.rc-xvq9j" in namespace "gc-7865"
    Jan 29 03:10:00.772: INFO: Deleting pod "simpletest.rc-z567w" in namespace "gc-7865"
    Jan 29 03:10:00.895: INFO: Deleting pod "simpletest.rc-z6k76" in namespace "gc-7865"
    Jan 29 03:10:01.011: INFO: Deleting pod "simpletest.rc-z8vm8" in namespace "gc-7865"
    Jan 29 03:10:01.121: INFO: Deleting pod "simpletest.rc-zj5fw" in namespace "gc-7865"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan 29 03:10:01.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-7865" for this suite. 01/29/23 03:10:01.274
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:10:01.317
Jan 29 03:10:01.317: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename configmap 01/29/23 03:10:01.319
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:10:01.419
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:10:01.427
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
STEP: Creating configMap with name configmap-test-volume-aff49004-877a-4742-817d-383eecfe87f7 01/29/23 03:10:01.434
STEP: Creating a pod to test consume configMaps 01/29/23 03:10:01.454
Jan 29 03:10:01.485: INFO: Waiting up to 5m0s for pod "pod-configmaps-e248760c-1ee4-4f37-96a0-89e8fb225985" in namespace "configmap-7724" to be "Succeeded or Failed"
Jan 29 03:10:01.493: INFO: Pod "pod-configmaps-e248760c-1ee4-4f37-96a0-89e8fb225985": Phase="Pending", Reason="", readiness=false. Elapsed: 8.227478ms
Jan 29 03:10:03.502: INFO: Pod "pod-configmaps-e248760c-1ee4-4f37-96a0-89e8fb225985": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017580179s
Jan 29 03:10:05.500: INFO: Pod "pod-configmaps-e248760c-1ee4-4f37-96a0-89e8fb225985": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014983117s
STEP: Saw pod success 01/29/23 03:10:05.5
Jan 29 03:10:05.500: INFO: Pod "pod-configmaps-e248760c-1ee4-4f37-96a0-89e8fb225985" satisfied condition "Succeeded or Failed"
Jan 29 03:10:05.505: INFO: Trying to get logs from node slave2 pod pod-configmaps-e248760c-1ee4-4f37-96a0-89e8fb225985 container configmap-volume-test: <nil>
STEP: delete the pod 01/29/23 03:10:05.537
Jan 29 03:10:05.638: INFO: Waiting for pod pod-configmaps-e248760c-1ee4-4f37-96a0-89e8fb225985 to disappear
Jan 29 03:10:05.644: INFO: Pod pod-configmaps-e248760c-1ee4-4f37-96a0-89e8fb225985 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 29 03:10:05.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7724" for this suite. 01/29/23 03:10:05.66
{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":45,"skipped":745,"failed":0}
------------------------------
• [4.357 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:10:01.317
    Jan 29 03:10:01.317: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename configmap 01/29/23 03:10:01.319
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:10:01.419
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:10:01.427
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:422
    STEP: Creating configMap with name configmap-test-volume-aff49004-877a-4742-817d-383eecfe87f7 01/29/23 03:10:01.434
    STEP: Creating a pod to test consume configMaps 01/29/23 03:10:01.454
    Jan 29 03:10:01.485: INFO: Waiting up to 5m0s for pod "pod-configmaps-e248760c-1ee4-4f37-96a0-89e8fb225985" in namespace "configmap-7724" to be "Succeeded or Failed"
    Jan 29 03:10:01.493: INFO: Pod "pod-configmaps-e248760c-1ee4-4f37-96a0-89e8fb225985": Phase="Pending", Reason="", readiness=false. Elapsed: 8.227478ms
    Jan 29 03:10:03.502: INFO: Pod "pod-configmaps-e248760c-1ee4-4f37-96a0-89e8fb225985": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017580179s
    Jan 29 03:10:05.500: INFO: Pod "pod-configmaps-e248760c-1ee4-4f37-96a0-89e8fb225985": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014983117s
    STEP: Saw pod success 01/29/23 03:10:05.5
    Jan 29 03:10:05.500: INFO: Pod "pod-configmaps-e248760c-1ee4-4f37-96a0-89e8fb225985" satisfied condition "Succeeded or Failed"
    Jan 29 03:10:05.505: INFO: Trying to get logs from node slave2 pod pod-configmaps-e248760c-1ee4-4f37-96a0-89e8fb225985 container configmap-volume-test: <nil>
    STEP: delete the pod 01/29/23 03:10:05.537
    Jan 29 03:10:05.638: INFO: Waiting for pod pod-configmaps-e248760c-1ee4-4f37-96a0-89e8fb225985 to disappear
    Jan 29 03:10:05.644: INFO: Pod pod-configmaps-e248760c-1ee4-4f37-96a0-89e8fb225985 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 29 03:10:05.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-7724" for this suite. 01/29/23 03:10:05.66
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:10:05.674
Jan 29 03:10:05.674: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename emptydir 01/29/23 03:10:05.676
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:10:05.711
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:10:05.717
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
STEP: Creating Pod 01/29/23 03:10:05.723
Jan 29 03:10:05.743: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-b065bfa2-91ee-4d49-a1c9-6e05f034bf0d" in namespace "emptydir-5945" to be "running"
Jan 29 03:10:05.750: INFO: Pod "pod-sharedvolume-b065bfa2-91ee-4d49-a1c9-6e05f034bf0d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.285191ms
Jan 29 03:10:07.758: INFO: Pod "pod-sharedvolume-b065bfa2-91ee-4d49-a1c9-6e05f034bf0d": Phase="Running", Reason="", readiness=false. Elapsed: 2.0148604s
Jan 29 03:10:07.758: INFO: Pod "pod-sharedvolume-b065bfa2-91ee-4d49-a1c9-6e05f034bf0d" satisfied condition "running"
STEP: Reading file content from the nginx-container 01/29/23 03:10:07.758
Jan 29 03:10:07.758: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-5945 PodName:pod-sharedvolume-b065bfa2-91ee-4d49-a1c9-6e05f034bf0d ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 29 03:10:07.758: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
Jan 29 03:10:07.759: INFO: ExecWithOptions: Clientset creation
Jan 29 03:10:07.759: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/emptydir-5945/pods/pod-sharedvolume-b065bfa2-91ee-4d49-a1c9-6e05f034bf0d/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Jan 29 03:10:07.896: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 29 03:10:07.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5945" for this suite. 01/29/23 03:10:07.907
{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","completed":46,"skipped":745,"failed":0}
------------------------------
• [2.248 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:10:05.674
    Jan 29 03:10:05.674: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename emptydir 01/29/23 03:10:05.676
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:10:05.711
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:10:05.717
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:226
    STEP: Creating Pod 01/29/23 03:10:05.723
    Jan 29 03:10:05.743: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-b065bfa2-91ee-4d49-a1c9-6e05f034bf0d" in namespace "emptydir-5945" to be "running"
    Jan 29 03:10:05.750: INFO: Pod "pod-sharedvolume-b065bfa2-91ee-4d49-a1c9-6e05f034bf0d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.285191ms
    Jan 29 03:10:07.758: INFO: Pod "pod-sharedvolume-b065bfa2-91ee-4d49-a1c9-6e05f034bf0d": Phase="Running", Reason="", readiness=false. Elapsed: 2.0148604s
    Jan 29 03:10:07.758: INFO: Pod "pod-sharedvolume-b065bfa2-91ee-4d49-a1c9-6e05f034bf0d" satisfied condition "running"
    STEP: Reading file content from the nginx-container 01/29/23 03:10:07.758
    Jan 29 03:10:07.758: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-5945 PodName:pod-sharedvolume-b065bfa2-91ee-4d49-a1c9-6e05f034bf0d ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 29 03:10:07.758: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    Jan 29 03:10:07.759: INFO: ExecWithOptions: Clientset creation
    Jan 29 03:10:07.759: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/emptydir-5945/pods/pod-sharedvolume-b065bfa2-91ee-4d49-a1c9-6e05f034bf0d/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Jan 29 03:10:07.896: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 29 03:10:07.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-5945" for this suite. 01/29/23 03:10:07.907
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:10:07.923
Jan 29 03:10:07.923: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename resourcequota 01/29/23 03:10:07.925
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:10:07.953
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:10:07.959
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
STEP: Creating a ResourceQuota with terminating scope 01/29/23 03:10:07.965
STEP: Ensuring ResourceQuota status is calculated 01/29/23 03:10:07.975
STEP: Creating a ResourceQuota with not terminating scope 01/29/23 03:10:09.983
STEP: Ensuring ResourceQuota status is calculated 01/29/23 03:10:09.991
STEP: Creating a long running pod 01/29/23 03:10:11.999
STEP: Ensuring resource quota with not terminating scope captures the pod usage 01/29/23 03:10:12.029
STEP: Ensuring resource quota with terminating scope ignored the pod usage 01/29/23 03:10:14.044
STEP: Deleting the pod 01/29/23 03:10:16.052
STEP: Ensuring resource quota status released the pod usage 01/29/23 03:10:16.15
STEP: Creating a terminating pod 01/29/23 03:10:18.16
STEP: Ensuring resource quota with terminating scope captures the pod usage 01/29/23 03:10:18.178
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 01/29/23 03:10:20.187
STEP: Deleting the pod 01/29/23 03:10:22.194
STEP: Ensuring resource quota status released the pod usage 01/29/23 03:10:22.287
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 29 03:10:24.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1379" for this suite. 01/29/23 03:10:24.309
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","completed":47,"skipped":754,"failed":0}
------------------------------
• [SLOW TEST] [16.401 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:10:07.923
    Jan 29 03:10:07.923: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename resourcequota 01/29/23 03:10:07.925
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:10:07.953
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:10:07.959
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:680
    STEP: Creating a ResourceQuota with terminating scope 01/29/23 03:10:07.965
    STEP: Ensuring ResourceQuota status is calculated 01/29/23 03:10:07.975
    STEP: Creating a ResourceQuota with not terminating scope 01/29/23 03:10:09.983
    STEP: Ensuring ResourceQuota status is calculated 01/29/23 03:10:09.991
    STEP: Creating a long running pod 01/29/23 03:10:11.999
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 01/29/23 03:10:12.029
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 01/29/23 03:10:14.044
    STEP: Deleting the pod 01/29/23 03:10:16.052
    STEP: Ensuring resource quota status released the pod usage 01/29/23 03:10:16.15
    STEP: Creating a terminating pod 01/29/23 03:10:18.16
    STEP: Ensuring resource quota with terminating scope captures the pod usage 01/29/23 03:10:18.178
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 01/29/23 03:10:20.187
    STEP: Deleting the pod 01/29/23 03:10:22.194
    STEP: Ensuring resource quota status released the pod usage 01/29/23 03:10:22.287
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 29 03:10:24.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-1379" for this suite. 01/29/23 03:10:24.309
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:10:24.325
Jan 29 03:10:24.325: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename prestop 01/29/23 03:10:24.327
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:10:24.357
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:10:24.363
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-2571 01/29/23 03:10:24.371
STEP: Waiting for pods to come up. 01/29/23 03:10:24.391
Jan 29 03:10:24.391: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-2571" to be "running"
Jan 29 03:10:24.403: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 11.845843ms
Jan 29 03:10:26.411: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.019938035s
Jan 29 03:10:26.411: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-2571 01/29/23 03:10:26.417
Jan 29 03:10:26.432: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-2571" to be "running"
Jan 29 03:10:26.441: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 9.192305ms
Jan 29 03:10:28.450: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.017863301s
Jan 29 03:10:28.450: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 01/29/23 03:10:28.45
Jan 29 03:10:33.497: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 01/29/23 03:10:33.497
[AfterEach] [sig-node] PreStop
  test/e2e/framework/framework.go:187
Jan 29 03:10:33.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-2571" for this suite. 01/29/23 03:10:33.6
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","completed":48,"skipped":758,"failed":0}
------------------------------
• [SLOW TEST] [9.286 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:10:24.325
    Jan 29 03:10:24.325: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename prestop 01/29/23 03:10:24.327
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:10:24.357
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:10:24.363
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-2571 01/29/23 03:10:24.371
    STEP: Waiting for pods to come up. 01/29/23 03:10:24.391
    Jan 29 03:10:24.391: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-2571" to be "running"
    Jan 29 03:10:24.403: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 11.845843ms
    Jan 29 03:10:26.411: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.019938035s
    Jan 29 03:10:26.411: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-2571 01/29/23 03:10:26.417
    Jan 29 03:10:26.432: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-2571" to be "running"
    Jan 29 03:10:26.441: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 9.192305ms
    Jan 29 03:10:28.450: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.017863301s
    Jan 29 03:10:28.450: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 01/29/23 03:10:28.45
    Jan 29 03:10:33.497: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 01/29/23 03:10:33.497
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/framework.go:187
    Jan 29 03:10:33.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "prestop-2571" for this suite. 01/29/23 03:10:33.6
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:10:33.613
Jan 29 03:10:33.613: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename crd-publish-openapi 01/29/23 03:10:33.614
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:10:33.645
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:10:33.65
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 01/29/23 03:10:33.655
Jan 29 03:10:33.656: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
Jan 29 03:10:41.509: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 29 03:11:05.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2897" for this suite. 01/29/23 03:11:05.563
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","completed":49,"skipped":778,"failed":0}
------------------------------
• [SLOW TEST] [31.971 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:10:33.613
    Jan 29 03:10:33.613: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename crd-publish-openapi 01/29/23 03:10:33.614
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:10:33.645
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:10:33.65
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:275
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 01/29/23 03:10:33.655
    Jan 29 03:10:33.656: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    Jan 29 03:10:41.509: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 29 03:11:05.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-2897" for this suite. 01/29/23 03:11:05.563
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:11:05.585
Jan 29 03:11:05.586: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename secrets 01/29/23 03:11:05.587
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:11:05.62
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:11:05.627
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
STEP: Creating secret with name secret-test-map-94dd9680-cfbd-46e6-b860-a9a611dc99e5 01/29/23 03:11:05.634
STEP: Creating a pod to test consume secrets 01/29/23 03:11:05.643
Jan 29 03:11:05.665: INFO: Waiting up to 5m0s for pod "pod-secrets-2b0aeb10-5be0-40ac-b0b8-ce434639f083" in namespace "secrets-3910" to be "Succeeded or Failed"
Jan 29 03:11:05.672: INFO: Pod "pod-secrets-2b0aeb10-5be0-40ac-b0b8-ce434639f083": Phase="Pending", Reason="", readiness=false. Elapsed: 7.456592ms
Jan 29 03:11:07.679: INFO: Pod "pod-secrets-2b0aeb10-5be0-40ac-b0b8-ce434639f083": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014444997s
Jan 29 03:11:09.681: INFO: Pod "pod-secrets-2b0aeb10-5be0-40ac-b0b8-ce434639f083": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016153205s
STEP: Saw pod success 01/29/23 03:11:09.681
Jan 29 03:11:09.682: INFO: Pod "pod-secrets-2b0aeb10-5be0-40ac-b0b8-ce434639f083" satisfied condition "Succeeded or Failed"
Jan 29 03:11:09.689: INFO: Trying to get logs from node slave2 pod pod-secrets-2b0aeb10-5be0-40ac-b0b8-ce434639f083 container secret-volume-test: <nil>
STEP: delete the pod 01/29/23 03:11:09.704
Jan 29 03:11:09.800: INFO: Waiting for pod pod-secrets-2b0aeb10-5be0-40ac-b0b8-ce434639f083 to disappear
Jan 29 03:11:09.813: INFO: Pod pod-secrets-2b0aeb10-5be0-40ac-b0b8-ce434639f083 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 29 03:11:09.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3910" for this suite. 01/29/23 03:11:09.826
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":50,"skipped":790,"failed":0}
------------------------------
• [4.254 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:11:05.585
    Jan 29 03:11:05.586: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename secrets 01/29/23 03:11:05.587
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:11:05.62
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:11:05.627
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:88
    STEP: Creating secret with name secret-test-map-94dd9680-cfbd-46e6-b860-a9a611dc99e5 01/29/23 03:11:05.634
    STEP: Creating a pod to test consume secrets 01/29/23 03:11:05.643
    Jan 29 03:11:05.665: INFO: Waiting up to 5m0s for pod "pod-secrets-2b0aeb10-5be0-40ac-b0b8-ce434639f083" in namespace "secrets-3910" to be "Succeeded or Failed"
    Jan 29 03:11:05.672: INFO: Pod "pod-secrets-2b0aeb10-5be0-40ac-b0b8-ce434639f083": Phase="Pending", Reason="", readiness=false. Elapsed: 7.456592ms
    Jan 29 03:11:07.679: INFO: Pod "pod-secrets-2b0aeb10-5be0-40ac-b0b8-ce434639f083": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014444997s
    Jan 29 03:11:09.681: INFO: Pod "pod-secrets-2b0aeb10-5be0-40ac-b0b8-ce434639f083": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016153205s
    STEP: Saw pod success 01/29/23 03:11:09.681
    Jan 29 03:11:09.682: INFO: Pod "pod-secrets-2b0aeb10-5be0-40ac-b0b8-ce434639f083" satisfied condition "Succeeded or Failed"
    Jan 29 03:11:09.689: INFO: Trying to get logs from node slave2 pod pod-secrets-2b0aeb10-5be0-40ac-b0b8-ce434639f083 container secret-volume-test: <nil>
    STEP: delete the pod 01/29/23 03:11:09.704
    Jan 29 03:11:09.800: INFO: Waiting for pod pod-secrets-2b0aeb10-5be0-40ac-b0b8-ce434639f083 to disappear
    Jan 29 03:11:09.813: INFO: Pod pod-secrets-2b0aeb10-5be0-40ac-b0b8-ce434639f083 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 29 03:11:09.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-3910" for this suite. 01/29/23 03:11:09.826
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:11:09.841
Jan 29 03:11:09.841: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename container-lifecycle-hook 01/29/23 03:11:09.842
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:11:09.872
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:11:09.877
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 01/29/23 03:11:09.892
Jan 29 03:11:09.910: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6920" to be "running and ready"
Jan 29 03:11:09.916: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 5.524659ms
Jan 29 03:11:09.916: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 29 03:11:11.922: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.012185141s
Jan 29 03:11:11.923: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan 29 03:11:11.923: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
STEP: create the pod with lifecycle hook 01/29/23 03:11:11.928
Jan 29 03:11:11.940: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-6920" to be "running and ready"
Jan 29 03:11:11.946: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 5.840181ms
Jan 29 03:11:11.946: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 29 03:11:13.956: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.015601405s
Jan 29 03:11:13.956: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Jan 29 03:11:13.956: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 01/29/23 03:11:13.962
Jan 29 03:11:14.001: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 29 03:11:14.008: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 29 03:11:16.009: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 29 03:11:16.018: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 29 03:11:18.008: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 29 03:11:18.014: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 01/29/23 03:11:18.014
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Jan 29 03:11:18.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6920" for this suite. 01/29/23 03:11:18.057
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","completed":51,"skipped":798,"failed":0}
------------------------------
• [SLOW TEST] [8.227 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:114

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:11:09.841
    Jan 29 03:11:09.841: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/29/23 03:11:09.842
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:11:09.872
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:11:09.877
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 01/29/23 03:11:09.892
    Jan 29 03:11:09.910: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6920" to be "running and ready"
    Jan 29 03:11:09.916: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 5.524659ms
    Jan 29 03:11:09.916: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 29 03:11:11.922: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.012185141s
    Jan 29 03:11:11.923: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan 29 03:11:11.923: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:114
    STEP: create the pod with lifecycle hook 01/29/23 03:11:11.928
    Jan 29 03:11:11.940: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-6920" to be "running and ready"
    Jan 29 03:11:11.946: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 5.840181ms
    Jan 29 03:11:11.946: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 29 03:11:13.956: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.015601405s
    Jan 29 03:11:13.956: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Jan 29 03:11:13.956: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 01/29/23 03:11:13.962
    Jan 29 03:11:14.001: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jan 29 03:11:14.008: INFO: Pod pod-with-prestop-exec-hook still exists
    Jan 29 03:11:16.009: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jan 29 03:11:16.018: INFO: Pod pod-with-prestop-exec-hook still exists
    Jan 29 03:11:18.008: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jan 29 03:11:18.014: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 01/29/23 03:11:18.014
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Jan 29 03:11:18.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-6920" for this suite. 01/29/23 03:11:18.057
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:11:18.073
Jan 29 03:11:18.073: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename emptydir 01/29/23 03:11:18.074
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:11:18.104
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:11:18.11
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
STEP: Creating a pod to test emptydir 0666 on tmpfs 01/29/23 03:11:18.115
Jan 29 03:11:18.132: INFO: Waiting up to 5m0s for pod "pod-e09bb846-78db-4cb3-8dcc-9de0820b18ce" in namespace "emptydir-5554" to be "Succeeded or Failed"
Jan 29 03:11:18.138: INFO: Pod "pod-e09bb846-78db-4cb3-8dcc-9de0820b18ce": Phase="Pending", Reason="", readiness=false. Elapsed: 5.72706ms
Jan 29 03:11:20.145: INFO: Pod "pod-e09bb846-78db-4cb3-8dcc-9de0820b18ce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013173448s
Jan 29 03:11:22.146: INFO: Pod "pod-e09bb846-78db-4cb3-8dcc-9de0820b18ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014153451s
STEP: Saw pod success 01/29/23 03:11:22.146
Jan 29 03:11:22.146: INFO: Pod "pod-e09bb846-78db-4cb3-8dcc-9de0820b18ce" satisfied condition "Succeeded or Failed"
Jan 29 03:11:22.152: INFO: Trying to get logs from node slave2 pod pod-e09bb846-78db-4cb3-8dcc-9de0820b18ce container test-container: <nil>
STEP: delete the pod 01/29/23 03:11:22.168
Jan 29 03:11:22.263: INFO: Waiting for pod pod-e09bb846-78db-4cb3-8dcc-9de0820b18ce to disappear
Jan 29 03:11:22.270: INFO: Pod pod-e09bb846-78db-4cb3-8dcc-9de0820b18ce no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 29 03:11:22.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5554" for this suite. 01/29/23 03:11:22.28
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":52,"skipped":880,"failed":0}
------------------------------
• [4.220 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:11:18.073
    Jan 29 03:11:18.073: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename emptydir 01/29/23 03:11:18.074
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:11:18.104
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:11:18.11
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:136
    STEP: Creating a pod to test emptydir 0666 on tmpfs 01/29/23 03:11:18.115
    Jan 29 03:11:18.132: INFO: Waiting up to 5m0s for pod "pod-e09bb846-78db-4cb3-8dcc-9de0820b18ce" in namespace "emptydir-5554" to be "Succeeded or Failed"
    Jan 29 03:11:18.138: INFO: Pod "pod-e09bb846-78db-4cb3-8dcc-9de0820b18ce": Phase="Pending", Reason="", readiness=false. Elapsed: 5.72706ms
    Jan 29 03:11:20.145: INFO: Pod "pod-e09bb846-78db-4cb3-8dcc-9de0820b18ce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013173448s
    Jan 29 03:11:22.146: INFO: Pod "pod-e09bb846-78db-4cb3-8dcc-9de0820b18ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014153451s
    STEP: Saw pod success 01/29/23 03:11:22.146
    Jan 29 03:11:22.146: INFO: Pod "pod-e09bb846-78db-4cb3-8dcc-9de0820b18ce" satisfied condition "Succeeded or Failed"
    Jan 29 03:11:22.152: INFO: Trying to get logs from node slave2 pod pod-e09bb846-78db-4cb3-8dcc-9de0820b18ce container test-container: <nil>
    STEP: delete the pod 01/29/23 03:11:22.168
    Jan 29 03:11:22.263: INFO: Waiting for pod pod-e09bb846-78db-4cb3-8dcc-9de0820b18ce to disappear
    Jan 29 03:11:22.270: INFO: Pod pod-e09bb846-78db-4cb3-8dcc-9de0820b18ce no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 29 03:11:22.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-5554" for this suite. 01/29/23 03:11:22.28
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:11:22.294
Jan 29 03:11:22.294: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename deployment 01/29/23 03:11:22.295
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:11:22.327
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:11:22.332
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 01/29/23 03:11:22.343
STEP: waiting for Deployment to be created 01/29/23 03:11:22.355
STEP: waiting for all Replicas to be Ready 01/29/23 03:11:22.358
Jan 29 03:11:22.360: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 29 03:11:22.360: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 29 03:11:22.386: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 29 03:11:22.386: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 29 03:11:22.403: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 29 03:11:22.403: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 29 03:11:22.524: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 29 03:11:22.524: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 29 03:11:24.112: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jan 29 03:11:24.112: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jan 29 03:11:24.142: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 01/29/23 03:11:24.142
W0129 03:11:24.151393      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jan 29 03:11:24.154: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 01/29/23 03:11:24.154
Jan 29 03:11:24.158: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 0
Jan 29 03:11:24.158: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 0
Jan 29 03:11:24.158: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 0
Jan 29 03:11:24.158: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 0
Jan 29 03:11:24.158: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 0
Jan 29 03:11:24.158: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 0
Jan 29 03:11:24.158: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 0
Jan 29 03:11:24.158: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 0
Jan 29 03:11:24.158: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 1
Jan 29 03:11:24.158: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 1
Jan 29 03:11:24.158: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 2
Jan 29 03:11:24.158: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 2
Jan 29 03:11:24.158: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 2
Jan 29 03:11:24.158: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 2
Jan 29 03:11:24.167: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 2
Jan 29 03:11:24.167: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 2
Jan 29 03:11:24.202: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 2
Jan 29 03:11:24.202: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 2
Jan 29 03:11:24.228: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 1
Jan 29 03:11:24.228: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 1
Jan 29 03:11:24.238: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 1
Jan 29 03:11:24.238: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 1
Jan 29 03:11:26.251: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 2
Jan 29 03:11:26.251: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 2
Jan 29 03:11:26.307: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 1
STEP: listing Deployments 01/29/23 03:11:26.307
Jan 29 03:11:26.316: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 01/29/23 03:11:26.316
Jan 29 03:11:26.339: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 01/29/23 03:11:26.339
Jan 29 03:11:26.355: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 29 03:11:26.355: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 29 03:11:26.386: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 29 03:11:26.401: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 29 03:11:26.419: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 29 03:11:28.333: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 29 03:11:28.355: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
Jan 29 03:11:28.416: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
Jan 29 03:11:28.432: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 29 03:11:30.439: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 01/29/23 03:11:30.491
STEP: fetching the DeploymentStatus 01/29/23 03:11:30.502
Jan 29 03:11:30.512: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 1
Jan 29 03:11:30.512: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 1
Jan 29 03:11:30.512: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 1
Jan 29 03:11:30.512: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 1
Jan 29 03:11:30.513: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 1
Jan 29 03:11:30.513: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 2
Jan 29 03:11:30.513: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 3
Jan 29 03:11:30.513: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 3
Jan 29 03:11:30.513: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 2
Jan 29 03:11:30.513: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 3
STEP: deleting the Deployment 01/29/23 03:11:30.513
Jan 29 03:11:30.533: INFO: observed event type MODIFIED
Jan 29 03:11:30.533: INFO: observed event type MODIFIED
Jan 29 03:11:30.533: INFO: observed event type MODIFIED
Jan 29 03:11:30.533: INFO: observed event type MODIFIED
Jan 29 03:11:30.534: INFO: observed event type MODIFIED
Jan 29 03:11:30.534: INFO: observed event type MODIFIED
Jan 29 03:11:30.534: INFO: observed event type MODIFIED
Jan 29 03:11:30.534: INFO: observed event type MODIFIED
Jan 29 03:11:30.534: INFO: observed event type MODIFIED
Jan 29 03:11:30.534: INFO: observed event type MODIFIED
Jan 29 03:11:30.534: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 29 03:11:30.543: INFO: Log out all the ReplicaSets if there is no deployment created
Jan 29 03:11:30.553: INFO: ReplicaSet "test-deployment-54cc775c4b":
&ReplicaSet{ObjectMeta:{test-deployment-54cc775c4b  deployment-365  dd430050-0515-4eed-86ea-8e60b8c00463 5947980 4 2023-01-29 03:11:24 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 3093dfbc-e9fa-46d2-9b3e-98feb8d68bc5 0x4000d357a7 0x4000d357a8}] [] []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 54cc775c4b,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0x4000d35868 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Jan 29 03:11:30.563: INFO: pod: "test-deployment-54cc775c4b-2qh78":
&Pod{ObjectMeta:{test-deployment-54cc775c4b-2qh78 test-deployment-54cc775c4b- deployment-365  f28ad83e-8466-4624-88a7-bcffab814e77 5947976 0 2023-01-29 03:11:26 +0000 UTC 2023-01-29 03:11:31 +0000 UTC 0x40001bd2f8 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "100.101.51.28"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "100.101.51.28"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet test-deployment-54cc775c4b dd430050-0515-4eed-86ea-8e60b8c00463 0x40001bd327 0x40001bd328}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vshht,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vshht,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:11:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:11:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:11:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:11:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.244,PodIP:100.101.51.28,StartTime:2023-01-29 03:11:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-29 03:11:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry-jinan-lab.inspurcloud.cn/library/cke/pause-arm64:3.8,ImageID:docker-pullable://registry-jinan-lab.inspurcloud.cn/library/cke/pause-arm64@sha256:9479939a0f907bdc87fefd971159c5a03a2bc34f18316501f9a7f1c76de5e38d,ContainerID:docker://197afbc6781aecee1bb2a8c4001a401974573405158699b0e2157306ca311cac,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.101.51.28,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jan 29 03:11:30.563: INFO: ReplicaSet "test-deployment-7c7d8d58c8":
&ReplicaSet{ObjectMeta:{test-deployment-7c7d8d58c8  deployment-365  44ee3a4a-e19d-47aa-a0cd-b6c380352a78 5947972 2 2023-01-29 03:11:26 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 3093dfbc-e9fa-46d2-9b3e-98feb8d68bc5 0x4000d3596e 0x4000d3596f}] [] []},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7c7d8d58c8,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0x4000d35c20 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Jan 29 03:11:30.573: INFO: pod: "test-deployment-7c7d8d58c8-2g9q7":
&Pod{ObjectMeta:{test-deployment-7c7d8d58c8-2g9q7 test-deployment-7c7d8d58c8- deployment-365  140f1190-5a62-4a8c-8a33-a729335fb86c 5947928 0 2023-01-29 03:11:26 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "100.101.49.167"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "100.101.49.167"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 44ee3a4a-e19d-47aa-a0cd-b6c380352a78 0x400405a8b7 0x400405a8b8}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8tzsp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8tzsp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:11:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:11:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:11:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:11:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.245,PodIP:100.101.49.167,StartTime:2023-01-29 03:11:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-29 03:11:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry-jinan-lab.inspurcloud.cn/library/cke/httpd-arm64:2.4.38-2,ImageID:docker-pullable://registry-jinan-lab.inspurcloud.cn/library/cke/httpd-arm64@sha256:a4f0279e65f8d239ea154d08f0572c5419f43e4ed748bee8a1a1b762d0d36893,ContainerID:docker://0f559bee19b455fe7a5482d20b66f94b33998f206c471a4c39b27f327f663e6f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.101.49.167,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jan 29 03:11:30.573: INFO: pod: "test-deployment-7c7d8d58c8-pzsrr":
&Pod{ObjectMeta:{test-deployment-7c7d8d58c8-pzsrr test-deployment-7c7d8d58c8- deployment-365  bb8eb831-acd3-423a-84f0-2c55c88cd48b 5947971 0 2023-01-29 03:11:28 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "100.101.51.41"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "100.101.51.41"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 44ee3a4a-e19d-47aa-a0cd-b6c380352a78 0x400405aa87 0x400405aa88}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wbdkz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wbdkz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:11:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:11:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:11:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:11:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.244,PodIP:100.101.51.41,StartTime:2023-01-29 03:11:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-29 03:11:29 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry-jinan-lab.inspurcloud.cn/library/cke/httpd-arm64:2.4.38-2,ImageID:docker-pullable://registry-jinan-lab.inspurcloud.cn/library/cke/httpd-arm64@sha256:a4f0279e65f8d239ea154d08f0572c5419f43e4ed748bee8a1a1b762d0d36893,ContainerID:docker://6d100eccffce38ce714f57352a281098264fc4ef4f5a2c11950ed1763ac91348,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.101.51.41,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jan 29 03:11:30.574: INFO: ReplicaSet "test-deployment-8594bb6fdd":
&ReplicaSet{ObjectMeta:{test-deployment-8594bb6fdd  deployment-365  9ce9fb8d-0236-48c9-a8f7-05d09c1da388 5947857 3 2023-01-29 03:11:22 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 3093dfbc-e9fa-46d2-9b3e-98feb8d68bc5 0x4000d35e77 0x4000d35e78}] [] []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 8594bb6fdd,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0x4000d35ec8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan 29 03:11:30.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-365" for this suite. 01/29/23 03:11:30.596
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","completed":53,"skipped":882,"failed":0}
------------------------------
• [SLOW TEST] [8.316 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:11:22.294
    Jan 29 03:11:22.294: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename deployment 01/29/23 03:11:22.295
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:11:22.327
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:11:22.332
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 01/29/23 03:11:22.343
    STEP: waiting for Deployment to be created 01/29/23 03:11:22.355
    STEP: waiting for all Replicas to be Ready 01/29/23 03:11:22.358
    Jan 29 03:11:22.360: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 29 03:11:22.360: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 29 03:11:22.386: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 29 03:11:22.386: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 29 03:11:22.403: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 29 03:11:22.403: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 29 03:11:22.524: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 29 03:11:22.524: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 29 03:11:24.112: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Jan 29 03:11:24.112: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Jan 29 03:11:24.142: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 01/29/23 03:11:24.142
    W0129 03:11:24.151393      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jan 29 03:11:24.154: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 01/29/23 03:11:24.154
    Jan 29 03:11:24.158: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 0
    Jan 29 03:11:24.158: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 0
    Jan 29 03:11:24.158: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 0
    Jan 29 03:11:24.158: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 0
    Jan 29 03:11:24.158: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 0
    Jan 29 03:11:24.158: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 0
    Jan 29 03:11:24.158: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 0
    Jan 29 03:11:24.158: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 0
    Jan 29 03:11:24.158: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 1
    Jan 29 03:11:24.158: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 1
    Jan 29 03:11:24.158: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 2
    Jan 29 03:11:24.158: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 2
    Jan 29 03:11:24.158: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 2
    Jan 29 03:11:24.158: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 2
    Jan 29 03:11:24.167: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 2
    Jan 29 03:11:24.167: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 2
    Jan 29 03:11:24.202: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 2
    Jan 29 03:11:24.202: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 2
    Jan 29 03:11:24.228: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 1
    Jan 29 03:11:24.228: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 1
    Jan 29 03:11:24.238: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 1
    Jan 29 03:11:24.238: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 1
    Jan 29 03:11:26.251: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 2
    Jan 29 03:11:26.251: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 2
    Jan 29 03:11:26.307: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 1
    STEP: listing Deployments 01/29/23 03:11:26.307
    Jan 29 03:11:26.316: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 01/29/23 03:11:26.316
    Jan 29 03:11:26.339: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 01/29/23 03:11:26.339
    Jan 29 03:11:26.355: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 29 03:11:26.355: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 29 03:11:26.386: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 29 03:11:26.401: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 29 03:11:26.419: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 29 03:11:28.333: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 29 03:11:28.355: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 29 03:11:28.416: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 29 03:11:28.432: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 29 03:11:30.439: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 01/29/23 03:11:30.491
    STEP: fetching the DeploymentStatus 01/29/23 03:11:30.502
    Jan 29 03:11:30.512: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 1
    Jan 29 03:11:30.512: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 1
    Jan 29 03:11:30.512: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 1
    Jan 29 03:11:30.512: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 1
    Jan 29 03:11:30.513: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 1
    Jan 29 03:11:30.513: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 2
    Jan 29 03:11:30.513: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 3
    Jan 29 03:11:30.513: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 3
    Jan 29 03:11:30.513: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 2
    Jan 29 03:11:30.513: INFO: observed Deployment test-deployment in namespace deployment-365 with ReadyReplicas 3
    STEP: deleting the Deployment 01/29/23 03:11:30.513
    Jan 29 03:11:30.533: INFO: observed event type MODIFIED
    Jan 29 03:11:30.533: INFO: observed event type MODIFIED
    Jan 29 03:11:30.533: INFO: observed event type MODIFIED
    Jan 29 03:11:30.533: INFO: observed event type MODIFIED
    Jan 29 03:11:30.534: INFO: observed event type MODIFIED
    Jan 29 03:11:30.534: INFO: observed event type MODIFIED
    Jan 29 03:11:30.534: INFO: observed event type MODIFIED
    Jan 29 03:11:30.534: INFO: observed event type MODIFIED
    Jan 29 03:11:30.534: INFO: observed event type MODIFIED
    Jan 29 03:11:30.534: INFO: observed event type MODIFIED
    Jan 29 03:11:30.534: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 29 03:11:30.543: INFO: Log out all the ReplicaSets if there is no deployment created
    Jan 29 03:11:30.553: INFO: ReplicaSet "test-deployment-54cc775c4b":
    &ReplicaSet{ObjectMeta:{test-deployment-54cc775c4b  deployment-365  dd430050-0515-4eed-86ea-8e60b8c00463 5947980 4 2023-01-29 03:11:24 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 3093dfbc-e9fa-46d2-9b3e-98feb8d68bc5 0x4000d357a7 0x4000d357a8}] [] []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 54cc775c4b,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0x4000d35868 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    Jan 29 03:11:30.563: INFO: pod: "test-deployment-54cc775c4b-2qh78":
    &Pod{ObjectMeta:{test-deployment-54cc775c4b-2qh78 test-deployment-54cc775c4b- deployment-365  f28ad83e-8466-4624-88a7-bcffab814e77 5947976 0 2023-01-29 03:11:26 +0000 UTC 2023-01-29 03:11:31 +0000 UTC 0x40001bd2f8 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "",
        "ips": [
            "100.101.51.28"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "",
        "ips": [
            "100.101.51.28"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet test-deployment-54cc775c4b dd430050-0515-4eed-86ea-8e60b8c00463 0x40001bd327 0x40001bd328}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vshht,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vshht,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:11:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:11:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:11:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:11:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.244,PodIP:100.101.51.28,StartTime:2023-01-29 03:11:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-29 03:11:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry-jinan-lab.inspurcloud.cn/library/cke/pause-arm64:3.8,ImageID:docker-pullable://registry-jinan-lab.inspurcloud.cn/library/cke/pause-arm64@sha256:9479939a0f907bdc87fefd971159c5a03a2bc34f18316501f9a7f1c76de5e38d,ContainerID:docker://197afbc6781aecee1bb2a8c4001a401974573405158699b0e2157306ca311cac,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.101.51.28,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jan 29 03:11:30.563: INFO: ReplicaSet "test-deployment-7c7d8d58c8":
    &ReplicaSet{ObjectMeta:{test-deployment-7c7d8d58c8  deployment-365  44ee3a4a-e19d-47aa-a0cd-b6c380352a78 5947972 2 2023-01-29 03:11:26 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 3093dfbc-e9fa-46d2-9b3e-98feb8d68bc5 0x4000d3596e 0x4000d3596f}] [] []},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7c7d8d58c8,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0x4000d35c20 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

    Jan 29 03:11:30.573: INFO: pod: "test-deployment-7c7d8d58c8-2g9q7":
    &Pod{ObjectMeta:{test-deployment-7c7d8d58c8-2g9q7 test-deployment-7c7d8d58c8- deployment-365  140f1190-5a62-4a8c-8a33-a729335fb86c 5947928 0 2023-01-29 03:11:26 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "",
        "ips": [
            "100.101.49.167"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "",
        "ips": [
            "100.101.49.167"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 44ee3a4a-e19d-47aa-a0cd-b6c380352a78 0x400405a8b7 0x400405a8b8}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8tzsp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8tzsp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:11:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:11:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:11:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:11:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.245,PodIP:100.101.49.167,StartTime:2023-01-29 03:11:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-29 03:11:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry-jinan-lab.inspurcloud.cn/library/cke/httpd-arm64:2.4.38-2,ImageID:docker-pullable://registry-jinan-lab.inspurcloud.cn/library/cke/httpd-arm64@sha256:a4f0279e65f8d239ea154d08f0572c5419f43e4ed748bee8a1a1b762d0d36893,ContainerID:docker://0f559bee19b455fe7a5482d20b66f94b33998f206c471a4c39b27f327f663e6f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.101.49.167,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jan 29 03:11:30.573: INFO: pod: "test-deployment-7c7d8d58c8-pzsrr":
    &Pod{ObjectMeta:{test-deployment-7c7d8d58c8-pzsrr test-deployment-7c7d8d58c8- deployment-365  bb8eb831-acd3-423a-84f0-2c55c88cd48b 5947971 0 2023-01-29 03:11:28 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "",
        "ips": [
            "100.101.51.41"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "",
        "ips": [
            "100.101.51.41"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 44ee3a4a-e19d-47aa-a0cd-b6c380352a78 0x400405aa87 0x400405aa88}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wbdkz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wbdkz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:11:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:11:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:11:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:11:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.244,PodIP:100.101.51.41,StartTime:2023-01-29 03:11:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-29 03:11:29 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry-jinan-lab.inspurcloud.cn/library/cke/httpd-arm64:2.4.38-2,ImageID:docker-pullable://registry-jinan-lab.inspurcloud.cn/library/cke/httpd-arm64@sha256:a4f0279e65f8d239ea154d08f0572c5419f43e4ed748bee8a1a1b762d0d36893,ContainerID:docker://6d100eccffce38ce714f57352a281098264fc4ef4f5a2c11950ed1763ac91348,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.101.51.41,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jan 29 03:11:30.574: INFO: ReplicaSet "test-deployment-8594bb6fdd":
    &ReplicaSet{ObjectMeta:{test-deployment-8594bb6fdd  deployment-365  9ce9fb8d-0236-48c9-a8f7-05d09c1da388 5947857 3 2023-01-29 03:11:22 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 3093dfbc-e9fa-46d2-9b3e-98feb8d68bc5 0x4000d35e77 0x4000d35e78}] [] []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 8594bb6fdd,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0x4000d35ec8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan 29 03:11:30.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-365" for this suite. 01/29/23 03:11:30.596
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:11:30.612
Jan 29 03:11:30.612: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename emptydir 01/29/23 03:11:30.613
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:11:30.655
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:11:30.662
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
STEP: Creating a pod to test emptydir 0644 on node default medium 01/29/23 03:11:30.672
Jan 29 03:11:30.698: INFO: Waiting up to 5m0s for pod "pod-5e441618-d362-453f-834e-8d1b05cc45e1" in namespace "emptydir-2803" to be "Succeeded or Failed"
Jan 29 03:11:30.705: INFO: Pod "pod-5e441618-d362-453f-834e-8d1b05cc45e1": Phase="Pending", Reason="", readiness=false. Elapsed: 7.005229ms
Jan 29 03:11:32.712: INFO: Pod "pod-5e441618-d362-453f-834e-8d1b05cc45e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013634931s
Jan 29 03:11:34.713: INFO: Pod "pod-5e441618-d362-453f-834e-8d1b05cc45e1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014997577s
STEP: Saw pod success 01/29/23 03:11:34.714
Jan 29 03:11:34.714: INFO: Pod "pod-5e441618-d362-453f-834e-8d1b05cc45e1" satisfied condition "Succeeded or Failed"
Jan 29 03:11:34.720: INFO: Trying to get logs from node slave2 pod pod-5e441618-d362-453f-834e-8d1b05cc45e1 container test-container: <nil>
STEP: delete the pod 01/29/23 03:11:34.737
Jan 29 03:11:34.830: INFO: Waiting for pod pod-5e441618-d362-453f-834e-8d1b05cc45e1 to disappear
Jan 29 03:11:34.836: INFO: Pod pod-5e441618-d362-453f-834e-8d1b05cc45e1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 29 03:11:34.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2803" for this suite. 01/29/23 03:11:34.846
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":54,"skipped":883,"failed":0}
------------------------------
• [4.245 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:11:30.612
    Jan 29 03:11:30.612: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename emptydir 01/29/23 03:11:30.613
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:11:30.655
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:11:30.662
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:196
    STEP: Creating a pod to test emptydir 0644 on node default medium 01/29/23 03:11:30.672
    Jan 29 03:11:30.698: INFO: Waiting up to 5m0s for pod "pod-5e441618-d362-453f-834e-8d1b05cc45e1" in namespace "emptydir-2803" to be "Succeeded or Failed"
    Jan 29 03:11:30.705: INFO: Pod "pod-5e441618-d362-453f-834e-8d1b05cc45e1": Phase="Pending", Reason="", readiness=false. Elapsed: 7.005229ms
    Jan 29 03:11:32.712: INFO: Pod "pod-5e441618-d362-453f-834e-8d1b05cc45e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013634931s
    Jan 29 03:11:34.713: INFO: Pod "pod-5e441618-d362-453f-834e-8d1b05cc45e1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014997577s
    STEP: Saw pod success 01/29/23 03:11:34.714
    Jan 29 03:11:34.714: INFO: Pod "pod-5e441618-d362-453f-834e-8d1b05cc45e1" satisfied condition "Succeeded or Failed"
    Jan 29 03:11:34.720: INFO: Trying to get logs from node slave2 pod pod-5e441618-d362-453f-834e-8d1b05cc45e1 container test-container: <nil>
    STEP: delete the pod 01/29/23 03:11:34.737
    Jan 29 03:11:34.830: INFO: Waiting for pod pod-5e441618-d362-453f-834e-8d1b05cc45e1 to disappear
    Jan 29 03:11:34.836: INFO: Pod pod-5e441618-d362-453f-834e-8d1b05cc45e1 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 29 03:11:34.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-2803" for this suite. 01/29/23 03:11:34.846
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:11:34.857
Jan 29 03:11:34.857: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename projected 01/29/23 03:11:34.859
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:11:34.889
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:11:34.895
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
STEP: Creating the pod 01/29/23 03:11:34.9
Jan 29 03:11:34.918: INFO: Waiting up to 5m0s for pod "annotationupdate73468c17-6408-4069-b0bc-bb492bcc0a10" in namespace "projected-8622" to be "running and ready"
Jan 29 03:11:34.925: INFO: Pod "annotationupdate73468c17-6408-4069-b0bc-bb492bcc0a10": Phase="Pending", Reason="", readiness=false. Elapsed: 6.223443ms
Jan 29 03:11:34.925: INFO: The phase of Pod annotationupdate73468c17-6408-4069-b0bc-bb492bcc0a10 is Pending, waiting for it to be Running (with Ready = true)
Jan 29 03:11:36.935: INFO: Pod "annotationupdate73468c17-6408-4069-b0bc-bb492bcc0a10": Phase="Running", Reason="", readiness=true. Elapsed: 2.016662972s
Jan 29 03:11:36.935: INFO: The phase of Pod annotationupdate73468c17-6408-4069-b0bc-bb492bcc0a10 is Running (Ready = true)
Jan 29 03:11:36.935: INFO: Pod "annotationupdate73468c17-6408-4069-b0bc-bb492bcc0a10" satisfied condition "running and ready"
Jan 29 03:11:37.480: INFO: Successfully updated pod "annotationupdate73468c17-6408-4069-b0bc-bb492bcc0a10"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 29 03:11:39.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8622" for this suite. 01/29/23 03:11:39.526
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","completed":55,"skipped":888,"failed":0}
------------------------------
• [4.681 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:11:34.857
    Jan 29 03:11:34.857: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename projected 01/29/23 03:11:34.859
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:11:34.889
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:11:34.895
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:161
    STEP: Creating the pod 01/29/23 03:11:34.9
    Jan 29 03:11:34.918: INFO: Waiting up to 5m0s for pod "annotationupdate73468c17-6408-4069-b0bc-bb492bcc0a10" in namespace "projected-8622" to be "running and ready"
    Jan 29 03:11:34.925: INFO: Pod "annotationupdate73468c17-6408-4069-b0bc-bb492bcc0a10": Phase="Pending", Reason="", readiness=false. Elapsed: 6.223443ms
    Jan 29 03:11:34.925: INFO: The phase of Pod annotationupdate73468c17-6408-4069-b0bc-bb492bcc0a10 is Pending, waiting for it to be Running (with Ready = true)
    Jan 29 03:11:36.935: INFO: Pod "annotationupdate73468c17-6408-4069-b0bc-bb492bcc0a10": Phase="Running", Reason="", readiness=true. Elapsed: 2.016662972s
    Jan 29 03:11:36.935: INFO: The phase of Pod annotationupdate73468c17-6408-4069-b0bc-bb492bcc0a10 is Running (Ready = true)
    Jan 29 03:11:36.935: INFO: Pod "annotationupdate73468c17-6408-4069-b0bc-bb492bcc0a10" satisfied condition "running and ready"
    Jan 29 03:11:37.480: INFO: Successfully updated pod "annotationupdate73468c17-6408-4069-b0bc-bb492bcc0a10"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 29 03:11:39.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8622" for this suite. 01/29/23 03:11:39.526
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:11:39.54
Jan 29 03:11:39.540: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename services 01/29/23 03:11:39.541
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:11:39.571
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:11:39.577
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231
STEP: creating an Endpoint 01/29/23 03:11:39.589
STEP: waiting for available Endpoint 01/29/23 03:11:39.596
STEP: listing all Endpoints 01/29/23 03:11:39.599
STEP: updating the Endpoint 01/29/23 03:11:39.606
STEP: fetching the Endpoint 01/29/23 03:11:39.615
STEP: patching the Endpoint 01/29/23 03:11:39.621
STEP: fetching the Endpoint 01/29/23 03:11:39.631
STEP: deleting the Endpoint by Collection 01/29/23 03:11:39.637
STEP: waiting for Endpoint deletion 01/29/23 03:11:39.65
STEP: fetching the Endpoint 01/29/23 03:11:39.653
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 29 03:11:39.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2475" for this suite. 01/29/23 03:11:39.668
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","completed":56,"skipped":903,"failed":0}
------------------------------
• [0.140 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:11:39.54
    Jan 29 03:11:39.540: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename services 01/29/23 03:11:39.541
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:11:39.571
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:11:39.577
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3231
    STEP: creating an Endpoint 01/29/23 03:11:39.589
    STEP: waiting for available Endpoint 01/29/23 03:11:39.596
    STEP: listing all Endpoints 01/29/23 03:11:39.599
    STEP: updating the Endpoint 01/29/23 03:11:39.606
    STEP: fetching the Endpoint 01/29/23 03:11:39.615
    STEP: patching the Endpoint 01/29/23 03:11:39.621
    STEP: fetching the Endpoint 01/29/23 03:11:39.631
    STEP: deleting the Endpoint by Collection 01/29/23 03:11:39.637
    STEP: waiting for Endpoint deletion 01/29/23 03:11:39.65
    STEP: fetching the Endpoint 01/29/23 03:11:39.653
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 29 03:11:39.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-2475" for this suite. 01/29/23 03:11:39.668
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:11:39.683
Jan 29 03:11:39.683: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename kubectl 01/29/23 03:11:39.685
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:11:39.712
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:11:39.718
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
STEP: creating Agnhost RC 01/29/23 03:11:39.723
Jan 29 03:11:39.724: INFO: namespace kubectl-3713
Jan 29 03:11:39.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-3713 create -f -'
Jan 29 03:11:40.770: INFO: stderr: ""
Jan 29 03:11:40.770: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 01/29/23 03:11:40.77
Jan 29 03:11:41.777: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 29 03:11:41.777: INFO: Found 0 / 1
Jan 29 03:11:42.778: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 29 03:11:42.778: INFO: Found 1 / 1
Jan 29 03:11:42.778: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 29 03:11:42.784: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 29 03:11:42.784: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 29 03:11:42.785: INFO: wait on agnhost-primary startup in kubectl-3713 
Jan 29 03:11:42.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-3713 logs agnhost-primary-bzvl5 agnhost-primary'
Jan 29 03:11:42.926: INFO: stderr: ""
Jan 29 03:11:42.927: INFO: stdout: "Paused\n"
STEP: exposing RC 01/29/23 03:11:42.927
Jan 29 03:11:42.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-3713 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Jan 29 03:11:43.060: INFO: stderr: ""
Jan 29 03:11:43.060: INFO: stdout: "service/rm2 exposed\n"
Jan 29 03:11:43.067: INFO: Service rm2 in namespace kubectl-3713 found.
STEP: exposing service 01/29/23 03:11:45.098
Jan 29 03:11:45.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-3713 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Jan 29 03:11:45.242: INFO: stderr: ""
Jan 29 03:11:45.242: INFO: stdout: "service/rm3 exposed\n"
Jan 29 03:11:45.250: INFO: Service rm3 in namespace kubectl-3713 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 29 03:11:47.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3713" for this suite. 01/29/23 03:11:47.276
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","completed":57,"skipped":940,"failed":0}
------------------------------
• [SLOW TEST] [7.607 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1407
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:11:39.683
    Jan 29 03:11:39.683: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename kubectl 01/29/23 03:11:39.685
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:11:39.712
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:11:39.718
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1413
    STEP: creating Agnhost RC 01/29/23 03:11:39.723
    Jan 29 03:11:39.724: INFO: namespace kubectl-3713
    Jan 29 03:11:39.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-3713 create -f -'
    Jan 29 03:11:40.770: INFO: stderr: ""
    Jan 29 03:11:40.770: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 01/29/23 03:11:40.77
    Jan 29 03:11:41.777: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 29 03:11:41.777: INFO: Found 0 / 1
    Jan 29 03:11:42.778: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 29 03:11:42.778: INFO: Found 1 / 1
    Jan 29 03:11:42.778: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Jan 29 03:11:42.784: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 29 03:11:42.784: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jan 29 03:11:42.785: INFO: wait on agnhost-primary startup in kubectl-3713 
    Jan 29 03:11:42.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-3713 logs agnhost-primary-bzvl5 agnhost-primary'
    Jan 29 03:11:42.926: INFO: stderr: ""
    Jan 29 03:11:42.927: INFO: stdout: "Paused\n"
    STEP: exposing RC 01/29/23 03:11:42.927
    Jan 29 03:11:42.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-3713 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Jan 29 03:11:43.060: INFO: stderr: ""
    Jan 29 03:11:43.060: INFO: stdout: "service/rm2 exposed\n"
    Jan 29 03:11:43.067: INFO: Service rm2 in namespace kubectl-3713 found.
    STEP: exposing service 01/29/23 03:11:45.098
    Jan 29 03:11:45.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-3713 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Jan 29 03:11:45.242: INFO: stderr: ""
    Jan 29 03:11:45.242: INFO: stdout: "service/rm3 exposed\n"
    Jan 29 03:11:45.250: INFO: Service rm3 in namespace kubectl-3713 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 29 03:11:47.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-3713" for this suite. 01/29/23 03:11:47.276
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:11:47.291
Jan 29 03:11:47.292: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename services 01/29/23 03:11:47.293
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:11:47.324
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:11:47.33
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-9461 01/29/23 03:11:47.336
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/29/23 03:11:47.355
STEP: creating service externalsvc in namespace services-9461 01/29/23 03:11:47.356
STEP: creating replication controller externalsvc in namespace services-9461 01/29/23 03:11:47.389
I0129 03:11:47.402602      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-9461, replica count: 2
I0129 03:11:50.453911      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 01/29/23 03:11:50.472
Jan 29 03:11:50.505: INFO: Creating new exec pod
Jan 29 03:11:50.531: INFO: Waiting up to 5m0s for pod "execpoddbprq" in namespace "services-9461" to be "running"
Jan 29 03:11:50.540: INFO: Pod "execpoddbprq": Phase="Pending", Reason="", readiness=false. Elapsed: 9.592267ms
Jan 29 03:11:52.549: INFO: Pod "execpoddbprq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017890401s
Jan 29 03:11:54.548: INFO: Pod "execpoddbprq": Phase="Running", Reason="", readiness=true. Elapsed: 4.017188893s
Jan 29 03:11:54.548: INFO: Pod "execpoddbprq" satisfied condition "running"
Jan 29 03:11:54.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-9461 exec execpoddbprq -- /bin/sh -x -c nslookup clusterip-service.services-9461.svc.cluster.local'
Jan 29 03:11:54.812: INFO: stderr: "+ nslookup clusterip-service.services-9461.svc.cluster.local\n"
Jan 29 03:11:54.812: INFO: stdout: "Server:\t\t100.105.0.3\nAddress:\t100.105.0.3#53\n\nclusterip-service.services-9461.svc.cluster.local\tcanonical name = externalsvc.services-9461.svc.cluster.local.\nName:\texternalsvc.services-9461.svc.cluster.local\nAddress: 100.105.143.29\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-9461, will wait for the garbage collector to delete the pods 01/29/23 03:11:54.812
Jan 29 03:11:54.881: INFO: Deleting ReplicationController externalsvc took: 11.014757ms
Jan 29 03:11:54.982: INFO: Terminating ReplicationController externalsvc pods took: 101.251049ms
Jan 29 03:11:57.817: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 29 03:11:57.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9461" for this suite. 01/29/23 03:11:57.85
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","completed":58,"skipped":944,"failed":0}
------------------------------
• [SLOW TEST] [10.569 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:11:47.291
    Jan 29 03:11:47.292: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename services 01/29/23 03:11:47.293
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:11:47.324
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:11:47.33
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1481
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-9461 01/29/23 03:11:47.336
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/29/23 03:11:47.355
    STEP: creating service externalsvc in namespace services-9461 01/29/23 03:11:47.356
    STEP: creating replication controller externalsvc in namespace services-9461 01/29/23 03:11:47.389
    I0129 03:11:47.402602      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-9461, replica count: 2
    I0129 03:11:50.453911      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 01/29/23 03:11:50.472
    Jan 29 03:11:50.505: INFO: Creating new exec pod
    Jan 29 03:11:50.531: INFO: Waiting up to 5m0s for pod "execpoddbprq" in namespace "services-9461" to be "running"
    Jan 29 03:11:50.540: INFO: Pod "execpoddbprq": Phase="Pending", Reason="", readiness=false. Elapsed: 9.592267ms
    Jan 29 03:11:52.549: INFO: Pod "execpoddbprq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017890401s
    Jan 29 03:11:54.548: INFO: Pod "execpoddbprq": Phase="Running", Reason="", readiness=true. Elapsed: 4.017188893s
    Jan 29 03:11:54.548: INFO: Pod "execpoddbprq" satisfied condition "running"
    Jan 29 03:11:54.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-9461 exec execpoddbprq -- /bin/sh -x -c nslookup clusterip-service.services-9461.svc.cluster.local'
    Jan 29 03:11:54.812: INFO: stderr: "+ nslookup clusterip-service.services-9461.svc.cluster.local\n"
    Jan 29 03:11:54.812: INFO: stdout: "Server:\t\t100.105.0.3\nAddress:\t100.105.0.3#53\n\nclusterip-service.services-9461.svc.cluster.local\tcanonical name = externalsvc.services-9461.svc.cluster.local.\nName:\texternalsvc.services-9461.svc.cluster.local\nAddress: 100.105.143.29\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-9461, will wait for the garbage collector to delete the pods 01/29/23 03:11:54.812
    Jan 29 03:11:54.881: INFO: Deleting ReplicationController externalsvc took: 11.014757ms
    Jan 29 03:11:54.982: INFO: Terminating ReplicationController externalsvc pods took: 101.251049ms
    Jan 29 03:11:57.817: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 29 03:11:57.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-9461" for this suite. 01/29/23 03:11:57.85
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:11:57.864
Jan 29 03:11:57.864: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename resourcequota 01/29/23 03:11:57.866
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:11:57.894
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:11:57.9
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
STEP: Creating a ResourceQuota 01/29/23 03:11:57.906
STEP: Getting a ResourceQuota 01/29/23 03:11:57.916
STEP: Updating a ResourceQuota 01/29/23 03:11:57.923
STEP: Verifying a ResourceQuota was modified 01/29/23 03:11:57.93
STEP: Deleting a ResourceQuota 01/29/23 03:11:57.936
STEP: Verifying the deleted ResourceQuota 01/29/23 03:11:57.946
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 29 03:11:57.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2302" for this suite. 01/29/23 03:11:57.963
{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","completed":59,"skipped":1010,"failed":0}
------------------------------
• [0.114 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:11:57.864
    Jan 29 03:11:57.864: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename resourcequota 01/29/23 03:11:57.866
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:11:57.894
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:11:57.9
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:874
    STEP: Creating a ResourceQuota 01/29/23 03:11:57.906
    STEP: Getting a ResourceQuota 01/29/23 03:11:57.916
    STEP: Updating a ResourceQuota 01/29/23 03:11:57.923
    STEP: Verifying a ResourceQuota was modified 01/29/23 03:11:57.93
    STEP: Deleting a ResourceQuota 01/29/23 03:11:57.936
    STEP: Verifying the deleted ResourceQuota 01/29/23 03:11:57.946
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 29 03:11:57.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-2302" for this suite. 01/29/23 03:11:57.963
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:11:57.978
Jan 29 03:11:57.978: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename subpath 01/29/23 03:11:57.98
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:11:58.014
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:11:58.019
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/29/23 03:11:58.025
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-j427 01/29/23 03:11:58.039
STEP: Creating a pod to test atomic-volume-subpath 01/29/23 03:11:58.04
Jan 29 03:11:58.062: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-j427" in namespace "subpath-3008" to be "Succeeded or Failed"
Jan 29 03:11:58.077: INFO: Pod "pod-subpath-test-projected-j427": Phase="Pending", Reason="", readiness=false. Elapsed: 15.154627ms
Jan 29 03:12:00.085: INFO: Pod "pod-subpath-test-projected-j427": Phase="Running", Reason="", readiness=true. Elapsed: 2.022558394s
Jan 29 03:12:02.086: INFO: Pod "pod-subpath-test-projected-j427": Phase="Running", Reason="", readiness=true. Elapsed: 4.024332923s
Jan 29 03:12:04.086: INFO: Pod "pod-subpath-test-projected-j427": Phase="Running", Reason="", readiness=true. Elapsed: 6.023704274s
Jan 29 03:12:06.084: INFO: Pod "pod-subpath-test-projected-j427": Phase="Running", Reason="", readiness=true. Elapsed: 8.021634736s
Jan 29 03:12:08.085: INFO: Pod "pod-subpath-test-projected-j427": Phase="Running", Reason="", readiness=true. Elapsed: 10.02281114s
Jan 29 03:12:10.085: INFO: Pod "pod-subpath-test-projected-j427": Phase="Running", Reason="", readiness=true. Elapsed: 12.022957157s
Jan 29 03:12:12.084: INFO: Pod "pod-subpath-test-projected-j427": Phase="Running", Reason="", readiness=true. Elapsed: 14.021915106s
Jan 29 03:12:14.103: INFO: Pod "pod-subpath-test-projected-j427": Phase="Running", Reason="", readiness=true. Elapsed: 16.040773714s
Jan 29 03:12:16.085: INFO: Pod "pod-subpath-test-projected-j427": Phase="Running", Reason="", readiness=true. Elapsed: 18.022848945s
Jan 29 03:12:18.085: INFO: Pod "pod-subpath-test-projected-j427": Phase="Running", Reason="", readiness=true. Elapsed: 20.02277548s
Jan 29 03:12:20.091: INFO: Pod "pod-subpath-test-projected-j427": Phase="Running", Reason="", readiness=false. Elapsed: 22.029305742s
Jan 29 03:12:22.085: INFO: Pod "pod-subpath-test-projected-j427": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.022630991s
STEP: Saw pod success 01/29/23 03:12:22.085
Jan 29 03:12:22.085: INFO: Pod "pod-subpath-test-projected-j427" satisfied condition "Succeeded or Failed"
Jan 29 03:12:22.091: INFO: Trying to get logs from node slave2 pod pod-subpath-test-projected-j427 container test-container-subpath-projected-j427: <nil>
STEP: delete the pod 01/29/23 03:12:22.108
Jan 29 03:12:22.207: INFO: Waiting for pod pod-subpath-test-projected-j427 to disappear
Jan 29 03:12:22.213: INFO: Pod pod-subpath-test-projected-j427 no longer exists
STEP: Deleting pod pod-subpath-test-projected-j427 01/29/23 03:12:22.213
Jan 29 03:12:22.213: INFO: Deleting pod "pod-subpath-test-projected-j427" in namespace "subpath-3008"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jan 29 03:12:22.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3008" for this suite. 01/29/23 03:12:22.23
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]","completed":60,"skipped":1010,"failed":0}
------------------------------
• [SLOW TEST] [24.263 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:11:57.978
    Jan 29 03:11:57.978: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename subpath 01/29/23 03:11:57.98
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:11:58.014
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:11:58.019
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/29/23 03:11:58.025
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-j427 01/29/23 03:11:58.039
    STEP: Creating a pod to test atomic-volume-subpath 01/29/23 03:11:58.04
    Jan 29 03:11:58.062: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-j427" in namespace "subpath-3008" to be "Succeeded or Failed"
    Jan 29 03:11:58.077: INFO: Pod "pod-subpath-test-projected-j427": Phase="Pending", Reason="", readiness=false. Elapsed: 15.154627ms
    Jan 29 03:12:00.085: INFO: Pod "pod-subpath-test-projected-j427": Phase="Running", Reason="", readiness=true. Elapsed: 2.022558394s
    Jan 29 03:12:02.086: INFO: Pod "pod-subpath-test-projected-j427": Phase="Running", Reason="", readiness=true. Elapsed: 4.024332923s
    Jan 29 03:12:04.086: INFO: Pod "pod-subpath-test-projected-j427": Phase="Running", Reason="", readiness=true. Elapsed: 6.023704274s
    Jan 29 03:12:06.084: INFO: Pod "pod-subpath-test-projected-j427": Phase="Running", Reason="", readiness=true. Elapsed: 8.021634736s
    Jan 29 03:12:08.085: INFO: Pod "pod-subpath-test-projected-j427": Phase="Running", Reason="", readiness=true. Elapsed: 10.02281114s
    Jan 29 03:12:10.085: INFO: Pod "pod-subpath-test-projected-j427": Phase="Running", Reason="", readiness=true. Elapsed: 12.022957157s
    Jan 29 03:12:12.084: INFO: Pod "pod-subpath-test-projected-j427": Phase="Running", Reason="", readiness=true. Elapsed: 14.021915106s
    Jan 29 03:12:14.103: INFO: Pod "pod-subpath-test-projected-j427": Phase="Running", Reason="", readiness=true. Elapsed: 16.040773714s
    Jan 29 03:12:16.085: INFO: Pod "pod-subpath-test-projected-j427": Phase="Running", Reason="", readiness=true. Elapsed: 18.022848945s
    Jan 29 03:12:18.085: INFO: Pod "pod-subpath-test-projected-j427": Phase="Running", Reason="", readiness=true. Elapsed: 20.02277548s
    Jan 29 03:12:20.091: INFO: Pod "pod-subpath-test-projected-j427": Phase="Running", Reason="", readiness=false. Elapsed: 22.029305742s
    Jan 29 03:12:22.085: INFO: Pod "pod-subpath-test-projected-j427": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.022630991s
    STEP: Saw pod success 01/29/23 03:12:22.085
    Jan 29 03:12:22.085: INFO: Pod "pod-subpath-test-projected-j427" satisfied condition "Succeeded or Failed"
    Jan 29 03:12:22.091: INFO: Trying to get logs from node slave2 pod pod-subpath-test-projected-j427 container test-container-subpath-projected-j427: <nil>
    STEP: delete the pod 01/29/23 03:12:22.108
    Jan 29 03:12:22.207: INFO: Waiting for pod pod-subpath-test-projected-j427 to disappear
    Jan 29 03:12:22.213: INFO: Pod pod-subpath-test-projected-j427 no longer exists
    STEP: Deleting pod pod-subpath-test-projected-j427 01/29/23 03:12:22.213
    Jan 29 03:12:22.213: INFO: Deleting pod "pod-subpath-test-projected-j427" in namespace "subpath-3008"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jan 29 03:12:22.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-3008" for this suite. 01/29/23 03:12:22.23
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:12:22.244
Jan 29 03:12:22.244: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename job 01/29/23 03:12:22.245
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:12:22.277
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:12:22.283
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
STEP: Creating a suspended job 01/29/23 03:12:22.297
STEP: Patching the Job 01/29/23 03:12:22.311
STEP: Watching for Job to be patched 01/29/23 03:12:22.362
Jan 29 03:12:22.365: INFO: Event ADDED observed for Job e2e-gw969 in namespace job-220 with labels: map[e2e-job-label:e2e-gw969] and annotations: map[batch.kubernetes.io/job-tracking: cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated]
Jan 29 03:12:22.365: INFO: Event MODIFIED observed for Job e2e-gw969 in namespace job-220 with labels: map[e2e-job-label:e2e-gw969] and annotations: map[batch.kubernetes.io/job-tracking: cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated]
Jan 29 03:12:22.365: INFO: Event MODIFIED found for Job e2e-gw969 in namespace job-220 with labels: map[e2e-gw969:patched e2e-job-label:e2e-gw969] and annotations: map[batch.kubernetes.io/job-tracking: cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated]
STEP: Updating the job 01/29/23 03:12:22.365
STEP: Watching for Job to be updated 01/29/23 03:12:22.38
Jan 29 03:12:22.383: INFO: Event MODIFIED found for Job e2e-gw969 in namespace job-220 with labels: map[e2e-gw969:patched e2e-job-label:e2e-gw969] and annotations: map[batch.kubernetes.io/job-tracking: cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated updated:true]
Jan 29 03:12:22.383: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "cke-admission.inspur.com/protect":"true", "cke-admission.inspur.com/status":"mutated", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 01/29/23 03:12:22.383
Jan 29 03:12:22.391: INFO: Job: e2e-gw969 as labels: map[e2e-gw969:patched e2e-job-label:e2e-gw969]
STEP: Waiting for job to complete 01/29/23 03:12:22.391
STEP: Delete a job collection with a labelselector 01/29/23 03:12:32.4
STEP: Watching for Job to be deleted 01/29/23 03:12:32.415
Jan 29 03:12:32.419: INFO: Event MODIFIED observed for Job e2e-gw969 in namespace job-220 with labels: map[e2e-gw969:patched e2e-job-label:e2e-gw969] and annotations: map[batch.kubernetes.io/job-tracking: cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated updated:true]
Jan 29 03:12:32.419: INFO: Event MODIFIED observed for Job e2e-gw969 in namespace job-220 with labels: map[e2e-gw969:patched e2e-job-label:e2e-gw969] and annotations: map[batch.kubernetes.io/job-tracking: cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated updated:true]
Jan 29 03:12:32.419: INFO: Event MODIFIED observed for Job e2e-gw969 in namespace job-220 with labels: map[e2e-gw969:patched e2e-job-label:e2e-gw969] and annotations: map[batch.kubernetes.io/job-tracking: cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated updated:true]
Jan 29 03:12:32.419: INFO: Event MODIFIED observed for Job e2e-gw969 in namespace job-220 with labels: map[e2e-gw969:patched e2e-job-label:e2e-gw969] and annotations: map[batch.kubernetes.io/job-tracking: cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated updated:true]
Jan 29 03:12:32.419: INFO: Event MODIFIED observed for Job e2e-gw969 in namespace job-220 with labels: map[e2e-gw969:patched e2e-job-label:e2e-gw969] and annotations: map[batch.kubernetes.io/job-tracking: cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated updated:true]
Jan 29 03:12:32.419: INFO: Event DELETED found for Job e2e-gw969 in namespace job-220 with labels: map[e2e-gw969:patched e2e-job-label:e2e-gw969] and annotations: map[batch.kubernetes.io/job-tracking: cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated updated:true]
STEP: Relist jobs to confirm deletion 01/29/23 03:12:32.419
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jan 29 03:12:32.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-220" for this suite. 01/29/23 03:12:32.441
{"msg":"PASSED [sig-apps] Job should manage the lifecycle of a job [Conformance]","completed":61,"skipped":1034,"failed":0}
------------------------------
• [SLOW TEST] [10.212 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:12:22.244
    Jan 29 03:12:22.244: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename job 01/29/23 03:12:22.245
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:12:22.277
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:12:22.283
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:531
    STEP: Creating a suspended job 01/29/23 03:12:22.297
    STEP: Patching the Job 01/29/23 03:12:22.311
    STEP: Watching for Job to be patched 01/29/23 03:12:22.362
    Jan 29 03:12:22.365: INFO: Event ADDED observed for Job e2e-gw969 in namespace job-220 with labels: map[e2e-job-label:e2e-gw969] and annotations: map[batch.kubernetes.io/job-tracking: cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated]
    Jan 29 03:12:22.365: INFO: Event MODIFIED observed for Job e2e-gw969 in namespace job-220 with labels: map[e2e-job-label:e2e-gw969] and annotations: map[batch.kubernetes.io/job-tracking: cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated]
    Jan 29 03:12:22.365: INFO: Event MODIFIED found for Job e2e-gw969 in namespace job-220 with labels: map[e2e-gw969:patched e2e-job-label:e2e-gw969] and annotations: map[batch.kubernetes.io/job-tracking: cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated]
    STEP: Updating the job 01/29/23 03:12:22.365
    STEP: Watching for Job to be updated 01/29/23 03:12:22.38
    Jan 29 03:12:22.383: INFO: Event MODIFIED found for Job e2e-gw969 in namespace job-220 with labels: map[e2e-gw969:patched e2e-job-label:e2e-gw969] and annotations: map[batch.kubernetes.io/job-tracking: cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated updated:true]
    Jan 29 03:12:22.383: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "cke-admission.inspur.com/protect":"true", "cke-admission.inspur.com/status":"mutated", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 01/29/23 03:12:22.383
    Jan 29 03:12:22.391: INFO: Job: e2e-gw969 as labels: map[e2e-gw969:patched e2e-job-label:e2e-gw969]
    STEP: Waiting for job to complete 01/29/23 03:12:22.391
    STEP: Delete a job collection with a labelselector 01/29/23 03:12:32.4
    STEP: Watching for Job to be deleted 01/29/23 03:12:32.415
    Jan 29 03:12:32.419: INFO: Event MODIFIED observed for Job e2e-gw969 in namespace job-220 with labels: map[e2e-gw969:patched e2e-job-label:e2e-gw969] and annotations: map[batch.kubernetes.io/job-tracking: cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated updated:true]
    Jan 29 03:12:32.419: INFO: Event MODIFIED observed for Job e2e-gw969 in namespace job-220 with labels: map[e2e-gw969:patched e2e-job-label:e2e-gw969] and annotations: map[batch.kubernetes.io/job-tracking: cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated updated:true]
    Jan 29 03:12:32.419: INFO: Event MODIFIED observed for Job e2e-gw969 in namespace job-220 with labels: map[e2e-gw969:patched e2e-job-label:e2e-gw969] and annotations: map[batch.kubernetes.io/job-tracking: cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated updated:true]
    Jan 29 03:12:32.419: INFO: Event MODIFIED observed for Job e2e-gw969 in namespace job-220 with labels: map[e2e-gw969:patched e2e-job-label:e2e-gw969] and annotations: map[batch.kubernetes.io/job-tracking: cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated updated:true]
    Jan 29 03:12:32.419: INFO: Event MODIFIED observed for Job e2e-gw969 in namespace job-220 with labels: map[e2e-gw969:patched e2e-job-label:e2e-gw969] and annotations: map[batch.kubernetes.io/job-tracking: cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated updated:true]
    Jan 29 03:12:32.419: INFO: Event DELETED found for Job e2e-gw969 in namespace job-220 with labels: map[e2e-gw969:patched e2e-job-label:e2e-gw969] and annotations: map[batch.kubernetes.io/job-tracking: cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated updated:true]
    STEP: Relist jobs to confirm deletion 01/29/23 03:12:32.419
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jan 29 03:12:32.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-220" for this suite. 01/29/23 03:12:32.441
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:12:32.457
Jan 29 03:12:32.457: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename disruption 01/29/23 03:12:32.459
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:12:32.5
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:12:32.507
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
STEP: Creating a pdb that targets all three pods in a test replica set 01/29/23 03:12:32.514
STEP: Waiting for the pdb to be processed 01/29/23 03:12:32.528
STEP: First trying to evict a pod which shouldn't be evictable 01/29/23 03:12:34.573
STEP: Waiting for all pods to be running 01/29/23 03:12:34.573
Jan 29 03:12:34.581: INFO: pods: 0 < 3
Jan 29 03:12:36.589: INFO: running pods: 1 < 3
STEP: locating a running pod 01/29/23 03:12:38.591
STEP: Updating the pdb to allow a pod to be evicted 01/29/23 03:12:38.609
STEP: Waiting for the pdb to be processed 01/29/23 03:12:38.622
STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/29/23 03:12:40.637
STEP: Waiting for all pods to be running 01/29/23 03:12:40.637
STEP: Waiting for the pdb to observed all healthy pods 01/29/23 03:12:40.645
STEP: Patching the pdb to disallow a pod to be evicted 01/29/23 03:12:40.687
STEP: Waiting for the pdb to be processed 01/29/23 03:12:40.709
STEP: Waiting for all pods to be running 01/29/23 03:12:42.729
Jan 29 03:12:42.736: INFO: running pods: 2 < 3
STEP: locating a running pod 01/29/23 03:12:44.745
STEP: Deleting the pdb to allow a pod to be evicted 01/29/23 03:12:44.761
STEP: Waiting for the pdb to be deleted 01/29/23 03:12:44.771
STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/29/23 03:12:44.777
STEP: Waiting for all pods to be running 01/29/23 03:12:44.777
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jan 29 03:12:44.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-1133" for this suite. 01/29/23 03:12:44.815
{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","completed":62,"skipped":1034,"failed":0}
------------------------------
• [SLOW TEST] [12.377 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:12:32.457
    Jan 29 03:12:32.457: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename disruption 01/29/23 03:12:32.459
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:12:32.5
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:12:32.507
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:346
    STEP: Creating a pdb that targets all three pods in a test replica set 01/29/23 03:12:32.514
    STEP: Waiting for the pdb to be processed 01/29/23 03:12:32.528
    STEP: First trying to evict a pod which shouldn't be evictable 01/29/23 03:12:34.573
    STEP: Waiting for all pods to be running 01/29/23 03:12:34.573
    Jan 29 03:12:34.581: INFO: pods: 0 < 3
    Jan 29 03:12:36.589: INFO: running pods: 1 < 3
    STEP: locating a running pod 01/29/23 03:12:38.591
    STEP: Updating the pdb to allow a pod to be evicted 01/29/23 03:12:38.609
    STEP: Waiting for the pdb to be processed 01/29/23 03:12:38.622
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/29/23 03:12:40.637
    STEP: Waiting for all pods to be running 01/29/23 03:12:40.637
    STEP: Waiting for the pdb to observed all healthy pods 01/29/23 03:12:40.645
    STEP: Patching the pdb to disallow a pod to be evicted 01/29/23 03:12:40.687
    STEP: Waiting for the pdb to be processed 01/29/23 03:12:40.709
    STEP: Waiting for all pods to be running 01/29/23 03:12:42.729
    Jan 29 03:12:42.736: INFO: running pods: 2 < 3
    STEP: locating a running pod 01/29/23 03:12:44.745
    STEP: Deleting the pdb to allow a pod to be evicted 01/29/23 03:12:44.761
    STEP: Waiting for the pdb to be deleted 01/29/23 03:12:44.771
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/29/23 03:12:44.777
    STEP: Waiting for all pods to be running 01/29/23 03:12:44.777
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jan 29 03:12:44.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-1133" for this suite. 01/29/23 03:12:44.815
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:12:44.835
Jan 29 03:12:44.835: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename dns 01/29/23 03:12:44.837
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:12:44.878
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:12:44.884
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 01/29/23 03:12:44.89
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5006.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-5006.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5006.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5006.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-5006.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-5006.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-5006.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-5006.svc.cluster.local;sleep 1; done
 01/29/23 03:12:44.903
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5006.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-5006.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5006.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-5006.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-5006.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-5006.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-5006.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-5006.svc.cluster.local;sleep 1; done
 01/29/23 03:12:44.903
STEP: creating a pod to probe DNS 01/29/23 03:12:44.903
STEP: submitting the pod to kubernetes 01/29/23 03:12:44.904
Jan 29 03:12:44.936: INFO: Waiting up to 15m0s for pod "dns-test-11f1a066-04d8-4223-803a-97e9b294c378" in namespace "dns-5006" to be "running"
Jan 29 03:12:44.943: INFO: Pod "dns-test-11f1a066-04d8-4223-803a-97e9b294c378": Phase="Pending", Reason="", readiness=false. Elapsed: 6.748967ms
Jan 29 03:12:46.951: INFO: Pod "dns-test-11f1a066-04d8-4223-803a-97e9b294c378": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014939521s
Jan 29 03:12:48.951: INFO: Pod "dns-test-11f1a066-04d8-4223-803a-97e9b294c378": Phase="Running", Reason="", readiness=true. Elapsed: 4.014636314s
Jan 29 03:12:48.951: INFO: Pod "dns-test-11f1a066-04d8-4223-803a-97e9b294c378" satisfied condition "running"
STEP: retrieving the pod 01/29/23 03:12:48.951
STEP: looking for the results for each expected name from probers 01/29/23 03:12:48.957
Jan 29 03:12:48.966: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5006.svc.cluster.local from pod dns-5006/dns-test-11f1a066-04d8-4223-803a-97e9b294c378: the server could not find the requested resource (get pods dns-test-11f1a066-04d8-4223-803a-97e9b294c378)
Jan 29 03:12:48.973: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5006.svc.cluster.local from pod dns-5006/dns-test-11f1a066-04d8-4223-803a-97e9b294c378: the server could not find the requested resource (get pods dns-test-11f1a066-04d8-4223-803a-97e9b294c378)
Jan 29 03:12:48.980: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5006.svc.cluster.local from pod dns-5006/dns-test-11f1a066-04d8-4223-803a-97e9b294c378: the server could not find the requested resource (get pods dns-test-11f1a066-04d8-4223-803a-97e9b294c378)
Jan 29 03:12:48.987: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5006.svc.cluster.local from pod dns-5006/dns-test-11f1a066-04d8-4223-803a-97e9b294c378: the server could not find the requested resource (get pods dns-test-11f1a066-04d8-4223-803a-97e9b294c378)
Jan 29 03:12:48.993: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5006.svc.cluster.local from pod dns-5006/dns-test-11f1a066-04d8-4223-803a-97e9b294c378: the server could not find the requested resource (get pods dns-test-11f1a066-04d8-4223-803a-97e9b294c378)
Jan 29 03:12:49.008: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5006.svc.cluster.local from pod dns-5006/dns-test-11f1a066-04d8-4223-803a-97e9b294c378: the server could not find the requested resource (get pods dns-test-11f1a066-04d8-4223-803a-97e9b294c378)
Jan 29 03:12:49.015: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5006.svc.cluster.local from pod dns-5006/dns-test-11f1a066-04d8-4223-803a-97e9b294c378: the server could not find the requested resource (get pods dns-test-11f1a066-04d8-4223-803a-97e9b294c378)
Jan 29 03:12:49.016: INFO: Lookups using dns-5006/dns-test-11f1a066-04d8-4223-803a-97e9b294c378 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5006.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5006.svc.cluster.local wheezy_udp@dns-test-service-2.dns-5006.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5006.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5006.svc.cluster.local jessie_udp@dns-test-service-2.dns-5006.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5006.svc.cluster.local]

Jan 29 03:12:54.069: INFO: DNS probes using dns-5006/dns-test-11f1a066-04d8-4223-803a-97e9b294c378 succeeded

STEP: deleting the pod 01/29/23 03:12:54.069
STEP: deleting the test headless service 01/29/23 03:12:54.203
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan 29 03:12:54.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5006" for this suite. 01/29/23 03:13:15.315
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","completed":63,"skipped":1035,"failed":0}
------------------------------
• [SLOW TEST] [30.496 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:12:44.835
    Jan 29 03:12:44.835: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename dns 01/29/23 03:12:44.837
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:12:44.878
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:12:44.884
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 01/29/23 03:12:44.89
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5006.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-5006.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5006.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5006.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-5006.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-5006.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-5006.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-5006.svc.cluster.local;sleep 1; done
     01/29/23 03:12:44.903
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5006.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-5006.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5006.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-5006.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-5006.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-5006.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-5006.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-5006.svc.cluster.local;sleep 1; done
     01/29/23 03:12:44.903
    STEP: creating a pod to probe DNS 01/29/23 03:12:44.903
    STEP: submitting the pod to kubernetes 01/29/23 03:12:44.904
    Jan 29 03:12:44.936: INFO: Waiting up to 15m0s for pod "dns-test-11f1a066-04d8-4223-803a-97e9b294c378" in namespace "dns-5006" to be "running"
    Jan 29 03:12:44.943: INFO: Pod "dns-test-11f1a066-04d8-4223-803a-97e9b294c378": Phase="Pending", Reason="", readiness=false. Elapsed: 6.748967ms
    Jan 29 03:12:46.951: INFO: Pod "dns-test-11f1a066-04d8-4223-803a-97e9b294c378": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014939521s
    Jan 29 03:12:48.951: INFO: Pod "dns-test-11f1a066-04d8-4223-803a-97e9b294c378": Phase="Running", Reason="", readiness=true. Elapsed: 4.014636314s
    Jan 29 03:12:48.951: INFO: Pod "dns-test-11f1a066-04d8-4223-803a-97e9b294c378" satisfied condition "running"
    STEP: retrieving the pod 01/29/23 03:12:48.951
    STEP: looking for the results for each expected name from probers 01/29/23 03:12:48.957
    Jan 29 03:12:48.966: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5006.svc.cluster.local from pod dns-5006/dns-test-11f1a066-04d8-4223-803a-97e9b294c378: the server could not find the requested resource (get pods dns-test-11f1a066-04d8-4223-803a-97e9b294c378)
    Jan 29 03:12:48.973: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5006.svc.cluster.local from pod dns-5006/dns-test-11f1a066-04d8-4223-803a-97e9b294c378: the server could not find the requested resource (get pods dns-test-11f1a066-04d8-4223-803a-97e9b294c378)
    Jan 29 03:12:48.980: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5006.svc.cluster.local from pod dns-5006/dns-test-11f1a066-04d8-4223-803a-97e9b294c378: the server could not find the requested resource (get pods dns-test-11f1a066-04d8-4223-803a-97e9b294c378)
    Jan 29 03:12:48.987: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5006.svc.cluster.local from pod dns-5006/dns-test-11f1a066-04d8-4223-803a-97e9b294c378: the server could not find the requested resource (get pods dns-test-11f1a066-04d8-4223-803a-97e9b294c378)
    Jan 29 03:12:48.993: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5006.svc.cluster.local from pod dns-5006/dns-test-11f1a066-04d8-4223-803a-97e9b294c378: the server could not find the requested resource (get pods dns-test-11f1a066-04d8-4223-803a-97e9b294c378)
    Jan 29 03:12:49.008: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5006.svc.cluster.local from pod dns-5006/dns-test-11f1a066-04d8-4223-803a-97e9b294c378: the server could not find the requested resource (get pods dns-test-11f1a066-04d8-4223-803a-97e9b294c378)
    Jan 29 03:12:49.015: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5006.svc.cluster.local from pod dns-5006/dns-test-11f1a066-04d8-4223-803a-97e9b294c378: the server could not find the requested resource (get pods dns-test-11f1a066-04d8-4223-803a-97e9b294c378)
    Jan 29 03:12:49.016: INFO: Lookups using dns-5006/dns-test-11f1a066-04d8-4223-803a-97e9b294c378 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5006.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5006.svc.cluster.local wheezy_udp@dns-test-service-2.dns-5006.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5006.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5006.svc.cluster.local jessie_udp@dns-test-service-2.dns-5006.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5006.svc.cluster.local]

    Jan 29 03:12:54.069: INFO: DNS probes using dns-5006/dns-test-11f1a066-04d8-4223-803a-97e9b294c378 succeeded

    STEP: deleting the pod 01/29/23 03:12:54.069
    STEP: deleting the test headless service 01/29/23 03:12:54.203
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan 29 03:12:54.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-5006" for this suite. 01/29/23 03:13:15.315
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:13:15.335
Jan 29 03:13:15.335: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename var-expansion 01/29/23 03:13:15.337
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:13:15.373
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:13:15.379
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
STEP: Creating a pod to test substitution in container's args 01/29/23 03:13:15.384
Jan 29 03:13:15.401: INFO: Waiting up to 5m0s for pod "var-expansion-f22d62e2-8cf4-4f30-ae85-356d62e80f8a" in namespace "var-expansion-1314" to be "Succeeded or Failed"
Jan 29 03:13:15.408: INFO: Pod "var-expansion-f22d62e2-8cf4-4f30-ae85-356d62e80f8a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.014629ms
Jan 29 03:13:17.415: INFO: Pod "var-expansion-f22d62e2-8cf4-4f30-ae85-356d62e80f8a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013959154s
Jan 29 03:13:19.416: INFO: Pod "var-expansion-f22d62e2-8cf4-4f30-ae85-356d62e80f8a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015719722s
STEP: Saw pod success 01/29/23 03:13:19.416
Jan 29 03:13:19.417: INFO: Pod "var-expansion-f22d62e2-8cf4-4f30-ae85-356d62e80f8a" satisfied condition "Succeeded or Failed"
Jan 29 03:13:19.422: INFO: Trying to get logs from node slave2 pod var-expansion-f22d62e2-8cf4-4f30-ae85-356d62e80f8a container dapi-container: <nil>
STEP: delete the pod 01/29/23 03:13:19.437
Jan 29 03:13:19.532: INFO: Waiting for pod var-expansion-f22d62e2-8cf4-4f30-ae85-356d62e80f8a to disappear
Jan 29 03:13:19.537: INFO: Pod var-expansion-f22d62e2-8cf4-4f30-ae85-356d62e80f8a no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan 29 03:13:19.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1314" for this suite. 01/29/23 03:13:19.547
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","completed":64,"skipped":1087,"failed":0}
------------------------------
• [4.223 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:13:15.335
    Jan 29 03:13:15.335: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename var-expansion 01/29/23 03:13:15.337
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:13:15.373
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:13:15.379
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:91
    STEP: Creating a pod to test substitution in container's args 01/29/23 03:13:15.384
    Jan 29 03:13:15.401: INFO: Waiting up to 5m0s for pod "var-expansion-f22d62e2-8cf4-4f30-ae85-356d62e80f8a" in namespace "var-expansion-1314" to be "Succeeded or Failed"
    Jan 29 03:13:15.408: INFO: Pod "var-expansion-f22d62e2-8cf4-4f30-ae85-356d62e80f8a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.014629ms
    Jan 29 03:13:17.415: INFO: Pod "var-expansion-f22d62e2-8cf4-4f30-ae85-356d62e80f8a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013959154s
    Jan 29 03:13:19.416: INFO: Pod "var-expansion-f22d62e2-8cf4-4f30-ae85-356d62e80f8a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015719722s
    STEP: Saw pod success 01/29/23 03:13:19.416
    Jan 29 03:13:19.417: INFO: Pod "var-expansion-f22d62e2-8cf4-4f30-ae85-356d62e80f8a" satisfied condition "Succeeded or Failed"
    Jan 29 03:13:19.422: INFO: Trying to get logs from node slave2 pod var-expansion-f22d62e2-8cf4-4f30-ae85-356d62e80f8a container dapi-container: <nil>
    STEP: delete the pod 01/29/23 03:13:19.437
    Jan 29 03:13:19.532: INFO: Waiting for pod var-expansion-f22d62e2-8cf4-4f30-ae85-356d62e80f8a to disappear
    Jan 29 03:13:19.537: INFO: Pod var-expansion-f22d62e2-8cf4-4f30-ae85-356d62e80f8a no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan 29 03:13:19.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-1314" for this suite. 01/29/23 03:13:19.547
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:13:19.56
Jan 29 03:13:19.560: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename crd-publish-openapi 01/29/23 03:13:19.562
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:13:19.596
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:13:19.603
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
STEP: set up a multi version CRD 01/29/23 03:13:19.609
Jan 29 03:13:19.610: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: rename a version 01/29/23 03:13:34.889
STEP: check the new version name is served 01/29/23 03:13:34.914
STEP: check the old version name is removed 01/29/23 03:13:41.431
STEP: check the other version is not changed 01/29/23 03:13:44.057
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 29 03:13:56.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3712" for this suite. 01/29/23 03:13:56.1
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","completed":65,"skipped":1101,"failed":0}
------------------------------
• [SLOW TEST] [36.551 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:13:19.56
    Jan 29 03:13:19.560: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename crd-publish-openapi 01/29/23 03:13:19.562
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:13:19.596
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:13:19.603
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:390
    STEP: set up a multi version CRD 01/29/23 03:13:19.609
    Jan 29 03:13:19.610: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: rename a version 01/29/23 03:13:34.889
    STEP: check the new version name is served 01/29/23 03:13:34.914
    STEP: check the old version name is removed 01/29/23 03:13:41.431
    STEP: check the other version is not changed 01/29/23 03:13:44.057
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 29 03:13:56.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-3712" for this suite. 01/29/23 03:13:56.1
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:13:56.113
Jan 29 03:13:56.113: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename kubectl 01/29/23 03:13:56.115
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:13:56.148
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:13:56.154
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1732
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/29/23 03:13:56.161
Jan 29 03:13:56.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-1653 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jan 29 03:13:56.278: INFO: stderr: ""
Jan 29 03:13:56.278: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 01/29/23 03:13:56.278
STEP: verifying the pod e2e-test-httpd-pod was created 01/29/23 03:14:01.331
Jan 29 03:14:01.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-1653 get pod e2e-test-httpd-pod -o json'
Jan 29 03:14:01.445: INFO: stderr: ""
Jan 29 03:14:01.445: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"k8s.v1.cni.cncf.io/network-status\": \"[{\\n    \\\"name\\\": \\\"\\\",\\n    \\\"ips\\\": [\\n        \\\"100.101.49.180\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"k8s.v1.cni.cncf.io/networks-status\": \"[{\\n    \\\"name\\\": \\\"\\\",\\n    \\\"ips\\\": [\\n        \\\"100.101.49.180\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\"\n        },\n        \"creationTimestamp\": \"2023-01-29T03:13:56Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-1653\",\n        \"resourceVersion\": \"5949376\",\n        \"uid\": \"611aa2d6-727c-414c-8e26-2842f5639029\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-h5fbz\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"slave2\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 10000000,\n        \"priorityClassName\": \"priority-class-apps\",\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-h5fbz\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-29T03:13:56Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-29T03:13:58Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-29T03:13:58Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-29T03:13:56Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://a62af2056681bf7b27955ae91c59937ce48818b06c78b1030d6e53e46902ab20\",\n                \"image\": \"registry-jinan-lab.inspurcloud.cn/library/cke/httpd-arm64:2.4.38-2\",\n                \"imageID\": \"docker-pullable://registry-jinan-lab.inspurcloud.cn/library/cke/httpd-arm64@sha256:a4f0279e65f8d239ea154d08f0572c5419f43e4ed748bee8a1a1b762d0d36893\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-01-29T03:13:57Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.122.245\",\n        \"phase\": \"Running\",\n        \"podIP\": \"100.101.49.180\",\n        \"podIPs\": [\n            {\n                \"ip\": \"100.101.49.180\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-01-29T03:13:56Z\"\n    }\n}\n"
STEP: replace the image in the pod 01/29/23 03:14:01.446
Jan 29 03:14:01.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-1653 replace -f -'
Jan 29 03:14:02.505: INFO: stderr: ""
Jan 29 03:14:02.505: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-2 01/29/23 03:14:02.505
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1736
Jan 29 03:14:02.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-1653 delete pods e2e-test-httpd-pod'
Jan 29 03:14:04.551: INFO: stderr: ""
Jan 29 03:14:04.551: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 29 03:14:04.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1653" for this suite. 01/29/23 03:14:04.572
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","completed":66,"skipped":1125,"failed":0}
------------------------------
• [SLOW TEST] [8.474 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1729
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1745

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:13:56.113
    Jan 29 03:13:56.113: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename kubectl 01/29/23 03:13:56.115
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:13:56.148
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:13:56.154
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1732
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1745
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/29/23 03:13:56.161
    Jan 29 03:13:56.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-1653 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Jan 29 03:13:56.278: INFO: stderr: ""
    Jan 29 03:13:56.278: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 01/29/23 03:13:56.278
    STEP: verifying the pod e2e-test-httpd-pod was created 01/29/23 03:14:01.331
    Jan 29 03:14:01.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-1653 get pod e2e-test-httpd-pod -o json'
    Jan 29 03:14:01.445: INFO: stderr: ""
    Jan 29 03:14:01.445: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"k8s.v1.cni.cncf.io/network-status\": \"[{\\n    \\\"name\\\": \\\"\\\",\\n    \\\"ips\\\": [\\n        \\\"100.101.49.180\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"k8s.v1.cni.cncf.io/networks-status\": \"[{\\n    \\\"name\\\": \\\"\\\",\\n    \\\"ips\\\": [\\n        \\\"100.101.49.180\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\"\n        },\n        \"creationTimestamp\": \"2023-01-29T03:13:56Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-1653\",\n        \"resourceVersion\": \"5949376\",\n        \"uid\": \"611aa2d6-727c-414c-8e26-2842f5639029\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-h5fbz\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"slave2\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 10000000,\n        \"priorityClassName\": \"priority-class-apps\",\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-h5fbz\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-29T03:13:56Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-29T03:13:58Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-29T03:13:58Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-29T03:13:56Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://a62af2056681bf7b27955ae91c59937ce48818b06c78b1030d6e53e46902ab20\",\n                \"image\": \"registry-jinan-lab.inspurcloud.cn/library/cke/httpd-arm64:2.4.38-2\",\n                \"imageID\": \"docker-pullable://registry-jinan-lab.inspurcloud.cn/library/cke/httpd-arm64@sha256:a4f0279e65f8d239ea154d08f0572c5419f43e4ed748bee8a1a1b762d0d36893\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-01-29T03:13:57Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.122.245\",\n        \"phase\": \"Running\",\n        \"podIP\": \"100.101.49.180\",\n        \"podIPs\": [\n            {\n                \"ip\": \"100.101.49.180\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-01-29T03:13:56Z\"\n    }\n}\n"
    STEP: replace the image in the pod 01/29/23 03:14:01.446
    Jan 29 03:14:01.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-1653 replace -f -'
    Jan 29 03:14:02.505: INFO: stderr: ""
    Jan 29 03:14:02.505: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-2 01/29/23 03:14:02.505
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1736
    Jan 29 03:14:02.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-1653 delete pods e2e-test-httpd-pod'
    Jan 29 03:14:04.551: INFO: stderr: ""
    Jan 29 03:14:04.551: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 29 03:14:04.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-1653" for this suite. 01/29/23 03:14:04.572
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:14:04.593
Jan 29 03:14:04.593: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename conformance-tests 01/29/23 03:14:04.595
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:14:04.638
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:14:04.644
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 01/29/23 03:14:04.65
Jan 29 03:14:04.650: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:187
Jan 29 03:14:04.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "conformance-tests-9691" for this suite. 01/29/23 03:14:04.676
{"msg":"PASSED [sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]","completed":67,"skipped":1166,"failed":0}
------------------------------
• [0.095 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:14:04.593
    Jan 29 03:14:04.593: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename conformance-tests 01/29/23 03:14:04.595
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:14:04.638
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:14:04.644
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 01/29/23 03:14:04.65
    Jan 29 03:14:04.650: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:187
    Jan 29 03:14:04.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "conformance-tests-9691" for this suite. 01/29/23 03:14:04.676
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:14:04.692
Jan 29 03:14:04.692: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename replication-controller 01/29/23 03:14:04.694
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:14:04.733
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:14:04.739
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
STEP: Creating replication controller my-hostname-basic-c64099a8-54c9-4c6a-9bbd-f8b3f403745e 01/29/23 03:14:04.744
Jan 29 03:14:04.766: INFO: Pod name my-hostname-basic-c64099a8-54c9-4c6a-9bbd-f8b3f403745e: Found 0 pods out of 1
Jan 29 03:14:09.775: INFO: Pod name my-hostname-basic-c64099a8-54c9-4c6a-9bbd-f8b3f403745e: Found 1 pods out of 1
Jan 29 03:14:09.775: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-c64099a8-54c9-4c6a-9bbd-f8b3f403745e" are running
Jan 29 03:14:09.775: INFO: Waiting up to 5m0s for pod "my-hostname-basic-c64099a8-54c9-4c6a-9bbd-f8b3f403745e-9bhh9" in namespace "replication-controller-1656" to be "running"
Jan 29 03:14:09.781: INFO: Pod "my-hostname-basic-c64099a8-54c9-4c6a-9bbd-f8b3f403745e-9bhh9": Phase="Running", Reason="", readiness=true. Elapsed: 6.218044ms
Jan 29 03:14:09.781: INFO: Pod "my-hostname-basic-c64099a8-54c9-4c6a-9bbd-f8b3f403745e-9bhh9" satisfied condition "running"
Jan 29 03:14:09.781: INFO: Pod "my-hostname-basic-c64099a8-54c9-4c6a-9bbd-f8b3f403745e-9bhh9" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-29 03:14:04 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-29 03:14:06 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-29 03:14:06 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-29 03:14:04 +0000 UTC Reason: Message:}])
Jan 29 03:14:09.781: INFO: Trying to dial the pod
Jan 29 03:14:14.802: INFO: Controller my-hostname-basic-c64099a8-54c9-4c6a-9bbd-f8b3f403745e: Got expected result from replica 1 [my-hostname-basic-c64099a8-54c9-4c6a-9bbd-f8b3f403745e-9bhh9]: "my-hostname-basic-c64099a8-54c9-4c6a-9bbd-f8b3f403745e-9bhh9", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jan 29 03:14:14.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1656" for this suite. 01/29/23 03:14:14.812
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","completed":68,"skipped":1214,"failed":0}
------------------------------
• [SLOW TEST] [10.131 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:14:04.692
    Jan 29 03:14:04.692: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename replication-controller 01/29/23 03:14:04.694
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:14:04.733
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:14:04.739
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:66
    STEP: Creating replication controller my-hostname-basic-c64099a8-54c9-4c6a-9bbd-f8b3f403745e 01/29/23 03:14:04.744
    Jan 29 03:14:04.766: INFO: Pod name my-hostname-basic-c64099a8-54c9-4c6a-9bbd-f8b3f403745e: Found 0 pods out of 1
    Jan 29 03:14:09.775: INFO: Pod name my-hostname-basic-c64099a8-54c9-4c6a-9bbd-f8b3f403745e: Found 1 pods out of 1
    Jan 29 03:14:09.775: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-c64099a8-54c9-4c6a-9bbd-f8b3f403745e" are running
    Jan 29 03:14:09.775: INFO: Waiting up to 5m0s for pod "my-hostname-basic-c64099a8-54c9-4c6a-9bbd-f8b3f403745e-9bhh9" in namespace "replication-controller-1656" to be "running"
    Jan 29 03:14:09.781: INFO: Pod "my-hostname-basic-c64099a8-54c9-4c6a-9bbd-f8b3f403745e-9bhh9": Phase="Running", Reason="", readiness=true. Elapsed: 6.218044ms
    Jan 29 03:14:09.781: INFO: Pod "my-hostname-basic-c64099a8-54c9-4c6a-9bbd-f8b3f403745e-9bhh9" satisfied condition "running"
    Jan 29 03:14:09.781: INFO: Pod "my-hostname-basic-c64099a8-54c9-4c6a-9bbd-f8b3f403745e-9bhh9" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-29 03:14:04 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-29 03:14:06 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-29 03:14:06 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-29 03:14:04 +0000 UTC Reason: Message:}])
    Jan 29 03:14:09.781: INFO: Trying to dial the pod
    Jan 29 03:14:14.802: INFO: Controller my-hostname-basic-c64099a8-54c9-4c6a-9bbd-f8b3f403745e: Got expected result from replica 1 [my-hostname-basic-c64099a8-54c9-4c6a-9bbd-f8b3f403745e-9bhh9]: "my-hostname-basic-c64099a8-54c9-4c6a-9bbd-f8b3f403745e-9bhh9", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jan 29 03:14:14.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-1656" for this suite. 01/29/23 03:14:14.812
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:14:14.83
Jan 29 03:14:14.830: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename init-container 01/29/23 03:14:14.832
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:14:14.86
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:14:14.866
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
STEP: creating the pod 01/29/23 03:14:14.871
Jan 29 03:14:14.872: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Jan 29 03:14:20.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3970" for this suite. 01/29/23 03:14:20.064
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","completed":69,"skipped":1271,"failed":0}
------------------------------
• [SLOW TEST] [5.247 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:14:14.83
    Jan 29 03:14:14.830: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename init-container 01/29/23 03:14:14.832
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:14:14.86
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:14:14.866
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:457
    STEP: creating the pod 01/29/23 03:14:14.871
    Jan 29 03:14:14.872: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan 29 03:14:20.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-3970" for this suite. 01/29/23 03:14:20.064
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:14:20.078
Jan 29 03:14:20.078: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename emptydir 01/29/23 03:14:20.079
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:14:20.113
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:14:20.119
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
STEP: Creating a pod to test emptydir volume type on tmpfs 01/29/23 03:14:20.125
Jan 29 03:14:20.144: INFO: Waiting up to 5m0s for pod "pod-0f5dc824-ca98-42ba-9690-056e1af9de61" in namespace "emptydir-191" to be "Succeeded or Failed"
Jan 29 03:14:20.151: INFO: Pod "pod-0f5dc824-ca98-42ba-9690-056e1af9de61": Phase="Pending", Reason="", readiness=false. Elapsed: 6.683006ms
Jan 29 03:14:22.160: INFO: Pod "pod-0f5dc824-ca98-42ba-9690-056e1af9de61": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015007858s
Jan 29 03:14:24.160: INFO: Pod "pod-0f5dc824-ca98-42ba-9690-056e1af9de61": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014957851s
Jan 29 03:14:26.159: INFO: Pod "pod-0f5dc824-ca98-42ba-9690-056e1af9de61": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014266999s
STEP: Saw pod success 01/29/23 03:14:26.159
Jan 29 03:14:26.159: INFO: Pod "pod-0f5dc824-ca98-42ba-9690-056e1af9de61" satisfied condition "Succeeded or Failed"
Jan 29 03:14:26.165: INFO: Trying to get logs from node slave2 pod pod-0f5dc824-ca98-42ba-9690-056e1af9de61 container test-container: <nil>
STEP: delete the pod 01/29/23 03:14:26.18
Jan 29 03:14:26.269: INFO: Waiting for pod pod-0f5dc824-ca98-42ba-9690-056e1af9de61 to disappear
Jan 29 03:14:26.275: INFO: Pod pod-0f5dc824-ca98-42ba-9690-056e1af9de61 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 29 03:14:26.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-191" for this suite. 01/29/23 03:14:26.284
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":70,"skipped":1282,"failed":0}
------------------------------
• [SLOW TEST] [6.217 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:14:20.078
    Jan 29 03:14:20.078: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename emptydir 01/29/23 03:14:20.079
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:14:20.113
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:14:20.119
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:86
    STEP: Creating a pod to test emptydir volume type on tmpfs 01/29/23 03:14:20.125
    Jan 29 03:14:20.144: INFO: Waiting up to 5m0s for pod "pod-0f5dc824-ca98-42ba-9690-056e1af9de61" in namespace "emptydir-191" to be "Succeeded or Failed"
    Jan 29 03:14:20.151: INFO: Pod "pod-0f5dc824-ca98-42ba-9690-056e1af9de61": Phase="Pending", Reason="", readiness=false. Elapsed: 6.683006ms
    Jan 29 03:14:22.160: INFO: Pod "pod-0f5dc824-ca98-42ba-9690-056e1af9de61": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015007858s
    Jan 29 03:14:24.160: INFO: Pod "pod-0f5dc824-ca98-42ba-9690-056e1af9de61": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014957851s
    Jan 29 03:14:26.159: INFO: Pod "pod-0f5dc824-ca98-42ba-9690-056e1af9de61": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014266999s
    STEP: Saw pod success 01/29/23 03:14:26.159
    Jan 29 03:14:26.159: INFO: Pod "pod-0f5dc824-ca98-42ba-9690-056e1af9de61" satisfied condition "Succeeded or Failed"
    Jan 29 03:14:26.165: INFO: Trying to get logs from node slave2 pod pod-0f5dc824-ca98-42ba-9690-056e1af9de61 container test-container: <nil>
    STEP: delete the pod 01/29/23 03:14:26.18
    Jan 29 03:14:26.269: INFO: Waiting for pod pod-0f5dc824-ca98-42ba-9690-056e1af9de61 to disappear
    Jan 29 03:14:26.275: INFO: Pod pod-0f5dc824-ca98-42ba-9690-056e1af9de61 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 29 03:14:26.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-191" for this suite. 01/29/23 03:14:26.284
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:14:26.297
Jan 29 03:14:26.297: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename sysctl 01/29/23 03:14:26.298
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:14:26.327
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:14:26.333
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 01/29/23 03:14:26.338
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
Jan 29 03:14:26.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-8671" for this suite. 01/29/23 03:14:26.359
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":71,"skipped":1310,"failed":0}
------------------------------
• [0.073 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:14:26.297
    Jan 29 03:14:26.297: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename sysctl 01/29/23 03:14:26.298
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:14:26.327
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:14:26.333
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 01/29/23 03:14:26.338
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan 29 03:14:26.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-8671" for this suite. 01/29/23 03:14:26.359
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:14:26.37
Jan 29 03:14:26.370: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename webhook 01/29/23 03:14:26.372
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:14:26.405
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:14:26.412
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/29/23 03:14:26.443
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/29/23 03:14:29.228
STEP: Deploying the webhook pod 01/29/23 03:14:29.255
STEP: Wait for the deployment to be ready 01/29/23 03:14:29.276
Jan 29 03:14:29.290: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/29/23 03:14:31.311
STEP: Verifying the service has paired with the endpoint 01/29/23 03:14:31.329
Jan 29 03:14:32.330: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 01/29/23 03:14:32.337
STEP: create a configmap that should be updated by the webhook 01/29/23 03:14:32.363
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 29 03:14:32.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-567" for this suite. 01/29/23 03:14:32.403
STEP: Destroying namespace "webhook-567-markers" for this suite. 01/29/23 03:14:32.412
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","completed":72,"skipped":1313,"failed":0}
------------------------------
• [SLOW TEST] [6.131 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:14:26.37
    Jan 29 03:14:26.370: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename webhook 01/29/23 03:14:26.372
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:14:26.405
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:14:26.412
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/29/23 03:14:26.443
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/29/23 03:14:29.228
    STEP: Deploying the webhook pod 01/29/23 03:14:29.255
    STEP: Wait for the deployment to be ready 01/29/23 03:14:29.276
    Jan 29 03:14:29.290: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/29/23 03:14:31.311
    STEP: Verifying the service has paired with the endpoint 01/29/23 03:14:31.329
    Jan 29 03:14:32.330: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:251
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 01/29/23 03:14:32.337
    STEP: create a configmap that should be updated by the webhook 01/29/23 03:14:32.363
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 29 03:14:32.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-567" for this suite. 01/29/23 03:14:32.403
    STEP: Destroying namespace "webhook-567-markers" for this suite. 01/29/23 03:14:32.412
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:14:32.508
Jan 29 03:14:32.508: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename secrets 01/29/23 03:14:32.509
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:14:32.551
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:14:32.557
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
STEP: creating secret secrets-7546/secret-test-54d77016-ed83-40a0-8c3e-2d1192e55f2e 01/29/23 03:14:32.562
STEP: Creating a pod to test consume secrets 01/29/23 03:14:32.57
Jan 29 03:14:32.588: INFO: Waiting up to 5m0s for pod "pod-configmaps-91fd7e49-14da-40a4-bb14-da389e508372" in namespace "secrets-7546" to be "Succeeded or Failed"
Jan 29 03:14:32.596: INFO: Pod "pod-configmaps-91fd7e49-14da-40a4-bb14-da389e508372": Phase="Pending", Reason="", readiness=false. Elapsed: 8.640861ms
Jan 29 03:14:34.605: INFO: Pod "pod-configmaps-91fd7e49-14da-40a4-bb14-da389e508372": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016727091s
Jan 29 03:14:36.604: INFO: Pod "pod-configmaps-91fd7e49-14da-40a4-bb14-da389e508372": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016480742s
Jan 29 03:14:38.603: INFO: Pod "pod-configmaps-91fd7e49-14da-40a4-bb14-da389e508372": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015213687s
STEP: Saw pod success 01/29/23 03:14:38.603
Jan 29 03:14:38.603: INFO: Pod "pod-configmaps-91fd7e49-14da-40a4-bb14-da389e508372" satisfied condition "Succeeded or Failed"
Jan 29 03:14:38.609: INFO: Trying to get logs from node slave2 pod pod-configmaps-91fd7e49-14da-40a4-bb14-da389e508372 container env-test: <nil>
STEP: delete the pod 01/29/23 03:14:38.624
Jan 29 03:14:38.692: INFO: Waiting for pod pod-configmaps-91fd7e49-14da-40a4-bb14-da389e508372 to disappear
Jan 29 03:14:38.697: INFO: Pod pod-configmaps-91fd7e49-14da-40a4-bb14-da389e508372 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Jan 29 03:14:38.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7546" for this suite. 01/29/23 03:14:38.707
{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","completed":73,"skipped":1317,"failed":0}
------------------------------
• [SLOW TEST] [6.213 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:14:32.508
    Jan 29 03:14:32.508: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename secrets 01/29/23 03:14:32.509
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:14:32.551
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:14:32.557
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:94
    STEP: creating secret secrets-7546/secret-test-54d77016-ed83-40a0-8c3e-2d1192e55f2e 01/29/23 03:14:32.562
    STEP: Creating a pod to test consume secrets 01/29/23 03:14:32.57
    Jan 29 03:14:32.588: INFO: Waiting up to 5m0s for pod "pod-configmaps-91fd7e49-14da-40a4-bb14-da389e508372" in namespace "secrets-7546" to be "Succeeded or Failed"
    Jan 29 03:14:32.596: INFO: Pod "pod-configmaps-91fd7e49-14da-40a4-bb14-da389e508372": Phase="Pending", Reason="", readiness=false. Elapsed: 8.640861ms
    Jan 29 03:14:34.605: INFO: Pod "pod-configmaps-91fd7e49-14da-40a4-bb14-da389e508372": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016727091s
    Jan 29 03:14:36.604: INFO: Pod "pod-configmaps-91fd7e49-14da-40a4-bb14-da389e508372": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016480742s
    Jan 29 03:14:38.603: INFO: Pod "pod-configmaps-91fd7e49-14da-40a4-bb14-da389e508372": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015213687s
    STEP: Saw pod success 01/29/23 03:14:38.603
    Jan 29 03:14:38.603: INFO: Pod "pod-configmaps-91fd7e49-14da-40a4-bb14-da389e508372" satisfied condition "Succeeded or Failed"
    Jan 29 03:14:38.609: INFO: Trying to get logs from node slave2 pod pod-configmaps-91fd7e49-14da-40a4-bb14-da389e508372 container env-test: <nil>
    STEP: delete the pod 01/29/23 03:14:38.624
    Jan 29 03:14:38.692: INFO: Waiting for pod pod-configmaps-91fd7e49-14da-40a4-bb14-da389e508372 to disappear
    Jan 29 03:14:38.697: INFO: Pod pod-configmaps-91fd7e49-14da-40a4-bb14-da389e508372 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Jan 29 03:14:38.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-7546" for this suite. 01/29/23 03:14:38.707
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:14:38.721
Jan 29 03:14:38.721: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename services 01/29/23 03:14:38.723
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:14:38.752
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:14:38.758
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 29 03:14:38.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6419" for this suite. 01/29/23 03:14:38.778
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","completed":74,"skipped":1323,"failed":0}
------------------------------
• [0.068 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:14:38.721
    Jan 29 03:14:38.721: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename services 01/29/23 03:14:38.723
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:14:38.752
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:14:38.758
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:781
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 29 03:14:38.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-6419" for this suite. 01/29/23 03:14:38.778
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:14:38.79
Jan 29 03:14:38.790: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename webhook 01/29/23 03:14:38.792
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:14:38.826
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:14:38.832
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/29/23 03:14:38.867
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/29/23 03:14:39.86
STEP: Deploying the webhook pod 01/29/23 03:14:39.868
STEP: Wait for the deployment to be ready 01/29/23 03:14:39.889
Jan 29 03:14:39.901: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jan 29 03:14:41.923: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 29, 3, 14, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 3, 14, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 29, 3, 14, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 3, 14, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/29/23 03:14:43.93
STEP: Verifying the service has paired with the endpoint 01/29/23 03:14:43.952
Jan 29 03:14:44.952: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
STEP: Setting timeout (1s) shorter than webhook latency (5s) 01/29/23 03:14:44.962
STEP: Registering slow webhook via the AdmissionRegistration API 01/29/23 03:14:44.962
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 01/29/23 03:14:44.988
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 01/29/23 03:14:46.007
STEP: Registering slow webhook via the AdmissionRegistration API 01/29/23 03:14:46.007
STEP: Having no error when timeout is longer than webhook latency 01/29/23 03:14:47.056
STEP: Registering slow webhook via the AdmissionRegistration API 01/29/23 03:14:47.056
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 01/29/23 03:14:52.112
STEP: Registering slow webhook via the AdmissionRegistration API 01/29/23 03:14:52.112
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 29 03:14:57.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6404" for this suite. 01/29/23 03:14:57.192
STEP: Destroying namespace "webhook-6404-markers" for this suite. 01/29/23 03:14:57.203
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","completed":75,"skipped":1333,"failed":0}
------------------------------
• [SLOW TEST] [18.527 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:14:38.79
    Jan 29 03:14:38.790: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename webhook 01/29/23 03:14:38.792
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:14:38.826
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:14:38.832
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/29/23 03:14:38.867
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/29/23 03:14:39.86
    STEP: Deploying the webhook pod 01/29/23 03:14:39.868
    STEP: Wait for the deployment to be ready 01/29/23 03:14:39.889
    Jan 29 03:14:39.901: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    Jan 29 03:14:41.923: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 29, 3, 14, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 3, 14, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 29, 3, 14, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 3, 14, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/29/23 03:14:43.93
    STEP: Verifying the service has paired with the endpoint 01/29/23 03:14:43.952
    Jan 29 03:14:44.952: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:380
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 01/29/23 03:14:44.962
    STEP: Registering slow webhook via the AdmissionRegistration API 01/29/23 03:14:44.962
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 01/29/23 03:14:44.988
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 01/29/23 03:14:46.007
    STEP: Registering slow webhook via the AdmissionRegistration API 01/29/23 03:14:46.007
    STEP: Having no error when timeout is longer than webhook latency 01/29/23 03:14:47.056
    STEP: Registering slow webhook via the AdmissionRegistration API 01/29/23 03:14:47.056
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 01/29/23 03:14:52.112
    STEP: Registering slow webhook via the AdmissionRegistration API 01/29/23 03:14:52.112
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 29 03:14:57.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-6404" for this suite. 01/29/23 03:14:57.192
    STEP: Destroying namespace "webhook-6404-markers" for this suite. 01/29/23 03:14:57.203
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:14:57.318
Jan 29 03:14:57.318: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename emptydir 01/29/23 03:14:57.32
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:14:57.376
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:14:57.383
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
STEP: Creating a pod to test emptydir 0777 on tmpfs 01/29/23 03:14:57.392
Jan 29 03:14:57.418: INFO: Waiting up to 5m0s for pod "pod-1de2388e-776d-4631-ae85-c703e3543f4f" in namespace "emptydir-6778" to be "Succeeded or Failed"
Jan 29 03:14:57.429: INFO: Pod "pod-1de2388e-776d-4631-ae85-c703e3543f4f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.981317ms
Jan 29 03:14:59.436: INFO: Pod "pod-1de2388e-776d-4631-ae85-c703e3543f4f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017675737s
Jan 29 03:15:01.438: INFO: Pod "pod-1de2388e-776d-4631-ae85-c703e3543f4f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020139207s
STEP: Saw pod success 01/29/23 03:15:01.438
Jan 29 03:15:01.438: INFO: Pod "pod-1de2388e-776d-4631-ae85-c703e3543f4f" satisfied condition "Succeeded or Failed"
Jan 29 03:15:01.444: INFO: Trying to get logs from node slave2 pod pod-1de2388e-776d-4631-ae85-c703e3543f4f container test-container: <nil>
STEP: delete the pod 01/29/23 03:15:01.46
Jan 29 03:15:01.559: INFO: Waiting for pod pod-1de2388e-776d-4631-ae85-c703e3543f4f to disappear
Jan 29 03:15:01.566: INFO: Pod pod-1de2388e-776d-4631-ae85-c703e3543f4f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 29 03:15:01.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6778" for this suite. 01/29/23 03:15:01.578
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":76,"skipped":1336,"failed":0}
------------------------------
• [4.276 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:14:57.318
    Jan 29 03:14:57.318: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename emptydir 01/29/23 03:14:57.32
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:14:57.376
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:14:57.383
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:146
    STEP: Creating a pod to test emptydir 0777 on tmpfs 01/29/23 03:14:57.392
    Jan 29 03:14:57.418: INFO: Waiting up to 5m0s for pod "pod-1de2388e-776d-4631-ae85-c703e3543f4f" in namespace "emptydir-6778" to be "Succeeded or Failed"
    Jan 29 03:14:57.429: INFO: Pod "pod-1de2388e-776d-4631-ae85-c703e3543f4f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.981317ms
    Jan 29 03:14:59.436: INFO: Pod "pod-1de2388e-776d-4631-ae85-c703e3543f4f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017675737s
    Jan 29 03:15:01.438: INFO: Pod "pod-1de2388e-776d-4631-ae85-c703e3543f4f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020139207s
    STEP: Saw pod success 01/29/23 03:15:01.438
    Jan 29 03:15:01.438: INFO: Pod "pod-1de2388e-776d-4631-ae85-c703e3543f4f" satisfied condition "Succeeded or Failed"
    Jan 29 03:15:01.444: INFO: Trying to get logs from node slave2 pod pod-1de2388e-776d-4631-ae85-c703e3543f4f container test-container: <nil>
    STEP: delete the pod 01/29/23 03:15:01.46
    Jan 29 03:15:01.559: INFO: Waiting for pod pod-1de2388e-776d-4631-ae85-c703e3543f4f to disappear
    Jan 29 03:15:01.566: INFO: Pod pod-1de2388e-776d-4631-ae85-c703e3543f4f no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 29 03:15:01.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-6778" for this suite. 01/29/23 03:15:01.578
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:15:01.597
Jan 29 03:15:01.597: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename crd-publish-openapi 01/29/23 03:15:01.598
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:15:01.63
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:15:01.647
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
Jan 29 03:15:01.654: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/29/23 03:15:06.581
Jan 29 03:15:06.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=crd-publish-openapi-9264 --namespace=crd-publish-openapi-9264 create -f -'
Jan 29 03:15:07.831: INFO: stderr: ""
Jan 29 03:15:07.831: INFO: stdout: "e2e-test-crd-publish-openapi-8966-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jan 29 03:15:07.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=crd-publish-openapi-9264 --namespace=crd-publish-openapi-9264 delete e2e-test-crd-publish-openapi-8966-crds test-cr'
Jan 29 03:15:07.957: INFO: stderr: ""
Jan 29 03:15:07.957: INFO: stdout: "e2e-test-crd-publish-openapi-8966-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Jan 29 03:15:07.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=crd-publish-openapi-9264 --namespace=crd-publish-openapi-9264 apply -f -'
Jan 29 03:15:09.261: INFO: stderr: ""
Jan 29 03:15:09.261: INFO: stdout: "e2e-test-crd-publish-openapi-8966-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jan 29 03:15:09.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=crd-publish-openapi-9264 --namespace=crd-publish-openapi-9264 delete e2e-test-crd-publish-openapi-8966-crds test-cr'
Jan 29 03:15:09.445: INFO: stderr: ""
Jan 29 03:15:09.445: INFO: stdout: "e2e-test-crd-publish-openapi-8966-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 01/29/23 03:15:09.445
Jan 29 03:15:09.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=crd-publish-openapi-9264 explain e2e-test-crd-publish-openapi-8966-crds'
Jan 29 03:15:10.513: INFO: stderr: ""
Jan 29 03:15:10.513: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8966-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 29 03:15:17.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9264" for this suite. 01/29/23 03:15:17.769
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","completed":77,"skipped":1371,"failed":0}
------------------------------
• [SLOW TEST] [16.192 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:15:01.597
    Jan 29 03:15:01.597: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename crd-publish-openapi 01/29/23 03:15:01.598
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:15:01.63
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:15:01.647
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:235
    Jan 29 03:15:01.654: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/29/23 03:15:06.581
    Jan 29 03:15:06.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=crd-publish-openapi-9264 --namespace=crd-publish-openapi-9264 create -f -'
    Jan 29 03:15:07.831: INFO: stderr: ""
    Jan 29 03:15:07.831: INFO: stdout: "e2e-test-crd-publish-openapi-8966-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Jan 29 03:15:07.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=crd-publish-openapi-9264 --namespace=crd-publish-openapi-9264 delete e2e-test-crd-publish-openapi-8966-crds test-cr'
    Jan 29 03:15:07.957: INFO: stderr: ""
    Jan 29 03:15:07.957: INFO: stdout: "e2e-test-crd-publish-openapi-8966-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Jan 29 03:15:07.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=crd-publish-openapi-9264 --namespace=crd-publish-openapi-9264 apply -f -'
    Jan 29 03:15:09.261: INFO: stderr: ""
    Jan 29 03:15:09.261: INFO: stdout: "e2e-test-crd-publish-openapi-8966-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Jan 29 03:15:09.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=crd-publish-openapi-9264 --namespace=crd-publish-openapi-9264 delete e2e-test-crd-publish-openapi-8966-crds test-cr'
    Jan 29 03:15:09.445: INFO: stderr: ""
    Jan 29 03:15:09.445: INFO: stdout: "e2e-test-crd-publish-openapi-8966-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 01/29/23 03:15:09.445
    Jan 29 03:15:09.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=crd-publish-openapi-9264 explain e2e-test-crd-publish-openapi-8966-crds'
    Jan 29 03:15:10.513: INFO: stderr: ""
    Jan 29 03:15:10.513: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8966-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 29 03:15:17.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-9264" for this suite. 01/29/23 03:15:17.769
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:15:17.789
Jan 29 03:15:17.790: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename downward-api 01/29/23 03:15:17.791
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:15:17.821
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:15:17.827
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
STEP: Creating a pod to test downward api env vars 01/29/23 03:15:17.832
Jan 29 03:15:17.852: INFO: Waiting up to 5m0s for pod "downward-api-99ec5315-3fa4-405b-bade-33be6696fb5f" in namespace "downward-api-3056" to be "Succeeded or Failed"
Jan 29 03:15:17.858: INFO: Pod "downward-api-99ec5315-3fa4-405b-bade-33be6696fb5f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008622ms
Jan 29 03:15:19.869: INFO: Pod "downward-api-99ec5315-3fa4-405b-bade-33be6696fb5f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016618569s
Jan 29 03:15:21.870: INFO: Pod "downward-api-99ec5315-3fa4-405b-bade-33be6696fb5f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017846171s
Jan 29 03:15:23.867: INFO: Pod "downward-api-99ec5315-3fa4-405b-bade-33be6696fb5f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014454721s
STEP: Saw pod success 01/29/23 03:15:23.867
Jan 29 03:15:23.867: INFO: Pod "downward-api-99ec5315-3fa4-405b-bade-33be6696fb5f" satisfied condition "Succeeded or Failed"
Jan 29 03:15:23.873: INFO: Trying to get logs from node slave2 pod downward-api-99ec5315-3fa4-405b-bade-33be6696fb5f container dapi-container: <nil>
STEP: delete the pod 01/29/23 03:15:23.89
Jan 29 03:15:23.987: INFO: Waiting for pod downward-api-99ec5315-3fa4-405b-bade-33be6696fb5f to disappear
Jan 29 03:15:23.994: INFO: Pod downward-api-99ec5315-3fa4-405b-bade-33be6696fb5f no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jan 29 03:15:23.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3056" for this suite. 01/29/23 03:15:24.003
{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","completed":78,"skipped":1381,"failed":0}
------------------------------
• [SLOW TEST] [6.226 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:15:17.789
    Jan 29 03:15:17.790: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename downward-api 01/29/23 03:15:17.791
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:15:17.821
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:15:17.827
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:165
    STEP: Creating a pod to test downward api env vars 01/29/23 03:15:17.832
    Jan 29 03:15:17.852: INFO: Waiting up to 5m0s for pod "downward-api-99ec5315-3fa4-405b-bade-33be6696fb5f" in namespace "downward-api-3056" to be "Succeeded or Failed"
    Jan 29 03:15:17.858: INFO: Pod "downward-api-99ec5315-3fa4-405b-bade-33be6696fb5f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008622ms
    Jan 29 03:15:19.869: INFO: Pod "downward-api-99ec5315-3fa4-405b-bade-33be6696fb5f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016618569s
    Jan 29 03:15:21.870: INFO: Pod "downward-api-99ec5315-3fa4-405b-bade-33be6696fb5f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017846171s
    Jan 29 03:15:23.867: INFO: Pod "downward-api-99ec5315-3fa4-405b-bade-33be6696fb5f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014454721s
    STEP: Saw pod success 01/29/23 03:15:23.867
    Jan 29 03:15:23.867: INFO: Pod "downward-api-99ec5315-3fa4-405b-bade-33be6696fb5f" satisfied condition "Succeeded or Failed"
    Jan 29 03:15:23.873: INFO: Trying to get logs from node slave2 pod downward-api-99ec5315-3fa4-405b-bade-33be6696fb5f container dapi-container: <nil>
    STEP: delete the pod 01/29/23 03:15:23.89
    Jan 29 03:15:23.987: INFO: Waiting for pod downward-api-99ec5315-3fa4-405b-bade-33be6696fb5f to disappear
    Jan 29 03:15:23.994: INFO: Pod downward-api-99ec5315-3fa4-405b-bade-33be6696fb5f no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jan 29 03:15:23.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-3056" for this suite. 01/29/23 03:15:24.003
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:15:24.019
Jan 29 03:15:24.019: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename deployment 01/29/23 03:15:24.02
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:15:24.059
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:15:24.065
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Jan 29 03:15:24.071: INFO: Creating deployment "webserver-deployment"
Jan 29 03:15:24.084: INFO: Waiting for observed generation 1
Jan 29 03:15:26.101: INFO: Waiting for all required pods to come up
Jan 29 03:15:26.112: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 01/29/23 03:15:26.112
Jan 29 03:15:26.112: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-296rm" in namespace "deployment-6385" to be "running"
Jan 29 03:15:26.113: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-nkr4j" in namespace "deployment-6385" to be "running"
Jan 29 03:15:26.113: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-4hgdz" in namespace "deployment-6385" to be "running"
Jan 29 03:15:26.113: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-7vw4s" in namespace "deployment-6385" to be "running"
Jan 29 03:15:26.113: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-qhfwk" in namespace "deployment-6385" to be "running"
Jan 29 03:15:26.113: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-gvkms" in namespace "deployment-6385" to be "running"
Jan 29 03:15:26.113: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-w6pj7" in namespace "deployment-6385" to be "running"
Jan 29 03:15:26.113: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-x8pxn" in namespace "deployment-6385" to be "running"
Jan 29 03:15:26.114: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-xgmh8" in namespace "deployment-6385" to be "running"
Jan 29 03:15:26.126: INFO: Pod "webserver-deployment-845c8977d9-296rm": Phase="Pending", Reason="", readiness=false. Elapsed: 13.454254ms
Jan 29 03:15:26.134: INFO: Pod "webserver-deployment-845c8977d9-x8pxn": Phase="Pending", Reason="", readiness=false. Elapsed: 20.576524ms
Jan 29 03:15:26.134: INFO: Pod "webserver-deployment-845c8977d9-xgmh8": Phase="Pending", Reason="", readiness=false. Elapsed: 20.586744ms
Jan 29 03:15:26.134: INFO: Pod "webserver-deployment-845c8977d9-w6pj7": Phase="Pending", Reason="", readiness=false. Elapsed: 20.900126ms
Jan 29 03:15:26.135: INFO: Pod "webserver-deployment-845c8977d9-4hgdz": Phase="Pending", Reason="", readiness=false. Elapsed: 22.100935ms
Jan 29 03:15:26.135: INFO: Pod "webserver-deployment-845c8977d9-gvkms": Phase="Pending", Reason="", readiness=false. Elapsed: 21.567851ms
Jan 29 03:15:26.135: INFO: Pod "webserver-deployment-845c8977d9-qhfwk": Phase="Pending", Reason="", readiness=false. Elapsed: 21.876273ms
Jan 29 03:15:26.136: INFO: Pod "webserver-deployment-845c8977d9-7vw4s": Phase="Pending", Reason="", readiness=false. Elapsed: 22.994921ms
Jan 29 03:15:26.136: INFO: Pod "webserver-deployment-845c8977d9-nkr4j": Phase="Pending", Reason="", readiness=false. Elapsed: 23.477605ms
Jan 29 03:15:28.133: INFO: Pod "webserver-deployment-845c8977d9-296rm": Phase="Running", Reason="", readiness=true. Elapsed: 2.020317315s
Jan 29 03:15:28.133: INFO: Pod "webserver-deployment-845c8977d9-296rm" satisfied condition "running"
Jan 29 03:15:28.143: INFO: Pod "webserver-deployment-845c8977d9-xgmh8": Phase="Running", Reason="", readiness=true. Elapsed: 2.028977336s
Jan 29 03:15:28.143: INFO: Pod "webserver-deployment-845c8977d9-xgmh8" satisfied condition "running"
Jan 29 03:15:28.145: INFO: Pod "webserver-deployment-845c8977d9-nkr4j": Phase="Running", Reason="", readiness=true. Elapsed: 2.032178999s
Jan 29 03:15:28.145: INFO: Pod "webserver-deployment-845c8977d9-gvkms": Phase="Running", Reason="", readiness=true. Elapsed: 2.031397293s
Jan 29 03:15:28.145: INFO: Pod "webserver-deployment-845c8977d9-gvkms" satisfied condition "running"
Jan 29 03:15:28.145: INFO: Pod "webserver-deployment-845c8977d9-nkr4j" satisfied condition "running"
Jan 29 03:15:28.146: INFO: Pod "webserver-deployment-845c8977d9-w6pj7": Phase="Running", Reason="", readiness=true. Elapsed: 2.032167998s
Jan 29 03:15:28.146: INFO: Pod "webserver-deployment-845c8977d9-w6pj7" satisfied condition "running"
Jan 29 03:15:28.146: INFO: Pod "webserver-deployment-845c8977d9-qhfwk": Phase="Running", Reason="", readiness=true. Elapsed: 2.032510461s
Jan 29 03:15:28.146: INFO: Pod "webserver-deployment-845c8977d9-qhfwk" satisfied condition "running"
Jan 29 03:15:28.146: INFO: Pod "webserver-deployment-845c8977d9-x8pxn": Phase="Running", Reason="", readiness=true. Elapsed: 2.032295539s
Jan 29 03:15:28.146: INFO: Pod "webserver-deployment-845c8977d9-x8pxn" satisfied condition "running"
Jan 29 03:15:28.146: INFO: Pod "webserver-deployment-845c8977d9-7vw4s": Phase="Running", Reason="", readiness=true. Elapsed: 2.033148665s
Jan 29 03:15:28.146: INFO: Pod "webserver-deployment-845c8977d9-7vw4s" satisfied condition "running"
Jan 29 03:15:28.146: INFO: Pod "webserver-deployment-845c8977d9-4hgdz": Phase="Running", Reason="", readiness=true. Elapsed: 2.033372447s
Jan 29 03:15:28.146: INFO: Pod "webserver-deployment-845c8977d9-4hgdz" satisfied condition "running"
Jan 29 03:15:28.146: INFO: Waiting for deployment "webserver-deployment" to complete
Jan 29 03:15:28.160: INFO: Updating deployment "webserver-deployment" with a non-existent image
Jan 29 03:15:28.174: INFO: Updating deployment webserver-deployment
Jan 29 03:15:28.174: INFO: Waiting for observed generation 2
Jan 29 03:15:30.189: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jan 29 03:15:30.195: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jan 29 03:15:30.201: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jan 29 03:15:30.221: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jan 29 03:15:30.221: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jan 29 03:15:30.228: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jan 29 03:15:30.241: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Jan 29 03:15:30.241: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Jan 29 03:15:30.259: INFO: Updating deployment webserver-deployment
Jan 29 03:15:30.259: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Jan 29 03:15:30.275: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jan 29 03:15:32.306: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 29 03:15:32.336: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-6385  69ddc841-e39a-44de-a3e5-5c31d9737bcc 5950583 3 2023-01-29 03:15:24 +0000 UTC <nil> <nil> map[name:httpd] map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/revision:2] [] [] []},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0x4005755e18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-29 03:15:30 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2023-01-29 03:15:30 +0000 UTC,LastTransitionTime:2023-01-29 03:15:24 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Jan 29 03:15:32.378: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-6385  bf874850-3453-4d04-874b-3ec2c847e80f 5950580 3 2023-01-29 03:15:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 69ddc841-e39a-44de-a3e5-5c31d9737bcc 0x40057a9067 0x40057a9068}] [] []},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0x40057a90d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 29 03:15:32.378: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Jan 29 03:15:32.378: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-845c8977d9  deployment-6385  f4abe03d-ea57-4def-915f-cb677f890574 5950552 3 2023-01-29 03:15:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 69ddc841-e39a-44de-a3e5-5c31d9737bcc 0x40057a9147 0x40057a9148}] [] []},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0x40057a91a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Jan 29 03:15:32.393: INFO: Pod "webserver-deployment-69b7448995-55ksv" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-55ksv webserver-deployment-69b7448995- deployment-6385  d74383bb-c78d-4518-8edb-5a4f2713cda8 5950582 0 2023-01-29 03:15:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "100.101.32.110"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "100.101.32.110"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bf874850-3453-4d04-874b-3ec2c847e80f 0x40057a9717 0x40057a9718}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wstff,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wstff,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.243,PodIP:,StartTime:2023-01-29 03:15:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: Get https://registry-1.docker.io/v2/: dial tcp: lookup registry-1.docker.io on 100.105.0.3:53: server misbehaving,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 03:15:32.394: INFO: Pod "webserver-deployment-69b7448995-5m767" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-5m767 webserver-deployment-69b7448995- deployment-6385  1fbfbd04-6a65-4f2d-83f1-9e6986a9cbac 5950702 0 2023-01-29 03:15:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bf874850-3453-4d04-874b-3ec2c847e80f 0x40057a9937 0x40057a9938}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dlzlh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dlzlh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.245,PodIP:,StartTime:2023-01-29 03:15:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 03:15:32.394: INFO: Pod "webserver-deployment-69b7448995-cr99n" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-cr99n webserver-deployment-69b7448995- deployment-6385  5830d4b8-caad-4e79-9d25-897e775551c2 5950612 0 2023-01-29 03:15:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bf874850-3453-4d04-874b-3ec2c847e80f 0x40057a9b67 0x40057a9b68}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ngnzw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ngnzw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.245,PodIP:,StartTime:2023-01-29 03:15:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 03:15:32.394: INFO: Pod "webserver-deployment-69b7448995-dctdx" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-dctdx webserver-deployment-69b7448995- deployment-6385  3a30023e-3ff3-4b4d-ac36-8b187207288a 5950458 0 2023-01-29 03:15:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "100.101.161.202"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "100.101.161.202"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bf874850-3453-4d04-874b-3ec2c847e80f 0x40057a9d87 0x40057a9d88}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l5nt9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l5nt9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.241,PodIP:,StartTime:2023-01-29 03:15:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 03:15:32.395: INFO: Pod "webserver-deployment-69b7448995-frq7q" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-frq7q webserver-deployment-69b7448995- deployment-6385  26360ce1-cdb2-4921-a661-56b8b4848a1b 5950534 0 2023-01-29 03:15:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bf874850-3453-4d04-874b-3ec2c847e80f 0x40057a9fa7 0x40057a9fa8}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9kg5d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9kg5d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.244,PodIP:,StartTime:2023-01-29 03:15:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 03:15:32.395: INFO: Pod "webserver-deployment-69b7448995-gbvzk" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-gbvzk webserver-deployment-69b7448995- deployment-6385  2b634fbe-ea2e-4026-92a0-65a051cd16b5 5950699 0 2023-01-29 03:15:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "100.101.51.37"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "100.101.51.37"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bf874850-3453-4d04-874b-3ec2c847e80f 0x40057f0167 0x40057f0168}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qjb5l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qjb5l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.244,PodIP:,StartTime:2023-01-29 03:15:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: Get https://registry-1.docker.io/v2/: dial tcp: lookup registry-1.docker.io on 100.105.0.3:53: server misbehaving,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 03:15:32.396: INFO: Pod "webserver-deployment-69b7448995-gfvlt" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-gfvlt webserver-deployment-69b7448995- deployment-6385  b0762cbc-288b-49f5-81f8-f9e573334939 5950631 0 2023-01-29 03:15:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bf874850-3453-4d04-874b-3ec2c847e80f 0x40057f0337 0x40057f0338}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ngd99,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ngd99,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.244,PodIP:,StartTime:2023-01-29 03:15:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 03:15:32.396: INFO: Pod "webserver-deployment-69b7448995-p8f4t" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-p8f4t webserver-deployment-69b7448995- deployment-6385  08c81725-159f-4def-afc8-ebf2c092074d 5950554 0 2023-01-29 03:15:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bf874850-3453-4d04-874b-3ec2c847e80f 0x40057f04f7 0x40057f04f8}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mk9js,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mk9js,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.243,PodIP:,StartTime:2023-01-29 03:15:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 03:15:32.397: INFO: Pod "webserver-deployment-69b7448995-rlsjd" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-rlsjd webserver-deployment-69b7448995- deployment-6385  5c5b6c69-d1b3-4c8d-b834-db2229f4601a 5950625 0 2023-01-29 03:15:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "100.101.49.208"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "100.101.49.208"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bf874850-3453-4d04-874b-3ec2c847e80f 0x40057f06b7 0x40057f06b8}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qfkbn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qfkbn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.245,PodIP:,StartTime:2023-01-29 03:15:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 03:15:32.397: INFO: Pod "webserver-deployment-69b7448995-thdjf" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-thdjf webserver-deployment-69b7448995- deployment-6385  414e587b-56a5-41f0-9db5-b4ac16aa5122 5950658 0 2023-01-29 03:15:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "100.101.208.222"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "100.101.208.222"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bf874850-3453-4d04-874b-3ec2c847e80f 0x40057f08b7 0x40057f08b8}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v86kh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v86kh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.242,PodIP:,StartTime:2023-01-29 03:15:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 03:15:32.398: INFO: Pod "webserver-deployment-69b7448995-vqvw7" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-vqvw7 webserver-deployment-69b7448995- deployment-6385  52e5bdf2-feec-4e5c-8aee-1171894b7ca6 5950620 0 2023-01-29 03:15:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "100.101.208.223"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "100.101.208.223"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bf874850-3453-4d04-874b-3ec2c847e80f 0x40057f0ac7 0x40057f0ac8}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9vw2h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9vw2h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.242,PodIP:100.101.208.223,StartTime:2023-01-29 03:15:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "webserver:404",},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.101.208.223,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 03:15:32.398: INFO: Pod "webserver-deployment-69b7448995-xjwvp" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-xjwvp webserver-deployment-69b7448995- deployment-6385  34ba3f4f-7ca4-4723-99be-6c633deb743f 5950585 0 2023-01-29 03:15:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "100.101.49.211"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "100.101.49.211"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bf874850-3453-4d04-874b-3ec2c847e80f 0x40057f0d17 0x40057f0d18}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m24k4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m24k4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.245,PodIP:100.101.49.211,StartTime:2023-01-29 03:15:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "webserver:404",},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.101.49.211,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 03:15:32.399: INFO: Pod "webserver-deployment-69b7448995-xsv92" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-xsv92 webserver-deployment-69b7448995- deployment-6385  cf953761-a9a2-4ac0-9a30-82369e2da538 5950644 0 2023-01-29 03:15:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "100.101.161.204"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "100.101.161.204"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bf874850-3453-4d04-874b-3ec2c847e80f 0x40057f0f87 0x40057f0f88}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dlrpp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dlrpp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.241,PodIP:,StartTime:2023-01-29 03:15:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 03:15:32.399: INFO: Pod "webserver-deployment-845c8977d9-2vvkn" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-2vvkn webserver-deployment-845c8977d9- deployment-6385  b9855367-d366-497f-b0b1-f5282aa0d593 5950677 0 2023-01-29 03:15:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "100.101.208.224"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "100.101.208.224"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 f4abe03d-ea57-4def-915f-cb677f890574 0x40057f1157 0x40057f1158}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7vbbh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7vbbh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.242,PodIP:,StartTime:2023-01-29 03:15:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 03:15:32.400: INFO: Pod "webserver-deployment-845c8977d9-4gpnc" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-4gpnc webserver-deployment-845c8977d9- deployment-6385  63e0fba5-6646-434e-9192-96a55baf3fba 5950695 0 2023-01-29 03:15:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "100.101.208.225"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "100.101.208.225"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 f4abe03d-ea57-4def-915f-cb677f890574 0x40057f1307 0x40057f1308}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nvh22,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nvh22,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.242,PodIP:,StartTime:2023-01-29 03:15:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 03:15:32.400: INFO: Pod "webserver-deployment-845c8977d9-4hgdz" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-4hgdz webserver-deployment-845c8977d9- deployment-6385  3c492c50-0a08-468b-b143-214e249e101d 5950338 0 2023-01-29 03:15:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "100.101.32.109"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "100.101.32.109"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 f4abe03d-ea57-4def-915f-cb677f890574 0x40057f1517 0x40057f1518}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r94xw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r94xw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.243,PodIP:100.101.32.109,StartTime:2023-01-29 03:15:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-29 03:15:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry-jinan-lab.inspurcloud.cn/library/cke/httpd-arm64:2.4.38-2,ImageID:docker-pullable://registry-jinan-lab.inspurcloud.cn/library/cke/httpd-arm64@sha256:a4f0279e65f8d239ea154d08f0572c5419f43e4ed748bee8a1a1b762d0d36893,ContainerID:docker://e3d18946ae146bcc1ac17f0acf4a54df2d3b390af17e2244fba47acee4dbaa76,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.101.32.109,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 03:15:32.401: INFO: Pod "webserver-deployment-845c8977d9-7vw4s" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-7vw4s webserver-deployment-845c8977d9- deployment-6385  f4f0eb3a-555a-42e3-9ffb-aba5f790e2d1 5950331 0 2023-01-29 03:15:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "100.101.49.190"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "100.101.49.190"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 f4abe03d-ea57-4def-915f-cb677f890574 0x40057f16f7 0x40057f16f8}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fpdl8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fpdl8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.245,PodIP:100.101.49.190,StartTime:2023-01-29 03:15:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-29 03:15:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry-jinan-lab.inspurcloud.cn/library/cke/httpd-arm64:2.4.38-2,ImageID:docker-pullable://registry-jinan-lab.inspurcloud.cn/library/cke/httpd-arm64@sha256:a4f0279e65f8d239ea154d08f0572c5419f43e4ed748bee8a1a1b762d0d36893,ContainerID:docker://159ac32f2b9755505655c0deff6b62d726b8cc59a3572a759cf004f5d4e48197,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.101.49.190,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 03:15:32.401: INFO: Pod "webserver-deployment-845c8977d9-b75j5" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-b75j5 webserver-deployment-845c8977d9- deployment-6385  caaaa32f-317c-4624-ac22-1839d862d4a5 5950682 0 2023-01-29 03:15:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 f4abe03d-ea57-4def-915f-cb677f890574 0x40057f1917 0x40057f1918}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4tzr9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4tzr9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.244,PodIP:,StartTime:2023-01-29 03:15:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 03:15:32.402: INFO: Pod "webserver-deployment-845c8977d9-cl2qb" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-cl2qb webserver-deployment-845c8977d9- deployment-6385  2446f7f3-b43e-4bfc-b7a8-f472a9729884 5950678 0 2023-01-29 03:15:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "100.101.51.34"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "100.101.51.34"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 f4abe03d-ea57-4def-915f-cb677f890574 0x40057f1af7 0x40057f1af8}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l7tm2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l7tm2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.244,PodIP:,StartTime:2023-01-29 03:15:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 03:15:32.403: INFO: Pod "webserver-deployment-845c8977d9-fsj9x" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-fsj9x webserver-deployment-845c8977d9- deployment-6385  097bf848-059f-45a9-be29-7960879307ba 5950645 0 2023-01-29 03:15:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 f4abe03d-ea57-4def-915f-cb677f890574 0x40057f1ca7 0x40057f1ca8}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n8qhl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n8qhl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.245,PodIP:,StartTime:2023-01-29 03:15:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 03:15:32.403: INFO: Pod "webserver-deployment-845c8977d9-gvkms" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-gvkms webserver-deployment-845c8977d9- deployment-6385  9ae14bf3-8bf9-45cc-87cc-2e4fd95f76c6 5950360 0 2023-01-29 03:15:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "100.101.51.23"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "100.101.51.23"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 f4abe03d-ea57-4def-915f-cb677f890574 0x40057f1e47 0x40057f1e48}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dlkb6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dlkb6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.244,PodIP:100.101.51.23,StartTime:2023-01-29 03:15:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-29 03:15:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry-jinan-lab.inspurcloud.cn/library/cke/httpd-arm64:2.4.38-2,ImageID:docker-pullable://registry-jinan-lab.inspurcloud.cn/library/cke/httpd-arm64@sha256:a4f0279e65f8d239ea154d08f0572c5419f43e4ed748bee8a1a1b762d0d36893,ContainerID:docker://3d62ea6aefed8e214559eb9dc4dca327ca12c992f03c58f8c6e5dcadc41e4d2a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.101.51.23,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 03:15:32.403: INFO: Pod "webserver-deployment-845c8977d9-mhvr5" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-mhvr5 webserver-deployment-845c8977d9- deployment-6385  ed9b2c22-6da1-481d-986f-aacc334dd691 5950659 0 2023-01-29 03:15:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "100.101.161.203"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "100.101.161.203"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 f4abe03d-ea57-4def-915f-cb677f890574 0x4005816077 0x4005816078}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q98jv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q98jv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.241,PodIP:,StartTime:2023-01-29 03:15:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 03:15:32.404: INFO: Pod "webserver-deployment-845c8977d9-nkr4j" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-nkr4j webserver-deployment-845c8977d9- deployment-6385  51192260-efda-435b-8384-6a5146828a48 5950341 0 2023-01-29 03:15:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "100.101.161.200"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "100.101.161.200"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 f4abe03d-ea57-4def-915f-cb677f890574 0x4005816257 0x4005816258}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-x952v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-x952v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.241,PodIP:100.101.161.200,StartTime:2023-01-29 03:15:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-29 03:15:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:dominicyin/httpd:2.4.38-2,ImageID:docker-pullable://dominicyin/httpd@sha256:a4f0279e65f8d239ea154d08f0572c5419f43e4ed748bee8a1a1b762d0d36893,ContainerID:docker://f1d88c43b52277db567776f0fb6b1fdc2f35b7a44ade6215723f9cc2a554bf4e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.101.161.200,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 03:15:32.405: INFO: Pod "webserver-deployment-845c8977d9-qhfwk" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-qhfwk webserver-deployment-845c8977d9- deployment-6385  e6477e11-af3a-40d2-9962-ffc18b404b22 5950354 0 2023-01-29 03:15:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "100.101.208.221"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "100.101.208.221"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 f4abe03d-ea57-4def-915f-cb677f890574 0x4005816477 0x4005816478}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jcdd9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jcdd9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.242,PodIP:100.101.208.221,StartTime:2023-01-29 03:15:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-29 03:15:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry-jinan-lab.inspurcloud.cn/library/cke/httpd-arm64:2.4.38-2,ImageID:docker-pullable://registry-jinan-lab.inspurcloud.cn/library/cke/httpd-arm64@sha256:a4f0279e65f8d239ea154d08f0572c5419f43e4ed748bee8a1a1b762d0d36893,ContainerID:docker://df1db6c528cc9b745241284f02c52d6bf026380dd00ad613a67092a0b06533cd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.101.208.221,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 03:15:32.405: INFO: Pod "webserver-deployment-845c8977d9-r7bgx" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-r7bgx webserver-deployment-845c8977d9- deployment-6385  ee9cc1f5-907a-4142-88c4-da72242f00d8 5950626 0 2023-01-29 03:15:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "100.101.49.207"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "100.101.49.207"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 f4abe03d-ea57-4def-915f-cb677f890574 0x4005816667 0x4005816668}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tzkx7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tzkx7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.245,PodIP:,StartTime:2023-01-29 03:15:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 03:15:32.405: INFO: Pod "webserver-deployment-845c8977d9-rbnmx" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-rbnmx webserver-deployment-845c8977d9- deployment-6385  f52d3799-b447-4bfd-ab24-8dd48fe2125f 5950636 0 2023-01-29 03:15:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "100.101.51.42"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "100.101.51.42"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 f4abe03d-ea57-4def-915f-cb677f890574 0x4005816817 0x4005816818}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n4mvm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n4mvm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.244,PodIP:,StartTime:2023-01-29 03:15:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 03:15:32.406: INFO: Pod "webserver-deployment-845c8977d9-rdj7s" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-rdj7s webserver-deployment-845c8977d9- deployment-6385  cbbde9f8-5fdd-4dcc-8fff-037da592476a 5950700 0 2023-01-29 03:15:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "100.101.32.111"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "100.101.32.111"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 f4abe03d-ea57-4def-915f-cb677f890574 0x40058169b7 0x40058169b8}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9ng7d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9ng7d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.243,PodIP:,StartTime:2023-01-29 03:15:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 03:15:32.406: INFO: Pod "webserver-deployment-845c8977d9-rgvk4" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-rgvk4 webserver-deployment-845c8977d9- deployment-6385  c35ab847-ec16-4f86-ac11-b7a5fa4f15eb 5950577 0 2023-01-29 03:15:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 f4abe03d-ea57-4def-915f-cb677f890574 0x4005816ba7 0x4005816ba8}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f5mng,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f5mng,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.244,PodIP:,StartTime:2023-01-29 03:15:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 03:15:32.407: INFO: Pod "webserver-deployment-845c8977d9-w6pj7" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-w6pj7 webserver-deployment-845c8977d9- deployment-6385  6136687c-7370-4733-9143-1464128f758a 5950357 0 2023-01-29 03:15:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "100.101.208.220"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "100.101.208.220"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 f4abe03d-ea57-4def-915f-cb677f890574 0x4005816d97 0x4005816d98}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ggt8w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ggt8w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.242,PodIP:100.101.208.220,StartTime:2023-01-29 03:15:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-29 03:15:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry-jinan-lab.inspurcloud.cn/library/cke/httpd-arm64:2.4.38-2,ImageID:docker-pullable://registry-jinan-lab.inspurcloud.cn/library/cke/httpd-arm64@sha256:a4f0279e65f8d239ea154d08f0572c5419f43e4ed748bee8a1a1b762d0d36893,ContainerID:docker://aa9c17461cdc28b93c9d9e5c81762fb60a401ebe453e41a564817ec26d98405d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.101.208.220,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 03:15:32.407: INFO: Pod "webserver-deployment-845c8977d9-whdq4" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-whdq4 webserver-deployment-845c8977d9- deployment-6385  2b611d76-cc86-47b4-a7e0-a169dd6c4542 5950632 0 2023-01-29 03:15:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "100.101.49.183"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "100.101.49.183"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 f4abe03d-ea57-4def-915f-cb677f890574 0x4005816f57 0x4005816f58}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dp684,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dp684,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.245,PodIP:,StartTime:2023-01-29 03:15:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 03:15:32.408: INFO: Pod "webserver-deployment-845c8977d9-x8pxn" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-x8pxn webserver-deployment-845c8977d9- deployment-6385  a15ad0ad-9ab0-4d42-ae17-143914493338 5950334 0 2023-01-29 03:15:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "100.101.49.209"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "100.101.49.209"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 f4abe03d-ea57-4def-915f-cb677f890574 0x40058170f7 0x40058170f8}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w9wm6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w9wm6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.245,PodIP:100.101.49.209,StartTime:2023-01-29 03:15:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-29 03:15:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry-jinan-lab.inspurcloud.cn/library/cke/httpd-arm64:2.4.38-2,ImageID:docker-pullable://registry-jinan-lab.inspurcloud.cn/library/cke/httpd-arm64@sha256:a4f0279e65f8d239ea154d08f0572c5419f43e4ed748bee8a1a1b762d0d36893,ContainerID:docker://7e5c2a8a58a48670b3bd66f42eadf34fabfb43e0edbee3b56f904c2099808c61,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.101.49.209,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 03:15:32.408: INFO: Pod "webserver-deployment-845c8977d9-xgmh8" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-xgmh8 webserver-deployment-845c8977d9- deployment-6385  5f8c148e-d780-40ed-a760-92dc9f596425 5950347 0 2023-01-29 03:15:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "100.101.161.201"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "100.101.161.201"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 f4abe03d-ea57-4def-915f-cb677f890574 0x40058172b7 0x40058172b8}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pbm4q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pbm4q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.241,PodIP:100.101.161.201,StartTime:2023-01-29 03:15:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-29 03:15:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:dominicyin/httpd:2.4.38-2,ImageID:docker-pullable://dominicyin/httpd@sha256:a4f0279e65f8d239ea154d08f0572c5419f43e4ed748bee8a1a1b762d0d36893,ContainerID:docker://0622ee738047a190676e2a8ff221a4deb2245c21cc97d089bedbd58d848604e8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.101.161.201,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 03:15:32.408: INFO: Pod "webserver-deployment-845c8977d9-z7vwk" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-z7vwk webserver-deployment-845c8977d9- deployment-6385  7ecb32b0-44ce-47fa-9b38-f598b543ba97 5950681 0 2023-01-29 03:15:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "100.101.32.112"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "100.101.32.112"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 f4abe03d-ea57-4def-915f-cb677f890574 0x4005817477 0x4005817478}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r5tr9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r5tr9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.243,PodIP:,StartTime:2023-01-29 03:15:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan 29 03:15:32.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6385" for this suite. 01/29/23 03:15:32.425
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","completed":79,"skipped":1435,"failed":0}
------------------------------
• [SLOW TEST] [8.439 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:15:24.019
    Jan 29 03:15:24.019: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename deployment 01/29/23 03:15:24.02
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:15:24.059
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:15:24.065
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Jan 29 03:15:24.071: INFO: Creating deployment "webserver-deployment"
    Jan 29 03:15:24.084: INFO: Waiting for observed generation 1
    Jan 29 03:15:26.101: INFO: Waiting for all required pods to come up
    Jan 29 03:15:26.112: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 01/29/23 03:15:26.112
    Jan 29 03:15:26.112: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-296rm" in namespace "deployment-6385" to be "running"
    Jan 29 03:15:26.113: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-nkr4j" in namespace "deployment-6385" to be "running"
    Jan 29 03:15:26.113: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-4hgdz" in namespace "deployment-6385" to be "running"
    Jan 29 03:15:26.113: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-7vw4s" in namespace "deployment-6385" to be "running"
    Jan 29 03:15:26.113: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-qhfwk" in namespace "deployment-6385" to be "running"
    Jan 29 03:15:26.113: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-gvkms" in namespace "deployment-6385" to be "running"
    Jan 29 03:15:26.113: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-w6pj7" in namespace "deployment-6385" to be "running"
    Jan 29 03:15:26.113: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-x8pxn" in namespace "deployment-6385" to be "running"
    Jan 29 03:15:26.114: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-xgmh8" in namespace "deployment-6385" to be "running"
    Jan 29 03:15:26.126: INFO: Pod "webserver-deployment-845c8977d9-296rm": Phase="Pending", Reason="", readiness=false. Elapsed: 13.454254ms
    Jan 29 03:15:26.134: INFO: Pod "webserver-deployment-845c8977d9-x8pxn": Phase="Pending", Reason="", readiness=false. Elapsed: 20.576524ms
    Jan 29 03:15:26.134: INFO: Pod "webserver-deployment-845c8977d9-xgmh8": Phase="Pending", Reason="", readiness=false. Elapsed: 20.586744ms
    Jan 29 03:15:26.134: INFO: Pod "webserver-deployment-845c8977d9-w6pj7": Phase="Pending", Reason="", readiness=false. Elapsed: 20.900126ms
    Jan 29 03:15:26.135: INFO: Pod "webserver-deployment-845c8977d9-4hgdz": Phase="Pending", Reason="", readiness=false. Elapsed: 22.100935ms
    Jan 29 03:15:26.135: INFO: Pod "webserver-deployment-845c8977d9-gvkms": Phase="Pending", Reason="", readiness=false. Elapsed: 21.567851ms
    Jan 29 03:15:26.135: INFO: Pod "webserver-deployment-845c8977d9-qhfwk": Phase="Pending", Reason="", readiness=false. Elapsed: 21.876273ms
    Jan 29 03:15:26.136: INFO: Pod "webserver-deployment-845c8977d9-7vw4s": Phase="Pending", Reason="", readiness=false. Elapsed: 22.994921ms
    Jan 29 03:15:26.136: INFO: Pod "webserver-deployment-845c8977d9-nkr4j": Phase="Pending", Reason="", readiness=false. Elapsed: 23.477605ms
    Jan 29 03:15:28.133: INFO: Pod "webserver-deployment-845c8977d9-296rm": Phase="Running", Reason="", readiness=true. Elapsed: 2.020317315s
    Jan 29 03:15:28.133: INFO: Pod "webserver-deployment-845c8977d9-296rm" satisfied condition "running"
    Jan 29 03:15:28.143: INFO: Pod "webserver-deployment-845c8977d9-xgmh8": Phase="Running", Reason="", readiness=true. Elapsed: 2.028977336s
    Jan 29 03:15:28.143: INFO: Pod "webserver-deployment-845c8977d9-xgmh8" satisfied condition "running"
    Jan 29 03:15:28.145: INFO: Pod "webserver-deployment-845c8977d9-nkr4j": Phase="Running", Reason="", readiness=true. Elapsed: 2.032178999s
    Jan 29 03:15:28.145: INFO: Pod "webserver-deployment-845c8977d9-gvkms": Phase="Running", Reason="", readiness=true. Elapsed: 2.031397293s
    Jan 29 03:15:28.145: INFO: Pod "webserver-deployment-845c8977d9-gvkms" satisfied condition "running"
    Jan 29 03:15:28.145: INFO: Pod "webserver-deployment-845c8977d9-nkr4j" satisfied condition "running"
    Jan 29 03:15:28.146: INFO: Pod "webserver-deployment-845c8977d9-w6pj7": Phase="Running", Reason="", readiness=true. Elapsed: 2.032167998s
    Jan 29 03:15:28.146: INFO: Pod "webserver-deployment-845c8977d9-w6pj7" satisfied condition "running"
    Jan 29 03:15:28.146: INFO: Pod "webserver-deployment-845c8977d9-qhfwk": Phase="Running", Reason="", readiness=true. Elapsed: 2.032510461s
    Jan 29 03:15:28.146: INFO: Pod "webserver-deployment-845c8977d9-qhfwk" satisfied condition "running"
    Jan 29 03:15:28.146: INFO: Pod "webserver-deployment-845c8977d9-x8pxn": Phase="Running", Reason="", readiness=true. Elapsed: 2.032295539s
    Jan 29 03:15:28.146: INFO: Pod "webserver-deployment-845c8977d9-x8pxn" satisfied condition "running"
    Jan 29 03:15:28.146: INFO: Pod "webserver-deployment-845c8977d9-7vw4s": Phase="Running", Reason="", readiness=true. Elapsed: 2.033148665s
    Jan 29 03:15:28.146: INFO: Pod "webserver-deployment-845c8977d9-7vw4s" satisfied condition "running"
    Jan 29 03:15:28.146: INFO: Pod "webserver-deployment-845c8977d9-4hgdz": Phase="Running", Reason="", readiness=true. Elapsed: 2.033372447s
    Jan 29 03:15:28.146: INFO: Pod "webserver-deployment-845c8977d9-4hgdz" satisfied condition "running"
    Jan 29 03:15:28.146: INFO: Waiting for deployment "webserver-deployment" to complete
    Jan 29 03:15:28.160: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Jan 29 03:15:28.174: INFO: Updating deployment webserver-deployment
    Jan 29 03:15:28.174: INFO: Waiting for observed generation 2
    Jan 29 03:15:30.189: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Jan 29 03:15:30.195: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Jan 29 03:15:30.201: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Jan 29 03:15:30.221: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Jan 29 03:15:30.221: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Jan 29 03:15:30.228: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Jan 29 03:15:30.241: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Jan 29 03:15:30.241: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Jan 29 03:15:30.259: INFO: Updating deployment webserver-deployment
    Jan 29 03:15:30.259: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Jan 29 03:15:30.275: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Jan 29 03:15:32.306: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 29 03:15:32.336: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-6385  69ddc841-e39a-44de-a3e5-5c31d9737bcc 5950583 3 2023-01-29 03:15:24 +0000 UTC <nil> <nil> map[name:httpd] map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/revision:2] [] [] []},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0x4005755e18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-29 03:15:30 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2023-01-29 03:15:30 +0000 UTC,LastTransitionTime:2023-01-29 03:15:24 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    Jan 29 03:15:32.378: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-6385  bf874850-3453-4d04-874b-3ec2c847e80f 5950580 3 2023-01-29 03:15:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 69ddc841-e39a-44de-a3e5-5c31d9737bcc 0x40057a9067 0x40057a9068}] [] []},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0x40057a90d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 29 03:15:32.378: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Jan 29 03:15:32.378: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-845c8977d9  deployment-6385  f4abe03d-ea57-4def-915f-cb677f890574 5950552 3 2023-01-29 03:15:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 69ddc841-e39a-44de-a3e5-5c31d9737bcc 0x40057a9147 0x40057a9148}] [] []},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0x40057a91a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    Jan 29 03:15:32.393: INFO: Pod "webserver-deployment-69b7448995-55ksv" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-55ksv webserver-deployment-69b7448995- deployment-6385  d74383bb-c78d-4518-8edb-5a4f2713cda8 5950582 0 2023-01-29 03:15:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "",
        "ips": [
            "100.101.32.110"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "",
        "ips": [
            "100.101.32.110"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bf874850-3453-4d04-874b-3ec2c847e80f 0x40057a9717 0x40057a9718}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wstff,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wstff,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.243,PodIP:,StartTime:2023-01-29 03:15:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: Get https://registry-1.docker.io/v2/: dial tcp: lookup registry-1.docker.io on 100.105.0.3:53: server misbehaving,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 29 03:15:32.394: INFO: Pod "webserver-deployment-69b7448995-5m767" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-5m767 webserver-deployment-69b7448995- deployment-6385  1fbfbd04-6a65-4f2d-83f1-9e6986a9cbac 5950702 0 2023-01-29 03:15:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bf874850-3453-4d04-874b-3ec2c847e80f 0x40057a9937 0x40057a9938}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dlzlh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dlzlh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.245,PodIP:,StartTime:2023-01-29 03:15:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 29 03:15:32.394: INFO: Pod "webserver-deployment-69b7448995-cr99n" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-cr99n webserver-deployment-69b7448995- deployment-6385  5830d4b8-caad-4e79-9d25-897e775551c2 5950612 0 2023-01-29 03:15:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bf874850-3453-4d04-874b-3ec2c847e80f 0x40057a9b67 0x40057a9b68}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ngnzw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ngnzw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.245,PodIP:,StartTime:2023-01-29 03:15:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 29 03:15:32.394: INFO: Pod "webserver-deployment-69b7448995-dctdx" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-dctdx webserver-deployment-69b7448995- deployment-6385  3a30023e-3ff3-4b4d-ac36-8b187207288a 5950458 0 2023-01-29 03:15:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "",
        "ips": [
            "100.101.161.202"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "",
        "ips": [
            "100.101.161.202"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bf874850-3453-4d04-874b-3ec2c847e80f 0x40057a9d87 0x40057a9d88}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l5nt9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l5nt9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.241,PodIP:,StartTime:2023-01-29 03:15:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 29 03:15:32.395: INFO: Pod "webserver-deployment-69b7448995-frq7q" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-frq7q webserver-deployment-69b7448995- deployment-6385  26360ce1-cdb2-4921-a661-56b8b4848a1b 5950534 0 2023-01-29 03:15:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bf874850-3453-4d04-874b-3ec2c847e80f 0x40057a9fa7 0x40057a9fa8}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9kg5d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9kg5d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.244,PodIP:,StartTime:2023-01-29 03:15:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 29 03:15:32.395: INFO: Pod "webserver-deployment-69b7448995-gbvzk" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-gbvzk webserver-deployment-69b7448995- deployment-6385  2b634fbe-ea2e-4026-92a0-65a051cd16b5 5950699 0 2023-01-29 03:15:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "",
        "ips": [
            "100.101.51.37"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "",
        "ips": [
            "100.101.51.37"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bf874850-3453-4d04-874b-3ec2c847e80f 0x40057f0167 0x40057f0168}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qjb5l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qjb5l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.244,PodIP:,StartTime:2023-01-29 03:15:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: Get https://registry-1.docker.io/v2/: dial tcp: lookup registry-1.docker.io on 100.105.0.3:53: server misbehaving,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 29 03:15:32.396: INFO: Pod "webserver-deployment-69b7448995-gfvlt" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-gfvlt webserver-deployment-69b7448995- deployment-6385  b0762cbc-288b-49f5-81f8-f9e573334939 5950631 0 2023-01-29 03:15:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bf874850-3453-4d04-874b-3ec2c847e80f 0x40057f0337 0x40057f0338}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ngd99,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ngd99,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.244,PodIP:,StartTime:2023-01-29 03:15:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 29 03:15:32.396: INFO: Pod "webserver-deployment-69b7448995-p8f4t" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-p8f4t webserver-deployment-69b7448995- deployment-6385  08c81725-159f-4def-afc8-ebf2c092074d 5950554 0 2023-01-29 03:15:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bf874850-3453-4d04-874b-3ec2c847e80f 0x40057f04f7 0x40057f04f8}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mk9js,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mk9js,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.243,PodIP:,StartTime:2023-01-29 03:15:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 29 03:15:32.397: INFO: Pod "webserver-deployment-69b7448995-rlsjd" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-rlsjd webserver-deployment-69b7448995- deployment-6385  5c5b6c69-d1b3-4c8d-b834-db2229f4601a 5950625 0 2023-01-29 03:15:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "",
        "ips": [
            "100.101.49.208"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "",
        "ips": [
            "100.101.49.208"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bf874850-3453-4d04-874b-3ec2c847e80f 0x40057f06b7 0x40057f06b8}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qfkbn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qfkbn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.245,PodIP:,StartTime:2023-01-29 03:15:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 29 03:15:32.397: INFO: Pod "webserver-deployment-69b7448995-thdjf" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-thdjf webserver-deployment-69b7448995- deployment-6385  414e587b-56a5-41f0-9db5-b4ac16aa5122 5950658 0 2023-01-29 03:15:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "",
        "ips": [
            "100.101.208.222"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "",
        "ips": [
            "100.101.208.222"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bf874850-3453-4d04-874b-3ec2c847e80f 0x40057f08b7 0x40057f08b8}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v86kh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v86kh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.242,PodIP:,StartTime:2023-01-29 03:15:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 29 03:15:32.398: INFO: Pod "webserver-deployment-69b7448995-vqvw7" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-vqvw7 webserver-deployment-69b7448995- deployment-6385  52e5bdf2-feec-4e5c-8aee-1171894b7ca6 5950620 0 2023-01-29 03:15:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "",
        "ips": [
            "100.101.208.223"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "",
        "ips": [
            "100.101.208.223"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bf874850-3453-4d04-874b-3ec2c847e80f 0x40057f0ac7 0x40057f0ac8}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9vw2h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9vw2h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.242,PodIP:100.101.208.223,StartTime:2023-01-29 03:15:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "webserver:404",},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.101.208.223,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 29 03:15:32.398: INFO: Pod "webserver-deployment-69b7448995-xjwvp" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-xjwvp webserver-deployment-69b7448995- deployment-6385  34ba3f4f-7ca4-4723-99be-6c633deb743f 5950585 0 2023-01-29 03:15:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "",
        "ips": [
            "100.101.49.211"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "",
        "ips": [
            "100.101.49.211"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bf874850-3453-4d04-874b-3ec2c847e80f 0x40057f0d17 0x40057f0d18}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m24k4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m24k4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.245,PodIP:100.101.49.211,StartTime:2023-01-29 03:15:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "webserver:404",},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.101.49.211,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 29 03:15:32.399: INFO: Pod "webserver-deployment-69b7448995-xsv92" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-xsv92 webserver-deployment-69b7448995- deployment-6385  cf953761-a9a2-4ac0-9a30-82369e2da538 5950644 0 2023-01-29 03:15:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "",
        "ips": [
            "100.101.161.204"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "",
        "ips": [
            "100.101.161.204"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bf874850-3453-4d04-874b-3ec2c847e80f 0x40057f0f87 0x40057f0f88}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dlrpp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dlrpp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.241,PodIP:,StartTime:2023-01-29 03:15:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 29 03:15:32.399: INFO: Pod "webserver-deployment-845c8977d9-2vvkn" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-2vvkn webserver-deployment-845c8977d9- deployment-6385  b9855367-d366-497f-b0b1-f5282aa0d593 5950677 0 2023-01-29 03:15:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "",
        "ips": [
            "100.101.208.224"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "",
        "ips": [
            "100.101.208.224"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 f4abe03d-ea57-4def-915f-cb677f890574 0x40057f1157 0x40057f1158}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7vbbh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7vbbh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.242,PodIP:,StartTime:2023-01-29 03:15:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 29 03:15:32.400: INFO: Pod "webserver-deployment-845c8977d9-4gpnc" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-4gpnc webserver-deployment-845c8977d9- deployment-6385  63e0fba5-6646-434e-9192-96a55baf3fba 5950695 0 2023-01-29 03:15:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "",
        "ips": [
            "100.101.208.225"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "",
        "ips": [
            "100.101.208.225"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 f4abe03d-ea57-4def-915f-cb677f890574 0x40057f1307 0x40057f1308}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nvh22,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nvh22,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.242,PodIP:,StartTime:2023-01-29 03:15:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 29 03:15:32.400: INFO: Pod "webserver-deployment-845c8977d9-4hgdz" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-4hgdz webserver-deployment-845c8977d9- deployment-6385  3c492c50-0a08-468b-b143-214e249e101d 5950338 0 2023-01-29 03:15:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "",
        "ips": [
            "100.101.32.109"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "",
        "ips": [
            "100.101.32.109"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 f4abe03d-ea57-4def-915f-cb677f890574 0x40057f1517 0x40057f1518}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r94xw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r94xw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.243,PodIP:100.101.32.109,StartTime:2023-01-29 03:15:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-29 03:15:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry-jinan-lab.inspurcloud.cn/library/cke/httpd-arm64:2.4.38-2,ImageID:docker-pullable://registry-jinan-lab.inspurcloud.cn/library/cke/httpd-arm64@sha256:a4f0279e65f8d239ea154d08f0572c5419f43e4ed748bee8a1a1b762d0d36893,ContainerID:docker://e3d18946ae146bcc1ac17f0acf4a54df2d3b390af17e2244fba47acee4dbaa76,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.101.32.109,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 29 03:15:32.401: INFO: Pod "webserver-deployment-845c8977d9-7vw4s" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-7vw4s webserver-deployment-845c8977d9- deployment-6385  f4f0eb3a-555a-42e3-9ffb-aba5f790e2d1 5950331 0 2023-01-29 03:15:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "",
        "ips": [
            "100.101.49.190"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "",
        "ips": [
            "100.101.49.190"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 f4abe03d-ea57-4def-915f-cb677f890574 0x40057f16f7 0x40057f16f8}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fpdl8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fpdl8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.245,PodIP:100.101.49.190,StartTime:2023-01-29 03:15:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-29 03:15:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry-jinan-lab.inspurcloud.cn/library/cke/httpd-arm64:2.4.38-2,ImageID:docker-pullable://registry-jinan-lab.inspurcloud.cn/library/cke/httpd-arm64@sha256:a4f0279e65f8d239ea154d08f0572c5419f43e4ed748bee8a1a1b762d0d36893,ContainerID:docker://159ac32f2b9755505655c0deff6b62d726b8cc59a3572a759cf004f5d4e48197,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.101.49.190,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 29 03:15:32.401: INFO: Pod "webserver-deployment-845c8977d9-b75j5" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-b75j5 webserver-deployment-845c8977d9- deployment-6385  caaaa32f-317c-4624-ac22-1839d862d4a5 5950682 0 2023-01-29 03:15:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 f4abe03d-ea57-4def-915f-cb677f890574 0x40057f1917 0x40057f1918}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4tzr9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4tzr9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.244,PodIP:,StartTime:2023-01-29 03:15:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 29 03:15:32.402: INFO: Pod "webserver-deployment-845c8977d9-cl2qb" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-cl2qb webserver-deployment-845c8977d9- deployment-6385  2446f7f3-b43e-4bfc-b7a8-f472a9729884 5950678 0 2023-01-29 03:15:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "",
        "ips": [
            "100.101.51.34"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "",
        "ips": [
            "100.101.51.34"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 f4abe03d-ea57-4def-915f-cb677f890574 0x40057f1af7 0x40057f1af8}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l7tm2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l7tm2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.244,PodIP:,StartTime:2023-01-29 03:15:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 29 03:15:32.403: INFO: Pod "webserver-deployment-845c8977d9-fsj9x" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-fsj9x webserver-deployment-845c8977d9- deployment-6385  097bf848-059f-45a9-be29-7960879307ba 5950645 0 2023-01-29 03:15:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 f4abe03d-ea57-4def-915f-cb677f890574 0x40057f1ca7 0x40057f1ca8}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n8qhl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n8qhl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.245,PodIP:,StartTime:2023-01-29 03:15:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 29 03:15:32.403: INFO: Pod "webserver-deployment-845c8977d9-gvkms" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-gvkms webserver-deployment-845c8977d9- deployment-6385  9ae14bf3-8bf9-45cc-87cc-2e4fd95f76c6 5950360 0 2023-01-29 03:15:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "",
        "ips": [
            "100.101.51.23"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "",
        "ips": [
            "100.101.51.23"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 f4abe03d-ea57-4def-915f-cb677f890574 0x40057f1e47 0x40057f1e48}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dlkb6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dlkb6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.244,PodIP:100.101.51.23,StartTime:2023-01-29 03:15:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-29 03:15:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry-jinan-lab.inspurcloud.cn/library/cke/httpd-arm64:2.4.38-2,ImageID:docker-pullable://registry-jinan-lab.inspurcloud.cn/library/cke/httpd-arm64@sha256:a4f0279e65f8d239ea154d08f0572c5419f43e4ed748bee8a1a1b762d0d36893,ContainerID:docker://3d62ea6aefed8e214559eb9dc4dca327ca12c992f03c58f8c6e5dcadc41e4d2a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.101.51.23,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 29 03:15:32.403: INFO: Pod "webserver-deployment-845c8977d9-mhvr5" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-mhvr5 webserver-deployment-845c8977d9- deployment-6385  ed9b2c22-6da1-481d-986f-aacc334dd691 5950659 0 2023-01-29 03:15:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "",
        "ips": [
            "100.101.161.203"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "",
        "ips": [
            "100.101.161.203"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 f4abe03d-ea57-4def-915f-cb677f890574 0x4005816077 0x4005816078}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q98jv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q98jv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.241,PodIP:,StartTime:2023-01-29 03:15:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 29 03:15:32.404: INFO: Pod "webserver-deployment-845c8977d9-nkr4j" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-nkr4j webserver-deployment-845c8977d9- deployment-6385  51192260-efda-435b-8384-6a5146828a48 5950341 0 2023-01-29 03:15:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "",
        "ips": [
            "100.101.161.200"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "",
        "ips": [
            "100.101.161.200"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 f4abe03d-ea57-4def-915f-cb677f890574 0x4005816257 0x4005816258}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-x952v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-x952v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.241,PodIP:100.101.161.200,StartTime:2023-01-29 03:15:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-29 03:15:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:dominicyin/httpd:2.4.38-2,ImageID:docker-pullable://dominicyin/httpd@sha256:a4f0279e65f8d239ea154d08f0572c5419f43e4ed748bee8a1a1b762d0d36893,ContainerID:docker://f1d88c43b52277db567776f0fb6b1fdc2f35b7a44ade6215723f9cc2a554bf4e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.101.161.200,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 29 03:15:32.405: INFO: Pod "webserver-deployment-845c8977d9-qhfwk" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-qhfwk webserver-deployment-845c8977d9- deployment-6385  e6477e11-af3a-40d2-9962-ffc18b404b22 5950354 0 2023-01-29 03:15:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "",
        "ips": [
            "100.101.208.221"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "",
        "ips": [
            "100.101.208.221"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 f4abe03d-ea57-4def-915f-cb677f890574 0x4005816477 0x4005816478}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jcdd9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jcdd9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.242,PodIP:100.101.208.221,StartTime:2023-01-29 03:15:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-29 03:15:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry-jinan-lab.inspurcloud.cn/library/cke/httpd-arm64:2.4.38-2,ImageID:docker-pullable://registry-jinan-lab.inspurcloud.cn/library/cke/httpd-arm64@sha256:a4f0279e65f8d239ea154d08f0572c5419f43e4ed748bee8a1a1b762d0d36893,ContainerID:docker://df1db6c528cc9b745241284f02c52d6bf026380dd00ad613a67092a0b06533cd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.101.208.221,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 29 03:15:32.405: INFO: Pod "webserver-deployment-845c8977d9-r7bgx" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-r7bgx webserver-deployment-845c8977d9- deployment-6385  ee9cc1f5-907a-4142-88c4-da72242f00d8 5950626 0 2023-01-29 03:15:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "",
        "ips": [
            "100.101.49.207"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "",
        "ips": [
            "100.101.49.207"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 f4abe03d-ea57-4def-915f-cb677f890574 0x4005816667 0x4005816668}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tzkx7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tzkx7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.245,PodIP:,StartTime:2023-01-29 03:15:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 29 03:15:32.405: INFO: Pod "webserver-deployment-845c8977d9-rbnmx" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-rbnmx webserver-deployment-845c8977d9- deployment-6385  f52d3799-b447-4bfd-ab24-8dd48fe2125f 5950636 0 2023-01-29 03:15:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "",
        "ips": [
            "100.101.51.42"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "",
        "ips": [
            "100.101.51.42"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 f4abe03d-ea57-4def-915f-cb677f890574 0x4005816817 0x4005816818}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n4mvm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n4mvm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.244,PodIP:,StartTime:2023-01-29 03:15:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 29 03:15:32.406: INFO: Pod "webserver-deployment-845c8977d9-rdj7s" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-rdj7s webserver-deployment-845c8977d9- deployment-6385  cbbde9f8-5fdd-4dcc-8fff-037da592476a 5950700 0 2023-01-29 03:15:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "",
        "ips": [
            "100.101.32.111"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "",
        "ips": [
            "100.101.32.111"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 f4abe03d-ea57-4def-915f-cb677f890574 0x40058169b7 0x40058169b8}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9ng7d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9ng7d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.243,PodIP:,StartTime:2023-01-29 03:15:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 29 03:15:32.406: INFO: Pod "webserver-deployment-845c8977d9-rgvk4" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-rgvk4 webserver-deployment-845c8977d9- deployment-6385  c35ab847-ec16-4f86-ac11-b7a5fa4f15eb 5950577 0 2023-01-29 03:15:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 f4abe03d-ea57-4def-915f-cb677f890574 0x4005816ba7 0x4005816ba8}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f5mng,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f5mng,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.244,PodIP:,StartTime:2023-01-29 03:15:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 29 03:15:32.407: INFO: Pod "webserver-deployment-845c8977d9-w6pj7" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-w6pj7 webserver-deployment-845c8977d9- deployment-6385  6136687c-7370-4733-9143-1464128f758a 5950357 0 2023-01-29 03:15:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "",
        "ips": [
            "100.101.208.220"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "",
        "ips": [
            "100.101.208.220"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 f4abe03d-ea57-4def-915f-cb677f890574 0x4005816d97 0x4005816d98}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ggt8w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ggt8w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.242,PodIP:100.101.208.220,StartTime:2023-01-29 03:15:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-29 03:15:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry-jinan-lab.inspurcloud.cn/library/cke/httpd-arm64:2.4.38-2,ImageID:docker-pullable://registry-jinan-lab.inspurcloud.cn/library/cke/httpd-arm64@sha256:a4f0279e65f8d239ea154d08f0572c5419f43e4ed748bee8a1a1b762d0d36893,ContainerID:docker://aa9c17461cdc28b93c9d9e5c81762fb60a401ebe453e41a564817ec26d98405d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.101.208.220,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 29 03:15:32.407: INFO: Pod "webserver-deployment-845c8977d9-whdq4" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-whdq4 webserver-deployment-845c8977d9- deployment-6385  2b611d76-cc86-47b4-a7e0-a169dd6c4542 5950632 0 2023-01-29 03:15:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "",
        "ips": [
            "100.101.49.183"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "",
        "ips": [
            "100.101.49.183"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 f4abe03d-ea57-4def-915f-cb677f890574 0x4005816f57 0x4005816f58}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dp684,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dp684,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.245,PodIP:,StartTime:2023-01-29 03:15:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 29 03:15:32.408: INFO: Pod "webserver-deployment-845c8977d9-x8pxn" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-x8pxn webserver-deployment-845c8977d9- deployment-6385  a15ad0ad-9ab0-4d42-ae17-143914493338 5950334 0 2023-01-29 03:15:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "",
        "ips": [
            "100.101.49.209"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "",
        "ips": [
            "100.101.49.209"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 f4abe03d-ea57-4def-915f-cb677f890574 0x40058170f7 0x40058170f8}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w9wm6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w9wm6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.245,PodIP:100.101.49.209,StartTime:2023-01-29 03:15:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-29 03:15:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry-jinan-lab.inspurcloud.cn/library/cke/httpd-arm64:2.4.38-2,ImageID:docker-pullable://registry-jinan-lab.inspurcloud.cn/library/cke/httpd-arm64@sha256:a4f0279e65f8d239ea154d08f0572c5419f43e4ed748bee8a1a1b762d0d36893,ContainerID:docker://7e5c2a8a58a48670b3bd66f42eadf34fabfb43e0edbee3b56f904c2099808c61,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.101.49.209,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 29 03:15:32.408: INFO: Pod "webserver-deployment-845c8977d9-xgmh8" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-xgmh8 webserver-deployment-845c8977d9- deployment-6385  5f8c148e-d780-40ed-a760-92dc9f596425 5950347 0 2023-01-29 03:15:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "",
        "ips": [
            "100.101.161.201"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "",
        "ips": [
            "100.101.161.201"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 f4abe03d-ea57-4def-915f-cb677f890574 0x40058172b7 0x40058172b8}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pbm4q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pbm4q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.241,PodIP:100.101.161.201,StartTime:2023-01-29 03:15:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-29 03:15:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:dominicyin/httpd:2.4.38-2,ImageID:docker-pullable://dominicyin/httpd@sha256:a4f0279e65f8d239ea154d08f0572c5419f43e4ed748bee8a1a1b762d0d36893,ContainerID:docker://0622ee738047a190676e2a8ff221a4deb2245c21cc97d089bedbd58d848604e8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.101.161.201,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 29 03:15:32.408: INFO: Pod "webserver-deployment-845c8977d9-z7vwk" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-z7vwk webserver-deployment-845c8977d9- deployment-6385  7ecb32b0-44ce-47fa-9b38-f598b543ba97 5950681 0 2023-01-29 03:15:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "",
        "ips": [
            "100.101.32.112"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "",
        "ips": [
            "100.101.32.112"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 f4abe03d-ea57-4def-915f-cb677f890574 0x4005817477 0x4005817478}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r5tr9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r5tr9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:15:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.243,PodIP:,StartTime:2023-01-29 03:15:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan 29 03:15:32.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-6385" for this suite. 01/29/23 03:15:32.425
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:15:32.462
Jan 29 03:15:32.462: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename job 01/29/23 03:15:32.464
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:15:32.532
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:15:32.54
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
STEP: Creating Indexed job 01/29/23 03:15:32.547
STEP: Ensuring job reaches completions 01/29/23 03:15:32.58
STEP: Ensuring pods with index for job exist 01/29/23 03:15:46.588
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jan 29 03:15:46.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-460" for this suite. 01/29/23 03:15:46.604
{"msg":"PASSED [sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]","completed":80,"skipped":1459,"failed":0}
------------------------------
• [SLOW TEST] [14.154 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:15:32.462
    Jan 29 03:15:32.462: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename job 01/29/23 03:15:32.464
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:15:32.532
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:15:32.54
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:194
    STEP: Creating Indexed job 01/29/23 03:15:32.547
    STEP: Ensuring job reaches completions 01/29/23 03:15:32.58
    STEP: Ensuring pods with index for job exist 01/29/23 03:15:46.588
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jan 29 03:15:46.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-460" for this suite. 01/29/23 03:15:46.604
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:15:46.617
Jan 29 03:15:46.618: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename container-runtime 01/29/23 03:15:46.619
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:15:46.659
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:15:46.665
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
STEP: create the container 01/29/23 03:15:46.674
STEP: wait for the container to reach Succeeded 01/29/23 03:15:46.693
STEP: get the container status 01/29/23 03:15:50.728
STEP: the container should be terminated 01/29/23 03:15:50.734
STEP: the termination message should be set 01/29/23 03:15:50.734
Jan 29 03:15:50.734: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 01/29/23 03:15:50.734
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jan 29 03:15:50.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4005" for this suite. 01/29/23 03:15:50.844
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","completed":81,"skipped":1470,"failed":0}
------------------------------
• [4.240 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:15:46.617
    Jan 29 03:15:46.618: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename container-runtime 01/29/23 03:15:46.619
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:15:46.659
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:15:46.665
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194
    STEP: create the container 01/29/23 03:15:46.674
    STEP: wait for the container to reach Succeeded 01/29/23 03:15:46.693
    STEP: get the container status 01/29/23 03:15:50.728
    STEP: the container should be terminated 01/29/23 03:15:50.734
    STEP: the termination message should be set 01/29/23 03:15:50.734
    Jan 29 03:15:50.734: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 01/29/23 03:15:50.734
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jan 29 03:15:50.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-4005" for this suite. 01/29/23 03:15:50.844
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:15:50.859
Jan 29 03:15:50.859: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename configmap 01/29/23 03:15:50.861
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:15:50.901
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:15:50.908
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
STEP: Creating configMap with name configmap-test-volume-fe3bf99a-ef9d-479c-8c52-76d214ab07c2 01/29/23 03:15:50.915
STEP: Creating a pod to test consume configMaps 01/29/23 03:15:50.922
Jan 29 03:15:50.942: INFO: Waiting up to 5m0s for pod "pod-configmaps-6aaf19d5-a70b-4c74-acf1-37807f1b84f4" in namespace "configmap-3275" to be "Succeeded or Failed"
Jan 29 03:15:50.949: INFO: Pod "pod-configmaps-6aaf19d5-a70b-4c74-acf1-37807f1b84f4": Phase="Pending", Reason="", readiness=false. Elapsed: 7.234111ms
Jan 29 03:15:52.957: INFO: Pod "pod-configmaps-6aaf19d5-a70b-4c74-acf1-37807f1b84f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01511344s
Jan 29 03:15:54.957: INFO: Pod "pod-configmaps-6aaf19d5-a70b-4c74-acf1-37807f1b84f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014883371s
STEP: Saw pod success 01/29/23 03:15:54.957
Jan 29 03:15:54.957: INFO: Pod "pod-configmaps-6aaf19d5-a70b-4c74-acf1-37807f1b84f4" satisfied condition "Succeeded or Failed"
Jan 29 03:15:54.964: INFO: Trying to get logs from node slave2 pod pod-configmaps-6aaf19d5-a70b-4c74-acf1-37807f1b84f4 container agnhost-container: <nil>
STEP: delete the pod 01/29/23 03:15:54.979
Jan 29 03:15:55.073: INFO: Waiting for pod pod-configmaps-6aaf19d5-a70b-4c74-acf1-37807f1b84f4 to disappear
Jan 29 03:15:55.079: INFO: Pod pod-configmaps-6aaf19d5-a70b-4c74-acf1-37807f1b84f4 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 29 03:15:55.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3275" for this suite. 01/29/23 03:15:55.089
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":82,"skipped":1479,"failed":0}
------------------------------
• [4.243 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:15:50.859
    Jan 29 03:15:50.859: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename configmap 01/29/23 03:15:50.861
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:15:50.901
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:15:50.908
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:73
    STEP: Creating configMap with name configmap-test-volume-fe3bf99a-ef9d-479c-8c52-76d214ab07c2 01/29/23 03:15:50.915
    STEP: Creating a pod to test consume configMaps 01/29/23 03:15:50.922
    Jan 29 03:15:50.942: INFO: Waiting up to 5m0s for pod "pod-configmaps-6aaf19d5-a70b-4c74-acf1-37807f1b84f4" in namespace "configmap-3275" to be "Succeeded or Failed"
    Jan 29 03:15:50.949: INFO: Pod "pod-configmaps-6aaf19d5-a70b-4c74-acf1-37807f1b84f4": Phase="Pending", Reason="", readiness=false. Elapsed: 7.234111ms
    Jan 29 03:15:52.957: INFO: Pod "pod-configmaps-6aaf19d5-a70b-4c74-acf1-37807f1b84f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01511344s
    Jan 29 03:15:54.957: INFO: Pod "pod-configmaps-6aaf19d5-a70b-4c74-acf1-37807f1b84f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014883371s
    STEP: Saw pod success 01/29/23 03:15:54.957
    Jan 29 03:15:54.957: INFO: Pod "pod-configmaps-6aaf19d5-a70b-4c74-acf1-37807f1b84f4" satisfied condition "Succeeded or Failed"
    Jan 29 03:15:54.964: INFO: Trying to get logs from node slave2 pod pod-configmaps-6aaf19d5-a70b-4c74-acf1-37807f1b84f4 container agnhost-container: <nil>
    STEP: delete the pod 01/29/23 03:15:54.979
    Jan 29 03:15:55.073: INFO: Waiting for pod pod-configmaps-6aaf19d5-a70b-4c74-acf1-37807f1b84f4 to disappear
    Jan 29 03:15:55.079: INFO: Pod pod-configmaps-6aaf19d5-a70b-4c74-acf1-37807f1b84f4 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 29 03:15:55.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-3275" for this suite. 01/29/23 03:15:55.089
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:15:55.105
Jan 29 03:15:55.105: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename server-version 01/29/23 03:15:55.107
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:15:55.139
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:15:55.145
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 01/29/23 03:15:55.151
STEP: Confirm major version 01/29/23 03:15:55.153
Jan 29 03:15:55.154: INFO: Major version: 1
STEP: Confirm minor version 01/29/23 03:15:55.154
Jan 29 03:15:55.154: INFO: cleanMinorVersion: 20
Jan 29 03:15:55.154: INFO: Minor version: 20
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:187
Jan 29 03:15:55.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-5051" for this suite. 01/29/23 03:15:55.163
{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","completed":83,"skipped":1509,"failed":0}
------------------------------
• [0.069 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:15:55.105
    Jan 29 03:15:55.105: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename server-version 01/29/23 03:15:55.107
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:15:55.139
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:15:55.145
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 01/29/23 03:15:55.151
    STEP: Confirm major version 01/29/23 03:15:55.153
    Jan 29 03:15:55.154: INFO: Major version: 1
    STEP: Confirm minor version 01/29/23 03:15:55.154
    Jan 29 03:15:55.154: INFO: cleanMinorVersion: 20
    Jan 29 03:15:55.154: INFO: Minor version: 20
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:187
    Jan 29 03:15:55.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "server-version-5051" for this suite. 01/29/23 03:15:55.163
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:15:55.176
Jan 29 03:15:55.176: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename kubectl 01/29/23 03:15:55.178
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:15:55.205
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:15:55.211
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1698
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/29/23 03:15:55.218
Jan 29 03:15:55.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-3201 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2'
Jan 29 03:15:55.345: INFO: stderr: ""
Jan 29 03:15:55.345: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 01/29/23 03:15:55.345
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1702
Jan 29 03:15:55.353: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-3201 delete pods e2e-test-httpd-pod'
Jan 29 03:15:57.645: INFO: stderr: ""
Jan 29 03:15:57.645: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 29 03:15:57.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3201" for this suite. 01/29/23 03:15:57.656
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","completed":84,"skipped":1536,"failed":0}
------------------------------
• [2.490 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1695
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1711

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:15:55.176
    Jan 29 03:15:55.176: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename kubectl 01/29/23 03:15:55.178
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:15:55.205
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:15:55.211
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1698
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1711
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/29/23 03:15:55.218
    Jan 29 03:15:55.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-3201 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2'
    Jan 29 03:15:55.345: INFO: stderr: ""
    Jan 29 03:15:55.345: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 01/29/23 03:15:55.345
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1702
    Jan 29 03:15:55.353: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-3201 delete pods e2e-test-httpd-pod'
    Jan 29 03:15:57.645: INFO: stderr: ""
    Jan 29 03:15:57.645: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 29 03:15:57.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-3201" for this suite. 01/29/23 03:15:57.656
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:15:57.667
Jan 29 03:15:57.667: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename custom-resource-definition 01/29/23 03:15:57.668
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:15:57.701
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:15:57.707
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Jan 29 03:15:57.721: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 29 03:15:58.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-382" for this suite. 01/29/23 03:15:58.295
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","completed":85,"skipped":1537,"failed":0}
------------------------------
• [0.639 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:15:57.667
    Jan 29 03:15:57.667: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename custom-resource-definition 01/29/23 03:15:57.668
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:15:57.701
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:15:57.707
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Jan 29 03:15:57.721: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 29 03:15:58.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-382" for this suite. 01/29/23 03:15:58.295
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:15:58.309
Jan 29 03:15:58.309: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename resourcequota 01/29/23 03:15:58.31
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:15:58.335
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:15:58.341
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
STEP: Counting existing ResourceQuota 01/29/23 03:15:58.346
STEP: Creating a ResourceQuota 01/29/23 03:16:03.353
STEP: Ensuring resource quota status is calculated 01/29/23 03:16:03.361
STEP: Creating a ReplicaSet 01/29/23 03:16:05.369
STEP: Ensuring resource quota status captures replicaset creation 01/29/23 03:16:05.391
STEP: Deleting a ReplicaSet 01/29/23 03:16:07.4
STEP: Ensuring resource quota status released usage 01/29/23 03:16:07.412
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 29 03:16:09.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-69" for this suite. 01/29/23 03:16:09.432
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","completed":86,"skipped":1566,"failed":0}
------------------------------
• [SLOW TEST] [11.135 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:15:58.309
    Jan 29 03:15:58.309: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename resourcequota 01/29/23 03:15:58.31
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:15:58.335
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:15:58.341
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:438
    STEP: Counting existing ResourceQuota 01/29/23 03:15:58.346
    STEP: Creating a ResourceQuota 01/29/23 03:16:03.353
    STEP: Ensuring resource quota status is calculated 01/29/23 03:16:03.361
    STEP: Creating a ReplicaSet 01/29/23 03:16:05.369
    STEP: Ensuring resource quota status captures replicaset creation 01/29/23 03:16:05.391
    STEP: Deleting a ReplicaSet 01/29/23 03:16:07.4
    STEP: Ensuring resource quota status released usage 01/29/23 03:16:07.412
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 29 03:16:09.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-69" for this suite. 01/29/23 03:16:09.432
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:16:09.445
Jan 29 03:16:09.445: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename configmap 01/29/23 03:16:09.447
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:16:09.489
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:16:09.495
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
STEP: Creating configMap with name configmap-test-volume-553946c0-ae6c-41bf-9da0-b90c2532235f 01/29/23 03:16:09.502
STEP: Creating a pod to test consume configMaps 01/29/23 03:16:09.51
Jan 29 03:16:09.531: INFO: Waiting up to 5m0s for pod "pod-configmaps-a9cbabf0-b36e-4e3c-9079-9c87bd703cad" in namespace "configmap-8255" to be "Succeeded or Failed"
Jan 29 03:16:09.537: INFO: Pod "pod-configmaps-a9cbabf0-b36e-4e3c-9079-9c87bd703cad": Phase="Pending", Reason="", readiness=false. Elapsed: 6.378045ms
Jan 29 03:16:11.545: INFO: Pod "pod-configmaps-a9cbabf0-b36e-4e3c-9079-9c87bd703cad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014332334s
Jan 29 03:16:13.545: INFO: Pod "pod-configmaps-a9cbabf0-b36e-4e3c-9079-9c87bd703cad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014014045s
STEP: Saw pod success 01/29/23 03:16:13.545
Jan 29 03:16:13.545: INFO: Pod "pod-configmaps-a9cbabf0-b36e-4e3c-9079-9c87bd703cad" satisfied condition "Succeeded or Failed"
Jan 29 03:16:13.553: INFO: Trying to get logs from node slave2 pod pod-configmaps-a9cbabf0-b36e-4e3c-9079-9c87bd703cad container agnhost-container: <nil>
STEP: delete the pod 01/29/23 03:16:13.569
Jan 29 03:16:13.670: INFO: Waiting for pod pod-configmaps-a9cbabf0-b36e-4e3c-9079-9c87bd703cad to disappear
Jan 29 03:16:13.678: INFO: Pod pod-configmaps-a9cbabf0-b36e-4e3c-9079-9c87bd703cad no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 29 03:16:13.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8255" for this suite. 01/29/23 03:16:13.69
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":87,"skipped":1585,"failed":0}
------------------------------
• [4.257 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:16:09.445
    Jan 29 03:16:09.445: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename configmap 01/29/23 03:16:09.447
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:16:09.489
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:16:09.495
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:46
    STEP: Creating configMap with name configmap-test-volume-553946c0-ae6c-41bf-9da0-b90c2532235f 01/29/23 03:16:09.502
    STEP: Creating a pod to test consume configMaps 01/29/23 03:16:09.51
    Jan 29 03:16:09.531: INFO: Waiting up to 5m0s for pod "pod-configmaps-a9cbabf0-b36e-4e3c-9079-9c87bd703cad" in namespace "configmap-8255" to be "Succeeded or Failed"
    Jan 29 03:16:09.537: INFO: Pod "pod-configmaps-a9cbabf0-b36e-4e3c-9079-9c87bd703cad": Phase="Pending", Reason="", readiness=false. Elapsed: 6.378045ms
    Jan 29 03:16:11.545: INFO: Pod "pod-configmaps-a9cbabf0-b36e-4e3c-9079-9c87bd703cad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014332334s
    Jan 29 03:16:13.545: INFO: Pod "pod-configmaps-a9cbabf0-b36e-4e3c-9079-9c87bd703cad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014014045s
    STEP: Saw pod success 01/29/23 03:16:13.545
    Jan 29 03:16:13.545: INFO: Pod "pod-configmaps-a9cbabf0-b36e-4e3c-9079-9c87bd703cad" satisfied condition "Succeeded or Failed"
    Jan 29 03:16:13.553: INFO: Trying to get logs from node slave2 pod pod-configmaps-a9cbabf0-b36e-4e3c-9079-9c87bd703cad container agnhost-container: <nil>
    STEP: delete the pod 01/29/23 03:16:13.569
    Jan 29 03:16:13.670: INFO: Waiting for pod pod-configmaps-a9cbabf0-b36e-4e3c-9079-9c87bd703cad to disappear
    Jan 29 03:16:13.678: INFO: Pod pod-configmaps-a9cbabf0-b36e-4e3c-9079-9c87bd703cad no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 29 03:16:13.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-8255" for this suite. 01/29/23 03:16:13.69
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:16:13.704
Jan 29 03:16:13.704: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename events 01/29/23 03:16:13.706
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:16:13.74
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:16:13.746
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 01/29/23 03:16:13.752
Jan 29 03:16:13.759: INFO: created test-event-1
Jan 29 03:16:13.766: INFO: created test-event-2
Jan 29 03:16:13.773: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 01/29/23 03:16:13.773
STEP: delete collection of events 01/29/23 03:16:13.78
Jan 29 03:16:13.780: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 01/29/23 03:16:13.822
Jan 29 03:16:13.822: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
Jan 29 03:16:13.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5045" for this suite. 01/29/23 03:16:13.84
{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","completed":88,"skipped":1594,"failed":0}
------------------------------
• [0.147 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:16:13.704
    Jan 29 03:16:13.704: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename events 01/29/23 03:16:13.706
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:16:13.74
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:16:13.746
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 01/29/23 03:16:13.752
    Jan 29 03:16:13.759: INFO: created test-event-1
    Jan 29 03:16:13.766: INFO: created test-event-2
    Jan 29 03:16:13.773: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 01/29/23 03:16:13.773
    STEP: delete collection of events 01/29/23 03:16:13.78
    Jan 29 03:16:13.780: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 01/29/23 03:16:13.822
    Jan 29 03:16:13.822: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    Jan 29 03:16:13.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-5045" for this suite. 01/29/23 03:16:13.84
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:16:13.852
Jan 29 03:16:13.852: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename sched-preemption 01/29/23 03:16:13.853
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:16:13.882
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:16:13.888
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jan 29 03:16:13.919: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 29 03:17:13.986: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
STEP: Create pods that use 4/5 of node resources. 01/29/23 03:17:14.001
Jan 29 03:17:14.053: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jan 29 03:17:14.070: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jan 29 03:17:14.100: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jan 29 03:17:14.132: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Jan 29 03:17:14.169: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Jan 29 03:17:14.184: INFO: Created pod: pod2-1-sched-preemption-medium-priority
Jan 29 03:17:14.220: INFO: Created pod: pod3-0-sched-preemption-medium-priority
Jan 29 03:17:14.253: INFO: Created pod: pod3-1-sched-preemption-medium-priority
Jan 29 03:17:14.281: INFO: Created pod: pod4-0-sched-preemption-medium-priority
Jan 29 03:17:14.298: INFO: Created pod: pod4-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 01/29/23 03:17:14.298
Jan 29 03:17:14.298: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-1064" to be "running"
Jan 29 03:17:14.305: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.284424ms
Jan 29 03:17:16.311: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013224827s
Jan 29 03:17:18.312: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013414483s
Jan 29 03:17:20.314: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016109477s
Jan 29 03:17:22.311: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.012718488s
Jan 29 03:17:24.315: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 10.016976033s
Jan 29 03:17:26.315: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 12.016814407s
Jan 29 03:17:28.311: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 14.012683093s
Jan 29 03:17:30.312: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 16.013843116s
Jan 29 03:17:32.312: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 18.014140393s
Jan 29 03:17:34.323: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 20.025160665s
Jan 29 03:17:36.313: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 22.014785288s
Jan 29 03:17:38.314: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 24.016133032s
Jan 29 03:17:40.318: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 26.019651352s
Jan 29 03:17:40.318: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Jan 29 03:17:40.318: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-1064" to be "running"
Jan 29 03:17:40.327: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.51852ms
Jan 29 03:17:40.327: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Jan 29 03:17:40.327: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-1064" to be "running"
Jan 29 03:17:40.343: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 16.585896ms
Jan 29 03:17:40.343: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Jan 29 03:17:40.343: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-1064" to be "running"
Jan 29 03:17:40.355: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 11.667402ms
Jan 29 03:17:40.355: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Jan 29 03:17:40.355: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-1064" to be "running"
Jan 29 03:17:40.366: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 10.897316ms
Jan 29 03:17:40.366: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Jan 29 03:17:40.366: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-1064" to be "running"
Jan 29 03:17:40.377: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 10.353072ms
Jan 29 03:17:40.377: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
Jan 29 03:17:40.377: INFO: Waiting up to 5m0s for pod "pod3-0-sched-preemption-medium-priority" in namespace "sched-preemption-1064" to be "running"
Jan 29 03:17:40.386: INFO: Pod "pod3-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.880842ms
Jan 29 03:17:40.386: INFO: Pod "pod3-0-sched-preemption-medium-priority" satisfied condition "running"
Jan 29 03:17:40.386: INFO: Waiting up to 5m0s for pod "pod3-1-sched-preemption-medium-priority" in namespace "sched-preemption-1064" to be "running"
Jan 29 03:17:40.396: INFO: Pod "pod3-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 10.607294ms
Jan 29 03:17:40.397: INFO: Pod "pod3-1-sched-preemption-medium-priority" satisfied condition "running"
Jan 29 03:17:40.397: INFO: Waiting up to 5m0s for pod "pod4-0-sched-preemption-medium-priority" in namespace "sched-preemption-1064" to be "running"
Jan 29 03:17:40.409: INFO: Pod "pod4-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 12.338866ms
Jan 29 03:17:40.409: INFO: Pod "pod4-0-sched-preemption-medium-priority" satisfied condition "running"
Jan 29 03:17:40.409: INFO: Waiting up to 5m0s for pod "pod4-1-sched-preemption-medium-priority" in namespace "sched-preemption-1064" to be "running"
Jan 29 03:17:40.417: INFO: Pod "pod4-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.043136ms
Jan 29 03:17:40.417: INFO: Pod "pod4-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 01/29/23 03:17:40.417
Jan 29 03:17:40.462: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Jan 29 03:17:40.474: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 11.719362ms
Jan 29 03:17:42.482: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019911534s
Jan 29 03:17:44.482: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.019899929s
Jan 29 03:17:44.482: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Jan 29 03:17:44.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-1064" for this suite. 01/29/23 03:17:44.659
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","completed":89,"skipped":1605,"failed":0}
------------------------------
• [SLOW TEST] [90.916 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:16:13.852
    Jan 29 03:16:13.852: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename sched-preemption 01/29/23 03:16:13.853
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:16:13.882
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:16:13.888
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Jan 29 03:16:13.919: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 29 03:17:13.986: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:218
    STEP: Create pods that use 4/5 of node resources. 01/29/23 03:17:14.001
    Jan 29 03:17:14.053: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Jan 29 03:17:14.070: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Jan 29 03:17:14.100: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Jan 29 03:17:14.132: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Jan 29 03:17:14.169: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Jan 29 03:17:14.184: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    Jan 29 03:17:14.220: INFO: Created pod: pod3-0-sched-preemption-medium-priority
    Jan 29 03:17:14.253: INFO: Created pod: pod3-1-sched-preemption-medium-priority
    Jan 29 03:17:14.281: INFO: Created pod: pod4-0-sched-preemption-medium-priority
    Jan 29 03:17:14.298: INFO: Created pod: pod4-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 01/29/23 03:17:14.298
    Jan 29 03:17:14.298: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-1064" to be "running"
    Jan 29 03:17:14.305: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.284424ms
    Jan 29 03:17:16.311: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013224827s
    Jan 29 03:17:18.312: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013414483s
    Jan 29 03:17:20.314: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016109477s
    Jan 29 03:17:22.311: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.012718488s
    Jan 29 03:17:24.315: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 10.016976033s
    Jan 29 03:17:26.315: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 12.016814407s
    Jan 29 03:17:28.311: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 14.012683093s
    Jan 29 03:17:30.312: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 16.013843116s
    Jan 29 03:17:32.312: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 18.014140393s
    Jan 29 03:17:34.323: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 20.025160665s
    Jan 29 03:17:36.313: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 22.014785288s
    Jan 29 03:17:38.314: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 24.016133032s
    Jan 29 03:17:40.318: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 26.019651352s
    Jan 29 03:17:40.318: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Jan 29 03:17:40.318: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-1064" to be "running"
    Jan 29 03:17:40.327: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.51852ms
    Jan 29 03:17:40.327: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan 29 03:17:40.327: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-1064" to be "running"
    Jan 29 03:17:40.343: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 16.585896ms
    Jan 29 03:17:40.343: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan 29 03:17:40.343: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-1064" to be "running"
    Jan 29 03:17:40.355: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 11.667402ms
    Jan 29 03:17:40.355: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan 29 03:17:40.355: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-1064" to be "running"
    Jan 29 03:17:40.366: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 10.897316ms
    Jan 29 03:17:40.366: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan 29 03:17:40.366: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-1064" to be "running"
    Jan 29 03:17:40.377: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 10.353072ms
    Jan 29 03:17:40.377: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan 29 03:17:40.377: INFO: Waiting up to 5m0s for pod "pod3-0-sched-preemption-medium-priority" in namespace "sched-preemption-1064" to be "running"
    Jan 29 03:17:40.386: INFO: Pod "pod3-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.880842ms
    Jan 29 03:17:40.386: INFO: Pod "pod3-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan 29 03:17:40.386: INFO: Waiting up to 5m0s for pod "pod3-1-sched-preemption-medium-priority" in namespace "sched-preemption-1064" to be "running"
    Jan 29 03:17:40.396: INFO: Pod "pod3-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 10.607294ms
    Jan 29 03:17:40.397: INFO: Pod "pod3-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan 29 03:17:40.397: INFO: Waiting up to 5m0s for pod "pod4-0-sched-preemption-medium-priority" in namespace "sched-preemption-1064" to be "running"
    Jan 29 03:17:40.409: INFO: Pod "pod4-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 12.338866ms
    Jan 29 03:17:40.409: INFO: Pod "pod4-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan 29 03:17:40.409: INFO: Waiting up to 5m0s for pod "pod4-1-sched-preemption-medium-priority" in namespace "sched-preemption-1064" to be "running"
    Jan 29 03:17:40.417: INFO: Pod "pod4-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.043136ms
    Jan 29 03:17:40.417: INFO: Pod "pod4-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 01/29/23 03:17:40.417
    Jan 29 03:17:40.462: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Jan 29 03:17:40.474: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 11.719362ms
    Jan 29 03:17:42.482: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019911534s
    Jan 29 03:17:44.482: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.019899929s
    Jan 29 03:17:44.482: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Jan 29 03:17:44.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-1064" for this suite. 01/29/23 03:17:44.659
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:17:44.769
Jan 29 03:17:44.769: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename pods 01/29/23 03:17:44.77
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:17:44.797
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:17:44.803
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
STEP: creating the pod 01/29/23 03:17:44.812
STEP: submitting the pod to kubernetes 01/29/23 03:17:44.812
Jan 29 03:17:44.829: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-5f150d9c-164a-4059-8395-4237582fc0e2" in namespace "pods-6732" to be "running and ready"
Jan 29 03:17:44.836: INFO: Pod "pod-update-activedeadlineseconds-5f150d9c-164a-4059-8395-4237582fc0e2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.339691ms
Jan 29 03:17:44.836: INFO: The phase of Pod pod-update-activedeadlineseconds-5f150d9c-164a-4059-8395-4237582fc0e2 is Pending, waiting for it to be Running (with Ready = true)
Jan 29 03:17:46.845: INFO: Pod "pod-update-activedeadlineseconds-5f150d9c-164a-4059-8395-4237582fc0e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01652831s
Jan 29 03:17:46.845: INFO: The phase of Pod pod-update-activedeadlineseconds-5f150d9c-164a-4059-8395-4237582fc0e2 is Pending, waiting for it to be Running (with Ready = true)
Jan 29 03:17:48.845: INFO: Pod "pod-update-activedeadlineseconds-5f150d9c-164a-4059-8395-4237582fc0e2": Phase="Running", Reason="", readiness=true. Elapsed: 4.01572772s
Jan 29 03:17:48.845: INFO: The phase of Pod pod-update-activedeadlineseconds-5f150d9c-164a-4059-8395-4237582fc0e2 is Running (Ready = true)
Jan 29 03:17:48.845: INFO: Pod "pod-update-activedeadlineseconds-5f150d9c-164a-4059-8395-4237582fc0e2" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 01/29/23 03:17:48.853
STEP: updating the pod 01/29/23 03:17:48.858
Jan 29 03:17:49.374: INFO: Successfully updated pod "pod-update-activedeadlineseconds-5f150d9c-164a-4059-8395-4237582fc0e2"
Jan 29 03:17:49.374: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-5f150d9c-164a-4059-8395-4237582fc0e2" in namespace "pods-6732" to be "terminated with reason DeadlineExceeded"
Jan 29 03:17:49.380: INFO: Pod "pod-update-activedeadlineseconds-5f150d9c-164a-4059-8395-4237582fc0e2": Phase="Running", Reason="", readiness=true. Elapsed: 6.547026ms
Jan 29 03:17:51.391: INFO: Pod "pod-update-activedeadlineseconds-5f150d9c-164a-4059-8395-4237582fc0e2": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.017492057s
Jan 29 03:17:51.391: INFO: Pod "pod-update-activedeadlineseconds-5f150d9c-164a-4059-8395-4237582fc0e2" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 29 03:17:51.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6732" for this suite. 01/29/23 03:17:51.404
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","completed":90,"skipped":1608,"failed":0}
------------------------------
• [SLOW TEST] [6.656 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:17:44.769
    Jan 29 03:17:44.769: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename pods 01/29/23 03:17:44.77
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:17:44.797
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:17:44.803
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:397
    STEP: creating the pod 01/29/23 03:17:44.812
    STEP: submitting the pod to kubernetes 01/29/23 03:17:44.812
    Jan 29 03:17:44.829: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-5f150d9c-164a-4059-8395-4237582fc0e2" in namespace "pods-6732" to be "running and ready"
    Jan 29 03:17:44.836: INFO: Pod "pod-update-activedeadlineseconds-5f150d9c-164a-4059-8395-4237582fc0e2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.339691ms
    Jan 29 03:17:44.836: INFO: The phase of Pod pod-update-activedeadlineseconds-5f150d9c-164a-4059-8395-4237582fc0e2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 29 03:17:46.845: INFO: Pod "pod-update-activedeadlineseconds-5f150d9c-164a-4059-8395-4237582fc0e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01652831s
    Jan 29 03:17:46.845: INFO: The phase of Pod pod-update-activedeadlineseconds-5f150d9c-164a-4059-8395-4237582fc0e2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 29 03:17:48.845: INFO: Pod "pod-update-activedeadlineseconds-5f150d9c-164a-4059-8395-4237582fc0e2": Phase="Running", Reason="", readiness=true. Elapsed: 4.01572772s
    Jan 29 03:17:48.845: INFO: The phase of Pod pod-update-activedeadlineseconds-5f150d9c-164a-4059-8395-4237582fc0e2 is Running (Ready = true)
    Jan 29 03:17:48.845: INFO: Pod "pod-update-activedeadlineseconds-5f150d9c-164a-4059-8395-4237582fc0e2" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 01/29/23 03:17:48.853
    STEP: updating the pod 01/29/23 03:17:48.858
    Jan 29 03:17:49.374: INFO: Successfully updated pod "pod-update-activedeadlineseconds-5f150d9c-164a-4059-8395-4237582fc0e2"
    Jan 29 03:17:49.374: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-5f150d9c-164a-4059-8395-4237582fc0e2" in namespace "pods-6732" to be "terminated with reason DeadlineExceeded"
    Jan 29 03:17:49.380: INFO: Pod "pod-update-activedeadlineseconds-5f150d9c-164a-4059-8395-4237582fc0e2": Phase="Running", Reason="", readiness=true. Elapsed: 6.547026ms
    Jan 29 03:17:51.391: INFO: Pod "pod-update-activedeadlineseconds-5f150d9c-164a-4059-8395-4237582fc0e2": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.017492057s
    Jan 29 03:17:51.391: INFO: Pod "pod-update-activedeadlineseconds-5f150d9c-164a-4059-8395-4237582fc0e2" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 29 03:17:51.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-6732" for this suite. 01/29/23 03:17:51.404
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:17:51.429
Jan 29 03:17:51.429: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename sched-pred 01/29/23 03:17:51.431
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:17:51.502
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:17:51.509
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jan 29 03:17:51.548: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 29 03:17:51.592: INFO: Waiting for terminating namespaces to be deleted...
Jan 29 03:17:51.604: INFO: 
Logging pods the apiserver thinks is on node master1 before test
Jan 29 03:17:51.644: INFO: calico-kube-controllers-5c6d4b68d6-6p9cm from kube-system started at 2023-01-11 07:48:50 +0000 UTC (1 container statuses recorded)
Jan 29 03:17:51.644: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jan 29 03:17:51.644: INFO: calico-node-j4qnr from kube-system started at 2023-01-11 07:49:07 +0000 UTC (1 container statuses recorded)
Jan 29 03:17:51.644: INFO: 	Container calico-node ready: true, restart count 2
Jan 29 03:17:51.644: INFO: cke-admission-daemonset-g5mvj from kube-system started at 2023-01-11 07:51:31 +0000 UTC (1 container statuses recorded)
Jan 29 03:17:51.644: INFO: 	Container cke-admission ready: true, restart count 0
Jan 29 03:17:51.644: INFO: cke-controller-manager-master1 from kube-system started at 2023-01-11 07:49:37 +0000 UTC (1 container statuses recorded)
Jan 29 03:17:51.644: INFO: 	Container cke-controller-manager ready: true, restart count 0
Jan 29 03:17:51.644: INFO: component-controller-manager-master1 from kube-system started at 2023-01-11 07:49:18 +0000 UTC (1 container statuses recorded)
Jan 29 03:17:51.644: INFO: 	Container component-controller-manager ready: true, restart count 0
Jan 29 03:17:51.644: INFO: coredns-tntlt from kube-system started at 2023-01-11 07:49:05 +0000 UTC (1 container statuses recorded)
Jan 29 03:17:51.644: INFO: 	Container coredns ready: true, restart count 0
Jan 29 03:17:51.644: INFO: keepalived-master1 from kube-system started at 2023-01-11 07:48:19 +0000 UTC (1 container statuses recorded)
Jan 29 03:17:51.644: INFO: 	Container keepalived ready: true, restart count 0
Jan 29 03:17:51.644: INFO: kube-apiserver-master1 from kube-system started at 2023-01-11 07:48:12 +0000 UTC (1 container statuses recorded)
Jan 29 03:17:51.644: INFO: 	Container kube-apiserver ready: true, restart count 0
Jan 29 03:17:51.644: INFO: kube-controller-manager-master1 from kube-system started at 2023-01-11 07:48:20 +0000 UTC (1 container statuses recorded)
Jan 29 03:17:51.644: INFO: 	Container kube-controller-manager ready: true, restart count 5
Jan 29 03:17:51.644: INFO: kube-multus-ds-8b89r from kube-system started at 2023-01-11 07:51:31 +0000 UTC (1 container statuses recorded)
Jan 29 03:17:51.644: INFO: 	Container kube-multus ready: true, restart count 0
Jan 29 03:17:51.644: INFO: kube-proxy-master1 from kube-system started at 2023-01-11 07:48:15 +0000 UTC (1 container statuses recorded)
Jan 29 03:17:51.644: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 29 03:17:51.644: INFO: kube-scheduler-master1 from kube-system started at 2023-01-11 07:48:21 +0000 UTC (1 container statuses recorded)
Jan 29 03:17:51.644: INFO: 	Container kube-scheduler ready: true, restart count 9
Jan 29 03:17:51.644: INFO: nginx-proxy-master1 from kube-system started at 2023-01-11 07:48:26 +0000 UTC (2 container statuses recorded)
Jan 29 03:17:51.644: INFO: 	Container kube-apiserver-healthcheck ready: true, restart count 0
Jan 29 03:17:51.644: INFO: 	Container nginx-proxy ready: true, restart count 0
Jan 29 03:17:51.644: INFO: node-problem-detector-dgrmp from kube-system started at 2023-01-11 07:51:49 +0000 UTC (1 container statuses recorded)
Jan 29 03:17:51.644: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan 29 03:17:51.644: INFO: pod0-1-sched-preemption-medium-priority from sched-preemption-1064 started at 2023-01-29 03:17:15 +0000 UTC (1 container statuses recorded)
Jan 29 03:17:51.644: INFO: 	Container pod0-1-sched-preemption-medium-priority ready: true, restart count 0
Jan 29 03:17:51.644: INFO: 
Logging pods the apiserver thinks is on node master2 before test
Jan 29 03:17:51.664: INFO: calico-node-sx6hm from kube-system started at 2023-01-11 07:49:07 +0000 UTC (1 container statuses recorded)
Jan 29 03:17:51.664: INFO: 	Container calico-node ready: true, restart count 2
Jan 29 03:17:51.664: INFO: cke-admission-daemonset-6v8vg from kube-system started at 2023-01-11 07:51:31 +0000 UTC (1 container statuses recorded)
Jan 29 03:17:51.664: INFO: 	Container cke-admission ready: true, restart count 0
Jan 29 03:17:51.664: INFO: cke-controller-manager-master2 from kube-system started at 2023-01-11 07:49:04 +0000 UTC (1 container statuses recorded)
Jan 29 03:17:51.664: INFO: 	Container cke-controller-manager ready: true, restart count 1
Jan 29 03:17:51.664: INFO: component-controller-manager-master2 from kube-system started at 2023-01-11 07:49:02 +0000 UTC (1 container statuses recorded)
Jan 29 03:17:51.664: INFO: 	Container component-controller-manager ready: true, restart count 1
Jan 29 03:17:51.664: INFO: coredns-m9lv7 from kube-system started at 2023-01-11 07:50:16 +0000 UTC (1 container statuses recorded)
Jan 29 03:17:51.664: INFO: 	Container coredns ready: true, restart count 0
Jan 29 03:17:51.664: INFO: keepalived-master2 from kube-system started at 2023-01-11 07:47:57 +0000 UTC (1 container statuses recorded)
Jan 29 03:17:51.664: INFO: 	Container keepalived ready: true, restart count 0
Jan 29 03:17:51.664: INFO: kube-apiserver-master2 from kube-system started at 2023-01-11 07:47:49 +0000 UTC (1 container statuses recorded)
Jan 29 03:17:51.664: INFO: 	Container kube-apiserver ready: true, restart count 0
Jan 29 03:17:51.664: INFO: kube-controller-manager-master2 from kube-system started at 2023-01-11 07:47:51 +0000 UTC (1 container statuses recorded)
Jan 29 03:17:51.664: INFO: 	Container kube-controller-manager ready: true, restart count 4
Jan 29 03:17:51.664: INFO: kube-multus-ds-qq7kz from kube-system started at 2023-01-11 07:51:31 +0000 UTC (1 container statuses recorded)
Jan 29 03:17:51.664: INFO: 	Container kube-multus ready: true, restart count 0
Jan 29 03:17:51.664: INFO: kube-proxy-master2 from kube-system started at 2023-01-11 07:47:55 +0000 UTC (1 container statuses recorded)
Jan 29 03:17:51.664: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 29 03:17:51.664: INFO: kube-scheduler-master2 from kube-system started at 2023-01-11 07:48:09 +0000 UTC (1 container statuses recorded)
Jan 29 03:17:51.664: INFO: 	Container kube-scheduler ready: true, restart count 9
Jan 29 03:17:51.664: INFO: nginx-proxy-master2 from kube-system started at 2023-01-11 07:48:01 +0000 UTC (2 container statuses recorded)
Jan 29 03:17:51.664: INFO: 	Container kube-apiserver-healthcheck ready: true, restart count 0
Jan 29 03:17:51.664: INFO: 	Container nginx-proxy ready: true, restart count 0
Jan 29 03:17:51.664: INFO: node-problem-detector-w69xv from kube-system started at 2023-01-11 07:51:49 +0000 UTC (1 container statuses recorded)
Jan 29 03:17:51.664: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan 29 03:17:51.665: INFO: pod1-0-sched-preemption-medium-priority from sched-preemption-1064 started at 2023-01-29 03:17:23 +0000 UTC (1 container statuses recorded)
Jan 29 03:17:51.665: INFO: 	Container pod1-0-sched-preemption-medium-priority ready: true, restart count 0
Jan 29 03:17:51.665: INFO: pod1-1-sched-preemption-medium-priority from sched-preemption-1064 started at 2023-01-29 03:17:23 +0000 UTC (1 container statuses recorded)
Jan 29 03:17:51.665: INFO: 	Container pod1-1-sched-preemption-medium-priority ready: true, restart count 0
Jan 29 03:17:51.665: INFO: 
Logging pods the apiserver thinks is on node master3 before test
Jan 29 03:17:51.698: INFO: calico-node-9b26s from kube-system started at 2023-01-11 07:49:07 +0000 UTC (1 container statuses recorded)
Jan 29 03:17:51.698: INFO: 	Container calico-node ready: true, restart count 0
Jan 29 03:17:51.698: INFO: cke-admission-daemonset-mqt4k from kube-system started at 2023-01-11 07:51:31 +0000 UTC (1 container statuses recorded)
Jan 29 03:17:51.698: INFO: 	Container cke-admission ready: true, restart count 0
Jan 29 03:17:51.698: INFO: cke-controller-manager-master3 from kube-system started at 2023-01-11 07:49:08 +0000 UTC (1 container statuses recorded)
Jan 29 03:17:51.698: INFO: 	Container cke-controller-manager ready: true, restart count 0
Jan 29 03:17:51.698: INFO: component-controller-manager-master3 from kube-system started at 2023-01-11 07:49:17 +0000 UTC (1 container statuses recorded)
Jan 29 03:17:51.698: INFO: 	Container component-controller-manager ready: true, restart count 0
Jan 29 03:17:51.698: INFO: coredns-hwvn8 from kube-system started at 2023-01-11 07:49:26 +0000 UTC (1 container statuses recorded)
Jan 29 03:17:51.698: INFO: 	Container coredns ready: true, restart count 0
Jan 29 03:17:51.698: INFO: keepalived-master3 from kube-system started at 2023-01-11 07:48:26 +0000 UTC (1 container statuses recorded)
Jan 29 03:17:51.698: INFO: 	Container keepalived ready: true, restart count 0
Jan 29 03:17:51.698: INFO: kube-apiserver-master3 from kube-system started at 2023-01-11 07:48:44 +0000 UTC (1 container statuses recorded)
Jan 29 03:17:51.698: INFO: 	Container kube-apiserver ready: true, restart count 0
Jan 29 03:17:51.698: INFO: kube-controller-manager-master3 from kube-system started at 2023-01-11 07:48:27 +0000 UTC (1 container statuses recorded)
Jan 29 03:17:51.698: INFO: 	Container kube-controller-manager ready: true, restart count 4
Jan 29 03:17:51.698: INFO: kube-multus-ds-wrhjf from kube-system started at 2023-01-11 07:51:31 +0000 UTC (1 container statuses recorded)
Jan 29 03:17:51.698: INFO: 	Container kube-multus ready: true, restart count 0
Jan 29 03:17:51.698: INFO: kube-proxy-master3 from kube-system started at 2023-01-11 07:48:37 +0000 UTC (1 container statuses recorded)
Jan 29 03:17:51.698: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 29 03:17:51.698: INFO: kube-scheduler-master3 from kube-system started at 2023-01-11 07:48:30 +0000 UTC (1 container statuses recorded)
Jan 29 03:17:51.698: INFO: 	Container kube-scheduler ready: true, restart count 5
Jan 29 03:17:51.698: INFO: metrics-server-5584f68fbf-l689z from kube-system started at 2023-01-11 07:50:55 +0000 UTC (1 container statuses recorded)
Jan 29 03:17:51.698: INFO: 	Container metrics-server ready: true, restart count 0
Jan 29 03:17:51.698: INFO: nginx-proxy-master3 from kube-system started at 2023-01-11 07:48:32 +0000 UTC (2 container statuses recorded)
Jan 29 03:17:51.698: INFO: 	Container kube-apiserver-healthcheck ready: true, restart count 0
Jan 29 03:17:51.698: INFO: 	Container nginx-proxy ready: true, restart count 0
Jan 29 03:17:51.698: INFO: node-problem-detector-9987m from kube-system started at 2023-01-11 07:51:49 +0000 UTC (1 container statuses recorded)
Jan 29 03:17:51.698: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan 29 03:17:51.698: INFO: pod2-0-sched-preemption-medium-priority from sched-preemption-1064 started at 2023-01-29 03:17:21 +0000 UTC (1 container statuses recorded)
Jan 29 03:17:51.698: INFO: 	Container pod2-0-sched-preemption-medium-priority ready: true, restart count 0
Jan 29 03:17:51.698: INFO: pod2-1-sched-preemption-medium-priority from sched-preemption-1064 started at 2023-01-29 03:17:21 +0000 UTC (1 container statuses recorded)
Jan 29 03:17:51.698: INFO: 	Container pod2-1-sched-preemption-medium-priority ready: true, restart count 0
Jan 29 03:17:51.698: INFO: 
Logging pods the apiserver thinks is on node slave1 before test
Jan 29 03:17:51.719: INFO: calico-node-scr7m from kube-system started at 2023-01-11 07:49:07 +0000 UTC (1 container statuses recorded)
Jan 29 03:17:51.719: INFO: 	Container calico-node ready: true, restart count 0
Jan 29 03:17:51.719: INFO: kube-multus-ds-ng7xc from kube-system started at 2023-01-29 02:51:08 +0000 UTC (1 container statuses recorded)
Jan 29 03:17:51.719: INFO: 	Container kube-multus ready: true, restart count 0
Jan 29 03:17:51.719: INFO: kube-proxy-slave1 from kube-system started at 2023-01-11 07:48:42 +0000 UTC (1 container statuses recorded)
Jan 29 03:17:51.719: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 29 03:17:51.719: INFO: node-problem-detector-8bg82 from kube-system started at 2023-01-11 07:51:49 +0000 UTC (1 container statuses recorded)
Jan 29 03:17:51.719: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan 29 03:17:51.719: INFO: pod3-0-sched-preemption-medium-priority from sched-preemption-1064 started at 2023-01-29 03:17:37 +0000 UTC (1 container statuses recorded)
Jan 29 03:17:51.719: INFO: 	Container pod3-0-sched-preemption-medium-priority ready: true, restart count 0
Jan 29 03:17:51.719: INFO: pod3-1-sched-preemption-medium-priority from sched-preemption-1064 started at 2023-01-29 03:17:37 +0000 UTC (1 container statuses recorded)
Jan 29 03:17:51.719: INFO: 	Container pod3-1-sched-preemption-medium-priority ready: true, restart count 0
Jan 29 03:17:51.719: INFO: sonobuoy from sonobuoy started at 2023-01-29 02:56:33 +0000 UTC (1 container statuses recorded)
Jan 29 03:17:51.719: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 29 03:17:51.719: INFO: sonobuoy-e2e-job-3f32079945944ddf from sonobuoy started at 2023-01-29 02:56:35 +0000 UTC (2 container statuses recorded)
Jan 29 03:17:51.719: INFO: 	Container e2e ready: true, restart count 0
Jan 29 03:17:51.719: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 29 03:17:51.719: INFO: 
Logging pods the apiserver thinks is on node slave2 before test
Jan 29 03:17:51.779: INFO: calico-node-qhb5r from kube-system started at 2023-01-11 07:58:46 +0000 UTC (1 container statuses recorded)
Jan 29 03:17:51.779: INFO: 	Container calico-node ready: true, restart count 2
Jan 29 03:17:51.779: INFO: kube-multus-ds-8gtzz from kube-system started at 2023-01-29 02:43:13 +0000 UTC (1 container statuses recorded)
Jan 29 03:17:51.779: INFO: 	Container kube-multus ready: true, restart count 0
Jan 29 03:17:51.779: INFO: kube-proxy-slave2 from kube-system started at 2023-01-11 07:58:39 +0000 UTC (1 container statuses recorded)
Jan 29 03:17:51.779: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 29 03:17:51.779: INFO: node-problem-detector-m8cck from kube-system started at 2023-01-11 07:58:46 +0000 UTC (1 container statuses recorded)
Jan 29 03:17:51.779: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan 29 03:17:51.779: INFO: pod-update-activedeadlineseconds-5f150d9c-164a-4059-8395-4237582fc0e2 from pods-6732 started at 2023-01-29 03:17:44 +0000 UTC (1 container statuses recorded)
Jan 29 03:17:51.779: INFO: 	Container pause ready: false, restart count 0
Jan 29 03:17:51.779: INFO: pod4-0-sched-preemption-medium-priority from sched-preemption-1064 started at 2023-01-29 03:17:15 +0000 UTC (1 container statuses recorded)
Jan 29 03:17:51.779: INFO: 	Container pod4-0-sched-preemption-medium-priority ready: true, restart count 0
Jan 29 03:17:51.779: INFO: pod4-1-sched-preemption-medium-priority from sched-preemption-1064 started at 2023-01-29 03:17:15 +0000 UTC (1 container statuses recorded)
Jan 29 03:17:51.779: INFO: 	Container pod4-1-sched-preemption-medium-priority ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
STEP: Trying to schedule Pod with nonempty NodeSelector. 01/29/23 03:17:51.779
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.173ea94fda71d0ca], Reason = [FailedScheduling], Message = [0/5 nodes are available: 5 node(s) didn't match Pod's node affinity/selector. preemption: 0/5 nodes are available: 5 Preemption is not helpful for scheduling.] 01/29/23 03:17:51.874
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Jan 29 03:17:52.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9214" for this suite. 01/29/23 03:17:52.9
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","completed":91,"skipped":1647,"failed":0}
------------------------------
• [1.505 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:17:51.429
    Jan 29 03:17:51.429: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename sched-pred 01/29/23 03:17:51.431
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:17:51.502
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:17:51.509
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Jan 29 03:17:51.548: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan 29 03:17:51.592: INFO: Waiting for terminating namespaces to be deleted...
    Jan 29 03:17:51.604: INFO: 
    Logging pods the apiserver thinks is on node master1 before test
    Jan 29 03:17:51.644: INFO: calico-kube-controllers-5c6d4b68d6-6p9cm from kube-system started at 2023-01-11 07:48:50 +0000 UTC (1 container statuses recorded)
    Jan 29 03:17:51.644: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Jan 29 03:17:51.644: INFO: calico-node-j4qnr from kube-system started at 2023-01-11 07:49:07 +0000 UTC (1 container statuses recorded)
    Jan 29 03:17:51.644: INFO: 	Container calico-node ready: true, restart count 2
    Jan 29 03:17:51.644: INFO: cke-admission-daemonset-g5mvj from kube-system started at 2023-01-11 07:51:31 +0000 UTC (1 container statuses recorded)
    Jan 29 03:17:51.644: INFO: 	Container cke-admission ready: true, restart count 0
    Jan 29 03:17:51.644: INFO: cke-controller-manager-master1 from kube-system started at 2023-01-11 07:49:37 +0000 UTC (1 container statuses recorded)
    Jan 29 03:17:51.644: INFO: 	Container cke-controller-manager ready: true, restart count 0
    Jan 29 03:17:51.644: INFO: component-controller-manager-master1 from kube-system started at 2023-01-11 07:49:18 +0000 UTC (1 container statuses recorded)
    Jan 29 03:17:51.644: INFO: 	Container component-controller-manager ready: true, restart count 0
    Jan 29 03:17:51.644: INFO: coredns-tntlt from kube-system started at 2023-01-11 07:49:05 +0000 UTC (1 container statuses recorded)
    Jan 29 03:17:51.644: INFO: 	Container coredns ready: true, restart count 0
    Jan 29 03:17:51.644: INFO: keepalived-master1 from kube-system started at 2023-01-11 07:48:19 +0000 UTC (1 container statuses recorded)
    Jan 29 03:17:51.644: INFO: 	Container keepalived ready: true, restart count 0
    Jan 29 03:17:51.644: INFO: kube-apiserver-master1 from kube-system started at 2023-01-11 07:48:12 +0000 UTC (1 container statuses recorded)
    Jan 29 03:17:51.644: INFO: 	Container kube-apiserver ready: true, restart count 0
    Jan 29 03:17:51.644: INFO: kube-controller-manager-master1 from kube-system started at 2023-01-11 07:48:20 +0000 UTC (1 container statuses recorded)
    Jan 29 03:17:51.644: INFO: 	Container kube-controller-manager ready: true, restart count 5
    Jan 29 03:17:51.644: INFO: kube-multus-ds-8b89r from kube-system started at 2023-01-11 07:51:31 +0000 UTC (1 container statuses recorded)
    Jan 29 03:17:51.644: INFO: 	Container kube-multus ready: true, restart count 0
    Jan 29 03:17:51.644: INFO: kube-proxy-master1 from kube-system started at 2023-01-11 07:48:15 +0000 UTC (1 container statuses recorded)
    Jan 29 03:17:51.644: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan 29 03:17:51.644: INFO: kube-scheduler-master1 from kube-system started at 2023-01-11 07:48:21 +0000 UTC (1 container statuses recorded)
    Jan 29 03:17:51.644: INFO: 	Container kube-scheduler ready: true, restart count 9
    Jan 29 03:17:51.644: INFO: nginx-proxy-master1 from kube-system started at 2023-01-11 07:48:26 +0000 UTC (2 container statuses recorded)
    Jan 29 03:17:51.644: INFO: 	Container kube-apiserver-healthcheck ready: true, restart count 0
    Jan 29 03:17:51.644: INFO: 	Container nginx-proxy ready: true, restart count 0
    Jan 29 03:17:51.644: INFO: node-problem-detector-dgrmp from kube-system started at 2023-01-11 07:51:49 +0000 UTC (1 container statuses recorded)
    Jan 29 03:17:51.644: INFO: 	Container node-problem-detector ready: true, restart count 0
    Jan 29 03:17:51.644: INFO: pod0-1-sched-preemption-medium-priority from sched-preemption-1064 started at 2023-01-29 03:17:15 +0000 UTC (1 container statuses recorded)
    Jan 29 03:17:51.644: INFO: 	Container pod0-1-sched-preemption-medium-priority ready: true, restart count 0
    Jan 29 03:17:51.644: INFO: 
    Logging pods the apiserver thinks is on node master2 before test
    Jan 29 03:17:51.664: INFO: calico-node-sx6hm from kube-system started at 2023-01-11 07:49:07 +0000 UTC (1 container statuses recorded)
    Jan 29 03:17:51.664: INFO: 	Container calico-node ready: true, restart count 2
    Jan 29 03:17:51.664: INFO: cke-admission-daemonset-6v8vg from kube-system started at 2023-01-11 07:51:31 +0000 UTC (1 container statuses recorded)
    Jan 29 03:17:51.664: INFO: 	Container cke-admission ready: true, restart count 0
    Jan 29 03:17:51.664: INFO: cke-controller-manager-master2 from kube-system started at 2023-01-11 07:49:04 +0000 UTC (1 container statuses recorded)
    Jan 29 03:17:51.664: INFO: 	Container cke-controller-manager ready: true, restart count 1
    Jan 29 03:17:51.664: INFO: component-controller-manager-master2 from kube-system started at 2023-01-11 07:49:02 +0000 UTC (1 container statuses recorded)
    Jan 29 03:17:51.664: INFO: 	Container component-controller-manager ready: true, restart count 1
    Jan 29 03:17:51.664: INFO: coredns-m9lv7 from kube-system started at 2023-01-11 07:50:16 +0000 UTC (1 container statuses recorded)
    Jan 29 03:17:51.664: INFO: 	Container coredns ready: true, restart count 0
    Jan 29 03:17:51.664: INFO: keepalived-master2 from kube-system started at 2023-01-11 07:47:57 +0000 UTC (1 container statuses recorded)
    Jan 29 03:17:51.664: INFO: 	Container keepalived ready: true, restart count 0
    Jan 29 03:17:51.664: INFO: kube-apiserver-master2 from kube-system started at 2023-01-11 07:47:49 +0000 UTC (1 container statuses recorded)
    Jan 29 03:17:51.664: INFO: 	Container kube-apiserver ready: true, restart count 0
    Jan 29 03:17:51.664: INFO: kube-controller-manager-master2 from kube-system started at 2023-01-11 07:47:51 +0000 UTC (1 container statuses recorded)
    Jan 29 03:17:51.664: INFO: 	Container kube-controller-manager ready: true, restart count 4
    Jan 29 03:17:51.664: INFO: kube-multus-ds-qq7kz from kube-system started at 2023-01-11 07:51:31 +0000 UTC (1 container statuses recorded)
    Jan 29 03:17:51.664: INFO: 	Container kube-multus ready: true, restart count 0
    Jan 29 03:17:51.664: INFO: kube-proxy-master2 from kube-system started at 2023-01-11 07:47:55 +0000 UTC (1 container statuses recorded)
    Jan 29 03:17:51.664: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan 29 03:17:51.664: INFO: kube-scheduler-master2 from kube-system started at 2023-01-11 07:48:09 +0000 UTC (1 container statuses recorded)
    Jan 29 03:17:51.664: INFO: 	Container kube-scheduler ready: true, restart count 9
    Jan 29 03:17:51.664: INFO: nginx-proxy-master2 from kube-system started at 2023-01-11 07:48:01 +0000 UTC (2 container statuses recorded)
    Jan 29 03:17:51.664: INFO: 	Container kube-apiserver-healthcheck ready: true, restart count 0
    Jan 29 03:17:51.664: INFO: 	Container nginx-proxy ready: true, restart count 0
    Jan 29 03:17:51.664: INFO: node-problem-detector-w69xv from kube-system started at 2023-01-11 07:51:49 +0000 UTC (1 container statuses recorded)
    Jan 29 03:17:51.664: INFO: 	Container node-problem-detector ready: true, restart count 0
    Jan 29 03:17:51.665: INFO: pod1-0-sched-preemption-medium-priority from sched-preemption-1064 started at 2023-01-29 03:17:23 +0000 UTC (1 container statuses recorded)
    Jan 29 03:17:51.665: INFO: 	Container pod1-0-sched-preemption-medium-priority ready: true, restart count 0
    Jan 29 03:17:51.665: INFO: pod1-1-sched-preemption-medium-priority from sched-preemption-1064 started at 2023-01-29 03:17:23 +0000 UTC (1 container statuses recorded)
    Jan 29 03:17:51.665: INFO: 	Container pod1-1-sched-preemption-medium-priority ready: true, restart count 0
    Jan 29 03:17:51.665: INFO: 
    Logging pods the apiserver thinks is on node master3 before test
    Jan 29 03:17:51.698: INFO: calico-node-9b26s from kube-system started at 2023-01-11 07:49:07 +0000 UTC (1 container statuses recorded)
    Jan 29 03:17:51.698: INFO: 	Container calico-node ready: true, restart count 0
    Jan 29 03:17:51.698: INFO: cke-admission-daemonset-mqt4k from kube-system started at 2023-01-11 07:51:31 +0000 UTC (1 container statuses recorded)
    Jan 29 03:17:51.698: INFO: 	Container cke-admission ready: true, restart count 0
    Jan 29 03:17:51.698: INFO: cke-controller-manager-master3 from kube-system started at 2023-01-11 07:49:08 +0000 UTC (1 container statuses recorded)
    Jan 29 03:17:51.698: INFO: 	Container cke-controller-manager ready: true, restart count 0
    Jan 29 03:17:51.698: INFO: component-controller-manager-master3 from kube-system started at 2023-01-11 07:49:17 +0000 UTC (1 container statuses recorded)
    Jan 29 03:17:51.698: INFO: 	Container component-controller-manager ready: true, restart count 0
    Jan 29 03:17:51.698: INFO: coredns-hwvn8 from kube-system started at 2023-01-11 07:49:26 +0000 UTC (1 container statuses recorded)
    Jan 29 03:17:51.698: INFO: 	Container coredns ready: true, restart count 0
    Jan 29 03:17:51.698: INFO: keepalived-master3 from kube-system started at 2023-01-11 07:48:26 +0000 UTC (1 container statuses recorded)
    Jan 29 03:17:51.698: INFO: 	Container keepalived ready: true, restart count 0
    Jan 29 03:17:51.698: INFO: kube-apiserver-master3 from kube-system started at 2023-01-11 07:48:44 +0000 UTC (1 container statuses recorded)
    Jan 29 03:17:51.698: INFO: 	Container kube-apiserver ready: true, restart count 0
    Jan 29 03:17:51.698: INFO: kube-controller-manager-master3 from kube-system started at 2023-01-11 07:48:27 +0000 UTC (1 container statuses recorded)
    Jan 29 03:17:51.698: INFO: 	Container kube-controller-manager ready: true, restart count 4
    Jan 29 03:17:51.698: INFO: kube-multus-ds-wrhjf from kube-system started at 2023-01-11 07:51:31 +0000 UTC (1 container statuses recorded)
    Jan 29 03:17:51.698: INFO: 	Container kube-multus ready: true, restart count 0
    Jan 29 03:17:51.698: INFO: kube-proxy-master3 from kube-system started at 2023-01-11 07:48:37 +0000 UTC (1 container statuses recorded)
    Jan 29 03:17:51.698: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan 29 03:17:51.698: INFO: kube-scheduler-master3 from kube-system started at 2023-01-11 07:48:30 +0000 UTC (1 container statuses recorded)
    Jan 29 03:17:51.698: INFO: 	Container kube-scheduler ready: true, restart count 5
    Jan 29 03:17:51.698: INFO: metrics-server-5584f68fbf-l689z from kube-system started at 2023-01-11 07:50:55 +0000 UTC (1 container statuses recorded)
    Jan 29 03:17:51.698: INFO: 	Container metrics-server ready: true, restart count 0
    Jan 29 03:17:51.698: INFO: nginx-proxy-master3 from kube-system started at 2023-01-11 07:48:32 +0000 UTC (2 container statuses recorded)
    Jan 29 03:17:51.698: INFO: 	Container kube-apiserver-healthcheck ready: true, restart count 0
    Jan 29 03:17:51.698: INFO: 	Container nginx-proxy ready: true, restart count 0
    Jan 29 03:17:51.698: INFO: node-problem-detector-9987m from kube-system started at 2023-01-11 07:51:49 +0000 UTC (1 container statuses recorded)
    Jan 29 03:17:51.698: INFO: 	Container node-problem-detector ready: true, restart count 0
    Jan 29 03:17:51.698: INFO: pod2-0-sched-preemption-medium-priority from sched-preemption-1064 started at 2023-01-29 03:17:21 +0000 UTC (1 container statuses recorded)
    Jan 29 03:17:51.698: INFO: 	Container pod2-0-sched-preemption-medium-priority ready: true, restart count 0
    Jan 29 03:17:51.698: INFO: pod2-1-sched-preemption-medium-priority from sched-preemption-1064 started at 2023-01-29 03:17:21 +0000 UTC (1 container statuses recorded)
    Jan 29 03:17:51.698: INFO: 	Container pod2-1-sched-preemption-medium-priority ready: true, restart count 0
    Jan 29 03:17:51.698: INFO: 
    Logging pods the apiserver thinks is on node slave1 before test
    Jan 29 03:17:51.719: INFO: calico-node-scr7m from kube-system started at 2023-01-11 07:49:07 +0000 UTC (1 container statuses recorded)
    Jan 29 03:17:51.719: INFO: 	Container calico-node ready: true, restart count 0
    Jan 29 03:17:51.719: INFO: kube-multus-ds-ng7xc from kube-system started at 2023-01-29 02:51:08 +0000 UTC (1 container statuses recorded)
    Jan 29 03:17:51.719: INFO: 	Container kube-multus ready: true, restart count 0
    Jan 29 03:17:51.719: INFO: kube-proxy-slave1 from kube-system started at 2023-01-11 07:48:42 +0000 UTC (1 container statuses recorded)
    Jan 29 03:17:51.719: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan 29 03:17:51.719: INFO: node-problem-detector-8bg82 from kube-system started at 2023-01-11 07:51:49 +0000 UTC (1 container statuses recorded)
    Jan 29 03:17:51.719: INFO: 	Container node-problem-detector ready: true, restart count 0
    Jan 29 03:17:51.719: INFO: pod3-0-sched-preemption-medium-priority from sched-preemption-1064 started at 2023-01-29 03:17:37 +0000 UTC (1 container statuses recorded)
    Jan 29 03:17:51.719: INFO: 	Container pod3-0-sched-preemption-medium-priority ready: true, restart count 0
    Jan 29 03:17:51.719: INFO: pod3-1-sched-preemption-medium-priority from sched-preemption-1064 started at 2023-01-29 03:17:37 +0000 UTC (1 container statuses recorded)
    Jan 29 03:17:51.719: INFO: 	Container pod3-1-sched-preemption-medium-priority ready: true, restart count 0
    Jan 29 03:17:51.719: INFO: sonobuoy from sonobuoy started at 2023-01-29 02:56:33 +0000 UTC (1 container statuses recorded)
    Jan 29 03:17:51.719: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan 29 03:17:51.719: INFO: sonobuoy-e2e-job-3f32079945944ddf from sonobuoy started at 2023-01-29 02:56:35 +0000 UTC (2 container statuses recorded)
    Jan 29 03:17:51.719: INFO: 	Container e2e ready: true, restart count 0
    Jan 29 03:17:51.719: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 29 03:17:51.719: INFO: 
    Logging pods the apiserver thinks is on node slave2 before test
    Jan 29 03:17:51.779: INFO: calico-node-qhb5r from kube-system started at 2023-01-11 07:58:46 +0000 UTC (1 container statuses recorded)
    Jan 29 03:17:51.779: INFO: 	Container calico-node ready: true, restart count 2
    Jan 29 03:17:51.779: INFO: kube-multus-ds-8gtzz from kube-system started at 2023-01-29 02:43:13 +0000 UTC (1 container statuses recorded)
    Jan 29 03:17:51.779: INFO: 	Container kube-multus ready: true, restart count 0
    Jan 29 03:17:51.779: INFO: kube-proxy-slave2 from kube-system started at 2023-01-11 07:58:39 +0000 UTC (1 container statuses recorded)
    Jan 29 03:17:51.779: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan 29 03:17:51.779: INFO: node-problem-detector-m8cck from kube-system started at 2023-01-11 07:58:46 +0000 UTC (1 container statuses recorded)
    Jan 29 03:17:51.779: INFO: 	Container node-problem-detector ready: true, restart count 0
    Jan 29 03:17:51.779: INFO: pod-update-activedeadlineseconds-5f150d9c-164a-4059-8395-4237582fc0e2 from pods-6732 started at 2023-01-29 03:17:44 +0000 UTC (1 container statuses recorded)
    Jan 29 03:17:51.779: INFO: 	Container pause ready: false, restart count 0
    Jan 29 03:17:51.779: INFO: pod4-0-sched-preemption-medium-priority from sched-preemption-1064 started at 2023-01-29 03:17:15 +0000 UTC (1 container statuses recorded)
    Jan 29 03:17:51.779: INFO: 	Container pod4-0-sched-preemption-medium-priority ready: true, restart count 0
    Jan 29 03:17:51.779: INFO: pod4-1-sched-preemption-medium-priority from sched-preemption-1064 started at 2023-01-29 03:17:15 +0000 UTC (1 container statuses recorded)
    Jan 29 03:17:51.779: INFO: 	Container pod4-1-sched-preemption-medium-priority ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:438
    STEP: Trying to schedule Pod with nonempty NodeSelector. 01/29/23 03:17:51.779
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.173ea94fda71d0ca], Reason = [FailedScheduling], Message = [0/5 nodes are available: 5 node(s) didn't match Pod's node affinity/selector. preemption: 0/5 nodes are available: 5 Preemption is not helpful for scheduling.] 01/29/23 03:17:51.874
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Jan 29 03:17:52.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-9214" for this suite. 01/29/23 03:17:52.9
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:17:52.935
Jan 29 03:17:52.935: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename webhook 01/29/23 03:17:52.938
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:17:53.014
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:17:53.022
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/29/23 03:17:53.078
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/29/23 03:17:53.745
STEP: Deploying the webhook pod 01/29/23 03:17:53.752
STEP: Wait for the deployment to be ready 01/29/23 03:17:53.774
Jan 29 03:17:53.788: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/29/23 03:17:55.808
STEP: Verifying the service has paired with the endpoint 01/29/23 03:17:55.825
Jan 29 03:17:56.826: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
STEP: fetching the /apis discovery document 01/29/23 03:17:56.839
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 01/29/23 03:17:56.841
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 01/29/23 03:17:56.841
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 01/29/23 03:17:56.841
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 01/29/23 03:17:56.843
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 01/29/23 03:17:56.844
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 01/29/23 03:17:56.846
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 29 03:17:56.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-910" for this suite. 01/29/23 03:17:56.856
STEP: Destroying namespace "webhook-910-markers" for this suite. 01/29/23 03:17:56.869
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","completed":92,"skipped":1651,"failed":0}
------------------------------
• [4.060 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:17:52.935
    Jan 29 03:17:52.935: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename webhook 01/29/23 03:17:52.938
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:17:53.014
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:17:53.022
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/29/23 03:17:53.078
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/29/23 03:17:53.745
    STEP: Deploying the webhook pod 01/29/23 03:17:53.752
    STEP: Wait for the deployment to be ready 01/29/23 03:17:53.774
    Jan 29 03:17:53.788: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/29/23 03:17:55.808
    STEP: Verifying the service has paired with the endpoint 01/29/23 03:17:55.825
    Jan 29 03:17:56.826: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:116
    STEP: fetching the /apis discovery document 01/29/23 03:17:56.839
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 01/29/23 03:17:56.841
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 01/29/23 03:17:56.841
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 01/29/23 03:17:56.841
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 01/29/23 03:17:56.843
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 01/29/23 03:17:56.844
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 01/29/23 03:17:56.846
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 29 03:17:56.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-910" for this suite. 01/29/23 03:17:56.856
    STEP: Destroying namespace "webhook-910-markers" for this suite. 01/29/23 03:17:56.869
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:17:56.997
Jan 29 03:17:56.997: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename webhook 01/29/23 03:17:56.998
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:17:57.039
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:17:57.053
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/29/23 03:17:57.085
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/29/23 03:17:57.814
STEP: Deploying the webhook pod 01/29/23 03:17:57.825
STEP: Wait for the deployment to be ready 01/29/23 03:17:57.854
Jan 29 03:17:57.866: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/29/23 03:17:59.887
STEP: Verifying the service has paired with the endpoint 01/29/23 03:17:59.903
Jan 29 03:18:00.903: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
STEP: Creating a validating webhook configuration 01/29/23 03:18:00.911
STEP: Creating a configMap that does not comply to the validation webhook rules 01/29/23 03:18:00.944
STEP: Updating a validating webhook configuration's rules to not include the create operation 01/29/23 03:18:00.96
STEP: Creating a configMap that does not comply to the validation webhook rules 01/29/23 03:18:00.98
STEP: Patching a validating webhook configuration's rules to include the create operation 01/29/23 03:18:00.998
STEP: Creating a configMap that does not comply to the validation webhook rules 01/29/23 03:18:01.007
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 29 03:18:01.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9488" for this suite. 01/29/23 03:18:01.032
STEP: Destroying namespace "webhook-9488-markers" for this suite. 01/29/23 03:18:01.043
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","completed":93,"skipped":1662,"failed":0}
------------------------------
• [4.163 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:17:56.997
    Jan 29 03:17:56.997: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename webhook 01/29/23 03:17:56.998
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:17:57.039
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:17:57.053
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/29/23 03:17:57.085
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/29/23 03:17:57.814
    STEP: Deploying the webhook pod 01/29/23 03:17:57.825
    STEP: Wait for the deployment to be ready 01/29/23 03:17:57.854
    Jan 29 03:17:57.866: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/29/23 03:17:59.887
    STEP: Verifying the service has paired with the endpoint 01/29/23 03:17:59.903
    Jan 29 03:18:00.903: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:412
    STEP: Creating a validating webhook configuration 01/29/23 03:18:00.911
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/29/23 03:18:00.944
    STEP: Updating a validating webhook configuration's rules to not include the create operation 01/29/23 03:18:00.96
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/29/23 03:18:00.98
    STEP: Patching a validating webhook configuration's rules to include the create operation 01/29/23 03:18:00.998
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/29/23 03:18:01.007
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 29 03:18:01.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-9488" for this suite. 01/29/23 03:18:01.032
    STEP: Destroying namespace "webhook-9488-markers" for this suite. 01/29/23 03:18:01.043
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:18:01.174
Jan 29 03:18:01.174: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename svc-latency 01/29/23 03:18:01.176
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:18:01.212
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:18:01.22
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Jan 29 03:18:01.225: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: creating replication controller svc-latency-rc in namespace svc-latency-1159 01/29/23 03:18:01.226
I0129 03:18:01.236228      22 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-1159, replica count: 1
I0129 03:18:02.287991      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0129 03:18:03.288274      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0129 03:18:04.288522      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 29 03:18:04.460: INFO: Created: latency-svc-f5rkf
Jan 29 03:18:04.492: INFO: Got endpoints: latency-svc-f5rkf [103.032961ms]
Jan 29 03:18:04.580: INFO: Created: latency-svc-fsk9p
Jan 29 03:18:04.619: INFO: Got endpoints: latency-svc-fsk9p [126.243083ms]
Jan 29 03:18:04.636: INFO: Created: latency-svc-g6d2v
Jan 29 03:18:04.650: INFO: Got endpoints: latency-svc-g6d2v [156.134132ms]
Jan 29 03:18:04.675: INFO: Created: latency-svc-m2xqf
Jan 29 03:18:04.700: INFO: Got endpoints: latency-svc-m2xqf [204.131068ms]
Jan 29 03:18:04.718: INFO: Created: latency-svc-prds2
Jan 29 03:18:04.742: INFO: Created: latency-svc-flsdh
Jan 29 03:18:04.742: INFO: Got endpoints: latency-svc-prds2 [245.342357ms]
Jan 29 03:18:04.757: INFO: Got endpoints: latency-svc-flsdh [263.298402ms]
Jan 29 03:18:04.771: INFO: Created: latency-svc-lrgrg
Jan 29 03:18:04.785: INFO: Got endpoints: latency-svc-lrgrg [290.478593ms]
Jan 29 03:18:04.806: INFO: Created: latency-svc-8wggf
Jan 29 03:18:04.820: INFO: Got endpoints: latency-svc-8wggf [323.662905ms]
Jan 29 03:18:04.834: INFO: Created: latency-svc-9x5jq
Jan 29 03:18:04.848: INFO: Got endpoints: latency-svc-9x5jq [351.61634ms]
Jan 29 03:18:04.852: INFO: Created: latency-svc-ncnrh
Jan 29 03:18:04.868: INFO: Got endpoints: latency-svc-ncnrh [373.915437ms]
Jan 29 03:18:04.874: INFO: Created: latency-svc-dqnq9
Jan 29 03:18:04.893: INFO: Got endpoints: latency-svc-dqnq9 [397.403801ms]
Jan 29 03:18:04.900: INFO: Created: latency-svc-gspjx
Jan 29 03:18:04.912: INFO: Got endpoints: latency-svc-gspjx [416.021111ms]
Jan 29 03:18:04.915: INFO: Created: latency-svc-p7lgj
Jan 29 03:18:04.926: INFO: Got endpoints: latency-svc-p7lgj [430.725614ms]
Jan 29 03:18:04.939: INFO: Created: latency-svc-wh8g8
Jan 29 03:18:04.948: INFO: Got endpoints: latency-svc-wh8g8 [452.031023ms]
Jan 29 03:18:04.957: INFO: Created: latency-svc-flmxb
Jan 29 03:18:04.967: INFO: Created: latency-svc-lzjz8
Jan 29 03:18:04.972: INFO: Got endpoints: latency-svc-flmxb [477.3388ms]
Jan 29 03:18:04.979: INFO: Got endpoints: latency-svc-lzjz8 [482.602197ms]
Jan 29 03:18:04.982: INFO: Created: latency-svc-qnt5x
Jan 29 03:18:04.990: INFO: Got endpoints: latency-svc-qnt5x [370.547973ms]
Jan 29 03:18:05.007: INFO: Created: latency-svc-m6qxj
Jan 29 03:18:05.012: INFO: Got endpoints: latency-svc-m6qxj [362.454457ms]
Jan 29 03:18:05.024: INFO: Created: latency-svc-zkx57
Jan 29 03:18:05.030: INFO: Got endpoints: latency-svc-zkx57 [329.931929ms]
Jan 29 03:18:05.035: INFO: Created: latency-svc-fsqff
Jan 29 03:18:05.045: INFO: Got endpoints: latency-svc-fsqff [302.540297ms]
Jan 29 03:18:05.047: INFO: Created: latency-svc-268s5
Jan 29 03:18:05.057: INFO: Got endpoints: latency-svc-268s5 [300.468542ms]
Jan 29 03:18:05.069: INFO: Created: latency-svc-x8w2v
Jan 29 03:18:05.076: INFO: Got endpoints: latency-svc-x8w2v [290.289072ms]
Jan 29 03:18:05.085: INFO: Created: latency-svc-q2kpl
Jan 29 03:18:05.094: INFO: Got endpoints: latency-svc-q2kpl [273.840797ms]
Jan 29 03:18:05.100: INFO: Created: latency-svc-jpqgl
Jan 29 03:18:05.107: INFO: Got endpoints: latency-svc-jpqgl [259.435295ms]
Jan 29 03:18:05.111: INFO: Created: latency-svc-fw4ls
Jan 29 03:18:05.118: INFO: Got endpoints: latency-svc-fw4ls [249.239524ms]
Jan 29 03:18:05.123: INFO: Created: latency-svc-dc4gh
Jan 29 03:18:05.130: INFO: Got endpoints: latency-svc-dc4gh [236.694076ms]
Jan 29 03:18:05.135: INFO: Created: latency-svc-cd54n
Jan 29 03:18:05.144: INFO: Got endpoints: latency-svc-cd54n [231.665761ms]
Jan 29 03:18:05.147: INFO: Created: latency-svc-cfcw9
Jan 29 03:18:05.153: INFO: Got endpoints: latency-svc-cfcw9 [227.445832ms]
Jan 29 03:18:05.160: INFO: Created: latency-svc-cgsbx
Jan 29 03:18:05.166: INFO: Got endpoints: latency-svc-cgsbx [218.362948ms]
Jan 29 03:18:05.172: INFO: Created: latency-svc-7nttd
Jan 29 03:18:05.179: INFO: Got endpoints: latency-svc-7nttd [206.329023ms]
Jan 29 03:18:05.190: INFO: Created: latency-svc-fc6tk
Jan 29 03:18:05.191: INFO: Got endpoints: latency-svc-fc6tk [212.121664ms]
Jan 29 03:18:05.197: INFO: Created: latency-svc-tzhvr
Jan 29 03:18:05.205: INFO: Got endpoints: latency-svc-tzhvr [215.066905ms]
Jan 29 03:18:05.210: INFO: Created: latency-svc-7kgl9
Jan 29 03:18:05.216: INFO: Got endpoints: latency-svc-7kgl9 [203.610325ms]
Jan 29 03:18:05.224: INFO: Created: latency-svc-tdmnw
Jan 29 03:18:05.234: INFO: Got endpoints: latency-svc-tdmnw [204.210969ms]
Jan 29 03:18:05.236: INFO: Created: latency-svc-vsbgs
Jan 29 03:18:05.242: INFO: Got endpoints: latency-svc-vsbgs [197.388841ms]
Jan 29 03:18:05.248: INFO: Created: latency-svc-lbsvr
Jan 29 03:18:05.255: INFO: Got endpoints: latency-svc-lbsvr [197.920245ms]
Jan 29 03:18:05.260: INFO: Created: latency-svc-n27xb
Jan 29 03:18:05.272: INFO: Got endpoints: latency-svc-n27xb [196.125033ms]
Jan 29 03:18:05.272: INFO: Created: latency-svc-z22s4
Jan 29 03:18:05.279: INFO: Got endpoints: latency-svc-z22s4 [185.585079ms]
Jan 29 03:18:05.284: INFO: Created: latency-svc-htfgc
Jan 29 03:18:05.291: INFO: Got endpoints: latency-svc-htfgc [183.814527ms]
Jan 29 03:18:05.297: INFO: Created: latency-svc-z4fgg
Jan 29 03:18:05.307: INFO: Got endpoints: latency-svc-z4fgg [189.162524ms]
Jan 29 03:18:05.311: INFO: Created: latency-svc-wpqmc
Jan 29 03:18:05.315: INFO: Got endpoints: latency-svc-wpqmc [185.456178ms]
Jan 29 03:18:05.320: INFO: Created: latency-svc-b7xfk
Jan 29 03:18:05.326: INFO: Got endpoints: latency-svc-b7xfk [182.680118ms]
Jan 29 03:18:05.332: INFO: Created: latency-svc-rp29z
Jan 29 03:18:05.339: INFO: Got endpoints: latency-svc-rp29z [185.549139ms]
Jan 29 03:18:05.343: INFO: Created: latency-svc-n6nvb
Jan 29 03:18:05.350: INFO: Got endpoints: latency-svc-n6nvb [183.396923ms]
Jan 29 03:18:05.356: INFO: Created: latency-svc-tgrg2
Jan 29 03:18:05.364: INFO: Got endpoints: latency-svc-tgrg2 [184.942474ms]
Jan 29 03:18:05.368: INFO: Created: latency-svc-l77dd
Jan 29 03:18:05.375: INFO: Got endpoints: latency-svc-l77dd [183.216903ms]
Jan 29 03:18:05.380: INFO: Created: latency-svc-xgdlc
Jan 29 03:18:05.392: INFO: Got endpoints: latency-svc-xgdlc [186.646626ms]
Jan 29 03:18:05.395: INFO: Created: latency-svc-zhbtr
Jan 29 03:18:05.413: INFO: Created: latency-svc-677z9
Jan 29 03:18:05.420: INFO: Created: latency-svc-89rl6
Jan 29 03:18:05.430: INFO: Created: latency-svc-6hnp9
Jan 29 03:18:05.442: INFO: Got endpoints: latency-svc-zhbtr [225.437818ms]
Jan 29 03:18:05.443: INFO: Created: latency-svc-c9cb5
Jan 29 03:18:05.454: INFO: Created: latency-svc-b4nzz
Jan 29 03:18:05.468: INFO: Created: latency-svc-zg88h
Jan 29 03:18:05.483: INFO: Created: latency-svc-qg6nc
Jan 29 03:18:05.494: INFO: Got endpoints: latency-svc-677z9 [259.575957ms]
Jan 29 03:18:05.501: INFO: Created: latency-svc-csdc4
Jan 29 03:18:05.517: INFO: Created: latency-svc-vlgg4
Jan 29 03:18:05.532: INFO: Created: latency-svc-6zx9z
Jan 29 03:18:05.547: INFO: Got endpoints: latency-svc-89rl6 [305.292497ms]
Jan 29 03:18:05.553: INFO: Created: latency-svc-xr8bm
Jan 29 03:18:05.570: INFO: Created: latency-svc-lmtmg
Jan 29 03:18:05.581: INFO: Created: latency-svc-t8qlk
Jan 29 03:18:05.595: INFO: Got endpoints: latency-svc-6hnp9 [339.873018ms]
Jan 29 03:18:05.604: INFO: Created: latency-svc-htv82
Jan 29 03:18:05.614: INFO: Created: latency-svc-4vg9q
Jan 29 03:18:05.628: INFO: Created: latency-svc-92755
Jan 29 03:18:05.644: INFO: Created: latency-svc-8bmjs
Jan 29 03:18:05.645: INFO: Got endpoints: latency-svc-c9cb5 [372.409746ms]
Jan 29 03:18:05.655: INFO: Created: latency-svc-hgvrg
Jan 29 03:18:05.667: INFO: Created: latency-svc-npvdk
Jan 29 03:18:05.692: INFO: Got endpoints: latency-svc-b4nzz [412.754548ms]
Jan 29 03:18:05.724: INFO: Created: latency-svc-4mdw7
Jan 29 03:18:05.749: INFO: Got endpoints: latency-svc-zg88h [457.875984ms]
Jan 29 03:18:05.794: INFO: Created: latency-svc-d4wcn
Jan 29 03:18:05.795: INFO: Got endpoints: latency-svc-qg6nc [488.461598ms]
Jan 29 03:18:05.829: INFO: Created: latency-svc-7xv9x
Jan 29 03:18:05.851: INFO: Got endpoints: latency-svc-csdc4 [535.92805ms]
Jan 29 03:18:05.885: INFO: Created: latency-svc-v7wkf
Jan 29 03:18:05.899: INFO: Got endpoints: latency-svc-vlgg4 [572.253925ms]
Jan 29 03:18:05.942: INFO: Created: latency-svc-tvwqs
Jan 29 03:18:05.948: INFO: Got endpoints: latency-svc-6zx9z [608.852281ms]
Jan 29 03:18:05.982: INFO: Created: latency-svc-674dq
Jan 29 03:18:05.999: INFO: Got endpoints: latency-svc-xr8bm [649.265464ms]
Jan 29 03:18:06.049: INFO: Got endpoints: latency-svc-lmtmg [685.628498ms]
Jan 29 03:18:06.051: INFO: Created: latency-svc-9p97k
Jan 29 03:18:06.094: INFO: Got endpoints: latency-svc-t8qlk [719.615016ms]
Jan 29 03:18:06.101: INFO: Created: latency-svc-j4w46
Jan 29 03:18:06.129: INFO: Created: latency-svc-lxfrt
Jan 29 03:18:06.145: INFO: Got endpoints: latency-svc-htv82 [752.132823ms]
Jan 29 03:18:06.189: INFO: Created: latency-svc-7q682
Jan 29 03:18:06.197: INFO: Got endpoints: latency-svc-4vg9q [755.692208ms]
Jan 29 03:18:06.224: INFO: Created: latency-svc-hrzgh
Jan 29 03:18:06.245: INFO: Got endpoints: latency-svc-92755 [751.148436ms]
Jan 29 03:18:06.282: INFO: Created: latency-svc-lrhhd
Jan 29 03:18:06.294: INFO: Got endpoints: latency-svc-8bmjs [746.534524ms]
Jan 29 03:18:06.317: INFO: Created: latency-svc-7dcsd
Jan 29 03:18:06.343: INFO: Got endpoints: latency-svc-hgvrg [747.41115ms]
Jan 29 03:18:06.362: INFO: Created: latency-svc-g5hpn
Jan 29 03:18:06.391: INFO: Got endpoints: latency-svc-npvdk [746.668964ms]
Jan 29 03:18:06.414: INFO: Created: latency-svc-vx4r8
Jan 29 03:18:06.441: INFO: Got endpoints: latency-svc-4mdw7 [748.618998ms]
Jan 29 03:18:06.470: INFO: Created: latency-svc-wb49n
Jan 29 03:18:06.499: INFO: Got endpoints: latency-svc-d4wcn [749.500904ms]
Jan 29 03:18:06.530: INFO: Created: latency-svc-bplrl
Jan 29 03:18:06.542: INFO: Got endpoints: latency-svc-7xv9x [746.732845ms]
Jan 29 03:18:06.568: INFO: Created: latency-svc-wmj9h
Jan 29 03:18:06.594: INFO: Got endpoints: latency-svc-v7wkf [742.879758ms]
Jan 29 03:18:06.622: INFO: Created: latency-svc-6skcz
Jan 29 03:18:06.650: INFO: Got endpoints: latency-svc-tvwqs [751.110816ms]
Jan 29 03:18:06.676: INFO: Created: latency-svc-zpvw9
Jan 29 03:18:06.694: INFO: Got endpoints: latency-svc-674dq [745.96966ms]
Jan 29 03:18:06.716: INFO: Created: latency-svc-psn7w
Jan 29 03:18:06.741: INFO: Got endpoints: latency-svc-9p97k [741.458928ms]
Jan 29 03:18:06.760: INFO: Created: latency-svc-4kpfj
Jan 29 03:18:06.792: INFO: Got endpoints: latency-svc-j4w46 [742.563176ms]
Jan 29 03:18:06.813: INFO: Created: latency-svc-rvd6m
Jan 29 03:18:06.844: INFO: Got endpoints: latency-svc-lxfrt [749.627066ms]
Jan 29 03:18:06.876: INFO: Created: latency-svc-h7fbk
Jan 29 03:18:06.899: INFO: Got endpoints: latency-svc-7q682 [754.412739ms]
Jan 29 03:18:06.927: INFO: Created: latency-svc-xmk5j
Jan 29 03:18:06.945: INFO: Got endpoints: latency-svc-hrzgh [747.172828ms]
Jan 29 03:18:06.973: INFO: Created: latency-svc-5twch
Jan 29 03:18:06.991: INFO: Got endpoints: latency-svc-lrhhd [746.507144ms]
Jan 29 03:18:07.017: INFO: Created: latency-svc-jgp7s
Jan 29 03:18:07.039: INFO: Got endpoints: latency-svc-7dcsd [744.965753ms]
Jan 29 03:18:07.061: INFO: Created: latency-svc-xklk7
Jan 29 03:18:07.090: INFO: Got endpoints: latency-svc-g5hpn [747.127488ms]
Jan 29 03:18:07.110: INFO: Created: latency-svc-fz8g6
Jan 29 03:18:07.140: INFO: Got endpoints: latency-svc-vx4r8 [748.559778ms]
Jan 29 03:18:07.158: INFO: Created: latency-svc-82gfr
Jan 29 03:18:07.193: INFO: Got endpoints: latency-svc-wb49n [752.210584ms]
Jan 29 03:18:07.210: INFO: Created: latency-svc-j5mcs
Jan 29 03:18:07.239: INFO: Got endpoints: latency-svc-bplrl [740.413761ms]
Jan 29 03:18:07.261: INFO: Created: latency-svc-8xjr5
Jan 29 03:18:07.290: INFO: Got endpoints: latency-svc-wmj9h [747.674372ms]
Jan 29 03:18:07.308: INFO: Created: latency-svc-bwbpf
Jan 29 03:18:07.339: INFO: Got endpoints: latency-svc-6skcz [745.175214ms]
Jan 29 03:18:07.362: INFO: Created: latency-svc-zxdkf
Jan 29 03:18:07.393: INFO: Got endpoints: latency-svc-zpvw9 [742.776138ms]
Jan 29 03:18:07.421: INFO: Created: latency-svc-mm5ff
Jan 29 03:18:07.442: INFO: Got endpoints: latency-svc-psn7w [748.311836ms]
Jan 29 03:18:07.462: INFO: Created: latency-svc-n7pxk
Jan 29 03:18:07.492: INFO: Got endpoints: latency-svc-4kpfj [751.234596ms]
Jan 29 03:18:07.520: INFO: Created: latency-svc-788lb
Jan 29 03:18:07.546: INFO: Got endpoints: latency-svc-rvd6m [753.722294ms]
Jan 29 03:18:07.566: INFO: Created: latency-svc-gqhh7
Jan 29 03:18:07.590: INFO: Got endpoints: latency-svc-h7fbk [746.296343ms]
Jan 29 03:18:07.608: INFO: Created: latency-svc-w4ldx
Jan 29 03:18:07.645: INFO: Got endpoints: latency-svc-xmk5j [745.812038ms]
Jan 29 03:18:07.662: INFO: Created: latency-svc-c898p
Jan 29 03:18:07.691: INFO: Got endpoints: latency-svc-5twch [746.343042ms]
Jan 29 03:18:07.708: INFO: Created: latency-svc-wm9ft
Jan 29 03:18:07.740: INFO: Got endpoints: latency-svc-jgp7s [748.534238ms]
Jan 29 03:18:07.756: INFO: Created: latency-svc-rd8qb
Jan 29 03:18:07.790: INFO: Got endpoints: latency-svc-xklk7 [750.802053ms]
Jan 29 03:18:07.807: INFO: Created: latency-svc-28fld
Jan 29 03:18:07.840: INFO: Got endpoints: latency-svc-fz8g6 [749.468484ms]
Jan 29 03:18:07.857: INFO: Created: latency-svc-htks9
Jan 29 03:18:07.891: INFO: Got endpoints: latency-svc-82gfr [750.783774ms]
Jan 29 03:18:07.908: INFO: Created: latency-svc-d6985
Jan 29 03:18:07.940: INFO: Got endpoints: latency-svc-j5mcs [746.558684ms]
Jan 29 03:18:07.958: INFO: Created: latency-svc-r6clp
Jan 29 03:18:07.990: INFO: Got endpoints: latency-svc-8xjr5 [750.566612ms]
Jan 29 03:18:08.010: INFO: Created: latency-svc-2zr7p
Jan 29 03:18:08.041: INFO: Got endpoints: latency-svc-bwbpf [751.238817ms]
Jan 29 03:18:08.059: INFO: Created: latency-svc-xd958
Jan 29 03:18:08.090: INFO: Got endpoints: latency-svc-zxdkf [750.971215ms]
Jan 29 03:18:08.108: INFO: Created: latency-svc-5c7pj
Jan 29 03:18:08.141: INFO: Got endpoints: latency-svc-mm5ff [748.130656ms]
Jan 29 03:18:08.157: INFO: Created: latency-svc-lhjsm
Jan 29 03:18:08.190: INFO: Got endpoints: latency-svc-n7pxk [747.42175ms]
Jan 29 03:18:08.210: INFO: Created: latency-svc-pll2q
Jan 29 03:18:08.239: INFO: Got endpoints: latency-svc-788lb [746.913066ms]
Jan 29 03:18:08.258: INFO: Created: latency-svc-sfw2z
Jan 29 03:18:08.298: INFO: Got endpoints: latency-svc-gqhh7 [752.154323ms]
Jan 29 03:18:08.351: INFO: Got endpoints: latency-svc-w4ldx [760.776124ms]
Jan 29 03:18:08.368: INFO: Created: latency-svc-rvbfh
Jan 29 03:18:08.402: INFO: Created: latency-svc-b5wd2
Jan 29 03:18:08.402: INFO: Got endpoints: latency-svc-c898p [757.331559ms]
Jan 29 03:18:08.430: INFO: Created: latency-svc-qnbcf
Jan 29 03:18:08.441: INFO: Got endpoints: latency-svc-wm9ft [749.667726ms]
Jan 29 03:18:08.461: INFO: Created: latency-svc-2q8qq
Jan 29 03:18:08.492: INFO: Got endpoints: latency-svc-rd8qb [751.959462ms]
Jan 29 03:18:08.514: INFO: Created: latency-svc-xwxrq
Jan 29 03:18:08.542: INFO: Got endpoints: latency-svc-28fld [751.922761ms]
Jan 29 03:18:08.562: INFO: Created: latency-svc-csm4z
Jan 29 03:18:08.592: INFO: Got endpoints: latency-svc-htks9 [752.137823ms]
Jan 29 03:18:08.613: INFO: Created: latency-svc-gtm94
Jan 29 03:18:08.640: INFO: Got endpoints: latency-svc-d6985 [748.621279ms]
Jan 29 03:18:08.657: INFO: Created: latency-svc-q9l52
Jan 29 03:18:08.689: INFO: Got endpoints: latency-svc-r6clp [749.604685ms]
Jan 29 03:18:08.706: INFO: Created: latency-svc-w2gnh
Jan 29 03:18:08.739: INFO: Got endpoints: latency-svc-2zr7p [748.666859ms]
Jan 29 03:18:08.756: INFO: Created: latency-svc-l6b86
Jan 29 03:18:08.789: INFO: Got endpoints: latency-svc-xd958 [747.800993ms]
Jan 29 03:18:08.805: INFO: Created: latency-svc-wdzl9
Jan 29 03:18:08.842: INFO: Got endpoints: latency-svc-5c7pj [751.165196ms]
Jan 29 03:18:08.862: INFO: Created: latency-svc-brqhx
Jan 29 03:18:08.891: INFO: Got endpoints: latency-svc-lhjsm [749.417784ms]
Jan 29 03:18:08.907: INFO: Created: latency-svc-6gdgp
Jan 29 03:18:08.940: INFO: Got endpoints: latency-svc-pll2q [749.771967ms]
Jan 29 03:18:08.968: INFO: Created: latency-svc-t9kvv
Jan 29 03:18:08.991: INFO: Got endpoints: latency-svc-sfw2z [751.489378ms]
Jan 29 03:18:09.009: INFO: Created: latency-svc-b7gw9
Jan 29 03:18:09.041: INFO: Got endpoints: latency-svc-rvbfh [742.663576ms]
Jan 29 03:18:09.059: INFO: Created: latency-svc-vxsj2
Jan 29 03:18:09.091: INFO: Got endpoints: latency-svc-b5wd2 [739.388274ms]
Jan 29 03:18:09.107: INFO: Created: latency-svc-mbpj2
Jan 29 03:18:09.141: INFO: Got endpoints: latency-svc-qnbcf [738.746589ms]
Jan 29 03:18:09.162: INFO: Created: latency-svc-npghm
Jan 29 03:18:09.191: INFO: Got endpoints: latency-svc-2q8qq [750.639013ms]
Jan 29 03:18:09.208: INFO: Created: latency-svc-x5xts
Jan 29 03:18:09.244: INFO: Got endpoints: latency-svc-xwxrq [751.427138ms]
Jan 29 03:18:09.263: INFO: Created: latency-svc-6nsdg
Jan 29 03:18:09.292: INFO: Got endpoints: latency-svc-csm4z [750.088529ms]
Jan 29 03:18:09.311: INFO: Created: latency-svc-cls9n
Jan 29 03:18:09.341: INFO: Got endpoints: latency-svc-gtm94 [749.226762ms]
Jan 29 03:18:09.366: INFO: Created: latency-svc-22fm2
Jan 29 03:18:09.391: INFO: Got endpoints: latency-svc-q9l52 [750.692533ms]
Jan 29 03:18:09.408: INFO: Created: latency-svc-42dhj
Jan 29 03:18:09.442: INFO: Got endpoints: latency-svc-w2gnh [752.775068ms]
Jan 29 03:18:09.461: INFO: Created: latency-svc-vt62n
Jan 29 03:18:09.491: INFO: Got endpoints: latency-svc-l6b86 [752.528925ms]
Jan 29 03:18:09.515: INFO: Created: latency-svc-dq5tk
Jan 29 03:18:09.554: INFO: Got endpoints: latency-svc-wdzl9 [764.166168ms]
Jan 29 03:18:09.580: INFO: Created: latency-svc-wp957
Jan 29 03:18:09.595: INFO: Got endpoints: latency-svc-brqhx [752.937948ms]
Jan 29 03:18:09.618: INFO: Created: latency-svc-mf69d
Jan 29 03:18:09.641: INFO: Got endpoints: latency-svc-6gdgp [750.787853ms]
Jan 29 03:18:09.660: INFO: Created: latency-svc-8fcs2
Jan 29 03:18:09.690: INFO: Got endpoints: latency-svc-t9kvv [749.699926ms]
Jan 29 03:18:09.705: INFO: Created: latency-svc-rrpmj
Jan 29 03:18:09.739: INFO: Got endpoints: latency-svc-b7gw9 [748.444777ms]
Jan 29 03:18:09.757: INFO: Created: latency-svc-4vc86
Jan 29 03:18:09.790: INFO: Got endpoints: latency-svc-vxsj2 [748.651818ms]
Jan 29 03:18:09.806: INFO: Created: latency-svc-zmtbk
Jan 29 03:18:09.840: INFO: Got endpoints: latency-svc-mbpj2 [748.791839ms]
Jan 29 03:18:09.857: INFO: Created: latency-svc-8thfl
Jan 29 03:18:09.890: INFO: Got endpoints: latency-svc-npghm [748.92164ms]
Jan 29 03:18:09.909: INFO: Created: latency-svc-6wq7c
Jan 29 03:18:09.941: INFO: Got endpoints: latency-svc-x5xts [748.96534ms]
Jan 29 03:18:09.959: INFO: Created: latency-svc-tc8dn
Jan 29 03:18:09.991: INFO: Got endpoints: latency-svc-6nsdg [747.217749ms]
Jan 29 03:18:10.008: INFO: Created: latency-svc-5nvkm
Jan 29 03:18:10.040: INFO: Got endpoints: latency-svc-cls9n [747.303389ms]
Jan 29 03:18:10.058: INFO: Created: latency-svc-6plkv
Jan 29 03:18:10.091: INFO: Got endpoints: latency-svc-22fm2 [749.197503ms]
Jan 29 03:18:10.107: INFO: Created: latency-svc-prbw9
Jan 29 03:18:10.140: INFO: Got endpoints: latency-svc-42dhj [748.992041ms]
Jan 29 03:18:10.156: INFO: Created: latency-svc-wlx79
Jan 29 03:18:10.191: INFO: Got endpoints: latency-svc-vt62n [748.308457ms]
Jan 29 03:18:10.209: INFO: Created: latency-svc-pczrl
Jan 29 03:18:10.240: INFO: Got endpoints: latency-svc-dq5tk [748.068874ms]
Jan 29 03:18:10.257: INFO: Created: latency-svc-g79zd
Jan 29 03:18:10.290: INFO: Got endpoints: latency-svc-wp957 [736.470914ms]
Jan 29 03:18:10.307: INFO: Created: latency-svc-r7vxd
Jan 29 03:18:10.340: INFO: Got endpoints: latency-svc-mf69d [745.321095ms]
Jan 29 03:18:10.357: INFO: Created: latency-svc-bncbb
Jan 29 03:18:10.395: INFO: Got endpoints: latency-svc-8fcs2 [753.01993ms]
Jan 29 03:18:10.415: INFO: Created: latency-svc-6ksxt
Jan 29 03:18:10.440: INFO: Got endpoints: latency-svc-rrpmj [750.545532ms]
Jan 29 03:18:10.462: INFO: Created: latency-svc-wgsb8
Jan 29 03:18:10.499: INFO: Got endpoints: latency-svc-4vc86 [759.915617ms]
Jan 29 03:18:10.523: INFO: Created: latency-svc-5jkwd
Jan 29 03:18:10.542: INFO: Got endpoints: latency-svc-zmtbk [752.628387ms]
Jan 29 03:18:10.588: INFO: Created: latency-svc-5xdpp
Jan 29 03:18:10.596: INFO: Got endpoints: latency-svc-8thfl [756.06073ms]
Jan 29 03:18:10.617: INFO: Created: latency-svc-r6n5j
Jan 29 03:18:10.642: INFO: Got endpoints: latency-svc-6wq7c [751.875901ms]
Jan 29 03:18:10.662: INFO: Created: latency-svc-tvxbg
Jan 29 03:18:10.690: INFO: Got endpoints: latency-svc-tc8dn [749.376144ms]
Jan 29 03:18:10.706: INFO: Created: latency-svc-lgd2j
Jan 29 03:18:10.740: INFO: Got endpoints: latency-svc-5nvkm [748.975561ms]
Jan 29 03:18:10.759: INFO: Created: latency-svc-b7p2n
Jan 29 03:18:10.790: INFO: Got endpoints: latency-svc-6plkv [749.891667ms]
Jan 29 03:18:10.807: INFO: Created: latency-svc-ggs89
Jan 29 03:18:10.840: INFO: Got endpoints: latency-svc-prbw9 [749.362444ms]
Jan 29 03:18:10.867: INFO: Created: latency-svc-4md5p
Jan 29 03:18:10.892: INFO: Got endpoints: latency-svc-wlx79 [752.687967ms]
Jan 29 03:18:10.911: INFO: Created: latency-svc-lzz5j
Jan 29 03:18:10.940: INFO: Got endpoints: latency-svc-pczrl [748.945801ms]
Jan 29 03:18:10.959: INFO: Created: latency-svc-dh5fj
Jan 29 03:18:10.992: INFO: Got endpoints: latency-svc-g79zd [752.202424ms]
Jan 29 03:18:11.009: INFO: Created: latency-svc-wrlfj
Jan 29 03:18:11.040: INFO: Got endpoints: latency-svc-r7vxd [749.483525ms]
Jan 29 03:18:11.057: INFO: Created: latency-svc-dn79w
Jan 29 03:18:11.090: INFO: Got endpoints: latency-svc-bncbb [749.890167ms]
Jan 29 03:18:11.108: INFO: Created: latency-svc-g4vvm
Jan 29 03:18:11.140: INFO: Got endpoints: latency-svc-6ksxt [745.461637ms]
Jan 29 03:18:11.158: INFO: Created: latency-svc-pgz2s
Jan 29 03:18:11.190: INFO: Got endpoints: latency-svc-wgsb8 [749.876887ms]
Jan 29 03:18:11.210: INFO: Created: latency-svc-fpp9s
Jan 29 03:18:11.240: INFO: Got endpoints: latency-svc-5jkwd [740.207239ms]
Jan 29 03:18:11.257: INFO: Created: latency-svc-wc299
Jan 29 03:18:11.291: INFO: Got endpoints: latency-svc-5xdpp [748.109115ms]
Jan 29 03:18:11.308: INFO: Created: latency-svc-82bhs
Jan 29 03:18:11.340: INFO: Got endpoints: latency-svc-r6n5j [744.131987ms]
Jan 29 03:18:11.357: INFO: Created: latency-svc-8wfsv
Jan 29 03:18:11.391: INFO: Got endpoints: latency-svc-tvxbg [749.259803ms]
Jan 29 03:18:11.407: INFO: Created: latency-svc-l6pt2
Jan 29 03:18:11.440: INFO: Got endpoints: latency-svc-lgd2j [749.809867ms]
Jan 29 03:18:11.460: INFO: Created: latency-svc-x4spt
Jan 29 03:18:11.491: INFO: Got endpoints: latency-svc-b7p2n [750.519992ms]
Jan 29 03:18:11.518: INFO: Created: latency-svc-5wq2j
Jan 29 03:18:11.548: INFO: Got endpoints: latency-svc-ggs89 [758.188725ms]
Jan 29 03:18:11.572: INFO: Created: latency-svc-wt5hn
Jan 29 03:18:11.591: INFO: Got endpoints: latency-svc-4md5p [750.542791ms]
Jan 29 03:18:11.611: INFO: Created: latency-svc-gzqq7
Jan 29 03:18:11.641: INFO: Got endpoints: latency-svc-lzz5j [748.527638ms]
Jan 29 03:18:11.662: INFO: Created: latency-svc-m4wdt
Jan 29 03:18:11.692: INFO: Got endpoints: latency-svc-dh5fj [751.832621ms]
Jan 29 03:18:11.710: INFO: Created: latency-svc-29978
Jan 29 03:18:11.739: INFO: Got endpoints: latency-svc-wrlfj [747.397469ms]
Jan 29 03:18:11.756: INFO: Created: latency-svc-fnnv2
Jan 29 03:18:11.791: INFO: Got endpoints: latency-svc-dn79w [750.936295ms]
Jan 29 03:18:11.810: INFO: Created: latency-svc-ng6qf
Jan 29 03:18:11.842: INFO: Got endpoints: latency-svc-g4vvm [752.338064ms]
Jan 29 03:18:11.860: INFO: Created: latency-svc-8tz2k
Jan 29 03:18:11.890: INFO: Got endpoints: latency-svc-pgz2s [749.534565ms]
Jan 29 03:18:11.906: INFO: Created: latency-svc-7jzvt
Jan 29 03:18:11.940: INFO: Got endpoints: latency-svc-fpp9s [749.302623ms]
Jan 29 03:18:11.956: INFO: Created: latency-svc-7wzdq
Jan 29 03:18:11.992: INFO: Got endpoints: latency-svc-wc299 [752.519846ms]
Jan 29 03:18:12.010: INFO: Created: latency-svc-x9hc8
Jan 29 03:18:12.042: INFO: Got endpoints: latency-svc-82bhs [750.904514ms]
Jan 29 03:18:12.061: INFO: Created: latency-svc-5q8vb
Jan 29 03:18:12.090: INFO: Got endpoints: latency-svc-8wfsv [750.24057ms]
Jan 29 03:18:12.108: INFO: Created: latency-svc-wxf5t
Jan 29 03:18:12.142: INFO: Got endpoints: latency-svc-l6pt2 [749.967948ms]
Jan 29 03:18:12.160: INFO: Created: latency-svc-pzh8n
Jan 29 03:18:12.190: INFO: Got endpoints: latency-svc-x4spt [749.653046ms]
Jan 29 03:18:12.213: INFO: Created: latency-svc-z2bhr
Jan 29 03:18:12.240: INFO: Got endpoints: latency-svc-5wq2j [748.85446ms]
Jan 29 03:18:12.257: INFO: Created: latency-svc-dkhxz
Jan 29 03:18:12.291: INFO: Got endpoints: latency-svc-wt5hn [743.18232ms]
Jan 29 03:18:12.307: INFO: Created: latency-svc-stwbg
Jan 29 03:18:12.341: INFO: Got endpoints: latency-svc-gzqq7 [749.791046ms]
Jan 29 03:18:12.359: INFO: Created: latency-svc-7fb26
Jan 29 03:18:12.390: INFO: Got endpoints: latency-svc-m4wdt [748.87154ms]
Jan 29 03:18:12.439: INFO: Got endpoints: latency-svc-29978 [747.33373ms]
Jan 29 03:18:12.492: INFO: Got endpoints: latency-svc-fnnv2 [752.126683ms]
Jan 29 03:18:12.543: INFO: Got endpoints: latency-svc-ng6qf [751.973282ms]
Jan 29 03:18:12.591: INFO: Got endpoints: latency-svc-8tz2k [748.600038ms]
Jan 29 03:18:12.642: INFO: Got endpoints: latency-svc-7jzvt [752.067563ms]
Jan 29 03:18:12.690: INFO: Got endpoints: latency-svc-7wzdq [750.425351ms]
Jan 29 03:18:12.748: INFO: Got endpoints: latency-svc-x9hc8 [756.01453ms]
Jan 29 03:18:12.792: INFO: Got endpoints: latency-svc-5q8vb [749.881307ms]
Jan 29 03:18:12.842: INFO: Got endpoints: latency-svc-wxf5t [751.65044ms]
Jan 29 03:18:12.892: INFO: Got endpoints: latency-svc-pzh8n [750.048549ms]
Jan 29 03:18:12.941: INFO: Got endpoints: latency-svc-z2bhr [750.854015ms]
Jan 29 03:18:12.990: INFO: Got endpoints: latency-svc-dkhxz [750.26259ms]
Jan 29 03:18:13.040: INFO: Got endpoints: latency-svc-stwbg [748.454977ms]
Jan 29 03:18:13.091: INFO: Got endpoints: latency-svc-7fb26 [750.472611ms]
Jan 29 03:18:13.092: INFO: Latencies: [126.243083ms 156.134132ms 182.680118ms 183.216903ms 183.396923ms 183.814527ms 184.942474ms 185.456178ms 185.549139ms 185.585079ms 186.646626ms 189.162524ms 196.125033ms 197.388841ms 197.920245ms 203.610325ms 204.131068ms 204.210969ms 206.329023ms 212.121664ms 215.066905ms 218.362948ms 225.437818ms 227.445832ms 231.665761ms 236.694076ms 245.342357ms 249.239524ms 259.435295ms 259.575957ms 263.298402ms 273.840797ms 290.289072ms 290.478593ms 300.468542ms 302.540297ms 305.292497ms 323.662905ms 329.931929ms 339.873018ms 351.61634ms 362.454457ms 370.547973ms 372.409746ms 373.915437ms 397.403801ms 412.754548ms 416.021111ms 430.725614ms 452.031023ms 457.875984ms 477.3388ms 482.602197ms 488.461598ms 535.92805ms 572.253925ms 608.852281ms 649.265464ms 685.628498ms 719.615016ms 736.470914ms 738.746589ms 739.388274ms 740.207239ms 740.413761ms 741.458928ms 742.563176ms 742.663576ms 742.776138ms 742.879758ms 743.18232ms 744.131987ms 744.965753ms 745.175214ms 745.321095ms 745.461637ms 745.812038ms 745.96966ms 746.296343ms 746.343042ms 746.507144ms 746.534524ms 746.558684ms 746.668964ms 746.732845ms 746.913066ms 747.127488ms 747.172828ms 747.217749ms 747.303389ms 747.33373ms 747.397469ms 747.41115ms 747.42175ms 747.674372ms 747.800993ms 748.068874ms 748.109115ms 748.130656ms 748.308457ms 748.311836ms 748.444777ms 748.454977ms 748.527638ms 748.534238ms 748.559778ms 748.600038ms 748.618998ms 748.621279ms 748.651818ms 748.666859ms 748.791839ms 748.85446ms 748.87154ms 748.92164ms 748.945801ms 748.96534ms 748.975561ms 748.992041ms 749.197503ms 749.226762ms 749.259803ms 749.302623ms 749.362444ms 749.376144ms 749.417784ms 749.468484ms 749.483525ms 749.500904ms 749.534565ms 749.604685ms 749.627066ms 749.653046ms 749.667726ms 749.699926ms 749.771967ms 749.791046ms 749.809867ms 749.876887ms 749.881307ms 749.890167ms 749.891667ms 749.967948ms 750.048549ms 750.088529ms 750.24057ms 750.26259ms 750.425351ms 750.472611ms 750.519992ms 750.542791ms 750.545532ms 750.566612ms 750.639013ms 750.692533ms 750.783774ms 750.787853ms 750.802053ms 750.854015ms 750.904514ms 750.936295ms 750.971215ms 751.110816ms 751.148436ms 751.165196ms 751.234596ms 751.238817ms 751.427138ms 751.489378ms 751.65044ms 751.832621ms 751.875901ms 751.922761ms 751.959462ms 751.973282ms 752.067563ms 752.126683ms 752.132823ms 752.137823ms 752.154323ms 752.202424ms 752.210584ms 752.338064ms 752.519846ms 752.528925ms 752.628387ms 752.687967ms 752.775068ms 752.937948ms 753.01993ms 753.722294ms 754.412739ms 755.692208ms 756.01453ms 756.06073ms 757.331559ms 758.188725ms 759.915617ms 760.776124ms 764.166168ms]
Jan 29 03:18:13.092: INFO: 50 %ile: 748.311836ms
Jan 29 03:18:13.092: INFO: 90 %ile: 752.202424ms
Jan 29 03:18:13.092: INFO: 99 %ile: 760.776124ms
Jan 29 03:18:13.092: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:187
Jan 29 03:18:13.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-1159" for this suite. 01/29/23 03:18:13.107
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","completed":94,"skipped":1706,"failed":0}
------------------------------
• [SLOW TEST] [11.945 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:18:01.174
    Jan 29 03:18:01.174: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename svc-latency 01/29/23 03:18:01.176
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:18:01.212
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:18:01.22
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Jan 29 03:18:01.225: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-1159 01/29/23 03:18:01.226
    I0129 03:18:01.236228      22 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-1159, replica count: 1
    I0129 03:18:02.287991      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0129 03:18:03.288274      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0129 03:18:04.288522      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 29 03:18:04.460: INFO: Created: latency-svc-f5rkf
    Jan 29 03:18:04.492: INFO: Got endpoints: latency-svc-f5rkf [103.032961ms]
    Jan 29 03:18:04.580: INFO: Created: latency-svc-fsk9p
    Jan 29 03:18:04.619: INFO: Got endpoints: latency-svc-fsk9p [126.243083ms]
    Jan 29 03:18:04.636: INFO: Created: latency-svc-g6d2v
    Jan 29 03:18:04.650: INFO: Got endpoints: latency-svc-g6d2v [156.134132ms]
    Jan 29 03:18:04.675: INFO: Created: latency-svc-m2xqf
    Jan 29 03:18:04.700: INFO: Got endpoints: latency-svc-m2xqf [204.131068ms]
    Jan 29 03:18:04.718: INFO: Created: latency-svc-prds2
    Jan 29 03:18:04.742: INFO: Created: latency-svc-flsdh
    Jan 29 03:18:04.742: INFO: Got endpoints: latency-svc-prds2 [245.342357ms]
    Jan 29 03:18:04.757: INFO: Got endpoints: latency-svc-flsdh [263.298402ms]
    Jan 29 03:18:04.771: INFO: Created: latency-svc-lrgrg
    Jan 29 03:18:04.785: INFO: Got endpoints: latency-svc-lrgrg [290.478593ms]
    Jan 29 03:18:04.806: INFO: Created: latency-svc-8wggf
    Jan 29 03:18:04.820: INFO: Got endpoints: latency-svc-8wggf [323.662905ms]
    Jan 29 03:18:04.834: INFO: Created: latency-svc-9x5jq
    Jan 29 03:18:04.848: INFO: Got endpoints: latency-svc-9x5jq [351.61634ms]
    Jan 29 03:18:04.852: INFO: Created: latency-svc-ncnrh
    Jan 29 03:18:04.868: INFO: Got endpoints: latency-svc-ncnrh [373.915437ms]
    Jan 29 03:18:04.874: INFO: Created: latency-svc-dqnq9
    Jan 29 03:18:04.893: INFO: Got endpoints: latency-svc-dqnq9 [397.403801ms]
    Jan 29 03:18:04.900: INFO: Created: latency-svc-gspjx
    Jan 29 03:18:04.912: INFO: Got endpoints: latency-svc-gspjx [416.021111ms]
    Jan 29 03:18:04.915: INFO: Created: latency-svc-p7lgj
    Jan 29 03:18:04.926: INFO: Got endpoints: latency-svc-p7lgj [430.725614ms]
    Jan 29 03:18:04.939: INFO: Created: latency-svc-wh8g8
    Jan 29 03:18:04.948: INFO: Got endpoints: latency-svc-wh8g8 [452.031023ms]
    Jan 29 03:18:04.957: INFO: Created: latency-svc-flmxb
    Jan 29 03:18:04.967: INFO: Created: latency-svc-lzjz8
    Jan 29 03:18:04.972: INFO: Got endpoints: latency-svc-flmxb [477.3388ms]
    Jan 29 03:18:04.979: INFO: Got endpoints: latency-svc-lzjz8 [482.602197ms]
    Jan 29 03:18:04.982: INFO: Created: latency-svc-qnt5x
    Jan 29 03:18:04.990: INFO: Got endpoints: latency-svc-qnt5x [370.547973ms]
    Jan 29 03:18:05.007: INFO: Created: latency-svc-m6qxj
    Jan 29 03:18:05.012: INFO: Got endpoints: latency-svc-m6qxj [362.454457ms]
    Jan 29 03:18:05.024: INFO: Created: latency-svc-zkx57
    Jan 29 03:18:05.030: INFO: Got endpoints: latency-svc-zkx57 [329.931929ms]
    Jan 29 03:18:05.035: INFO: Created: latency-svc-fsqff
    Jan 29 03:18:05.045: INFO: Got endpoints: latency-svc-fsqff [302.540297ms]
    Jan 29 03:18:05.047: INFO: Created: latency-svc-268s5
    Jan 29 03:18:05.057: INFO: Got endpoints: latency-svc-268s5 [300.468542ms]
    Jan 29 03:18:05.069: INFO: Created: latency-svc-x8w2v
    Jan 29 03:18:05.076: INFO: Got endpoints: latency-svc-x8w2v [290.289072ms]
    Jan 29 03:18:05.085: INFO: Created: latency-svc-q2kpl
    Jan 29 03:18:05.094: INFO: Got endpoints: latency-svc-q2kpl [273.840797ms]
    Jan 29 03:18:05.100: INFO: Created: latency-svc-jpqgl
    Jan 29 03:18:05.107: INFO: Got endpoints: latency-svc-jpqgl [259.435295ms]
    Jan 29 03:18:05.111: INFO: Created: latency-svc-fw4ls
    Jan 29 03:18:05.118: INFO: Got endpoints: latency-svc-fw4ls [249.239524ms]
    Jan 29 03:18:05.123: INFO: Created: latency-svc-dc4gh
    Jan 29 03:18:05.130: INFO: Got endpoints: latency-svc-dc4gh [236.694076ms]
    Jan 29 03:18:05.135: INFO: Created: latency-svc-cd54n
    Jan 29 03:18:05.144: INFO: Got endpoints: latency-svc-cd54n [231.665761ms]
    Jan 29 03:18:05.147: INFO: Created: latency-svc-cfcw9
    Jan 29 03:18:05.153: INFO: Got endpoints: latency-svc-cfcw9 [227.445832ms]
    Jan 29 03:18:05.160: INFO: Created: latency-svc-cgsbx
    Jan 29 03:18:05.166: INFO: Got endpoints: latency-svc-cgsbx [218.362948ms]
    Jan 29 03:18:05.172: INFO: Created: latency-svc-7nttd
    Jan 29 03:18:05.179: INFO: Got endpoints: latency-svc-7nttd [206.329023ms]
    Jan 29 03:18:05.190: INFO: Created: latency-svc-fc6tk
    Jan 29 03:18:05.191: INFO: Got endpoints: latency-svc-fc6tk [212.121664ms]
    Jan 29 03:18:05.197: INFO: Created: latency-svc-tzhvr
    Jan 29 03:18:05.205: INFO: Got endpoints: latency-svc-tzhvr [215.066905ms]
    Jan 29 03:18:05.210: INFO: Created: latency-svc-7kgl9
    Jan 29 03:18:05.216: INFO: Got endpoints: latency-svc-7kgl9 [203.610325ms]
    Jan 29 03:18:05.224: INFO: Created: latency-svc-tdmnw
    Jan 29 03:18:05.234: INFO: Got endpoints: latency-svc-tdmnw [204.210969ms]
    Jan 29 03:18:05.236: INFO: Created: latency-svc-vsbgs
    Jan 29 03:18:05.242: INFO: Got endpoints: latency-svc-vsbgs [197.388841ms]
    Jan 29 03:18:05.248: INFO: Created: latency-svc-lbsvr
    Jan 29 03:18:05.255: INFO: Got endpoints: latency-svc-lbsvr [197.920245ms]
    Jan 29 03:18:05.260: INFO: Created: latency-svc-n27xb
    Jan 29 03:18:05.272: INFO: Got endpoints: latency-svc-n27xb [196.125033ms]
    Jan 29 03:18:05.272: INFO: Created: latency-svc-z22s4
    Jan 29 03:18:05.279: INFO: Got endpoints: latency-svc-z22s4 [185.585079ms]
    Jan 29 03:18:05.284: INFO: Created: latency-svc-htfgc
    Jan 29 03:18:05.291: INFO: Got endpoints: latency-svc-htfgc [183.814527ms]
    Jan 29 03:18:05.297: INFO: Created: latency-svc-z4fgg
    Jan 29 03:18:05.307: INFO: Got endpoints: latency-svc-z4fgg [189.162524ms]
    Jan 29 03:18:05.311: INFO: Created: latency-svc-wpqmc
    Jan 29 03:18:05.315: INFO: Got endpoints: latency-svc-wpqmc [185.456178ms]
    Jan 29 03:18:05.320: INFO: Created: latency-svc-b7xfk
    Jan 29 03:18:05.326: INFO: Got endpoints: latency-svc-b7xfk [182.680118ms]
    Jan 29 03:18:05.332: INFO: Created: latency-svc-rp29z
    Jan 29 03:18:05.339: INFO: Got endpoints: latency-svc-rp29z [185.549139ms]
    Jan 29 03:18:05.343: INFO: Created: latency-svc-n6nvb
    Jan 29 03:18:05.350: INFO: Got endpoints: latency-svc-n6nvb [183.396923ms]
    Jan 29 03:18:05.356: INFO: Created: latency-svc-tgrg2
    Jan 29 03:18:05.364: INFO: Got endpoints: latency-svc-tgrg2 [184.942474ms]
    Jan 29 03:18:05.368: INFO: Created: latency-svc-l77dd
    Jan 29 03:18:05.375: INFO: Got endpoints: latency-svc-l77dd [183.216903ms]
    Jan 29 03:18:05.380: INFO: Created: latency-svc-xgdlc
    Jan 29 03:18:05.392: INFO: Got endpoints: latency-svc-xgdlc [186.646626ms]
    Jan 29 03:18:05.395: INFO: Created: latency-svc-zhbtr
    Jan 29 03:18:05.413: INFO: Created: latency-svc-677z9
    Jan 29 03:18:05.420: INFO: Created: latency-svc-89rl6
    Jan 29 03:18:05.430: INFO: Created: latency-svc-6hnp9
    Jan 29 03:18:05.442: INFO: Got endpoints: latency-svc-zhbtr [225.437818ms]
    Jan 29 03:18:05.443: INFO: Created: latency-svc-c9cb5
    Jan 29 03:18:05.454: INFO: Created: latency-svc-b4nzz
    Jan 29 03:18:05.468: INFO: Created: latency-svc-zg88h
    Jan 29 03:18:05.483: INFO: Created: latency-svc-qg6nc
    Jan 29 03:18:05.494: INFO: Got endpoints: latency-svc-677z9 [259.575957ms]
    Jan 29 03:18:05.501: INFO: Created: latency-svc-csdc4
    Jan 29 03:18:05.517: INFO: Created: latency-svc-vlgg4
    Jan 29 03:18:05.532: INFO: Created: latency-svc-6zx9z
    Jan 29 03:18:05.547: INFO: Got endpoints: latency-svc-89rl6 [305.292497ms]
    Jan 29 03:18:05.553: INFO: Created: latency-svc-xr8bm
    Jan 29 03:18:05.570: INFO: Created: latency-svc-lmtmg
    Jan 29 03:18:05.581: INFO: Created: latency-svc-t8qlk
    Jan 29 03:18:05.595: INFO: Got endpoints: latency-svc-6hnp9 [339.873018ms]
    Jan 29 03:18:05.604: INFO: Created: latency-svc-htv82
    Jan 29 03:18:05.614: INFO: Created: latency-svc-4vg9q
    Jan 29 03:18:05.628: INFO: Created: latency-svc-92755
    Jan 29 03:18:05.644: INFO: Created: latency-svc-8bmjs
    Jan 29 03:18:05.645: INFO: Got endpoints: latency-svc-c9cb5 [372.409746ms]
    Jan 29 03:18:05.655: INFO: Created: latency-svc-hgvrg
    Jan 29 03:18:05.667: INFO: Created: latency-svc-npvdk
    Jan 29 03:18:05.692: INFO: Got endpoints: latency-svc-b4nzz [412.754548ms]
    Jan 29 03:18:05.724: INFO: Created: latency-svc-4mdw7
    Jan 29 03:18:05.749: INFO: Got endpoints: latency-svc-zg88h [457.875984ms]
    Jan 29 03:18:05.794: INFO: Created: latency-svc-d4wcn
    Jan 29 03:18:05.795: INFO: Got endpoints: latency-svc-qg6nc [488.461598ms]
    Jan 29 03:18:05.829: INFO: Created: latency-svc-7xv9x
    Jan 29 03:18:05.851: INFO: Got endpoints: latency-svc-csdc4 [535.92805ms]
    Jan 29 03:18:05.885: INFO: Created: latency-svc-v7wkf
    Jan 29 03:18:05.899: INFO: Got endpoints: latency-svc-vlgg4 [572.253925ms]
    Jan 29 03:18:05.942: INFO: Created: latency-svc-tvwqs
    Jan 29 03:18:05.948: INFO: Got endpoints: latency-svc-6zx9z [608.852281ms]
    Jan 29 03:18:05.982: INFO: Created: latency-svc-674dq
    Jan 29 03:18:05.999: INFO: Got endpoints: latency-svc-xr8bm [649.265464ms]
    Jan 29 03:18:06.049: INFO: Got endpoints: latency-svc-lmtmg [685.628498ms]
    Jan 29 03:18:06.051: INFO: Created: latency-svc-9p97k
    Jan 29 03:18:06.094: INFO: Got endpoints: latency-svc-t8qlk [719.615016ms]
    Jan 29 03:18:06.101: INFO: Created: latency-svc-j4w46
    Jan 29 03:18:06.129: INFO: Created: latency-svc-lxfrt
    Jan 29 03:18:06.145: INFO: Got endpoints: latency-svc-htv82 [752.132823ms]
    Jan 29 03:18:06.189: INFO: Created: latency-svc-7q682
    Jan 29 03:18:06.197: INFO: Got endpoints: latency-svc-4vg9q [755.692208ms]
    Jan 29 03:18:06.224: INFO: Created: latency-svc-hrzgh
    Jan 29 03:18:06.245: INFO: Got endpoints: latency-svc-92755 [751.148436ms]
    Jan 29 03:18:06.282: INFO: Created: latency-svc-lrhhd
    Jan 29 03:18:06.294: INFO: Got endpoints: latency-svc-8bmjs [746.534524ms]
    Jan 29 03:18:06.317: INFO: Created: latency-svc-7dcsd
    Jan 29 03:18:06.343: INFO: Got endpoints: latency-svc-hgvrg [747.41115ms]
    Jan 29 03:18:06.362: INFO: Created: latency-svc-g5hpn
    Jan 29 03:18:06.391: INFO: Got endpoints: latency-svc-npvdk [746.668964ms]
    Jan 29 03:18:06.414: INFO: Created: latency-svc-vx4r8
    Jan 29 03:18:06.441: INFO: Got endpoints: latency-svc-4mdw7 [748.618998ms]
    Jan 29 03:18:06.470: INFO: Created: latency-svc-wb49n
    Jan 29 03:18:06.499: INFO: Got endpoints: latency-svc-d4wcn [749.500904ms]
    Jan 29 03:18:06.530: INFO: Created: latency-svc-bplrl
    Jan 29 03:18:06.542: INFO: Got endpoints: latency-svc-7xv9x [746.732845ms]
    Jan 29 03:18:06.568: INFO: Created: latency-svc-wmj9h
    Jan 29 03:18:06.594: INFO: Got endpoints: latency-svc-v7wkf [742.879758ms]
    Jan 29 03:18:06.622: INFO: Created: latency-svc-6skcz
    Jan 29 03:18:06.650: INFO: Got endpoints: latency-svc-tvwqs [751.110816ms]
    Jan 29 03:18:06.676: INFO: Created: latency-svc-zpvw9
    Jan 29 03:18:06.694: INFO: Got endpoints: latency-svc-674dq [745.96966ms]
    Jan 29 03:18:06.716: INFO: Created: latency-svc-psn7w
    Jan 29 03:18:06.741: INFO: Got endpoints: latency-svc-9p97k [741.458928ms]
    Jan 29 03:18:06.760: INFO: Created: latency-svc-4kpfj
    Jan 29 03:18:06.792: INFO: Got endpoints: latency-svc-j4w46 [742.563176ms]
    Jan 29 03:18:06.813: INFO: Created: latency-svc-rvd6m
    Jan 29 03:18:06.844: INFO: Got endpoints: latency-svc-lxfrt [749.627066ms]
    Jan 29 03:18:06.876: INFO: Created: latency-svc-h7fbk
    Jan 29 03:18:06.899: INFO: Got endpoints: latency-svc-7q682 [754.412739ms]
    Jan 29 03:18:06.927: INFO: Created: latency-svc-xmk5j
    Jan 29 03:18:06.945: INFO: Got endpoints: latency-svc-hrzgh [747.172828ms]
    Jan 29 03:18:06.973: INFO: Created: latency-svc-5twch
    Jan 29 03:18:06.991: INFO: Got endpoints: latency-svc-lrhhd [746.507144ms]
    Jan 29 03:18:07.017: INFO: Created: latency-svc-jgp7s
    Jan 29 03:18:07.039: INFO: Got endpoints: latency-svc-7dcsd [744.965753ms]
    Jan 29 03:18:07.061: INFO: Created: latency-svc-xklk7
    Jan 29 03:18:07.090: INFO: Got endpoints: latency-svc-g5hpn [747.127488ms]
    Jan 29 03:18:07.110: INFO: Created: latency-svc-fz8g6
    Jan 29 03:18:07.140: INFO: Got endpoints: latency-svc-vx4r8 [748.559778ms]
    Jan 29 03:18:07.158: INFO: Created: latency-svc-82gfr
    Jan 29 03:18:07.193: INFO: Got endpoints: latency-svc-wb49n [752.210584ms]
    Jan 29 03:18:07.210: INFO: Created: latency-svc-j5mcs
    Jan 29 03:18:07.239: INFO: Got endpoints: latency-svc-bplrl [740.413761ms]
    Jan 29 03:18:07.261: INFO: Created: latency-svc-8xjr5
    Jan 29 03:18:07.290: INFO: Got endpoints: latency-svc-wmj9h [747.674372ms]
    Jan 29 03:18:07.308: INFO: Created: latency-svc-bwbpf
    Jan 29 03:18:07.339: INFO: Got endpoints: latency-svc-6skcz [745.175214ms]
    Jan 29 03:18:07.362: INFO: Created: latency-svc-zxdkf
    Jan 29 03:18:07.393: INFO: Got endpoints: latency-svc-zpvw9 [742.776138ms]
    Jan 29 03:18:07.421: INFO: Created: latency-svc-mm5ff
    Jan 29 03:18:07.442: INFO: Got endpoints: latency-svc-psn7w [748.311836ms]
    Jan 29 03:18:07.462: INFO: Created: latency-svc-n7pxk
    Jan 29 03:18:07.492: INFO: Got endpoints: latency-svc-4kpfj [751.234596ms]
    Jan 29 03:18:07.520: INFO: Created: latency-svc-788lb
    Jan 29 03:18:07.546: INFO: Got endpoints: latency-svc-rvd6m [753.722294ms]
    Jan 29 03:18:07.566: INFO: Created: latency-svc-gqhh7
    Jan 29 03:18:07.590: INFO: Got endpoints: latency-svc-h7fbk [746.296343ms]
    Jan 29 03:18:07.608: INFO: Created: latency-svc-w4ldx
    Jan 29 03:18:07.645: INFO: Got endpoints: latency-svc-xmk5j [745.812038ms]
    Jan 29 03:18:07.662: INFO: Created: latency-svc-c898p
    Jan 29 03:18:07.691: INFO: Got endpoints: latency-svc-5twch [746.343042ms]
    Jan 29 03:18:07.708: INFO: Created: latency-svc-wm9ft
    Jan 29 03:18:07.740: INFO: Got endpoints: latency-svc-jgp7s [748.534238ms]
    Jan 29 03:18:07.756: INFO: Created: latency-svc-rd8qb
    Jan 29 03:18:07.790: INFO: Got endpoints: latency-svc-xklk7 [750.802053ms]
    Jan 29 03:18:07.807: INFO: Created: latency-svc-28fld
    Jan 29 03:18:07.840: INFO: Got endpoints: latency-svc-fz8g6 [749.468484ms]
    Jan 29 03:18:07.857: INFO: Created: latency-svc-htks9
    Jan 29 03:18:07.891: INFO: Got endpoints: latency-svc-82gfr [750.783774ms]
    Jan 29 03:18:07.908: INFO: Created: latency-svc-d6985
    Jan 29 03:18:07.940: INFO: Got endpoints: latency-svc-j5mcs [746.558684ms]
    Jan 29 03:18:07.958: INFO: Created: latency-svc-r6clp
    Jan 29 03:18:07.990: INFO: Got endpoints: latency-svc-8xjr5 [750.566612ms]
    Jan 29 03:18:08.010: INFO: Created: latency-svc-2zr7p
    Jan 29 03:18:08.041: INFO: Got endpoints: latency-svc-bwbpf [751.238817ms]
    Jan 29 03:18:08.059: INFO: Created: latency-svc-xd958
    Jan 29 03:18:08.090: INFO: Got endpoints: latency-svc-zxdkf [750.971215ms]
    Jan 29 03:18:08.108: INFO: Created: latency-svc-5c7pj
    Jan 29 03:18:08.141: INFO: Got endpoints: latency-svc-mm5ff [748.130656ms]
    Jan 29 03:18:08.157: INFO: Created: latency-svc-lhjsm
    Jan 29 03:18:08.190: INFO: Got endpoints: latency-svc-n7pxk [747.42175ms]
    Jan 29 03:18:08.210: INFO: Created: latency-svc-pll2q
    Jan 29 03:18:08.239: INFO: Got endpoints: latency-svc-788lb [746.913066ms]
    Jan 29 03:18:08.258: INFO: Created: latency-svc-sfw2z
    Jan 29 03:18:08.298: INFO: Got endpoints: latency-svc-gqhh7 [752.154323ms]
    Jan 29 03:18:08.351: INFO: Got endpoints: latency-svc-w4ldx [760.776124ms]
    Jan 29 03:18:08.368: INFO: Created: latency-svc-rvbfh
    Jan 29 03:18:08.402: INFO: Created: latency-svc-b5wd2
    Jan 29 03:18:08.402: INFO: Got endpoints: latency-svc-c898p [757.331559ms]
    Jan 29 03:18:08.430: INFO: Created: latency-svc-qnbcf
    Jan 29 03:18:08.441: INFO: Got endpoints: latency-svc-wm9ft [749.667726ms]
    Jan 29 03:18:08.461: INFO: Created: latency-svc-2q8qq
    Jan 29 03:18:08.492: INFO: Got endpoints: latency-svc-rd8qb [751.959462ms]
    Jan 29 03:18:08.514: INFO: Created: latency-svc-xwxrq
    Jan 29 03:18:08.542: INFO: Got endpoints: latency-svc-28fld [751.922761ms]
    Jan 29 03:18:08.562: INFO: Created: latency-svc-csm4z
    Jan 29 03:18:08.592: INFO: Got endpoints: latency-svc-htks9 [752.137823ms]
    Jan 29 03:18:08.613: INFO: Created: latency-svc-gtm94
    Jan 29 03:18:08.640: INFO: Got endpoints: latency-svc-d6985 [748.621279ms]
    Jan 29 03:18:08.657: INFO: Created: latency-svc-q9l52
    Jan 29 03:18:08.689: INFO: Got endpoints: latency-svc-r6clp [749.604685ms]
    Jan 29 03:18:08.706: INFO: Created: latency-svc-w2gnh
    Jan 29 03:18:08.739: INFO: Got endpoints: latency-svc-2zr7p [748.666859ms]
    Jan 29 03:18:08.756: INFO: Created: latency-svc-l6b86
    Jan 29 03:18:08.789: INFO: Got endpoints: latency-svc-xd958 [747.800993ms]
    Jan 29 03:18:08.805: INFO: Created: latency-svc-wdzl9
    Jan 29 03:18:08.842: INFO: Got endpoints: latency-svc-5c7pj [751.165196ms]
    Jan 29 03:18:08.862: INFO: Created: latency-svc-brqhx
    Jan 29 03:18:08.891: INFO: Got endpoints: latency-svc-lhjsm [749.417784ms]
    Jan 29 03:18:08.907: INFO: Created: latency-svc-6gdgp
    Jan 29 03:18:08.940: INFO: Got endpoints: latency-svc-pll2q [749.771967ms]
    Jan 29 03:18:08.968: INFO: Created: latency-svc-t9kvv
    Jan 29 03:18:08.991: INFO: Got endpoints: latency-svc-sfw2z [751.489378ms]
    Jan 29 03:18:09.009: INFO: Created: latency-svc-b7gw9
    Jan 29 03:18:09.041: INFO: Got endpoints: latency-svc-rvbfh [742.663576ms]
    Jan 29 03:18:09.059: INFO: Created: latency-svc-vxsj2
    Jan 29 03:18:09.091: INFO: Got endpoints: latency-svc-b5wd2 [739.388274ms]
    Jan 29 03:18:09.107: INFO: Created: latency-svc-mbpj2
    Jan 29 03:18:09.141: INFO: Got endpoints: latency-svc-qnbcf [738.746589ms]
    Jan 29 03:18:09.162: INFO: Created: latency-svc-npghm
    Jan 29 03:18:09.191: INFO: Got endpoints: latency-svc-2q8qq [750.639013ms]
    Jan 29 03:18:09.208: INFO: Created: latency-svc-x5xts
    Jan 29 03:18:09.244: INFO: Got endpoints: latency-svc-xwxrq [751.427138ms]
    Jan 29 03:18:09.263: INFO: Created: latency-svc-6nsdg
    Jan 29 03:18:09.292: INFO: Got endpoints: latency-svc-csm4z [750.088529ms]
    Jan 29 03:18:09.311: INFO: Created: latency-svc-cls9n
    Jan 29 03:18:09.341: INFO: Got endpoints: latency-svc-gtm94 [749.226762ms]
    Jan 29 03:18:09.366: INFO: Created: latency-svc-22fm2
    Jan 29 03:18:09.391: INFO: Got endpoints: latency-svc-q9l52 [750.692533ms]
    Jan 29 03:18:09.408: INFO: Created: latency-svc-42dhj
    Jan 29 03:18:09.442: INFO: Got endpoints: latency-svc-w2gnh [752.775068ms]
    Jan 29 03:18:09.461: INFO: Created: latency-svc-vt62n
    Jan 29 03:18:09.491: INFO: Got endpoints: latency-svc-l6b86 [752.528925ms]
    Jan 29 03:18:09.515: INFO: Created: latency-svc-dq5tk
    Jan 29 03:18:09.554: INFO: Got endpoints: latency-svc-wdzl9 [764.166168ms]
    Jan 29 03:18:09.580: INFO: Created: latency-svc-wp957
    Jan 29 03:18:09.595: INFO: Got endpoints: latency-svc-brqhx [752.937948ms]
    Jan 29 03:18:09.618: INFO: Created: latency-svc-mf69d
    Jan 29 03:18:09.641: INFO: Got endpoints: latency-svc-6gdgp [750.787853ms]
    Jan 29 03:18:09.660: INFO: Created: latency-svc-8fcs2
    Jan 29 03:18:09.690: INFO: Got endpoints: latency-svc-t9kvv [749.699926ms]
    Jan 29 03:18:09.705: INFO: Created: latency-svc-rrpmj
    Jan 29 03:18:09.739: INFO: Got endpoints: latency-svc-b7gw9 [748.444777ms]
    Jan 29 03:18:09.757: INFO: Created: latency-svc-4vc86
    Jan 29 03:18:09.790: INFO: Got endpoints: latency-svc-vxsj2 [748.651818ms]
    Jan 29 03:18:09.806: INFO: Created: latency-svc-zmtbk
    Jan 29 03:18:09.840: INFO: Got endpoints: latency-svc-mbpj2 [748.791839ms]
    Jan 29 03:18:09.857: INFO: Created: latency-svc-8thfl
    Jan 29 03:18:09.890: INFO: Got endpoints: latency-svc-npghm [748.92164ms]
    Jan 29 03:18:09.909: INFO: Created: latency-svc-6wq7c
    Jan 29 03:18:09.941: INFO: Got endpoints: latency-svc-x5xts [748.96534ms]
    Jan 29 03:18:09.959: INFO: Created: latency-svc-tc8dn
    Jan 29 03:18:09.991: INFO: Got endpoints: latency-svc-6nsdg [747.217749ms]
    Jan 29 03:18:10.008: INFO: Created: latency-svc-5nvkm
    Jan 29 03:18:10.040: INFO: Got endpoints: latency-svc-cls9n [747.303389ms]
    Jan 29 03:18:10.058: INFO: Created: latency-svc-6plkv
    Jan 29 03:18:10.091: INFO: Got endpoints: latency-svc-22fm2 [749.197503ms]
    Jan 29 03:18:10.107: INFO: Created: latency-svc-prbw9
    Jan 29 03:18:10.140: INFO: Got endpoints: latency-svc-42dhj [748.992041ms]
    Jan 29 03:18:10.156: INFO: Created: latency-svc-wlx79
    Jan 29 03:18:10.191: INFO: Got endpoints: latency-svc-vt62n [748.308457ms]
    Jan 29 03:18:10.209: INFO: Created: latency-svc-pczrl
    Jan 29 03:18:10.240: INFO: Got endpoints: latency-svc-dq5tk [748.068874ms]
    Jan 29 03:18:10.257: INFO: Created: latency-svc-g79zd
    Jan 29 03:18:10.290: INFO: Got endpoints: latency-svc-wp957 [736.470914ms]
    Jan 29 03:18:10.307: INFO: Created: latency-svc-r7vxd
    Jan 29 03:18:10.340: INFO: Got endpoints: latency-svc-mf69d [745.321095ms]
    Jan 29 03:18:10.357: INFO: Created: latency-svc-bncbb
    Jan 29 03:18:10.395: INFO: Got endpoints: latency-svc-8fcs2 [753.01993ms]
    Jan 29 03:18:10.415: INFO: Created: latency-svc-6ksxt
    Jan 29 03:18:10.440: INFO: Got endpoints: latency-svc-rrpmj [750.545532ms]
    Jan 29 03:18:10.462: INFO: Created: latency-svc-wgsb8
    Jan 29 03:18:10.499: INFO: Got endpoints: latency-svc-4vc86 [759.915617ms]
    Jan 29 03:18:10.523: INFO: Created: latency-svc-5jkwd
    Jan 29 03:18:10.542: INFO: Got endpoints: latency-svc-zmtbk [752.628387ms]
    Jan 29 03:18:10.588: INFO: Created: latency-svc-5xdpp
    Jan 29 03:18:10.596: INFO: Got endpoints: latency-svc-8thfl [756.06073ms]
    Jan 29 03:18:10.617: INFO: Created: latency-svc-r6n5j
    Jan 29 03:18:10.642: INFO: Got endpoints: latency-svc-6wq7c [751.875901ms]
    Jan 29 03:18:10.662: INFO: Created: latency-svc-tvxbg
    Jan 29 03:18:10.690: INFO: Got endpoints: latency-svc-tc8dn [749.376144ms]
    Jan 29 03:18:10.706: INFO: Created: latency-svc-lgd2j
    Jan 29 03:18:10.740: INFO: Got endpoints: latency-svc-5nvkm [748.975561ms]
    Jan 29 03:18:10.759: INFO: Created: latency-svc-b7p2n
    Jan 29 03:18:10.790: INFO: Got endpoints: latency-svc-6plkv [749.891667ms]
    Jan 29 03:18:10.807: INFO: Created: latency-svc-ggs89
    Jan 29 03:18:10.840: INFO: Got endpoints: latency-svc-prbw9 [749.362444ms]
    Jan 29 03:18:10.867: INFO: Created: latency-svc-4md5p
    Jan 29 03:18:10.892: INFO: Got endpoints: latency-svc-wlx79 [752.687967ms]
    Jan 29 03:18:10.911: INFO: Created: latency-svc-lzz5j
    Jan 29 03:18:10.940: INFO: Got endpoints: latency-svc-pczrl [748.945801ms]
    Jan 29 03:18:10.959: INFO: Created: latency-svc-dh5fj
    Jan 29 03:18:10.992: INFO: Got endpoints: latency-svc-g79zd [752.202424ms]
    Jan 29 03:18:11.009: INFO: Created: latency-svc-wrlfj
    Jan 29 03:18:11.040: INFO: Got endpoints: latency-svc-r7vxd [749.483525ms]
    Jan 29 03:18:11.057: INFO: Created: latency-svc-dn79w
    Jan 29 03:18:11.090: INFO: Got endpoints: latency-svc-bncbb [749.890167ms]
    Jan 29 03:18:11.108: INFO: Created: latency-svc-g4vvm
    Jan 29 03:18:11.140: INFO: Got endpoints: latency-svc-6ksxt [745.461637ms]
    Jan 29 03:18:11.158: INFO: Created: latency-svc-pgz2s
    Jan 29 03:18:11.190: INFO: Got endpoints: latency-svc-wgsb8 [749.876887ms]
    Jan 29 03:18:11.210: INFO: Created: latency-svc-fpp9s
    Jan 29 03:18:11.240: INFO: Got endpoints: latency-svc-5jkwd [740.207239ms]
    Jan 29 03:18:11.257: INFO: Created: latency-svc-wc299
    Jan 29 03:18:11.291: INFO: Got endpoints: latency-svc-5xdpp [748.109115ms]
    Jan 29 03:18:11.308: INFO: Created: latency-svc-82bhs
    Jan 29 03:18:11.340: INFO: Got endpoints: latency-svc-r6n5j [744.131987ms]
    Jan 29 03:18:11.357: INFO: Created: latency-svc-8wfsv
    Jan 29 03:18:11.391: INFO: Got endpoints: latency-svc-tvxbg [749.259803ms]
    Jan 29 03:18:11.407: INFO: Created: latency-svc-l6pt2
    Jan 29 03:18:11.440: INFO: Got endpoints: latency-svc-lgd2j [749.809867ms]
    Jan 29 03:18:11.460: INFO: Created: latency-svc-x4spt
    Jan 29 03:18:11.491: INFO: Got endpoints: latency-svc-b7p2n [750.519992ms]
    Jan 29 03:18:11.518: INFO: Created: latency-svc-5wq2j
    Jan 29 03:18:11.548: INFO: Got endpoints: latency-svc-ggs89 [758.188725ms]
    Jan 29 03:18:11.572: INFO: Created: latency-svc-wt5hn
    Jan 29 03:18:11.591: INFO: Got endpoints: latency-svc-4md5p [750.542791ms]
    Jan 29 03:18:11.611: INFO: Created: latency-svc-gzqq7
    Jan 29 03:18:11.641: INFO: Got endpoints: latency-svc-lzz5j [748.527638ms]
    Jan 29 03:18:11.662: INFO: Created: latency-svc-m4wdt
    Jan 29 03:18:11.692: INFO: Got endpoints: latency-svc-dh5fj [751.832621ms]
    Jan 29 03:18:11.710: INFO: Created: latency-svc-29978
    Jan 29 03:18:11.739: INFO: Got endpoints: latency-svc-wrlfj [747.397469ms]
    Jan 29 03:18:11.756: INFO: Created: latency-svc-fnnv2
    Jan 29 03:18:11.791: INFO: Got endpoints: latency-svc-dn79w [750.936295ms]
    Jan 29 03:18:11.810: INFO: Created: latency-svc-ng6qf
    Jan 29 03:18:11.842: INFO: Got endpoints: latency-svc-g4vvm [752.338064ms]
    Jan 29 03:18:11.860: INFO: Created: latency-svc-8tz2k
    Jan 29 03:18:11.890: INFO: Got endpoints: latency-svc-pgz2s [749.534565ms]
    Jan 29 03:18:11.906: INFO: Created: latency-svc-7jzvt
    Jan 29 03:18:11.940: INFO: Got endpoints: latency-svc-fpp9s [749.302623ms]
    Jan 29 03:18:11.956: INFO: Created: latency-svc-7wzdq
    Jan 29 03:18:11.992: INFO: Got endpoints: latency-svc-wc299 [752.519846ms]
    Jan 29 03:18:12.010: INFO: Created: latency-svc-x9hc8
    Jan 29 03:18:12.042: INFO: Got endpoints: latency-svc-82bhs [750.904514ms]
    Jan 29 03:18:12.061: INFO: Created: latency-svc-5q8vb
    Jan 29 03:18:12.090: INFO: Got endpoints: latency-svc-8wfsv [750.24057ms]
    Jan 29 03:18:12.108: INFO: Created: latency-svc-wxf5t
    Jan 29 03:18:12.142: INFO: Got endpoints: latency-svc-l6pt2 [749.967948ms]
    Jan 29 03:18:12.160: INFO: Created: latency-svc-pzh8n
    Jan 29 03:18:12.190: INFO: Got endpoints: latency-svc-x4spt [749.653046ms]
    Jan 29 03:18:12.213: INFO: Created: latency-svc-z2bhr
    Jan 29 03:18:12.240: INFO: Got endpoints: latency-svc-5wq2j [748.85446ms]
    Jan 29 03:18:12.257: INFO: Created: latency-svc-dkhxz
    Jan 29 03:18:12.291: INFO: Got endpoints: latency-svc-wt5hn [743.18232ms]
    Jan 29 03:18:12.307: INFO: Created: latency-svc-stwbg
    Jan 29 03:18:12.341: INFO: Got endpoints: latency-svc-gzqq7 [749.791046ms]
    Jan 29 03:18:12.359: INFO: Created: latency-svc-7fb26
    Jan 29 03:18:12.390: INFO: Got endpoints: latency-svc-m4wdt [748.87154ms]
    Jan 29 03:18:12.439: INFO: Got endpoints: latency-svc-29978 [747.33373ms]
    Jan 29 03:18:12.492: INFO: Got endpoints: latency-svc-fnnv2 [752.126683ms]
    Jan 29 03:18:12.543: INFO: Got endpoints: latency-svc-ng6qf [751.973282ms]
    Jan 29 03:18:12.591: INFO: Got endpoints: latency-svc-8tz2k [748.600038ms]
    Jan 29 03:18:12.642: INFO: Got endpoints: latency-svc-7jzvt [752.067563ms]
    Jan 29 03:18:12.690: INFO: Got endpoints: latency-svc-7wzdq [750.425351ms]
    Jan 29 03:18:12.748: INFO: Got endpoints: latency-svc-x9hc8 [756.01453ms]
    Jan 29 03:18:12.792: INFO: Got endpoints: latency-svc-5q8vb [749.881307ms]
    Jan 29 03:18:12.842: INFO: Got endpoints: latency-svc-wxf5t [751.65044ms]
    Jan 29 03:18:12.892: INFO: Got endpoints: latency-svc-pzh8n [750.048549ms]
    Jan 29 03:18:12.941: INFO: Got endpoints: latency-svc-z2bhr [750.854015ms]
    Jan 29 03:18:12.990: INFO: Got endpoints: latency-svc-dkhxz [750.26259ms]
    Jan 29 03:18:13.040: INFO: Got endpoints: latency-svc-stwbg [748.454977ms]
    Jan 29 03:18:13.091: INFO: Got endpoints: latency-svc-7fb26 [750.472611ms]
    Jan 29 03:18:13.092: INFO: Latencies: [126.243083ms 156.134132ms 182.680118ms 183.216903ms 183.396923ms 183.814527ms 184.942474ms 185.456178ms 185.549139ms 185.585079ms 186.646626ms 189.162524ms 196.125033ms 197.388841ms 197.920245ms 203.610325ms 204.131068ms 204.210969ms 206.329023ms 212.121664ms 215.066905ms 218.362948ms 225.437818ms 227.445832ms 231.665761ms 236.694076ms 245.342357ms 249.239524ms 259.435295ms 259.575957ms 263.298402ms 273.840797ms 290.289072ms 290.478593ms 300.468542ms 302.540297ms 305.292497ms 323.662905ms 329.931929ms 339.873018ms 351.61634ms 362.454457ms 370.547973ms 372.409746ms 373.915437ms 397.403801ms 412.754548ms 416.021111ms 430.725614ms 452.031023ms 457.875984ms 477.3388ms 482.602197ms 488.461598ms 535.92805ms 572.253925ms 608.852281ms 649.265464ms 685.628498ms 719.615016ms 736.470914ms 738.746589ms 739.388274ms 740.207239ms 740.413761ms 741.458928ms 742.563176ms 742.663576ms 742.776138ms 742.879758ms 743.18232ms 744.131987ms 744.965753ms 745.175214ms 745.321095ms 745.461637ms 745.812038ms 745.96966ms 746.296343ms 746.343042ms 746.507144ms 746.534524ms 746.558684ms 746.668964ms 746.732845ms 746.913066ms 747.127488ms 747.172828ms 747.217749ms 747.303389ms 747.33373ms 747.397469ms 747.41115ms 747.42175ms 747.674372ms 747.800993ms 748.068874ms 748.109115ms 748.130656ms 748.308457ms 748.311836ms 748.444777ms 748.454977ms 748.527638ms 748.534238ms 748.559778ms 748.600038ms 748.618998ms 748.621279ms 748.651818ms 748.666859ms 748.791839ms 748.85446ms 748.87154ms 748.92164ms 748.945801ms 748.96534ms 748.975561ms 748.992041ms 749.197503ms 749.226762ms 749.259803ms 749.302623ms 749.362444ms 749.376144ms 749.417784ms 749.468484ms 749.483525ms 749.500904ms 749.534565ms 749.604685ms 749.627066ms 749.653046ms 749.667726ms 749.699926ms 749.771967ms 749.791046ms 749.809867ms 749.876887ms 749.881307ms 749.890167ms 749.891667ms 749.967948ms 750.048549ms 750.088529ms 750.24057ms 750.26259ms 750.425351ms 750.472611ms 750.519992ms 750.542791ms 750.545532ms 750.566612ms 750.639013ms 750.692533ms 750.783774ms 750.787853ms 750.802053ms 750.854015ms 750.904514ms 750.936295ms 750.971215ms 751.110816ms 751.148436ms 751.165196ms 751.234596ms 751.238817ms 751.427138ms 751.489378ms 751.65044ms 751.832621ms 751.875901ms 751.922761ms 751.959462ms 751.973282ms 752.067563ms 752.126683ms 752.132823ms 752.137823ms 752.154323ms 752.202424ms 752.210584ms 752.338064ms 752.519846ms 752.528925ms 752.628387ms 752.687967ms 752.775068ms 752.937948ms 753.01993ms 753.722294ms 754.412739ms 755.692208ms 756.01453ms 756.06073ms 757.331559ms 758.188725ms 759.915617ms 760.776124ms 764.166168ms]
    Jan 29 03:18:13.092: INFO: 50 %ile: 748.311836ms
    Jan 29 03:18:13.092: INFO: 90 %ile: 752.202424ms
    Jan 29 03:18:13.092: INFO: 99 %ile: 760.776124ms
    Jan 29 03:18:13.092: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:187
    Jan 29 03:18:13.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svc-latency-1159" for this suite. 01/29/23 03:18:13.107
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:18:13.122
Jan 29 03:18:13.122: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename services 01/29/23 03:18:13.123
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:18:13.154
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:18:13.16
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189
STEP: creating service in namespace services-4070 01/29/23 03:18:13.166
STEP: creating service affinity-clusterip-transition in namespace services-4070 01/29/23 03:18:13.167
STEP: creating replication controller affinity-clusterip-transition in namespace services-4070 01/29/23 03:18:13.182
I0129 03:18:13.191660      22 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-4070, replica count: 3
I0129 03:18:16.242837      22 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 29 03:18:16.256: INFO: Creating new exec pod
Jan 29 03:18:16.273: INFO: Waiting up to 5m0s for pod "execpod-affinityzghmr" in namespace "services-4070" to be "running"
Jan 29 03:18:16.280: INFO: Pod "execpod-affinityzghmr": Phase="Pending", Reason="", readiness=false. Elapsed: 7.06341ms
Jan 29 03:18:18.294: INFO: Pod "execpod-affinityzghmr": Phase="Running", Reason="", readiness=true. Elapsed: 2.020373478s
Jan 29 03:18:18.294: INFO: Pod "execpod-affinityzghmr" satisfied condition "running"
Jan 29 03:18:19.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-4070 exec execpod-affinityzghmr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Jan 29 03:18:19.565: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Jan 29 03:18:19.566: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 29 03:18:19.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-4070 exec execpod-affinityzghmr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.105.182.98 80'
Jan 29 03:18:19.811: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.105.182.98 80\nConnection to 100.105.182.98 80 port [tcp/http] succeeded!\n"
Jan 29 03:18:19.811: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 29 03:18:19.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-4070 exec execpod-affinityzghmr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.105.182.98:80/ ; done'
Jan 29 03:18:20.180: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n"
Jan 29 03:18:20.180: INFO: stdout: "\naffinity-clusterip-transition-qc5xg\naffinity-clusterip-transition-qc5xg\naffinity-clusterip-transition-qc5xg\naffinity-clusterip-transition-qc5xg\naffinity-clusterip-transition-qc5xg\naffinity-clusterip-transition-qc5xg\naffinity-clusterip-transition-qc5xg\naffinity-clusterip-transition-qc5xg\naffinity-clusterip-transition-qc5xg\naffinity-clusterip-transition-qc5xg\naffinity-clusterip-transition-qc5xg\naffinity-clusterip-transition-qc5xg\naffinity-clusterip-transition-qc5xg\naffinity-clusterip-transition-qc5xg\naffinity-clusterip-transition-qc5xg\naffinity-clusterip-transition-qc5xg"
Jan 29 03:18:20.180: INFO: Received response from host: affinity-clusterip-transition-qc5xg
Jan 29 03:18:20.180: INFO: Received response from host: affinity-clusterip-transition-qc5xg
Jan 29 03:18:20.180: INFO: Received response from host: affinity-clusterip-transition-qc5xg
Jan 29 03:18:20.180: INFO: Received response from host: affinity-clusterip-transition-qc5xg
Jan 29 03:18:20.180: INFO: Received response from host: affinity-clusterip-transition-qc5xg
Jan 29 03:18:20.180: INFO: Received response from host: affinity-clusterip-transition-qc5xg
Jan 29 03:18:20.180: INFO: Received response from host: affinity-clusterip-transition-qc5xg
Jan 29 03:18:20.180: INFO: Received response from host: affinity-clusterip-transition-qc5xg
Jan 29 03:18:20.180: INFO: Received response from host: affinity-clusterip-transition-qc5xg
Jan 29 03:18:20.180: INFO: Received response from host: affinity-clusterip-transition-qc5xg
Jan 29 03:18:20.180: INFO: Received response from host: affinity-clusterip-transition-qc5xg
Jan 29 03:18:20.180: INFO: Received response from host: affinity-clusterip-transition-qc5xg
Jan 29 03:18:20.180: INFO: Received response from host: affinity-clusterip-transition-qc5xg
Jan 29 03:18:20.180: INFO: Received response from host: affinity-clusterip-transition-qc5xg
Jan 29 03:18:20.180: INFO: Received response from host: affinity-clusterip-transition-qc5xg
Jan 29 03:18:20.180: INFO: Received response from host: affinity-clusterip-transition-qc5xg
Jan 29 03:18:50.181: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-4070 exec execpod-affinityzghmr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.105.182.98:80/ ; done'
Jan 29 03:18:50.540: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n"
Jan 29 03:18:50.540: INFO: stdout: "\naffinity-clusterip-transition-hvvmq\naffinity-clusterip-transition-9zqxc\naffinity-clusterip-transition-qc5xg\naffinity-clusterip-transition-qc5xg\naffinity-clusterip-transition-qc5xg\naffinity-clusterip-transition-hvvmq\naffinity-clusterip-transition-qc5xg\naffinity-clusterip-transition-9zqxc\naffinity-clusterip-transition-hvvmq\naffinity-clusterip-transition-hvvmq\naffinity-clusterip-transition-9zqxc\naffinity-clusterip-transition-9zqxc\naffinity-clusterip-transition-9zqxc\naffinity-clusterip-transition-hvvmq\naffinity-clusterip-transition-qc5xg\naffinity-clusterip-transition-hvvmq"
Jan 29 03:18:50.540: INFO: Received response from host: affinity-clusterip-transition-hvvmq
Jan 29 03:18:50.540: INFO: Received response from host: affinity-clusterip-transition-9zqxc
Jan 29 03:18:50.540: INFO: Received response from host: affinity-clusterip-transition-qc5xg
Jan 29 03:18:50.540: INFO: Received response from host: affinity-clusterip-transition-qc5xg
Jan 29 03:18:50.540: INFO: Received response from host: affinity-clusterip-transition-qc5xg
Jan 29 03:18:50.540: INFO: Received response from host: affinity-clusterip-transition-hvvmq
Jan 29 03:18:50.540: INFO: Received response from host: affinity-clusterip-transition-qc5xg
Jan 29 03:18:50.540: INFO: Received response from host: affinity-clusterip-transition-9zqxc
Jan 29 03:18:50.540: INFO: Received response from host: affinity-clusterip-transition-hvvmq
Jan 29 03:18:50.540: INFO: Received response from host: affinity-clusterip-transition-hvvmq
Jan 29 03:18:50.540: INFO: Received response from host: affinity-clusterip-transition-9zqxc
Jan 29 03:18:50.540: INFO: Received response from host: affinity-clusterip-transition-9zqxc
Jan 29 03:18:50.540: INFO: Received response from host: affinity-clusterip-transition-9zqxc
Jan 29 03:18:50.540: INFO: Received response from host: affinity-clusterip-transition-hvvmq
Jan 29 03:18:50.540: INFO: Received response from host: affinity-clusterip-transition-qc5xg
Jan 29 03:18:50.540: INFO: Received response from host: affinity-clusterip-transition-hvvmq
Jan 29 03:18:50.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-4070 exec execpod-affinityzghmr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.105.182.98:80/ ; done'
Jan 29 03:18:50.923: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n"
Jan 29 03:18:50.923: INFO: stdout: "\naffinity-clusterip-transition-hvvmq\naffinity-clusterip-transition-hvvmq\naffinity-clusterip-transition-hvvmq\naffinity-clusterip-transition-hvvmq\naffinity-clusterip-transition-hvvmq\naffinity-clusterip-transition-hvvmq\naffinity-clusterip-transition-hvvmq\naffinity-clusterip-transition-hvvmq\naffinity-clusterip-transition-hvvmq\naffinity-clusterip-transition-hvvmq\naffinity-clusterip-transition-hvvmq\naffinity-clusterip-transition-hvvmq\naffinity-clusterip-transition-hvvmq\naffinity-clusterip-transition-hvvmq\naffinity-clusterip-transition-hvvmq\naffinity-clusterip-transition-hvvmq"
Jan 29 03:18:50.923: INFO: Received response from host: affinity-clusterip-transition-hvvmq
Jan 29 03:18:50.923: INFO: Received response from host: affinity-clusterip-transition-hvvmq
Jan 29 03:18:50.923: INFO: Received response from host: affinity-clusterip-transition-hvvmq
Jan 29 03:18:50.923: INFO: Received response from host: affinity-clusterip-transition-hvvmq
Jan 29 03:18:50.923: INFO: Received response from host: affinity-clusterip-transition-hvvmq
Jan 29 03:18:50.923: INFO: Received response from host: affinity-clusterip-transition-hvvmq
Jan 29 03:18:50.923: INFO: Received response from host: affinity-clusterip-transition-hvvmq
Jan 29 03:18:50.923: INFO: Received response from host: affinity-clusterip-transition-hvvmq
Jan 29 03:18:50.923: INFO: Received response from host: affinity-clusterip-transition-hvvmq
Jan 29 03:18:50.923: INFO: Received response from host: affinity-clusterip-transition-hvvmq
Jan 29 03:18:50.923: INFO: Received response from host: affinity-clusterip-transition-hvvmq
Jan 29 03:18:50.923: INFO: Received response from host: affinity-clusterip-transition-hvvmq
Jan 29 03:18:50.923: INFO: Received response from host: affinity-clusterip-transition-hvvmq
Jan 29 03:18:50.923: INFO: Received response from host: affinity-clusterip-transition-hvvmq
Jan 29 03:18:50.923: INFO: Received response from host: affinity-clusterip-transition-hvvmq
Jan 29 03:18:50.923: INFO: Received response from host: affinity-clusterip-transition-hvvmq
Jan 29 03:18:50.923: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-4070, will wait for the garbage collector to delete the pods 01/29/23 03:18:51.03
Jan 29 03:18:51.110: INFO: Deleting ReplicationController affinity-clusterip-transition took: 22.109055ms
Jan 29 03:18:51.210: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.663985ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 29 03:18:54.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4070" for this suite. 01/29/23 03:18:54.066
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","completed":95,"skipped":1729,"failed":0}
------------------------------
• [SLOW TEST] [40.955 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:18:13.122
    Jan 29 03:18:13.122: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename services 01/29/23 03:18:13.123
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:18:13.154
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:18:13.16
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2189
    STEP: creating service in namespace services-4070 01/29/23 03:18:13.166
    STEP: creating service affinity-clusterip-transition in namespace services-4070 01/29/23 03:18:13.167
    STEP: creating replication controller affinity-clusterip-transition in namespace services-4070 01/29/23 03:18:13.182
    I0129 03:18:13.191660      22 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-4070, replica count: 3
    I0129 03:18:16.242837      22 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 29 03:18:16.256: INFO: Creating new exec pod
    Jan 29 03:18:16.273: INFO: Waiting up to 5m0s for pod "execpod-affinityzghmr" in namespace "services-4070" to be "running"
    Jan 29 03:18:16.280: INFO: Pod "execpod-affinityzghmr": Phase="Pending", Reason="", readiness=false. Elapsed: 7.06341ms
    Jan 29 03:18:18.294: INFO: Pod "execpod-affinityzghmr": Phase="Running", Reason="", readiness=true. Elapsed: 2.020373478s
    Jan 29 03:18:18.294: INFO: Pod "execpod-affinityzghmr" satisfied condition "running"
    Jan 29 03:18:19.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-4070 exec execpod-affinityzghmr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
    Jan 29 03:18:19.565: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Jan 29 03:18:19.566: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 29 03:18:19.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-4070 exec execpod-affinityzghmr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.105.182.98 80'
    Jan 29 03:18:19.811: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.105.182.98 80\nConnection to 100.105.182.98 80 port [tcp/http] succeeded!\n"
    Jan 29 03:18:19.811: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 29 03:18:19.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-4070 exec execpod-affinityzghmr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.105.182.98:80/ ; done'
    Jan 29 03:18:20.180: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n"
    Jan 29 03:18:20.180: INFO: stdout: "\naffinity-clusterip-transition-qc5xg\naffinity-clusterip-transition-qc5xg\naffinity-clusterip-transition-qc5xg\naffinity-clusterip-transition-qc5xg\naffinity-clusterip-transition-qc5xg\naffinity-clusterip-transition-qc5xg\naffinity-clusterip-transition-qc5xg\naffinity-clusterip-transition-qc5xg\naffinity-clusterip-transition-qc5xg\naffinity-clusterip-transition-qc5xg\naffinity-clusterip-transition-qc5xg\naffinity-clusterip-transition-qc5xg\naffinity-clusterip-transition-qc5xg\naffinity-clusterip-transition-qc5xg\naffinity-clusterip-transition-qc5xg\naffinity-clusterip-transition-qc5xg"
    Jan 29 03:18:20.180: INFO: Received response from host: affinity-clusterip-transition-qc5xg
    Jan 29 03:18:20.180: INFO: Received response from host: affinity-clusterip-transition-qc5xg
    Jan 29 03:18:20.180: INFO: Received response from host: affinity-clusterip-transition-qc5xg
    Jan 29 03:18:20.180: INFO: Received response from host: affinity-clusterip-transition-qc5xg
    Jan 29 03:18:20.180: INFO: Received response from host: affinity-clusterip-transition-qc5xg
    Jan 29 03:18:20.180: INFO: Received response from host: affinity-clusterip-transition-qc5xg
    Jan 29 03:18:20.180: INFO: Received response from host: affinity-clusterip-transition-qc5xg
    Jan 29 03:18:20.180: INFO: Received response from host: affinity-clusterip-transition-qc5xg
    Jan 29 03:18:20.180: INFO: Received response from host: affinity-clusterip-transition-qc5xg
    Jan 29 03:18:20.180: INFO: Received response from host: affinity-clusterip-transition-qc5xg
    Jan 29 03:18:20.180: INFO: Received response from host: affinity-clusterip-transition-qc5xg
    Jan 29 03:18:20.180: INFO: Received response from host: affinity-clusterip-transition-qc5xg
    Jan 29 03:18:20.180: INFO: Received response from host: affinity-clusterip-transition-qc5xg
    Jan 29 03:18:20.180: INFO: Received response from host: affinity-clusterip-transition-qc5xg
    Jan 29 03:18:20.180: INFO: Received response from host: affinity-clusterip-transition-qc5xg
    Jan 29 03:18:20.180: INFO: Received response from host: affinity-clusterip-transition-qc5xg
    Jan 29 03:18:50.181: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-4070 exec execpod-affinityzghmr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.105.182.98:80/ ; done'
    Jan 29 03:18:50.540: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n"
    Jan 29 03:18:50.540: INFO: stdout: "\naffinity-clusterip-transition-hvvmq\naffinity-clusterip-transition-9zqxc\naffinity-clusterip-transition-qc5xg\naffinity-clusterip-transition-qc5xg\naffinity-clusterip-transition-qc5xg\naffinity-clusterip-transition-hvvmq\naffinity-clusterip-transition-qc5xg\naffinity-clusterip-transition-9zqxc\naffinity-clusterip-transition-hvvmq\naffinity-clusterip-transition-hvvmq\naffinity-clusterip-transition-9zqxc\naffinity-clusterip-transition-9zqxc\naffinity-clusterip-transition-9zqxc\naffinity-clusterip-transition-hvvmq\naffinity-clusterip-transition-qc5xg\naffinity-clusterip-transition-hvvmq"
    Jan 29 03:18:50.540: INFO: Received response from host: affinity-clusterip-transition-hvvmq
    Jan 29 03:18:50.540: INFO: Received response from host: affinity-clusterip-transition-9zqxc
    Jan 29 03:18:50.540: INFO: Received response from host: affinity-clusterip-transition-qc5xg
    Jan 29 03:18:50.540: INFO: Received response from host: affinity-clusterip-transition-qc5xg
    Jan 29 03:18:50.540: INFO: Received response from host: affinity-clusterip-transition-qc5xg
    Jan 29 03:18:50.540: INFO: Received response from host: affinity-clusterip-transition-hvvmq
    Jan 29 03:18:50.540: INFO: Received response from host: affinity-clusterip-transition-qc5xg
    Jan 29 03:18:50.540: INFO: Received response from host: affinity-clusterip-transition-9zqxc
    Jan 29 03:18:50.540: INFO: Received response from host: affinity-clusterip-transition-hvvmq
    Jan 29 03:18:50.540: INFO: Received response from host: affinity-clusterip-transition-hvvmq
    Jan 29 03:18:50.540: INFO: Received response from host: affinity-clusterip-transition-9zqxc
    Jan 29 03:18:50.540: INFO: Received response from host: affinity-clusterip-transition-9zqxc
    Jan 29 03:18:50.540: INFO: Received response from host: affinity-clusterip-transition-9zqxc
    Jan 29 03:18:50.540: INFO: Received response from host: affinity-clusterip-transition-hvvmq
    Jan 29 03:18:50.540: INFO: Received response from host: affinity-clusterip-transition-qc5xg
    Jan 29 03:18:50.540: INFO: Received response from host: affinity-clusterip-transition-hvvmq
    Jan 29 03:18:50.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-4070 exec execpod-affinityzghmr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.105.182.98:80/ ; done'
    Jan 29 03:18:50.923: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.182.98:80/\n"
    Jan 29 03:18:50.923: INFO: stdout: "\naffinity-clusterip-transition-hvvmq\naffinity-clusterip-transition-hvvmq\naffinity-clusterip-transition-hvvmq\naffinity-clusterip-transition-hvvmq\naffinity-clusterip-transition-hvvmq\naffinity-clusterip-transition-hvvmq\naffinity-clusterip-transition-hvvmq\naffinity-clusterip-transition-hvvmq\naffinity-clusterip-transition-hvvmq\naffinity-clusterip-transition-hvvmq\naffinity-clusterip-transition-hvvmq\naffinity-clusterip-transition-hvvmq\naffinity-clusterip-transition-hvvmq\naffinity-clusterip-transition-hvvmq\naffinity-clusterip-transition-hvvmq\naffinity-clusterip-transition-hvvmq"
    Jan 29 03:18:50.923: INFO: Received response from host: affinity-clusterip-transition-hvvmq
    Jan 29 03:18:50.923: INFO: Received response from host: affinity-clusterip-transition-hvvmq
    Jan 29 03:18:50.923: INFO: Received response from host: affinity-clusterip-transition-hvvmq
    Jan 29 03:18:50.923: INFO: Received response from host: affinity-clusterip-transition-hvvmq
    Jan 29 03:18:50.923: INFO: Received response from host: affinity-clusterip-transition-hvvmq
    Jan 29 03:18:50.923: INFO: Received response from host: affinity-clusterip-transition-hvvmq
    Jan 29 03:18:50.923: INFO: Received response from host: affinity-clusterip-transition-hvvmq
    Jan 29 03:18:50.923: INFO: Received response from host: affinity-clusterip-transition-hvvmq
    Jan 29 03:18:50.923: INFO: Received response from host: affinity-clusterip-transition-hvvmq
    Jan 29 03:18:50.923: INFO: Received response from host: affinity-clusterip-transition-hvvmq
    Jan 29 03:18:50.923: INFO: Received response from host: affinity-clusterip-transition-hvvmq
    Jan 29 03:18:50.923: INFO: Received response from host: affinity-clusterip-transition-hvvmq
    Jan 29 03:18:50.923: INFO: Received response from host: affinity-clusterip-transition-hvvmq
    Jan 29 03:18:50.923: INFO: Received response from host: affinity-clusterip-transition-hvvmq
    Jan 29 03:18:50.923: INFO: Received response from host: affinity-clusterip-transition-hvvmq
    Jan 29 03:18:50.923: INFO: Received response from host: affinity-clusterip-transition-hvvmq
    Jan 29 03:18:50.923: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-4070, will wait for the garbage collector to delete the pods 01/29/23 03:18:51.03
    Jan 29 03:18:51.110: INFO: Deleting ReplicationController affinity-clusterip-transition took: 22.109055ms
    Jan 29 03:18:51.210: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.663985ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 29 03:18:54.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4070" for this suite. 01/29/23 03:18:54.066
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:18:54.08
Jan 29 03:18:54.080: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename containers 01/29/23 03:18:54.081
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:18:54.11
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:18:54.117
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
STEP: Creating a pod to test override command 01/29/23 03:18:54.122
Jan 29 03:18:54.141: INFO: Waiting up to 5m0s for pod "client-containers-52af03f9-43c2-404b-86f8-182153b8de17" in namespace "containers-4926" to be "Succeeded or Failed"
Jan 29 03:18:54.147: INFO: Pod "client-containers-52af03f9-43c2-404b-86f8-182153b8de17": Phase="Pending", Reason="", readiness=false. Elapsed: 6.248364ms
Jan 29 03:18:56.155: INFO: Pod "client-containers-52af03f9-43c2-404b-86f8-182153b8de17": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013776676s
Jan 29 03:18:58.157: INFO: Pod "client-containers-52af03f9-43c2-404b-86f8-182153b8de17": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016218773s
STEP: Saw pod success 01/29/23 03:18:58.157
Jan 29 03:18:58.158: INFO: Pod "client-containers-52af03f9-43c2-404b-86f8-182153b8de17" satisfied condition "Succeeded or Failed"
Jan 29 03:18:58.164: INFO: Trying to get logs from node slave2 pod client-containers-52af03f9-43c2-404b-86f8-182153b8de17 container agnhost-container: <nil>
STEP: delete the pod 01/29/23 03:18:58.209
Jan 29 03:18:58.309: INFO: Waiting for pod client-containers-52af03f9-43c2-404b-86f8-182153b8de17 to disappear
Jan 29 03:18:58.315: INFO: Pod client-containers-52af03f9-43c2-404b-86f8-182153b8de17 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Jan 29 03:18:58.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4926" for this suite. 01/29/23 03:18:58.324
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]","completed":96,"skipped":1747,"failed":0}
------------------------------
• [4.255 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:18:54.08
    Jan 29 03:18:54.080: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename containers 01/29/23 03:18:54.081
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:18:54.11
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:18:54.117
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:72
    STEP: Creating a pod to test override command 01/29/23 03:18:54.122
    Jan 29 03:18:54.141: INFO: Waiting up to 5m0s for pod "client-containers-52af03f9-43c2-404b-86f8-182153b8de17" in namespace "containers-4926" to be "Succeeded or Failed"
    Jan 29 03:18:54.147: INFO: Pod "client-containers-52af03f9-43c2-404b-86f8-182153b8de17": Phase="Pending", Reason="", readiness=false. Elapsed: 6.248364ms
    Jan 29 03:18:56.155: INFO: Pod "client-containers-52af03f9-43c2-404b-86f8-182153b8de17": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013776676s
    Jan 29 03:18:58.157: INFO: Pod "client-containers-52af03f9-43c2-404b-86f8-182153b8de17": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016218773s
    STEP: Saw pod success 01/29/23 03:18:58.157
    Jan 29 03:18:58.158: INFO: Pod "client-containers-52af03f9-43c2-404b-86f8-182153b8de17" satisfied condition "Succeeded or Failed"
    Jan 29 03:18:58.164: INFO: Trying to get logs from node slave2 pod client-containers-52af03f9-43c2-404b-86f8-182153b8de17 container agnhost-container: <nil>
    STEP: delete the pod 01/29/23 03:18:58.209
    Jan 29 03:18:58.309: INFO: Waiting for pod client-containers-52af03f9-43c2-404b-86f8-182153b8de17 to disappear
    Jan 29 03:18:58.315: INFO: Pod client-containers-52af03f9-43c2-404b-86f8-182153b8de17 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Jan 29 03:18:58.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-4926" for this suite. 01/29/23 03:18:58.324
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:18:58.339
Jan 29 03:18:58.339: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename emptydir 01/29/23 03:18:58.34
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:18:58.369
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:18:58.375
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
STEP: Creating a pod to test emptydir 0777 on tmpfs 01/29/23 03:18:58.38
Jan 29 03:18:58.397: INFO: Waiting up to 5m0s for pod "pod-4c24fcbd-7121-405b-82ce-642ad9bf0567" in namespace "emptydir-9532" to be "Succeeded or Failed"
Jan 29 03:18:58.406: INFO: Pod "pod-4c24fcbd-7121-405b-82ce-642ad9bf0567": Phase="Pending", Reason="", readiness=false. Elapsed: 8.908502ms
Jan 29 03:19:00.414: INFO: Pod "pod-4c24fcbd-7121-405b-82ce-642ad9bf0567": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016448714s
Jan 29 03:19:02.413: INFO: Pod "pod-4c24fcbd-7121-405b-82ce-642ad9bf0567": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01594077s
STEP: Saw pod success 01/29/23 03:19:02.413
Jan 29 03:19:02.413: INFO: Pod "pod-4c24fcbd-7121-405b-82ce-642ad9bf0567" satisfied condition "Succeeded or Failed"
Jan 29 03:19:02.419: INFO: Trying to get logs from node slave2 pod pod-4c24fcbd-7121-405b-82ce-642ad9bf0567 container test-container: <nil>
STEP: delete the pod 01/29/23 03:19:02.434
Jan 29 03:19:02.504: INFO: Waiting for pod pod-4c24fcbd-7121-405b-82ce-642ad9bf0567 to disappear
Jan 29 03:19:02.509: INFO: Pod pod-4c24fcbd-7121-405b-82ce-642ad9bf0567 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 29 03:19:02.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9532" for this suite. 01/29/23 03:19:02.524
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":97,"skipped":1796,"failed":0}
------------------------------
• [4.196 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:18:58.339
    Jan 29 03:18:58.339: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename emptydir 01/29/23 03:18:58.34
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:18:58.369
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:18:58.375
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:116
    STEP: Creating a pod to test emptydir 0777 on tmpfs 01/29/23 03:18:58.38
    Jan 29 03:18:58.397: INFO: Waiting up to 5m0s for pod "pod-4c24fcbd-7121-405b-82ce-642ad9bf0567" in namespace "emptydir-9532" to be "Succeeded or Failed"
    Jan 29 03:18:58.406: INFO: Pod "pod-4c24fcbd-7121-405b-82ce-642ad9bf0567": Phase="Pending", Reason="", readiness=false. Elapsed: 8.908502ms
    Jan 29 03:19:00.414: INFO: Pod "pod-4c24fcbd-7121-405b-82ce-642ad9bf0567": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016448714s
    Jan 29 03:19:02.413: INFO: Pod "pod-4c24fcbd-7121-405b-82ce-642ad9bf0567": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01594077s
    STEP: Saw pod success 01/29/23 03:19:02.413
    Jan 29 03:19:02.413: INFO: Pod "pod-4c24fcbd-7121-405b-82ce-642ad9bf0567" satisfied condition "Succeeded or Failed"
    Jan 29 03:19:02.419: INFO: Trying to get logs from node slave2 pod pod-4c24fcbd-7121-405b-82ce-642ad9bf0567 container test-container: <nil>
    STEP: delete the pod 01/29/23 03:19:02.434
    Jan 29 03:19:02.504: INFO: Waiting for pod pod-4c24fcbd-7121-405b-82ce-642ad9bf0567 to disappear
    Jan 29 03:19:02.509: INFO: Pod pod-4c24fcbd-7121-405b-82ce-642ad9bf0567 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 29 03:19:02.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-9532" for this suite. 01/29/23 03:19:02.524
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:19:02.536
Jan 29 03:19:02.537: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename kubelet-test 01/29/23 03:19:02.538
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:19:02.565
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:19:02.57
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Jan 29 03:19:02.593: INFO: Waiting up to 5m0s for pod "busybox-readonly-fsba4eba6f-0704-48b9-aa31-9286561ef75c" in namespace "kubelet-test-5370" to be "running and ready"
Jan 29 03:19:02.599: INFO: Pod "busybox-readonly-fsba4eba6f-0704-48b9-aa31-9286561ef75c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.294344ms
Jan 29 03:19:02.599: INFO: The phase of Pod busybox-readonly-fsba4eba6f-0704-48b9-aa31-9286561ef75c is Pending, waiting for it to be Running (with Ready = true)
Jan 29 03:19:04.608: INFO: Pod "busybox-readonly-fsba4eba6f-0704-48b9-aa31-9286561ef75c": Phase="Running", Reason="", readiness=true. Elapsed: 2.015389727s
Jan 29 03:19:04.608: INFO: The phase of Pod busybox-readonly-fsba4eba6f-0704-48b9-aa31-9286561ef75c is Running (Ready = true)
Jan 29 03:19:04.608: INFO: Pod "busybox-readonly-fsba4eba6f-0704-48b9-aa31-9286561ef75c" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jan 29 03:19:04.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5370" for this suite. 01/29/23 03:19:04.645
{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","completed":98,"skipped":1816,"failed":0}
------------------------------
• [2.121 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:19:02.536
    Jan 29 03:19:02.537: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename kubelet-test 01/29/23 03:19:02.538
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:19:02.565
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:19:02.57
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Jan 29 03:19:02.593: INFO: Waiting up to 5m0s for pod "busybox-readonly-fsba4eba6f-0704-48b9-aa31-9286561ef75c" in namespace "kubelet-test-5370" to be "running and ready"
    Jan 29 03:19:02.599: INFO: Pod "busybox-readonly-fsba4eba6f-0704-48b9-aa31-9286561ef75c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.294344ms
    Jan 29 03:19:02.599: INFO: The phase of Pod busybox-readonly-fsba4eba6f-0704-48b9-aa31-9286561ef75c is Pending, waiting for it to be Running (with Ready = true)
    Jan 29 03:19:04.608: INFO: Pod "busybox-readonly-fsba4eba6f-0704-48b9-aa31-9286561ef75c": Phase="Running", Reason="", readiness=true. Elapsed: 2.015389727s
    Jan 29 03:19:04.608: INFO: The phase of Pod busybox-readonly-fsba4eba6f-0704-48b9-aa31-9286561ef75c is Running (Ready = true)
    Jan 29 03:19:04.608: INFO: Pod "busybox-readonly-fsba4eba6f-0704-48b9-aa31-9286561ef75c" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jan 29 03:19:04.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-5370" for this suite. 01/29/23 03:19:04.645
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:19:04.659
Jan 29 03:19:04.659: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename statefulset 01/29/23 03:19:04.66
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:19:04.693
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:19:04.698
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-3522 01/29/23 03:19:04.704
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
STEP: Creating a new StatefulSet 01/29/23 03:19:04.711
Jan 29 03:19:04.732: INFO: Found 0 stateful pods, waiting for 3
Jan 29 03:19:14.740: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 29 03:19:14.740: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 29 03:19:14.740: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 01/29/23 03:19:14.763
Jan 29 03:19:14.797: INFO: Updating stateful set ss2
STEP: Creating a new revision 01/29/23 03:19:14.797
STEP: Not applying an update when the partition is greater than the number of replicas 01/29/23 03:19:24.83
STEP: Performing a canary update 01/29/23 03:19:24.83
Jan 29 03:19:24.860: INFO: Updating stateful set ss2
Jan 29 03:19:24.875: INFO: Waiting for Pod statefulset-3522/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
STEP: Restoring Pods to the correct revision when they are deleted 01/29/23 03:19:34.893
Jan 29 03:19:35.076: INFO: Found 2 stateful pods, waiting for 3
Jan 29 03:19:45.088: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 29 03:19:45.088: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 29 03:19:45.088: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 01/29/23 03:19:45.101
Jan 29 03:19:45.126: INFO: Updating stateful set ss2
Jan 29 03:19:45.141: INFO: Waiting for Pod statefulset-3522/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
Jan 29 03:19:55.182: INFO: Updating stateful set ss2
Jan 29 03:19:55.195: INFO: Waiting for StatefulSet statefulset-3522/ss2 to complete update
Jan 29 03:19:55.195: INFO: Waiting for Pod statefulset-3522/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 29 03:20:05.210: INFO: Deleting all statefulset in ns statefulset-3522
Jan 29 03:20:05.217: INFO: Scaling statefulset ss2 to 0
Jan 29 03:20:15.272: INFO: Waiting for statefulset status.replicas updated to 0
Jan 29 03:20:15.278: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan 29 03:20:15.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3522" for this suite. 01/29/23 03:20:15.313
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","completed":99,"skipped":1832,"failed":0}
------------------------------
• [SLOW TEST] [70.670 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:315

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:19:04.659
    Jan 29 03:19:04.659: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename statefulset 01/29/23 03:19:04.66
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:19:04.693
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:19:04.698
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-3522 01/29/23 03:19:04.704
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:315
    STEP: Creating a new StatefulSet 01/29/23 03:19:04.711
    Jan 29 03:19:04.732: INFO: Found 0 stateful pods, waiting for 3
    Jan 29 03:19:14.740: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 29 03:19:14.740: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 29 03:19:14.740: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 01/29/23 03:19:14.763
    Jan 29 03:19:14.797: INFO: Updating stateful set ss2
    STEP: Creating a new revision 01/29/23 03:19:14.797
    STEP: Not applying an update when the partition is greater than the number of replicas 01/29/23 03:19:24.83
    STEP: Performing a canary update 01/29/23 03:19:24.83
    Jan 29 03:19:24.860: INFO: Updating stateful set ss2
    Jan 29 03:19:24.875: INFO: Waiting for Pod statefulset-3522/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    STEP: Restoring Pods to the correct revision when they are deleted 01/29/23 03:19:34.893
    Jan 29 03:19:35.076: INFO: Found 2 stateful pods, waiting for 3
    Jan 29 03:19:45.088: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 29 03:19:45.088: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 29 03:19:45.088: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 01/29/23 03:19:45.101
    Jan 29 03:19:45.126: INFO: Updating stateful set ss2
    Jan 29 03:19:45.141: INFO: Waiting for Pod statefulset-3522/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    Jan 29 03:19:55.182: INFO: Updating stateful set ss2
    Jan 29 03:19:55.195: INFO: Waiting for StatefulSet statefulset-3522/ss2 to complete update
    Jan 29 03:19:55.195: INFO: Waiting for Pod statefulset-3522/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan 29 03:20:05.210: INFO: Deleting all statefulset in ns statefulset-3522
    Jan 29 03:20:05.217: INFO: Scaling statefulset ss2 to 0
    Jan 29 03:20:15.272: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 29 03:20:15.278: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan 29 03:20:15.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-3522" for this suite. 01/29/23 03:20:15.313
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:20:15.333
Jan 29 03:20:15.333: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename svcaccounts 01/29/23 03:20:15.334
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:20:15.362
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:20:15.368
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
Jan 29 03:20:15.379: INFO: Got root ca configmap in namespace "svcaccounts-6252"
Jan 29 03:20:15.390: INFO: Deleted root ca configmap in namespace "svcaccounts-6252"
STEP: waiting for a new root ca configmap created 01/29/23 03:20:15.891
Jan 29 03:20:15.898: INFO: Recreated root ca configmap in namespace "svcaccounts-6252"
Jan 29 03:20:15.905: INFO: Updated root ca configmap in namespace "svcaccounts-6252"
STEP: waiting for the root ca configmap reconciled 01/29/23 03:20:16.406
Jan 29 03:20:16.414: INFO: Reconciled root ca configmap in namespace "svcaccounts-6252"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jan 29 03:20:16.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6252" for this suite. 01/29/23 03:20:16.423
{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","completed":100,"skipped":1858,"failed":0}
------------------------------
• [1.107 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:20:15.333
    Jan 29 03:20:15.333: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename svcaccounts 01/29/23 03:20:15.334
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:20:15.362
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:20:15.368
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:739
    Jan 29 03:20:15.379: INFO: Got root ca configmap in namespace "svcaccounts-6252"
    Jan 29 03:20:15.390: INFO: Deleted root ca configmap in namespace "svcaccounts-6252"
    STEP: waiting for a new root ca configmap created 01/29/23 03:20:15.891
    Jan 29 03:20:15.898: INFO: Recreated root ca configmap in namespace "svcaccounts-6252"
    Jan 29 03:20:15.905: INFO: Updated root ca configmap in namespace "svcaccounts-6252"
    STEP: waiting for the root ca configmap reconciled 01/29/23 03:20:16.406
    Jan 29 03:20:16.414: INFO: Reconciled root ca configmap in namespace "svcaccounts-6252"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jan 29 03:20:16.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-6252" for this suite. 01/29/23 03:20:16.423
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:20:16.44
Jan 29 03:20:16.440: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename services 01/29/23 03:20:16.442
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:20:16.469
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:20:16.475
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641
STEP: creating a collection of services 01/29/23 03:20:16.481
Jan 29 03:20:16.481: INFO: Creating e2e-svc-a-jx4nv
Jan 29 03:20:16.513: INFO: Creating e2e-svc-b-drqdk
Jan 29 03:20:16.534: INFO: Creating e2e-svc-c-7jbps
STEP: deleting service collection 01/29/23 03:20:16.561
Jan 29 03:20:16.647: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 29 03:20:16.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1944" for this suite. 01/29/23 03:20:16.664
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should delete a collection of services [Conformance]","completed":101,"skipped":1858,"failed":0}
------------------------------
• [0.242 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:20:16.44
    Jan 29 03:20:16.440: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename services 01/29/23 03:20:16.442
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:20:16.469
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:20:16.475
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3641
    STEP: creating a collection of services 01/29/23 03:20:16.481
    Jan 29 03:20:16.481: INFO: Creating e2e-svc-a-jx4nv
    Jan 29 03:20:16.513: INFO: Creating e2e-svc-b-drqdk
    Jan 29 03:20:16.534: INFO: Creating e2e-svc-c-7jbps
    STEP: deleting service collection 01/29/23 03:20:16.561
    Jan 29 03:20:16.647: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 29 03:20:16.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-1944" for this suite. 01/29/23 03:20:16.664
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:20:16.683
Jan 29 03:20:16.684: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename projected 01/29/23 03:20:16.686
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:20:16.719
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:20:16.726
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
STEP: Creating configMap with name projected-configmap-test-volume-0ec8c428-076d-457c-ac5e-961133876cf6 01/29/23 03:20:16.732
STEP: Creating a pod to test consume configMaps 01/29/23 03:20:16.74
Jan 29 03:20:16.758: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-114cbe57-6878-40bd-b9ea-78eb211af99e" in namespace "projected-1412" to be "Succeeded or Failed"
Jan 29 03:20:16.764: INFO: Pod "pod-projected-configmaps-114cbe57-6878-40bd-b9ea-78eb211af99e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.7948ms
Jan 29 03:20:18.771: INFO: Pod "pod-projected-configmaps-114cbe57-6878-40bd-b9ea-78eb211af99e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013178411s
Jan 29 03:20:20.771: INFO: Pod "pod-projected-configmaps-114cbe57-6878-40bd-b9ea-78eb211af99e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012782788s
Jan 29 03:20:22.772: INFO: Pod "pod-projected-configmaps-114cbe57-6878-40bd-b9ea-78eb211af99e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013793854s
STEP: Saw pod success 01/29/23 03:20:22.772
Jan 29 03:20:22.772: INFO: Pod "pod-projected-configmaps-114cbe57-6878-40bd-b9ea-78eb211af99e" satisfied condition "Succeeded or Failed"
Jan 29 03:20:22.783: INFO: Trying to get logs from node slave2 pod pod-projected-configmaps-114cbe57-6878-40bd-b9ea-78eb211af99e container agnhost-container: <nil>
STEP: delete the pod 01/29/23 03:20:22.798
Jan 29 03:20:22.901: INFO: Waiting for pod pod-projected-configmaps-114cbe57-6878-40bd-b9ea-78eb211af99e to disappear
Jan 29 03:20:22.906: INFO: Pod pod-projected-configmaps-114cbe57-6878-40bd-b9ea-78eb211af99e no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 29 03:20:22.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1412" for this suite. 01/29/23 03:20:22.917
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":102,"skipped":1863,"failed":0}
------------------------------
• [SLOW TEST] [6.245 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:20:16.683
    Jan 29 03:20:16.684: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename projected 01/29/23 03:20:16.686
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:20:16.719
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:20:16.726
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:56
    STEP: Creating configMap with name projected-configmap-test-volume-0ec8c428-076d-457c-ac5e-961133876cf6 01/29/23 03:20:16.732
    STEP: Creating a pod to test consume configMaps 01/29/23 03:20:16.74
    Jan 29 03:20:16.758: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-114cbe57-6878-40bd-b9ea-78eb211af99e" in namespace "projected-1412" to be "Succeeded or Failed"
    Jan 29 03:20:16.764: INFO: Pod "pod-projected-configmaps-114cbe57-6878-40bd-b9ea-78eb211af99e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.7948ms
    Jan 29 03:20:18.771: INFO: Pod "pod-projected-configmaps-114cbe57-6878-40bd-b9ea-78eb211af99e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013178411s
    Jan 29 03:20:20.771: INFO: Pod "pod-projected-configmaps-114cbe57-6878-40bd-b9ea-78eb211af99e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012782788s
    Jan 29 03:20:22.772: INFO: Pod "pod-projected-configmaps-114cbe57-6878-40bd-b9ea-78eb211af99e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013793854s
    STEP: Saw pod success 01/29/23 03:20:22.772
    Jan 29 03:20:22.772: INFO: Pod "pod-projected-configmaps-114cbe57-6878-40bd-b9ea-78eb211af99e" satisfied condition "Succeeded or Failed"
    Jan 29 03:20:22.783: INFO: Trying to get logs from node slave2 pod pod-projected-configmaps-114cbe57-6878-40bd-b9ea-78eb211af99e container agnhost-container: <nil>
    STEP: delete the pod 01/29/23 03:20:22.798
    Jan 29 03:20:22.901: INFO: Waiting for pod pod-projected-configmaps-114cbe57-6878-40bd-b9ea-78eb211af99e to disappear
    Jan 29 03:20:22.906: INFO: Pod pod-projected-configmaps-114cbe57-6878-40bd-b9ea-78eb211af99e no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 29 03:20:22.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1412" for this suite. 01/29/23 03:20:22.917
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:20:22.93
Jan 29 03:20:22.930: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename kubectl 01/29/23 03:20:22.932
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:20:22.962
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:20:22.969
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
STEP: creating a replication controller 01/29/23 03:20:22.977
Jan 29 03:20:22.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8820 create -f -'
Jan 29 03:20:24.329: INFO: stderr: ""
Jan 29 03:20:24.329: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/29/23 03:20:24.329
Jan 29 03:20:24.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8820 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 29 03:20:24.464: INFO: stderr: ""
Jan 29 03:20:24.464: INFO: stdout: "update-demo-nautilus-6bmlb update-demo-nautilus-bphrc "
Jan 29 03:20:24.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8820 get pods update-demo-nautilus-6bmlb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 29 03:20:24.580: INFO: stderr: ""
Jan 29 03:20:24.580: INFO: stdout: ""
Jan 29 03:20:24.580: INFO: update-demo-nautilus-6bmlb is created but not running
Jan 29 03:20:29.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8820 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 29 03:20:29.695: INFO: stderr: ""
Jan 29 03:20:29.695: INFO: stdout: "update-demo-nautilus-6bmlb update-demo-nautilus-bphrc "
Jan 29 03:20:29.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8820 get pods update-demo-nautilus-6bmlb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 29 03:20:29.809: INFO: stderr: ""
Jan 29 03:20:29.809: INFO: stdout: "true"
Jan 29 03:20:29.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8820 get pods update-demo-nautilus-6bmlb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 29 03:20:29.933: INFO: stderr: ""
Jan 29 03:20:29.933: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan 29 03:20:29.933: INFO: validating pod update-demo-nautilus-6bmlb
Jan 29 03:20:29.942: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 29 03:20:29.942: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 29 03:20:29.942: INFO: update-demo-nautilus-6bmlb is verified up and running
Jan 29 03:20:29.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8820 get pods update-demo-nautilus-bphrc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 29 03:20:30.059: INFO: stderr: ""
Jan 29 03:20:30.059: INFO: stdout: "true"
Jan 29 03:20:30.059: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8820 get pods update-demo-nautilus-bphrc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 29 03:20:30.183: INFO: stderr: ""
Jan 29 03:20:30.183: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan 29 03:20:30.183: INFO: validating pod update-demo-nautilus-bphrc
Jan 29 03:20:30.192: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 29 03:20:30.192: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 29 03:20:30.192: INFO: update-demo-nautilus-bphrc is verified up and running
STEP: scaling down the replication controller 01/29/23 03:20:30.192
Jan 29 03:20:30.195: INFO: scanned /root for discovery docs: <nil>
Jan 29 03:20:30.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8820 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Jan 29 03:20:31.346: INFO: stderr: ""
Jan 29 03:20:31.346: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/29/23 03:20:31.346
Jan 29 03:20:31.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8820 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 29 03:20:31.466: INFO: stderr: ""
Jan 29 03:20:31.466: INFO: stdout: "update-demo-nautilus-bphrc "
Jan 29 03:20:31.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8820 get pods update-demo-nautilus-bphrc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 29 03:20:31.573: INFO: stderr: ""
Jan 29 03:20:31.573: INFO: stdout: "true"
Jan 29 03:20:31.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8820 get pods update-demo-nautilus-bphrc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 29 03:20:31.680: INFO: stderr: ""
Jan 29 03:20:31.680: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan 29 03:20:31.680: INFO: validating pod update-demo-nautilus-bphrc
Jan 29 03:20:31.686: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 29 03:20:31.686: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 29 03:20:31.686: INFO: update-demo-nautilus-bphrc is verified up and running
STEP: scaling up the replication controller 01/29/23 03:20:31.686
Jan 29 03:20:31.689: INFO: scanned /root for discovery docs: <nil>
Jan 29 03:20:31.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8820 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Jan 29 03:20:32.833: INFO: stderr: ""
Jan 29 03:20:32.833: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/29/23 03:20:32.833
Jan 29 03:20:32.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8820 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 29 03:20:32.972: INFO: stderr: ""
Jan 29 03:20:32.972: INFO: stdout: "update-demo-nautilus-bphrc update-demo-nautilus-xk6lz "
Jan 29 03:20:32.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8820 get pods update-demo-nautilus-bphrc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 29 03:20:33.100: INFO: stderr: ""
Jan 29 03:20:33.100: INFO: stdout: "true"
Jan 29 03:20:33.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8820 get pods update-demo-nautilus-bphrc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 29 03:20:33.211: INFO: stderr: ""
Jan 29 03:20:33.211: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan 29 03:20:33.211: INFO: validating pod update-demo-nautilus-bphrc
Jan 29 03:20:33.217: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 29 03:20:33.218: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 29 03:20:33.218: INFO: update-demo-nautilus-bphrc is verified up and running
Jan 29 03:20:33.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8820 get pods update-demo-nautilus-xk6lz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 29 03:20:33.327: INFO: stderr: ""
Jan 29 03:20:33.327: INFO: stdout: ""
Jan 29 03:20:33.327: INFO: update-demo-nautilus-xk6lz is created but not running
Jan 29 03:20:38.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8820 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 29 03:20:38.445: INFO: stderr: ""
Jan 29 03:20:38.445: INFO: stdout: "update-demo-nautilus-bphrc update-demo-nautilus-xk6lz "
Jan 29 03:20:38.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8820 get pods update-demo-nautilus-bphrc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 29 03:20:38.558: INFO: stderr: ""
Jan 29 03:20:38.559: INFO: stdout: "true"
Jan 29 03:20:38.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8820 get pods update-demo-nautilus-bphrc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 29 03:20:38.674: INFO: stderr: ""
Jan 29 03:20:38.674: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan 29 03:20:38.674: INFO: validating pod update-demo-nautilus-bphrc
Jan 29 03:20:38.681: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 29 03:20:38.681: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 29 03:20:38.681: INFO: update-demo-nautilus-bphrc is verified up and running
Jan 29 03:20:38.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8820 get pods update-demo-nautilus-xk6lz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 29 03:20:38.800: INFO: stderr: ""
Jan 29 03:20:38.800: INFO: stdout: "true"
Jan 29 03:20:38.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8820 get pods update-demo-nautilus-xk6lz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 29 03:20:38.909: INFO: stderr: ""
Jan 29 03:20:38.910: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan 29 03:20:38.910: INFO: validating pod update-demo-nautilus-xk6lz
Jan 29 03:20:38.920: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 29 03:20:38.920: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 29 03:20:38.920: INFO: update-demo-nautilus-xk6lz is verified up and running
STEP: using delete to clean up resources 01/29/23 03:20:38.92
Jan 29 03:20:38.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8820 delete --grace-period=0 --force -f -'
Jan 29 03:20:39.043: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 29 03:20:39.043: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan 29 03:20:39.044: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8820 get rc,svc -l name=update-demo --no-headers'
Jan 29 03:20:39.208: INFO: stderr: "No resources found in kubectl-8820 namespace.\n"
Jan 29 03:20:39.208: INFO: stdout: ""
Jan 29 03:20:39.208: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8820 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 29 03:20:39.337: INFO: stderr: ""
Jan 29 03:20:39.337: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 29 03:20:39.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8820" for this suite. 01/29/23 03:20:39.348
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","completed":103,"skipped":1875,"failed":0}
------------------------------
• [SLOW TEST] [16.429 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:350

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:20:22.93
    Jan 29 03:20:22.930: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename kubectl 01/29/23 03:20:22.932
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:20:22.962
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:20:22.969
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:350
    STEP: creating a replication controller 01/29/23 03:20:22.977
    Jan 29 03:20:22.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8820 create -f -'
    Jan 29 03:20:24.329: INFO: stderr: ""
    Jan 29 03:20:24.329: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/29/23 03:20:24.329
    Jan 29 03:20:24.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8820 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 29 03:20:24.464: INFO: stderr: ""
    Jan 29 03:20:24.464: INFO: stdout: "update-demo-nautilus-6bmlb update-demo-nautilus-bphrc "
    Jan 29 03:20:24.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8820 get pods update-demo-nautilus-6bmlb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 29 03:20:24.580: INFO: stderr: ""
    Jan 29 03:20:24.580: INFO: stdout: ""
    Jan 29 03:20:24.580: INFO: update-demo-nautilus-6bmlb is created but not running
    Jan 29 03:20:29.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8820 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 29 03:20:29.695: INFO: stderr: ""
    Jan 29 03:20:29.695: INFO: stdout: "update-demo-nautilus-6bmlb update-demo-nautilus-bphrc "
    Jan 29 03:20:29.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8820 get pods update-demo-nautilus-6bmlb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 29 03:20:29.809: INFO: stderr: ""
    Jan 29 03:20:29.809: INFO: stdout: "true"
    Jan 29 03:20:29.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8820 get pods update-demo-nautilus-6bmlb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 29 03:20:29.933: INFO: stderr: ""
    Jan 29 03:20:29.933: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan 29 03:20:29.933: INFO: validating pod update-demo-nautilus-6bmlb
    Jan 29 03:20:29.942: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 29 03:20:29.942: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 29 03:20:29.942: INFO: update-demo-nautilus-6bmlb is verified up and running
    Jan 29 03:20:29.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8820 get pods update-demo-nautilus-bphrc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 29 03:20:30.059: INFO: stderr: ""
    Jan 29 03:20:30.059: INFO: stdout: "true"
    Jan 29 03:20:30.059: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8820 get pods update-demo-nautilus-bphrc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 29 03:20:30.183: INFO: stderr: ""
    Jan 29 03:20:30.183: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan 29 03:20:30.183: INFO: validating pod update-demo-nautilus-bphrc
    Jan 29 03:20:30.192: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 29 03:20:30.192: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 29 03:20:30.192: INFO: update-demo-nautilus-bphrc is verified up and running
    STEP: scaling down the replication controller 01/29/23 03:20:30.192
    Jan 29 03:20:30.195: INFO: scanned /root for discovery docs: <nil>
    Jan 29 03:20:30.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8820 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Jan 29 03:20:31.346: INFO: stderr: ""
    Jan 29 03:20:31.346: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/29/23 03:20:31.346
    Jan 29 03:20:31.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8820 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 29 03:20:31.466: INFO: stderr: ""
    Jan 29 03:20:31.466: INFO: stdout: "update-demo-nautilus-bphrc "
    Jan 29 03:20:31.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8820 get pods update-demo-nautilus-bphrc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 29 03:20:31.573: INFO: stderr: ""
    Jan 29 03:20:31.573: INFO: stdout: "true"
    Jan 29 03:20:31.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8820 get pods update-demo-nautilus-bphrc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 29 03:20:31.680: INFO: stderr: ""
    Jan 29 03:20:31.680: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan 29 03:20:31.680: INFO: validating pod update-demo-nautilus-bphrc
    Jan 29 03:20:31.686: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 29 03:20:31.686: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 29 03:20:31.686: INFO: update-demo-nautilus-bphrc is verified up and running
    STEP: scaling up the replication controller 01/29/23 03:20:31.686
    Jan 29 03:20:31.689: INFO: scanned /root for discovery docs: <nil>
    Jan 29 03:20:31.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8820 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Jan 29 03:20:32.833: INFO: stderr: ""
    Jan 29 03:20:32.833: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/29/23 03:20:32.833
    Jan 29 03:20:32.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8820 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 29 03:20:32.972: INFO: stderr: ""
    Jan 29 03:20:32.972: INFO: stdout: "update-demo-nautilus-bphrc update-demo-nautilus-xk6lz "
    Jan 29 03:20:32.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8820 get pods update-demo-nautilus-bphrc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 29 03:20:33.100: INFO: stderr: ""
    Jan 29 03:20:33.100: INFO: stdout: "true"
    Jan 29 03:20:33.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8820 get pods update-demo-nautilus-bphrc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 29 03:20:33.211: INFO: stderr: ""
    Jan 29 03:20:33.211: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan 29 03:20:33.211: INFO: validating pod update-demo-nautilus-bphrc
    Jan 29 03:20:33.217: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 29 03:20:33.218: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 29 03:20:33.218: INFO: update-demo-nautilus-bphrc is verified up and running
    Jan 29 03:20:33.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8820 get pods update-demo-nautilus-xk6lz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 29 03:20:33.327: INFO: stderr: ""
    Jan 29 03:20:33.327: INFO: stdout: ""
    Jan 29 03:20:33.327: INFO: update-demo-nautilus-xk6lz is created but not running
    Jan 29 03:20:38.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8820 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 29 03:20:38.445: INFO: stderr: ""
    Jan 29 03:20:38.445: INFO: stdout: "update-demo-nautilus-bphrc update-demo-nautilus-xk6lz "
    Jan 29 03:20:38.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8820 get pods update-demo-nautilus-bphrc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 29 03:20:38.558: INFO: stderr: ""
    Jan 29 03:20:38.559: INFO: stdout: "true"
    Jan 29 03:20:38.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8820 get pods update-demo-nautilus-bphrc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 29 03:20:38.674: INFO: stderr: ""
    Jan 29 03:20:38.674: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan 29 03:20:38.674: INFO: validating pod update-demo-nautilus-bphrc
    Jan 29 03:20:38.681: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 29 03:20:38.681: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 29 03:20:38.681: INFO: update-demo-nautilus-bphrc is verified up and running
    Jan 29 03:20:38.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8820 get pods update-demo-nautilus-xk6lz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 29 03:20:38.800: INFO: stderr: ""
    Jan 29 03:20:38.800: INFO: stdout: "true"
    Jan 29 03:20:38.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8820 get pods update-demo-nautilus-xk6lz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 29 03:20:38.909: INFO: stderr: ""
    Jan 29 03:20:38.910: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan 29 03:20:38.910: INFO: validating pod update-demo-nautilus-xk6lz
    Jan 29 03:20:38.920: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 29 03:20:38.920: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 29 03:20:38.920: INFO: update-demo-nautilus-xk6lz is verified up and running
    STEP: using delete to clean up resources 01/29/23 03:20:38.92
    Jan 29 03:20:38.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8820 delete --grace-period=0 --force -f -'
    Jan 29 03:20:39.043: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 29 03:20:39.043: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Jan 29 03:20:39.044: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8820 get rc,svc -l name=update-demo --no-headers'
    Jan 29 03:20:39.208: INFO: stderr: "No resources found in kubectl-8820 namespace.\n"
    Jan 29 03:20:39.208: INFO: stdout: ""
    Jan 29 03:20:39.208: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8820 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jan 29 03:20:39.337: INFO: stderr: ""
    Jan 29 03:20:39.337: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 29 03:20:39.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-8820" for this suite. 01/29/23 03:20:39.348
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:20:39.36
Jan 29 03:20:39.360: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename container-runtime 01/29/23 03:20:39.361
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:20:39.392
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:20:39.397
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
STEP: create the container 01/29/23 03:20:39.404
STEP: wait for the container to reach Succeeded 01/29/23 03:20:39.423
STEP: get the container status 01/29/23 03:20:43.469
STEP: the container should be terminated 01/29/23 03:20:43.476
STEP: the termination message should be set 01/29/23 03:20:43.476
Jan 29 03:20:43.476: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 01/29/23 03:20:43.476
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jan 29 03:20:43.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3630" for this suite. 01/29/23 03:20:43.58
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":104,"skipped":1878,"failed":0}
------------------------------
• [4.233 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:20:39.36
    Jan 29 03:20:39.360: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename container-runtime 01/29/23 03:20:39.361
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:20:39.392
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:20:39.397
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231
    STEP: create the container 01/29/23 03:20:39.404
    STEP: wait for the container to reach Succeeded 01/29/23 03:20:39.423
    STEP: get the container status 01/29/23 03:20:43.469
    STEP: the container should be terminated 01/29/23 03:20:43.476
    STEP: the termination message should be set 01/29/23 03:20:43.476
    Jan 29 03:20:43.476: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 01/29/23 03:20:43.476
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jan 29 03:20:43.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-3630" for this suite. 01/29/23 03:20:43.58
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:20:43.596
Jan 29 03:20:43.596: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename projected 01/29/23 03:20:43.598
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:20:43.626
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:20:43.632
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
STEP: Creating secret with name s-test-opt-del-1c3bdf07-364d-463d-90bf-4e305aac86ef 01/29/23 03:20:43.646
STEP: Creating secret with name s-test-opt-upd-1ca695bb-ea75-46e7-9cc1-56892acbca8e 01/29/23 03:20:43.653
STEP: Creating the pod 01/29/23 03:20:43.66
Jan 29 03:20:43.680: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ad039fb9-d59c-4882-acf7-7759b0d4679f" in namespace "projected-1461" to be "running and ready"
Jan 29 03:20:43.687: INFO: Pod "pod-projected-secrets-ad039fb9-d59c-4882-acf7-7759b0d4679f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.944728ms
Jan 29 03:20:43.687: INFO: The phase of Pod pod-projected-secrets-ad039fb9-d59c-4882-acf7-7759b0d4679f is Pending, waiting for it to be Running (with Ready = true)
Jan 29 03:20:45.695: INFO: Pod "pod-projected-secrets-ad039fb9-d59c-4882-acf7-7759b0d4679f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015105583s
Jan 29 03:20:45.695: INFO: The phase of Pod pod-projected-secrets-ad039fb9-d59c-4882-acf7-7759b0d4679f is Pending, waiting for it to be Running (with Ready = true)
Jan 29 03:20:47.695: INFO: Pod "pod-projected-secrets-ad039fb9-d59c-4882-acf7-7759b0d4679f": Phase="Running", Reason="", readiness=true. Elapsed: 4.014937778s
Jan 29 03:20:47.695: INFO: The phase of Pod pod-projected-secrets-ad039fb9-d59c-4882-acf7-7759b0d4679f is Running (Ready = true)
Jan 29 03:20:47.695: INFO: Pod "pod-projected-secrets-ad039fb9-d59c-4882-acf7-7759b0d4679f" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-1c3bdf07-364d-463d-90bf-4e305aac86ef 01/29/23 03:20:47.746
STEP: Updating secret s-test-opt-upd-1ca695bb-ea75-46e7-9cc1-56892acbca8e 01/29/23 03:20:47.756
STEP: Creating secret with name s-test-opt-create-fb58aa1f-a3b9-40f4-ac09-b6315d23b265 01/29/23 03:20:47.762
STEP: waiting to observe update in volume 01/29/23 03:20:47.769
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan 29 03:22:14.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1461" for this suite. 01/29/23 03:22:14.678
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":105,"skipped":1912,"failed":0}
------------------------------
• [SLOW TEST] [91.093 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:20:43.596
    Jan 29 03:20:43.596: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename projected 01/29/23 03:20:43.598
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:20:43.626
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:20:43.632
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:214
    STEP: Creating secret with name s-test-opt-del-1c3bdf07-364d-463d-90bf-4e305aac86ef 01/29/23 03:20:43.646
    STEP: Creating secret with name s-test-opt-upd-1ca695bb-ea75-46e7-9cc1-56892acbca8e 01/29/23 03:20:43.653
    STEP: Creating the pod 01/29/23 03:20:43.66
    Jan 29 03:20:43.680: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ad039fb9-d59c-4882-acf7-7759b0d4679f" in namespace "projected-1461" to be "running and ready"
    Jan 29 03:20:43.687: INFO: Pod "pod-projected-secrets-ad039fb9-d59c-4882-acf7-7759b0d4679f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.944728ms
    Jan 29 03:20:43.687: INFO: The phase of Pod pod-projected-secrets-ad039fb9-d59c-4882-acf7-7759b0d4679f is Pending, waiting for it to be Running (with Ready = true)
    Jan 29 03:20:45.695: INFO: Pod "pod-projected-secrets-ad039fb9-d59c-4882-acf7-7759b0d4679f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015105583s
    Jan 29 03:20:45.695: INFO: The phase of Pod pod-projected-secrets-ad039fb9-d59c-4882-acf7-7759b0d4679f is Pending, waiting for it to be Running (with Ready = true)
    Jan 29 03:20:47.695: INFO: Pod "pod-projected-secrets-ad039fb9-d59c-4882-acf7-7759b0d4679f": Phase="Running", Reason="", readiness=true. Elapsed: 4.014937778s
    Jan 29 03:20:47.695: INFO: The phase of Pod pod-projected-secrets-ad039fb9-d59c-4882-acf7-7759b0d4679f is Running (Ready = true)
    Jan 29 03:20:47.695: INFO: Pod "pod-projected-secrets-ad039fb9-d59c-4882-acf7-7759b0d4679f" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-1c3bdf07-364d-463d-90bf-4e305aac86ef 01/29/23 03:20:47.746
    STEP: Updating secret s-test-opt-upd-1ca695bb-ea75-46e7-9cc1-56892acbca8e 01/29/23 03:20:47.756
    STEP: Creating secret with name s-test-opt-create-fb58aa1f-a3b9-40f4-ac09-b6315d23b265 01/29/23 03:20:47.762
    STEP: waiting to observe update in volume 01/29/23 03:20:47.769
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan 29 03:22:14.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1461" for this suite. 01/29/23 03:22:14.678
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:22:14.691
Jan 29 03:22:14.691: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename sched-pred 01/29/23 03:22:14.693
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:22:14.733
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:22:14.739
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jan 29 03:22:14.752: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 29 03:22:14.771: INFO: Waiting for terminating namespaces to be deleted...
Jan 29 03:22:14.778: INFO: 
Logging pods the apiserver thinks is on node master1 before test
Jan 29 03:22:14.793: INFO: calico-kube-controllers-5c6d4b68d6-6p9cm from kube-system started at 2023-01-11 07:48:50 +0000 UTC (1 container statuses recorded)
Jan 29 03:22:14.793: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jan 29 03:22:14.793: INFO: calico-node-j4qnr from kube-system started at 2023-01-11 07:49:07 +0000 UTC (1 container statuses recorded)
Jan 29 03:22:14.793: INFO: 	Container calico-node ready: true, restart count 2
Jan 29 03:22:14.793: INFO: cke-admission-daemonset-g5mvj from kube-system started at 2023-01-11 07:51:31 +0000 UTC (1 container statuses recorded)
Jan 29 03:22:14.793: INFO: 	Container cke-admission ready: true, restart count 0
Jan 29 03:22:14.793: INFO: cke-controller-manager-master1 from kube-system started at 2023-01-11 07:49:37 +0000 UTC (1 container statuses recorded)
Jan 29 03:22:14.793: INFO: 	Container cke-controller-manager ready: true, restart count 0
Jan 29 03:22:14.793: INFO: component-controller-manager-master1 from kube-system started at 2023-01-11 07:49:18 +0000 UTC (1 container statuses recorded)
Jan 29 03:22:14.793: INFO: 	Container component-controller-manager ready: true, restart count 0
Jan 29 03:22:14.793: INFO: coredns-tntlt from kube-system started at 2023-01-11 07:49:05 +0000 UTC (1 container statuses recorded)
Jan 29 03:22:14.793: INFO: 	Container coredns ready: true, restart count 0
Jan 29 03:22:14.793: INFO: keepalived-master1 from kube-system started at 2023-01-11 07:48:19 +0000 UTC (1 container statuses recorded)
Jan 29 03:22:14.793: INFO: 	Container keepalived ready: true, restart count 0
Jan 29 03:22:14.793: INFO: kube-apiserver-master1 from kube-system started at 2023-01-11 07:48:12 +0000 UTC (1 container statuses recorded)
Jan 29 03:22:14.793: INFO: 	Container kube-apiserver ready: true, restart count 0
Jan 29 03:22:14.793: INFO: kube-controller-manager-master1 from kube-system started at 2023-01-11 07:48:20 +0000 UTC (1 container statuses recorded)
Jan 29 03:22:14.793: INFO: 	Container kube-controller-manager ready: true, restart count 5
Jan 29 03:22:14.793: INFO: kube-multus-ds-8b89r from kube-system started at 2023-01-11 07:51:31 +0000 UTC (1 container statuses recorded)
Jan 29 03:22:14.793: INFO: 	Container kube-multus ready: true, restart count 0
Jan 29 03:22:14.793: INFO: kube-proxy-master1 from kube-system started at 2023-01-11 07:48:15 +0000 UTC (1 container statuses recorded)
Jan 29 03:22:14.793: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 29 03:22:14.793: INFO: kube-scheduler-master1 from kube-system started at 2023-01-11 07:48:21 +0000 UTC (1 container statuses recorded)
Jan 29 03:22:14.793: INFO: 	Container kube-scheduler ready: true, restart count 9
Jan 29 03:22:14.793: INFO: nginx-proxy-master1 from kube-system started at 2023-01-11 07:48:26 +0000 UTC (2 container statuses recorded)
Jan 29 03:22:14.793: INFO: 	Container kube-apiserver-healthcheck ready: true, restart count 0
Jan 29 03:22:14.793: INFO: 	Container nginx-proxy ready: true, restart count 0
Jan 29 03:22:14.793: INFO: node-problem-detector-dgrmp from kube-system started at 2023-01-11 07:51:49 +0000 UTC (1 container statuses recorded)
Jan 29 03:22:14.793: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan 29 03:22:14.793: INFO: 
Logging pods the apiserver thinks is on node master2 before test
Jan 29 03:22:14.809: INFO: calico-node-sx6hm from kube-system started at 2023-01-11 07:49:07 +0000 UTC (1 container statuses recorded)
Jan 29 03:22:14.809: INFO: 	Container calico-node ready: true, restart count 2
Jan 29 03:22:14.809: INFO: cke-admission-daemonset-6v8vg from kube-system started at 2023-01-11 07:51:31 +0000 UTC (1 container statuses recorded)
Jan 29 03:22:14.809: INFO: 	Container cke-admission ready: true, restart count 0
Jan 29 03:22:14.809: INFO: cke-controller-manager-master2 from kube-system started at 2023-01-11 07:49:04 +0000 UTC (1 container statuses recorded)
Jan 29 03:22:14.809: INFO: 	Container cke-controller-manager ready: true, restart count 1
Jan 29 03:22:14.809: INFO: component-controller-manager-master2 from kube-system started at 2023-01-11 07:49:02 +0000 UTC (1 container statuses recorded)
Jan 29 03:22:14.809: INFO: 	Container component-controller-manager ready: true, restart count 1
Jan 29 03:22:14.809: INFO: coredns-m9lv7 from kube-system started at 2023-01-11 07:50:16 +0000 UTC (1 container statuses recorded)
Jan 29 03:22:14.809: INFO: 	Container coredns ready: true, restart count 0
Jan 29 03:22:14.809: INFO: keepalived-master2 from kube-system started at 2023-01-11 07:47:57 +0000 UTC (1 container statuses recorded)
Jan 29 03:22:14.809: INFO: 	Container keepalived ready: true, restart count 0
Jan 29 03:22:14.809: INFO: kube-apiserver-master2 from kube-system started at 2023-01-11 07:47:49 +0000 UTC (1 container statuses recorded)
Jan 29 03:22:14.809: INFO: 	Container kube-apiserver ready: true, restart count 0
Jan 29 03:22:14.809: INFO: kube-controller-manager-master2 from kube-system started at 2023-01-11 07:47:51 +0000 UTC (1 container statuses recorded)
Jan 29 03:22:14.809: INFO: 	Container kube-controller-manager ready: true, restart count 4
Jan 29 03:22:14.809: INFO: kube-multus-ds-qq7kz from kube-system started at 2023-01-11 07:51:31 +0000 UTC (1 container statuses recorded)
Jan 29 03:22:14.809: INFO: 	Container kube-multus ready: true, restart count 0
Jan 29 03:22:14.809: INFO: kube-proxy-master2 from kube-system started at 2023-01-11 07:47:55 +0000 UTC (1 container statuses recorded)
Jan 29 03:22:14.809: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 29 03:22:14.809: INFO: kube-scheduler-master2 from kube-system started at 2023-01-11 07:48:09 +0000 UTC (1 container statuses recorded)
Jan 29 03:22:14.809: INFO: 	Container kube-scheduler ready: true, restart count 9
Jan 29 03:22:14.809: INFO: nginx-proxy-master2 from kube-system started at 2023-01-11 07:48:01 +0000 UTC (2 container statuses recorded)
Jan 29 03:22:14.809: INFO: 	Container kube-apiserver-healthcheck ready: true, restart count 0
Jan 29 03:22:14.809: INFO: 	Container nginx-proxy ready: true, restart count 0
Jan 29 03:22:14.809: INFO: node-problem-detector-w69xv from kube-system started at 2023-01-11 07:51:49 +0000 UTC (1 container statuses recorded)
Jan 29 03:22:14.809: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan 29 03:22:14.809: INFO: 
Logging pods the apiserver thinks is on node master3 before test
Jan 29 03:22:14.827: INFO: calico-node-9b26s from kube-system started at 2023-01-11 07:49:07 +0000 UTC (1 container statuses recorded)
Jan 29 03:22:14.827: INFO: 	Container calico-node ready: true, restart count 0
Jan 29 03:22:14.827: INFO: cke-admission-daemonset-mqt4k from kube-system started at 2023-01-11 07:51:31 +0000 UTC (1 container statuses recorded)
Jan 29 03:22:14.827: INFO: 	Container cke-admission ready: true, restart count 0
Jan 29 03:22:14.827: INFO: cke-controller-manager-master3 from kube-system started at 2023-01-11 07:49:08 +0000 UTC (1 container statuses recorded)
Jan 29 03:22:14.827: INFO: 	Container cke-controller-manager ready: true, restart count 0
Jan 29 03:22:14.827: INFO: component-controller-manager-master3 from kube-system started at 2023-01-11 07:49:17 +0000 UTC (1 container statuses recorded)
Jan 29 03:22:14.827: INFO: 	Container component-controller-manager ready: true, restart count 0
Jan 29 03:22:14.827: INFO: coredns-hwvn8 from kube-system started at 2023-01-11 07:49:26 +0000 UTC (1 container statuses recorded)
Jan 29 03:22:14.827: INFO: 	Container coredns ready: true, restart count 0
Jan 29 03:22:14.827: INFO: keepalived-master3 from kube-system started at 2023-01-11 07:48:26 +0000 UTC (1 container statuses recorded)
Jan 29 03:22:14.827: INFO: 	Container keepalived ready: true, restart count 0
Jan 29 03:22:14.827: INFO: kube-apiserver-master3 from kube-system started at 2023-01-11 07:48:44 +0000 UTC (1 container statuses recorded)
Jan 29 03:22:14.827: INFO: 	Container kube-apiserver ready: true, restart count 0
Jan 29 03:22:14.827: INFO: kube-controller-manager-master3 from kube-system started at 2023-01-11 07:48:27 +0000 UTC (1 container statuses recorded)
Jan 29 03:22:14.827: INFO: 	Container kube-controller-manager ready: true, restart count 4
Jan 29 03:22:14.827: INFO: kube-multus-ds-wrhjf from kube-system started at 2023-01-11 07:51:31 +0000 UTC (1 container statuses recorded)
Jan 29 03:22:14.827: INFO: 	Container kube-multus ready: true, restart count 0
Jan 29 03:22:14.827: INFO: kube-proxy-master3 from kube-system started at 2023-01-11 07:48:37 +0000 UTC (1 container statuses recorded)
Jan 29 03:22:14.827: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 29 03:22:14.827: INFO: kube-scheduler-master3 from kube-system started at 2023-01-11 07:48:30 +0000 UTC (1 container statuses recorded)
Jan 29 03:22:14.827: INFO: 	Container kube-scheduler ready: true, restart count 5
Jan 29 03:22:14.827: INFO: metrics-server-5584f68fbf-l689z from kube-system started at 2023-01-11 07:50:55 +0000 UTC (1 container statuses recorded)
Jan 29 03:22:14.827: INFO: 	Container metrics-server ready: true, restart count 0
Jan 29 03:22:14.827: INFO: nginx-proxy-master3 from kube-system started at 2023-01-11 07:48:32 +0000 UTC (2 container statuses recorded)
Jan 29 03:22:14.827: INFO: 	Container kube-apiserver-healthcheck ready: true, restart count 0
Jan 29 03:22:14.827: INFO: 	Container nginx-proxy ready: true, restart count 0
Jan 29 03:22:14.827: INFO: node-problem-detector-9987m from kube-system started at 2023-01-11 07:51:49 +0000 UTC (1 container statuses recorded)
Jan 29 03:22:14.827: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan 29 03:22:14.827: INFO: 
Logging pods the apiserver thinks is on node slave1 before test
Jan 29 03:22:14.842: INFO: calico-node-scr7m from kube-system started at 2023-01-11 07:49:07 +0000 UTC (1 container statuses recorded)
Jan 29 03:22:14.842: INFO: 	Container calico-node ready: true, restart count 0
Jan 29 03:22:14.842: INFO: kube-multus-ds-ng7xc from kube-system started at 2023-01-29 02:51:08 +0000 UTC (1 container statuses recorded)
Jan 29 03:22:14.842: INFO: 	Container kube-multus ready: true, restart count 0
Jan 29 03:22:14.842: INFO: kube-proxy-slave1 from kube-system started at 2023-01-11 07:48:42 +0000 UTC (1 container statuses recorded)
Jan 29 03:22:14.842: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 29 03:22:14.842: INFO: node-problem-detector-8bg82 from kube-system started at 2023-01-11 07:51:49 +0000 UTC (1 container statuses recorded)
Jan 29 03:22:14.842: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan 29 03:22:14.842: INFO: sonobuoy from sonobuoy started at 2023-01-29 02:56:33 +0000 UTC (1 container statuses recorded)
Jan 29 03:22:14.842: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 29 03:22:14.842: INFO: sonobuoy-e2e-job-3f32079945944ddf from sonobuoy started at 2023-01-29 02:56:35 +0000 UTC (2 container statuses recorded)
Jan 29 03:22:14.842: INFO: 	Container e2e ready: true, restart count 0
Jan 29 03:22:14.842: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 29 03:22:14.842: INFO: 
Logging pods the apiserver thinks is on node slave2 before test
Jan 29 03:22:14.857: INFO: calico-node-qhb5r from kube-system started at 2023-01-11 07:58:46 +0000 UTC (1 container statuses recorded)
Jan 29 03:22:14.857: INFO: 	Container calico-node ready: true, restart count 2
Jan 29 03:22:14.857: INFO: kube-multus-ds-8gtzz from kube-system started at 2023-01-29 02:43:13 +0000 UTC (1 container statuses recorded)
Jan 29 03:22:14.857: INFO: 	Container kube-multus ready: true, restart count 0
Jan 29 03:22:14.857: INFO: kube-proxy-slave2 from kube-system started at 2023-01-11 07:58:39 +0000 UTC (1 container statuses recorded)
Jan 29 03:22:14.857: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 29 03:22:14.857: INFO: node-problem-detector-m8cck from kube-system started at 2023-01-11 07:58:46 +0000 UTC (1 container statuses recorded)
Jan 29 03:22:14.857: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan 29 03:22:14.857: INFO: pod-projected-secrets-ad039fb9-d59c-4882-acf7-7759b0d4679f from projected-1461 started at 2023-01-29 03:20:43 +0000 UTC (3 container statuses recorded)
Jan 29 03:22:14.857: INFO: 	Container creates-volume-test ready: true, restart count 0
Jan 29 03:22:14.857: INFO: 	Container dels-volume-test ready: true, restart count 0
Jan 29 03:22:14.857: INFO: 	Container upds-volume-test ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
STEP: Trying to launch a pod without a label to get a node which can launch it. 01/29/23 03:22:14.857
Jan 29 03:22:14.888: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-3171" to be "running"
Jan 29 03:22:14.896: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 7.554533ms
Jan 29 03:22:16.903: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.015227783s
Jan 29 03:22:16.904: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 01/29/23 03:22:16.91
STEP: Trying to apply a random label on the found node. 01/29/23 03:22:17.007
STEP: verifying the node has the label kubernetes.io/e2e-f4a7f539-f1d4-420d-901c-5f29e1cddcd7 42 01/29/23 03:22:17.019
STEP: Trying to relaunch the pod, now with labels. 01/29/23 03:22:17.026
Jan 29 03:22:17.049: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-3171" to be "not pending"
Jan 29 03:22:17.056: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 6.068602ms
Jan 29 03:22:19.064: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.014301675s
Jan 29 03:22:19.064: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-f4a7f539-f1d4-420d-901c-5f29e1cddcd7 off the node slave2 01/29/23 03:22:19.07
STEP: verifying the node doesn't have the label kubernetes.io/e2e-f4a7f539-f1d4-420d-901c-5f29e1cddcd7 01/29/23 03:22:19.088
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Jan 29 03:22:19.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3171" for this suite. 01/29/23 03:22:19.107
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","completed":106,"skipped":1930,"failed":0}
------------------------------
• [4.428 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:22:14.691
    Jan 29 03:22:14.691: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename sched-pred 01/29/23 03:22:14.693
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:22:14.733
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:22:14.739
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Jan 29 03:22:14.752: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan 29 03:22:14.771: INFO: Waiting for terminating namespaces to be deleted...
    Jan 29 03:22:14.778: INFO: 
    Logging pods the apiserver thinks is on node master1 before test
    Jan 29 03:22:14.793: INFO: calico-kube-controllers-5c6d4b68d6-6p9cm from kube-system started at 2023-01-11 07:48:50 +0000 UTC (1 container statuses recorded)
    Jan 29 03:22:14.793: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Jan 29 03:22:14.793: INFO: calico-node-j4qnr from kube-system started at 2023-01-11 07:49:07 +0000 UTC (1 container statuses recorded)
    Jan 29 03:22:14.793: INFO: 	Container calico-node ready: true, restart count 2
    Jan 29 03:22:14.793: INFO: cke-admission-daemonset-g5mvj from kube-system started at 2023-01-11 07:51:31 +0000 UTC (1 container statuses recorded)
    Jan 29 03:22:14.793: INFO: 	Container cke-admission ready: true, restart count 0
    Jan 29 03:22:14.793: INFO: cke-controller-manager-master1 from kube-system started at 2023-01-11 07:49:37 +0000 UTC (1 container statuses recorded)
    Jan 29 03:22:14.793: INFO: 	Container cke-controller-manager ready: true, restart count 0
    Jan 29 03:22:14.793: INFO: component-controller-manager-master1 from kube-system started at 2023-01-11 07:49:18 +0000 UTC (1 container statuses recorded)
    Jan 29 03:22:14.793: INFO: 	Container component-controller-manager ready: true, restart count 0
    Jan 29 03:22:14.793: INFO: coredns-tntlt from kube-system started at 2023-01-11 07:49:05 +0000 UTC (1 container statuses recorded)
    Jan 29 03:22:14.793: INFO: 	Container coredns ready: true, restart count 0
    Jan 29 03:22:14.793: INFO: keepalived-master1 from kube-system started at 2023-01-11 07:48:19 +0000 UTC (1 container statuses recorded)
    Jan 29 03:22:14.793: INFO: 	Container keepalived ready: true, restart count 0
    Jan 29 03:22:14.793: INFO: kube-apiserver-master1 from kube-system started at 2023-01-11 07:48:12 +0000 UTC (1 container statuses recorded)
    Jan 29 03:22:14.793: INFO: 	Container kube-apiserver ready: true, restart count 0
    Jan 29 03:22:14.793: INFO: kube-controller-manager-master1 from kube-system started at 2023-01-11 07:48:20 +0000 UTC (1 container statuses recorded)
    Jan 29 03:22:14.793: INFO: 	Container kube-controller-manager ready: true, restart count 5
    Jan 29 03:22:14.793: INFO: kube-multus-ds-8b89r from kube-system started at 2023-01-11 07:51:31 +0000 UTC (1 container statuses recorded)
    Jan 29 03:22:14.793: INFO: 	Container kube-multus ready: true, restart count 0
    Jan 29 03:22:14.793: INFO: kube-proxy-master1 from kube-system started at 2023-01-11 07:48:15 +0000 UTC (1 container statuses recorded)
    Jan 29 03:22:14.793: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan 29 03:22:14.793: INFO: kube-scheduler-master1 from kube-system started at 2023-01-11 07:48:21 +0000 UTC (1 container statuses recorded)
    Jan 29 03:22:14.793: INFO: 	Container kube-scheduler ready: true, restart count 9
    Jan 29 03:22:14.793: INFO: nginx-proxy-master1 from kube-system started at 2023-01-11 07:48:26 +0000 UTC (2 container statuses recorded)
    Jan 29 03:22:14.793: INFO: 	Container kube-apiserver-healthcheck ready: true, restart count 0
    Jan 29 03:22:14.793: INFO: 	Container nginx-proxy ready: true, restart count 0
    Jan 29 03:22:14.793: INFO: node-problem-detector-dgrmp from kube-system started at 2023-01-11 07:51:49 +0000 UTC (1 container statuses recorded)
    Jan 29 03:22:14.793: INFO: 	Container node-problem-detector ready: true, restart count 0
    Jan 29 03:22:14.793: INFO: 
    Logging pods the apiserver thinks is on node master2 before test
    Jan 29 03:22:14.809: INFO: calico-node-sx6hm from kube-system started at 2023-01-11 07:49:07 +0000 UTC (1 container statuses recorded)
    Jan 29 03:22:14.809: INFO: 	Container calico-node ready: true, restart count 2
    Jan 29 03:22:14.809: INFO: cke-admission-daemonset-6v8vg from kube-system started at 2023-01-11 07:51:31 +0000 UTC (1 container statuses recorded)
    Jan 29 03:22:14.809: INFO: 	Container cke-admission ready: true, restart count 0
    Jan 29 03:22:14.809: INFO: cke-controller-manager-master2 from kube-system started at 2023-01-11 07:49:04 +0000 UTC (1 container statuses recorded)
    Jan 29 03:22:14.809: INFO: 	Container cke-controller-manager ready: true, restart count 1
    Jan 29 03:22:14.809: INFO: component-controller-manager-master2 from kube-system started at 2023-01-11 07:49:02 +0000 UTC (1 container statuses recorded)
    Jan 29 03:22:14.809: INFO: 	Container component-controller-manager ready: true, restart count 1
    Jan 29 03:22:14.809: INFO: coredns-m9lv7 from kube-system started at 2023-01-11 07:50:16 +0000 UTC (1 container statuses recorded)
    Jan 29 03:22:14.809: INFO: 	Container coredns ready: true, restart count 0
    Jan 29 03:22:14.809: INFO: keepalived-master2 from kube-system started at 2023-01-11 07:47:57 +0000 UTC (1 container statuses recorded)
    Jan 29 03:22:14.809: INFO: 	Container keepalived ready: true, restart count 0
    Jan 29 03:22:14.809: INFO: kube-apiserver-master2 from kube-system started at 2023-01-11 07:47:49 +0000 UTC (1 container statuses recorded)
    Jan 29 03:22:14.809: INFO: 	Container kube-apiserver ready: true, restart count 0
    Jan 29 03:22:14.809: INFO: kube-controller-manager-master2 from kube-system started at 2023-01-11 07:47:51 +0000 UTC (1 container statuses recorded)
    Jan 29 03:22:14.809: INFO: 	Container kube-controller-manager ready: true, restart count 4
    Jan 29 03:22:14.809: INFO: kube-multus-ds-qq7kz from kube-system started at 2023-01-11 07:51:31 +0000 UTC (1 container statuses recorded)
    Jan 29 03:22:14.809: INFO: 	Container kube-multus ready: true, restart count 0
    Jan 29 03:22:14.809: INFO: kube-proxy-master2 from kube-system started at 2023-01-11 07:47:55 +0000 UTC (1 container statuses recorded)
    Jan 29 03:22:14.809: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan 29 03:22:14.809: INFO: kube-scheduler-master2 from kube-system started at 2023-01-11 07:48:09 +0000 UTC (1 container statuses recorded)
    Jan 29 03:22:14.809: INFO: 	Container kube-scheduler ready: true, restart count 9
    Jan 29 03:22:14.809: INFO: nginx-proxy-master2 from kube-system started at 2023-01-11 07:48:01 +0000 UTC (2 container statuses recorded)
    Jan 29 03:22:14.809: INFO: 	Container kube-apiserver-healthcheck ready: true, restart count 0
    Jan 29 03:22:14.809: INFO: 	Container nginx-proxy ready: true, restart count 0
    Jan 29 03:22:14.809: INFO: node-problem-detector-w69xv from kube-system started at 2023-01-11 07:51:49 +0000 UTC (1 container statuses recorded)
    Jan 29 03:22:14.809: INFO: 	Container node-problem-detector ready: true, restart count 0
    Jan 29 03:22:14.809: INFO: 
    Logging pods the apiserver thinks is on node master3 before test
    Jan 29 03:22:14.827: INFO: calico-node-9b26s from kube-system started at 2023-01-11 07:49:07 +0000 UTC (1 container statuses recorded)
    Jan 29 03:22:14.827: INFO: 	Container calico-node ready: true, restart count 0
    Jan 29 03:22:14.827: INFO: cke-admission-daemonset-mqt4k from kube-system started at 2023-01-11 07:51:31 +0000 UTC (1 container statuses recorded)
    Jan 29 03:22:14.827: INFO: 	Container cke-admission ready: true, restart count 0
    Jan 29 03:22:14.827: INFO: cke-controller-manager-master3 from kube-system started at 2023-01-11 07:49:08 +0000 UTC (1 container statuses recorded)
    Jan 29 03:22:14.827: INFO: 	Container cke-controller-manager ready: true, restart count 0
    Jan 29 03:22:14.827: INFO: component-controller-manager-master3 from kube-system started at 2023-01-11 07:49:17 +0000 UTC (1 container statuses recorded)
    Jan 29 03:22:14.827: INFO: 	Container component-controller-manager ready: true, restart count 0
    Jan 29 03:22:14.827: INFO: coredns-hwvn8 from kube-system started at 2023-01-11 07:49:26 +0000 UTC (1 container statuses recorded)
    Jan 29 03:22:14.827: INFO: 	Container coredns ready: true, restart count 0
    Jan 29 03:22:14.827: INFO: keepalived-master3 from kube-system started at 2023-01-11 07:48:26 +0000 UTC (1 container statuses recorded)
    Jan 29 03:22:14.827: INFO: 	Container keepalived ready: true, restart count 0
    Jan 29 03:22:14.827: INFO: kube-apiserver-master3 from kube-system started at 2023-01-11 07:48:44 +0000 UTC (1 container statuses recorded)
    Jan 29 03:22:14.827: INFO: 	Container kube-apiserver ready: true, restart count 0
    Jan 29 03:22:14.827: INFO: kube-controller-manager-master3 from kube-system started at 2023-01-11 07:48:27 +0000 UTC (1 container statuses recorded)
    Jan 29 03:22:14.827: INFO: 	Container kube-controller-manager ready: true, restart count 4
    Jan 29 03:22:14.827: INFO: kube-multus-ds-wrhjf from kube-system started at 2023-01-11 07:51:31 +0000 UTC (1 container statuses recorded)
    Jan 29 03:22:14.827: INFO: 	Container kube-multus ready: true, restart count 0
    Jan 29 03:22:14.827: INFO: kube-proxy-master3 from kube-system started at 2023-01-11 07:48:37 +0000 UTC (1 container statuses recorded)
    Jan 29 03:22:14.827: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan 29 03:22:14.827: INFO: kube-scheduler-master3 from kube-system started at 2023-01-11 07:48:30 +0000 UTC (1 container statuses recorded)
    Jan 29 03:22:14.827: INFO: 	Container kube-scheduler ready: true, restart count 5
    Jan 29 03:22:14.827: INFO: metrics-server-5584f68fbf-l689z from kube-system started at 2023-01-11 07:50:55 +0000 UTC (1 container statuses recorded)
    Jan 29 03:22:14.827: INFO: 	Container metrics-server ready: true, restart count 0
    Jan 29 03:22:14.827: INFO: nginx-proxy-master3 from kube-system started at 2023-01-11 07:48:32 +0000 UTC (2 container statuses recorded)
    Jan 29 03:22:14.827: INFO: 	Container kube-apiserver-healthcheck ready: true, restart count 0
    Jan 29 03:22:14.827: INFO: 	Container nginx-proxy ready: true, restart count 0
    Jan 29 03:22:14.827: INFO: node-problem-detector-9987m from kube-system started at 2023-01-11 07:51:49 +0000 UTC (1 container statuses recorded)
    Jan 29 03:22:14.827: INFO: 	Container node-problem-detector ready: true, restart count 0
    Jan 29 03:22:14.827: INFO: 
    Logging pods the apiserver thinks is on node slave1 before test
    Jan 29 03:22:14.842: INFO: calico-node-scr7m from kube-system started at 2023-01-11 07:49:07 +0000 UTC (1 container statuses recorded)
    Jan 29 03:22:14.842: INFO: 	Container calico-node ready: true, restart count 0
    Jan 29 03:22:14.842: INFO: kube-multus-ds-ng7xc from kube-system started at 2023-01-29 02:51:08 +0000 UTC (1 container statuses recorded)
    Jan 29 03:22:14.842: INFO: 	Container kube-multus ready: true, restart count 0
    Jan 29 03:22:14.842: INFO: kube-proxy-slave1 from kube-system started at 2023-01-11 07:48:42 +0000 UTC (1 container statuses recorded)
    Jan 29 03:22:14.842: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan 29 03:22:14.842: INFO: node-problem-detector-8bg82 from kube-system started at 2023-01-11 07:51:49 +0000 UTC (1 container statuses recorded)
    Jan 29 03:22:14.842: INFO: 	Container node-problem-detector ready: true, restart count 0
    Jan 29 03:22:14.842: INFO: sonobuoy from sonobuoy started at 2023-01-29 02:56:33 +0000 UTC (1 container statuses recorded)
    Jan 29 03:22:14.842: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan 29 03:22:14.842: INFO: sonobuoy-e2e-job-3f32079945944ddf from sonobuoy started at 2023-01-29 02:56:35 +0000 UTC (2 container statuses recorded)
    Jan 29 03:22:14.842: INFO: 	Container e2e ready: true, restart count 0
    Jan 29 03:22:14.842: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 29 03:22:14.842: INFO: 
    Logging pods the apiserver thinks is on node slave2 before test
    Jan 29 03:22:14.857: INFO: calico-node-qhb5r from kube-system started at 2023-01-11 07:58:46 +0000 UTC (1 container statuses recorded)
    Jan 29 03:22:14.857: INFO: 	Container calico-node ready: true, restart count 2
    Jan 29 03:22:14.857: INFO: kube-multus-ds-8gtzz from kube-system started at 2023-01-29 02:43:13 +0000 UTC (1 container statuses recorded)
    Jan 29 03:22:14.857: INFO: 	Container kube-multus ready: true, restart count 0
    Jan 29 03:22:14.857: INFO: kube-proxy-slave2 from kube-system started at 2023-01-11 07:58:39 +0000 UTC (1 container statuses recorded)
    Jan 29 03:22:14.857: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan 29 03:22:14.857: INFO: node-problem-detector-m8cck from kube-system started at 2023-01-11 07:58:46 +0000 UTC (1 container statuses recorded)
    Jan 29 03:22:14.857: INFO: 	Container node-problem-detector ready: true, restart count 0
    Jan 29 03:22:14.857: INFO: pod-projected-secrets-ad039fb9-d59c-4882-acf7-7759b0d4679f from projected-1461 started at 2023-01-29 03:20:43 +0000 UTC (3 container statuses recorded)
    Jan 29 03:22:14.857: INFO: 	Container creates-volume-test ready: true, restart count 0
    Jan 29 03:22:14.857: INFO: 	Container dels-volume-test ready: true, restart count 0
    Jan 29 03:22:14.857: INFO: 	Container upds-volume-test ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:461
    STEP: Trying to launch a pod without a label to get a node which can launch it. 01/29/23 03:22:14.857
    Jan 29 03:22:14.888: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-3171" to be "running"
    Jan 29 03:22:14.896: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 7.554533ms
    Jan 29 03:22:16.903: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.015227783s
    Jan 29 03:22:16.904: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 01/29/23 03:22:16.91
    STEP: Trying to apply a random label on the found node. 01/29/23 03:22:17.007
    STEP: verifying the node has the label kubernetes.io/e2e-f4a7f539-f1d4-420d-901c-5f29e1cddcd7 42 01/29/23 03:22:17.019
    STEP: Trying to relaunch the pod, now with labels. 01/29/23 03:22:17.026
    Jan 29 03:22:17.049: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-3171" to be "not pending"
    Jan 29 03:22:17.056: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 6.068602ms
    Jan 29 03:22:19.064: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.014301675s
    Jan 29 03:22:19.064: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-f4a7f539-f1d4-420d-901c-5f29e1cddcd7 off the node slave2 01/29/23 03:22:19.07
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-f4a7f539-f1d4-420d-901c-5f29e1cddcd7 01/29/23 03:22:19.088
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Jan 29 03:22:19.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-3171" for this suite. 01/29/23 03:22:19.107
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:22:19.122
Jan 29 03:22:19.122: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename custom-resource-definition 01/29/23 03:22:19.124
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:22:19.165
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:22:19.17
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 01/29/23 03:22:19.179
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 01/29/23 03:22:19.182
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 01/29/23 03:22:19.182
STEP: fetching the /apis/apiextensions.k8s.io discovery document 01/29/23 03:22:19.182
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 01/29/23 03:22:19.184
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 01/29/23 03:22:19.184
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 01/29/23 03:22:19.186
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 29 03:22:19.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7713" for this suite. 01/29/23 03:22:19.196
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","completed":107,"skipped":1977,"failed":0}
------------------------------
• [0.093 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:22:19.122
    Jan 29 03:22:19.122: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename custom-resource-definition 01/29/23 03:22:19.124
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:22:19.165
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:22:19.17
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 01/29/23 03:22:19.179
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 01/29/23 03:22:19.182
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 01/29/23 03:22:19.182
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 01/29/23 03:22:19.182
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 01/29/23 03:22:19.184
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 01/29/23 03:22:19.184
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 01/29/23 03:22:19.186
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 29 03:22:19.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-7713" for this suite. 01/29/23 03:22:19.196
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:22:19.219
Jan 29 03:22:19.219: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename container-lifecycle-hook 01/29/23 03:22:19.221
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:22:19.261
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:22:19.268
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 01/29/23 03:22:19.283
Jan 29 03:22:19.303: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-7735" to be "running and ready"
Jan 29 03:22:19.311: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 8.179377ms
Jan 29 03:22:19.311: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 29 03:22:21.320: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017568479s
Jan 29 03:22:21.321: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 29 03:22:23.319: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.016412666s
Jan 29 03:22:23.320: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan 29 03:22:23.320: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
STEP: create the pod with lifecycle hook 01/29/23 03:22:23.327
Jan 29 03:22:23.340: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-7735" to be "running and ready"
Jan 29 03:22:23.347: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 7.13043ms
Jan 29 03:22:23.348: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 29 03:22:25.354: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013785552s
Jan 29 03:22:25.354: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 29 03:22:27.355: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.014276171s
Jan 29 03:22:27.355: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Jan 29 03:22:27.355: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 01/29/23 03:22:27.361
Jan 29 03:22:27.401: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 29 03:22:27.415: INFO: Pod pod-with-prestop-http-hook still exists
Jan 29 03:22:29.415: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 29 03:22:29.422: INFO: Pod pod-with-prestop-http-hook still exists
Jan 29 03:22:31.416: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 29 03:22:31.422: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 01/29/23 03:22:31.422
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Jan 29 03:22:31.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7735" for this suite. 01/29/23 03:22:31.467
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","completed":108,"skipped":2011,"failed":0}
------------------------------
• [SLOW TEST] [12.261 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:22:19.219
    Jan 29 03:22:19.219: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/29/23 03:22:19.221
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:22:19.261
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:22:19.268
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 01/29/23 03:22:19.283
    Jan 29 03:22:19.303: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-7735" to be "running and ready"
    Jan 29 03:22:19.311: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 8.179377ms
    Jan 29 03:22:19.311: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 29 03:22:21.320: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017568479s
    Jan 29 03:22:21.321: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 29 03:22:23.319: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.016412666s
    Jan 29 03:22:23.320: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan 29 03:22:23.320: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:152
    STEP: create the pod with lifecycle hook 01/29/23 03:22:23.327
    Jan 29 03:22:23.340: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-7735" to be "running and ready"
    Jan 29 03:22:23.347: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 7.13043ms
    Jan 29 03:22:23.348: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 29 03:22:25.354: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013785552s
    Jan 29 03:22:25.354: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 29 03:22:27.355: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.014276171s
    Jan 29 03:22:27.355: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Jan 29 03:22:27.355: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 01/29/23 03:22:27.361
    Jan 29 03:22:27.401: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jan 29 03:22:27.415: INFO: Pod pod-with-prestop-http-hook still exists
    Jan 29 03:22:29.415: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jan 29 03:22:29.422: INFO: Pod pod-with-prestop-http-hook still exists
    Jan 29 03:22:31.416: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jan 29 03:22:31.422: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 01/29/23 03:22:31.422
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Jan 29 03:22:31.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-7735" for this suite. 01/29/23 03:22:31.467
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:22:31.481
Jan 29 03:22:31.481: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename configmap 01/29/23 03:22:31.483
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:22:31.511
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:22:31.516
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
STEP: Creating configMap with name configmap-test-upd-19c8939d-6a14-4dd2-9b82-0d68cd7e6bf6 01/29/23 03:22:31.531
STEP: Creating the pod 01/29/23 03:22:31.538
Jan 29 03:22:31.558: INFO: Waiting up to 5m0s for pod "pod-configmaps-cf96fc2c-8289-414f-9f78-e5ce8ddd88c5" in namespace "configmap-2961" to be "running"
Jan 29 03:22:31.564: INFO: Pod "pod-configmaps-cf96fc2c-8289-414f-9f78-e5ce8ddd88c5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.587879ms
Jan 29 03:22:33.572: INFO: Pod "pod-configmaps-cf96fc2c-8289-414f-9f78-e5ce8ddd88c5": Phase="Running", Reason="", readiness=false. Elapsed: 2.014311735s
Jan 29 03:22:33.572: INFO: Pod "pod-configmaps-cf96fc2c-8289-414f-9f78-e5ce8ddd88c5" satisfied condition "running"
STEP: Waiting for pod with text data 01/29/23 03:22:33.572
STEP: Waiting for pod with binary data 01/29/23 03:22:33.59
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 29 03:22:33.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2961" for this suite. 01/29/23 03:22:33.613
{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","completed":109,"skipped":2015,"failed":0}
------------------------------
• [2.143 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:22:31.481
    Jan 29 03:22:31.481: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename configmap 01/29/23 03:22:31.483
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:22:31.511
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:22:31.516
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:174
    STEP: Creating configMap with name configmap-test-upd-19c8939d-6a14-4dd2-9b82-0d68cd7e6bf6 01/29/23 03:22:31.531
    STEP: Creating the pod 01/29/23 03:22:31.538
    Jan 29 03:22:31.558: INFO: Waiting up to 5m0s for pod "pod-configmaps-cf96fc2c-8289-414f-9f78-e5ce8ddd88c5" in namespace "configmap-2961" to be "running"
    Jan 29 03:22:31.564: INFO: Pod "pod-configmaps-cf96fc2c-8289-414f-9f78-e5ce8ddd88c5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.587879ms
    Jan 29 03:22:33.572: INFO: Pod "pod-configmaps-cf96fc2c-8289-414f-9f78-e5ce8ddd88c5": Phase="Running", Reason="", readiness=false. Elapsed: 2.014311735s
    Jan 29 03:22:33.572: INFO: Pod "pod-configmaps-cf96fc2c-8289-414f-9f78-e5ce8ddd88c5" satisfied condition "running"
    STEP: Waiting for pod with text data 01/29/23 03:22:33.572
    STEP: Waiting for pod with binary data 01/29/23 03:22:33.59
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 29 03:22:33.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-2961" for this suite. 01/29/23 03:22:33.613
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:22:33.625
Jan 29 03:22:33.626: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename var-expansion 01/29/23 03:22:33.627
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:22:33.658
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:22:33.663
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
STEP: Creating a pod to test env composition 01/29/23 03:22:33.669
Jan 29 03:22:33.690: INFO: Waiting up to 5m0s for pod "var-expansion-57ecf6fb-4782-47f3-866a-a3eaae6269b4" in namespace "var-expansion-6416" to be "Succeeded or Failed"
Jan 29 03:22:33.698: INFO: Pod "var-expansion-57ecf6fb-4782-47f3-866a-a3eaae6269b4": Phase="Pending", Reason="", readiness=false. Elapsed: 7.376271ms
Jan 29 03:22:35.706: INFO: Pod "var-expansion-57ecf6fb-4782-47f3-866a-a3eaae6269b4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0150241s
Jan 29 03:22:37.708: INFO: Pod "var-expansion-57ecf6fb-4782-47f3-866a-a3eaae6269b4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017685575s
STEP: Saw pod success 01/29/23 03:22:37.708
Jan 29 03:22:37.708: INFO: Pod "var-expansion-57ecf6fb-4782-47f3-866a-a3eaae6269b4" satisfied condition "Succeeded or Failed"
Jan 29 03:22:37.716: INFO: Trying to get logs from node slave2 pod var-expansion-57ecf6fb-4782-47f3-866a-a3eaae6269b4 container dapi-container: <nil>
STEP: delete the pod 01/29/23 03:22:37.733
Jan 29 03:22:37.833: INFO: Waiting for pod var-expansion-57ecf6fb-4782-47f3-866a-a3eaae6269b4 to disappear
Jan 29 03:22:37.838: INFO: Pod var-expansion-57ecf6fb-4782-47f3-866a-a3eaae6269b4 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan 29 03:22:37.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6416" for this suite. 01/29/23 03:22:37.85
{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","completed":110,"skipped":2029,"failed":0}
------------------------------
• [4.238 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:22:33.625
    Jan 29 03:22:33.626: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename var-expansion 01/29/23 03:22:33.627
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:22:33.658
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:22:33.663
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:43
    STEP: Creating a pod to test env composition 01/29/23 03:22:33.669
    Jan 29 03:22:33.690: INFO: Waiting up to 5m0s for pod "var-expansion-57ecf6fb-4782-47f3-866a-a3eaae6269b4" in namespace "var-expansion-6416" to be "Succeeded or Failed"
    Jan 29 03:22:33.698: INFO: Pod "var-expansion-57ecf6fb-4782-47f3-866a-a3eaae6269b4": Phase="Pending", Reason="", readiness=false. Elapsed: 7.376271ms
    Jan 29 03:22:35.706: INFO: Pod "var-expansion-57ecf6fb-4782-47f3-866a-a3eaae6269b4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0150241s
    Jan 29 03:22:37.708: INFO: Pod "var-expansion-57ecf6fb-4782-47f3-866a-a3eaae6269b4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017685575s
    STEP: Saw pod success 01/29/23 03:22:37.708
    Jan 29 03:22:37.708: INFO: Pod "var-expansion-57ecf6fb-4782-47f3-866a-a3eaae6269b4" satisfied condition "Succeeded or Failed"
    Jan 29 03:22:37.716: INFO: Trying to get logs from node slave2 pod var-expansion-57ecf6fb-4782-47f3-866a-a3eaae6269b4 container dapi-container: <nil>
    STEP: delete the pod 01/29/23 03:22:37.733
    Jan 29 03:22:37.833: INFO: Waiting for pod var-expansion-57ecf6fb-4782-47f3-866a-a3eaae6269b4 to disappear
    Jan 29 03:22:37.838: INFO: Pod var-expansion-57ecf6fb-4782-47f3-866a-a3eaae6269b4 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan 29 03:22:37.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-6416" for this suite. 01/29/23 03:22:37.85
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:22:37.866
Jan 29 03:22:37.866: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename projected 01/29/23 03:22:37.868
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:22:37.898
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:22:37.904
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
STEP: Creating secret with name projected-secret-test-446af4e2-199e-4cc1-aec8-2b6cfe63f866 01/29/23 03:22:37.91
STEP: Creating a pod to test consume secrets 01/29/23 03:22:37.918
Jan 29 03:22:37.936: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4aea0424-1f0a-4519-9d8f-b05bd574ab18" in namespace "projected-8839" to be "Succeeded or Failed"
Jan 29 03:22:37.942: INFO: Pod "pod-projected-secrets-4aea0424-1f0a-4519-9d8f-b05bd574ab18": Phase="Pending", Reason="", readiness=false. Elapsed: 6.129603ms
Jan 29 03:22:39.951: INFO: Pod "pod-projected-secrets-4aea0424-1f0a-4519-9d8f-b05bd574ab18": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01490362s
Jan 29 03:22:41.952: INFO: Pod "pod-projected-secrets-4aea0424-1f0a-4519-9d8f-b05bd574ab18": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015721121s
STEP: Saw pod success 01/29/23 03:22:41.952
Jan 29 03:22:41.952: INFO: Pod "pod-projected-secrets-4aea0424-1f0a-4519-9d8f-b05bd574ab18" satisfied condition "Succeeded or Failed"
Jan 29 03:22:41.965: INFO: Trying to get logs from node slave2 pod pod-projected-secrets-4aea0424-1f0a-4519-9d8f-b05bd574ab18 container secret-volume-test: <nil>
STEP: delete the pod 01/29/23 03:22:41.986
Jan 29 03:22:42.105: INFO: Waiting for pod pod-projected-secrets-4aea0424-1f0a-4519-9d8f-b05bd574ab18 to disappear
Jan 29 03:22:42.110: INFO: Pod pod-projected-secrets-4aea0424-1f0a-4519-9d8f-b05bd574ab18 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan 29 03:22:42.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8839" for this suite. 01/29/23 03:22:42.119
{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":111,"skipped":2042,"failed":0}
------------------------------
• [4.265 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:22:37.866
    Jan 29 03:22:37.866: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename projected 01/29/23 03:22:37.868
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:22:37.898
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:22:37.904
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:118
    STEP: Creating secret with name projected-secret-test-446af4e2-199e-4cc1-aec8-2b6cfe63f866 01/29/23 03:22:37.91
    STEP: Creating a pod to test consume secrets 01/29/23 03:22:37.918
    Jan 29 03:22:37.936: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4aea0424-1f0a-4519-9d8f-b05bd574ab18" in namespace "projected-8839" to be "Succeeded or Failed"
    Jan 29 03:22:37.942: INFO: Pod "pod-projected-secrets-4aea0424-1f0a-4519-9d8f-b05bd574ab18": Phase="Pending", Reason="", readiness=false. Elapsed: 6.129603ms
    Jan 29 03:22:39.951: INFO: Pod "pod-projected-secrets-4aea0424-1f0a-4519-9d8f-b05bd574ab18": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01490362s
    Jan 29 03:22:41.952: INFO: Pod "pod-projected-secrets-4aea0424-1f0a-4519-9d8f-b05bd574ab18": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015721121s
    STEP: Saw pod success 01/29/23 03:22:41.952
    Jan 29 03:22:41.952: INFO: Pod "pod-projected-secrets-4aea0424-1f0a-4519-9d8f-b05bd574ab18" satisfied condition "Succeeded or Failed"
    Jan 29 03:22:41.965: INFO: Trying to get logs from node slave2 pod pod-projected-secrets-4aea0424-1f0a-4519-9d8f-b05bd574ab18 container secret-volume-test: <nil>
    STEP: delete the pod 01/29/23 03:22:41.986
    Jan 29 03:22:42.105: INFO: Waiting for pod pod-projected-secrets-4aea0424-1f0a-4519-9d8f-b05bd574ab18 to disappear
    Jan 29 03:22:42.110: INFO: Pod pod-projected-secrets-4aea0424-1f0a-4519-9d8f-b05bd574ab18 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan 29 03:22:42.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8839" for this suite. 01/29/23 03:22:42.119
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:22:42.131
Jan 29 03:22:42.132: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename runtimeclass 01/29/23 03:22:42.133
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:22:42.161
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:22:42.166
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Jan 29 03:22:42.204: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-8201 to be scheduled
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jan 29 03:22:42.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-8201" for this suite. 01/29/23 03:22:42.239
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]","completed":112,"skipped":2049,"failed":0}
------------------------------
• [0.130 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:22:42.131
    Jan 29 03:22:42.132: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename runtimeclass 01/29/23 03:22:42.133
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:22:42.161
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:22:42.166
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Jan 29 03:22:42.204: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-8201 to be scheduled
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jan 29 03:22:42.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-8201" for this suite. 01/29/23 03:22:42.239
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:22:42.262
Jan 29 03:22:42.262: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename projected 01/29/23 03:22:42.264
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:22:42.295
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:22:42.3
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
STEP: Creating projection with secret that has name projected-secret-test-83ca0ccf-a571-428b-83e5-5da3c235c272 01/29/23 03:22:42.306
STEP: Creating a pod to test consume secrets 01/29/23 03:22:42.313
Jan 29 03:22:42.331: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-cc2bcf4d-342e-4041-bd95-fabf1a97b223" in namespace "projected-229" to be "Succeeded or Failed"
Jan 29 03:22:42.337: INFO: Pod "pod-projected-secrets-cc2bcf4d-342e-4041-bd95-fabf1a97b223": Phase="Pending", Reason="", readiness=false. Elapsed: 5.82448ms
Jan 29 03:22:44.343: INFO: Pod "pod-projected-secrets-cc2bcf4d-342e-4041-bd95-fabf1a97b223": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011986319s
Jan 29 03:22:46.345: INFO: Pod "pod-projected-secrets-cc2bcf4d-342e-4041-bd95-fabf1a97b223": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01412623s
STEP: Saw pod success 01/29/23 03:22:46.345
Jan 29 03:22:46.345: INFO: Pod "pod-projected-secrets-cc2bcf4d-342e-4041-bd95-fabf1a97b223" satisfied condition "Succeeded or Failed"
Jan 29 03:22:46.352: INFO: Trying to get logs from node slave2 pod pod-projected-secrets-cc2bcf4d-342e-4041-bd95-fabf1a97b223 container projected-secret-volume-test: <nil>
STEP: delete the pod 01/29/23 03:22:46.367
Jan 29 03:22:46.461: INFO: Waiting for pod pod-projected-secrets-cc2bcf4d-342e-4041-bd95-fabf1a97b223 to disappear
Jan 29 03:22:46.467: INFO: Pod pod-projected-secrets-cc2bcf4d-342e-4041-bd95-fabf1a97b223 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan 29 03:22:46.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-229" for this suite. 01/29/23 03:22:46.477
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":113,"skipped":2049,"failed":0}
------------------------------
• [4.226 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:22:42.262
    Jan 29 03:22:42.262: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename projected 01/29/23 03:22:42.264
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:22:42.295
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:22:42.3
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:55
    STEP: Creating projection with secret that has name projected-secret-test-83ca0ccf-a571-428b-83e5-5da3c235c272 01/29/23 03:22:42.306
    STEP: Creating a pod to test consume secrets 01/29/23 03:22:42.313
    Jan 29 03:22:42.331: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-cc2bcf4d-342e-4041-bd95-fabf1a97b223" in namespace "projected-229" to be "Succeeded or Failed"
    Jan 29 03:22:42.337: INFO: Pod "pod-projected-secrets-cc2bcf4d-342e-4041-bd95-fabf1a97b223": Phase="Pending", Reason="", readiness=false. Elapsed: 5.82448ms
    Jan 29 03:22:44.343: INFO: Pod "pod-projected-secrets-cc2bcf4d-342e-4041-bd95-fabf1a97b223": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011986319s
    Jan 29 03:22:46.345: INFO: Pod "pod-projected-secrets-cc2bcf4d-342e-4041-bd95-fabf1a97b223": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01412623s
    STEP: Saw pod success 01/29/23 03:22:46.345
    Jan 29 03:22:46.345: INFO: Pod "pod-projected-secrets-cc2bcf4d-342e-4041-bd95-fabf1a97b223" satisfied condition "Succeeded or Failed"
    Jan 29 03:22:46.352: INFO: Trying to get logs from node slave2 pod pod-projected-secrets-cc2bcf4d-342e-4041-bd95-fabf1a97b223 container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/29/23 03:22:46.367
    Jan 29 03:22:46.461: INFO: Waiting for pod pod-projected-secrets-cc2bcf4d-342e-4041-bd95-fabf1a97b223 to disappear
    Jan 29 03:22:46.467: INFO: Pod pod-projected-secrets-cc2bcf4d-342e-4041-bd95-fabf1a97b223 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan 29 03:22:46.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-229" for this suite. 01/29/23 03:22:46.477
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:22:46.491
Jan 29 03:22:46.491: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename container-probe 01/29/23 03:22:46.492
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:22:46.522
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:22:46.527
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan 29 03:23:46.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9190" for this suite. 01/29/23 03:23:46.567
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","completed":114,"skipped":2082,"failed":0}
------------------------------
• [SLOW TEST] [60.086 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:22:46.491
    Jan 29 03:22:46.491: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename container-probe 01/29/23 03:22:46.492
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:22:46.522
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:22:46.527
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:104
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan 29 03:23:46.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-9190" for this suite. 01/29/23 03:23:46.567
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:23:46.579
Jan 29 03:23:46.579: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename namespaces 01/29/23 03:23:46.581
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:23:46.611
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:23:46.617
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
STEP: creating a Namespace 01/29/23 03:23:46.624
STEP: patching the Namespace 01/29/23 03:23:46.653
STEP: get the Namespace and ensuring it has the label 01/29/23 03:23:46.662
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Jan 29 03:23:46.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-8744" for this suite. 01/29/23 03:23:46.68
STEP: Destroying namespace "nspatchtest-aaf2e346-f969-4fa6-a67f-e203429eab24-2644" for this suite. 01/29/23 03:23:46.694
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","completed":115,"skipped":2098,"failed":0}
------------------------------
• [0.126 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:23:46.579
    Jan 29 03:23:46.579: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename namespaces 01/29/23 03:23:46.581
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:23:46.611
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:23:46.617
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:267
    STEP: creating a Namespace 01/29/23 03:23:46.624
    STEP: patching the Namespace 01/29/23 03:23:46.653
    STEP: get the Namespace and ensuring it has the label 01/29/23 03:23:46.662
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Jan 29 03:23:46.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-8744" for this suite. 01/29/23 03:23:46.68
    STEP: Destroying namespace "nspatchtest-aaf2e346-f969-4fa6-a67f-e203429eab24-2644" for this suite. 01/29/23 03:23:46.694
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:23:46.706
Jan 29 03:23:46.706: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename crd-publish-openapi 01/29/23 03:23:46.708
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:23:46.74
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:23:46.746
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
STEP: set up a multi version CRD 01/29/23 03:23:46.752
Jan 29 03:23:46.753: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: mark a version not serverd 01/29/23 03:23:59.613
STEP: check the unserved version gets removed 01/29/23 03:23:59.643
STEP: check the other version is not changed 01/29/23 03:24:06.375
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 29 03:24:18.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8139" for this suite. 01/29/23 03:24:18.377
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","completed":116,"skipped":2104,"failed":0}
------------------------------
• [SLOW TEST] [31.682 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:23:46.706
    Jan 29 03:23:46.706: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename crd-publish-openapi 01/29/23 03:23:46.708
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:23:46.74
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:23:46.746
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:441
    STEP: set up a multi version CRD 01/29/23 03:23:46.752
    Jan 29 03:23:46.753: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: mark a version not serverd 01/29/23 03:23:59.613
    STEP: check the unserved version gets removed 01/29/23 03:23:59.643
    STEP: check the other version is not changed 01/29/23 03:24:06.375
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 29 03:24:18.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-8139" for this suite. 01/29/23 03:24:18.377
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:24:18.39
Jan 29 03:24:18.390: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename projected 01/29/23 03:24:18.391
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:24:18.42
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:24:18.426
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
STEP: Creating projection with secret that has name projected-secret-test-map-dd684a31-047b-4167-bf64-75e2968cf6eb 01/29/23 03:24:18.432
STEP: Creating a pod to test consume secrets 01/29/23 03:24:18.439
Jan 29 03:24:18.469: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ae5a927f-0386-4b8c-bc4c-f6e729fdb539" in namespace "projected-2131" to be "Succeeded or Failed"
Jan 29 03:24:18.475: INFO: Pod "pod-projected-secrets-ae5a927f-0386-4b8c-bc4c-f6e729fdb539": Phase="Pending", Reason="", readiness=false. Elapsed: 6.050143ms
Jan 29 03:24:20.486: INFO: Pod "pod-projected-secrets-ae5a927f-0386-4b8c-bc4c-f6e729fdb539": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016966734s
Jan 29 03:24:22.482: INFO: Pod "pod-projected-secrets-ae5a927f-0386-4b8c-bc4c-f6e729fdb539": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0127817s
STEP: Saw pod success 01/29/23 03:24:22.482
Jan 29 03:24:22.482: INFO: Pod "pod-projected-secrets-ae5a927f-0386-4b8c-bc4c-f6e729fdb539" satisfied condition "Succeeded or Failed"
Jan 29 03:24:22.490: INFO: Trying to get logs from node slave2 pod pod-projected-secrets-ae5a927f-0386-4b8c-bc4c-f6e729fdb539 container projected-secret-volume-test: <nil>
STEP: delete the pod 01/29/23 03:24:22.523
Jan 29 03:24:22.625: INFO: Waiting for pod pod-projected-secrets-ae5a927f-0386-4b8c-bc4c-f6e729fdb539 to disappear
Jan 29 03:24:22.631: INFO: Pod pod-projected-secrets-ae5a927f-0386-4b8c-bc4c-f6e729fdb539 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan 29 03:24:22.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2131" for this suite. 01/29/23 03:24:22.64
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":117,"skipped":2129,"failed":0}
------------------------------
• [4.261 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:24:18.39
    Jan 29 03:24:18.390: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename projected 01/29/23 03:24:18.391
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:24:18.42
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:24:18.426
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:87
    STEP: Creating projection with secret that has name projected-secret-test-map-dd684a31-047b-4167-bf64-75e2968cf6eb 01/29/23 03:24:18.432
    STEP: Creating a pod to test consume secrets 01/29/23 03:24:18.439
    Jan 29 03:24:18.469: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ae5a927f-0386-4b8c-bc4c-f6e729fdb539" in namespace "projected-2131" to be "Succeeded or Failed"
    Jan 29 03:24:18.475: INFO: Pod "pod-projected-secrets-ae5a927f-0386-4b8c-bc4c-f6e729fdb539": Phase="Pending", Reason="", readiness=false. Elapsed: 6.050143ms
    Jan 29 03:24:20.486: INFO: Pod "pod-projected-secrets-ae5a927f-0386-4b8c-bc4c-f6e729fdb539": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016966734s
    Jan 29 03:24:22.482: INFO: Pod "pod-projected-secrets-ae5a927f-0386-4b8c-bc4c-f6e729fdb539": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0127817s
    STEP: Saw pod success 01/29/23 03:24:22.482
    Jan 29 03:24:22.482: INFO: Pod "pod-projected-secrets-ae5a927f-0386-4b8c-bc4c-f6e729fdb539" satisfied condition "Succeeded or Failed"
    Jan 29 03:24:22.490: INFO: Trying to get logs from node slave2 pod pod-projected-secrets-ae5a927f-0386-4b8c-bc4c-f6e729fdb539 container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/29/23 03:24:22.523
    Jan 29 03:24:22.625: INFO: Waiting for pod pod-projected-secrets-ae5a927f-0386-4b8c-bc4c-f6e729fdb539 to disappear
    Jan 29 03:24:22.631: INFO: Pod pod-projected-secrets-ae5a927f-0386-4b8c-bc4c-f6e729fdb539 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan 29 03:24:22.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2131" for this suite. 01/29/23 03:24:22.64
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:24:22.653
Jan 29 03:24:22.653: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename statefulset 01/29/23 03:24:22.654
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:24:22.694
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:24:22.7
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-4423 01/29/23 03:24:22.707
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
Jan 29 03:24:22.740: INFO: Found 0 stateful pods, waiting for 1
Jan 29 03:24:32.749: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 01/29/23 03:24:32.763
W0129 03:24:32.771114      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jan 29 03:24:32.786: INFO: Found 1 stateful pods, waiting for 2
Jan 29 03:24:42.794: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 29 03:24:42.794: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 01/29/23 03:24:42.808
STEP: Delete all of the StatefulSets 01/29/23 03:24:42.815
STEP: Verify that StatefulSets have been deleted 01/29/23 03:24:42.829
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 29 03:24:42.840: INFO: Deleting all statefulset in ns statefulset-4423
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan 29 03:24:42.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4423" for this suite. 01/29/23 03:24:42.878
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","completed":118,"skipped":2157,"failed":0}
------------------------------
• [SLOW TEST] [20.251 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:906

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:24:22.653
    Jan 29 03:24:22.653: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename statefulset 01/29/23 03:24:22.654
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:24:22.694
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:24:22.7
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-4423 01/29/23 03:24:22.707
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:906
    Jan 29 03:24:22.740: INFO: Found 0 stateful pods, waiting for 1
    Jan 29 03:24:32.749: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 01/29/23 03:24:32.763
    W0129 03:24:32.771114      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jan 29 03:24:32.786: INFO: Found 1 stateful pods, waiting for 2
    Jan 29 03:24:42.794: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 29 03:24:42.794: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 01/29/23 03:24:42.808
    STEP: Delete all of the StatefulSets 01/29/23 03:24:42.815
    STEP: Verify that StatefulSets have been deleted 01/29/23 03:24:42.829
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan 29 03:24:42.840: INFO: Deleting all statefulset in ns statefulset-4423
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan 29 03:24:42.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-4423" for this suite. 01/29/23 03:24:42.878
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:24:42.909
Jan 29 03:24:42.909: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename ingress 01/29/23 03:24:42.91
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:24:42.968
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:24:42.976
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 01/29/23 03:24:42.983
STEP: getting /apis/networking.k8s.io 01/29/23 03:24:42.989
STEP: getting /apis/networking.k8s.iov1 01/29/23 03:24:42.992
STEP: creating 01/29/23 03:24:42.995
STEP: getting 01/29/23 03:24:43.031
STEP: listing 01/29/23 03:24:43.037
STEP: watching 01/29/23 03:24:43.049
Jan 29 03:24:43.049: INFO: starting watch
STEP: cluster-wide listing 01/29/23 03:24:43.052
STEP: cluster-wide watching 01/29/23 03:24:43.058
Jan 29 03:24:43.058: INFO: starting watch
STEP: patching 01/29/23 03:24:43.06
STEP: updating 01/29/23 03:24:43.069
Jan 29 03:24:43.082: INFO: waiting for watch events with expected annotations
Jan 29 03:24:43.082: INFO: saw patched and updated annotations
STEP: patching /status 01/29/23 03:24:43.082
STEP: updating /status 01/29/23 03:24:43.091
STEP: get /status 01/29/23 03:24:43.113
STEP: deleting 01/29/23 03:24:43.119
STEP: deleting a collection 01/29/23 03:24:43.139
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:187
Jan 29 03:24:43.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-4005" for this suite. 01/29/23 03:24:43.185
{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","completed":119,"skipped":2222,"failed":0}
------------------------------
• [0.288 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:24:42.909
    Jan 29 03:24:42.909: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename ingress 01/29/23 03:24:42.91
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:24:42.968
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:24:42.976
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 01/29/23 03:24:42.983
    STEP: getting /apis/networking.k8s.io 01/29/23 03:24:42.989
    STEP: getting /apis/networking.k8s.iov1 01/29/23 03:24:42.992
    STEP: creating 01/29/23 03:24:42.995
    STEP: getting 01/29/23 03:24:43.031
    STEP: listing 01/29/23 03:24:43.037
    STEP: watching 01/29/23 03:24:43.049
    Jan 29 03:24:43.049: INFO: starting watch
    STEP: cluster-wide listing 01/29/23 03:24:43.052
    STEP: cluster-wide watching 01/29/23 03:24:43.058
    Jan 29 03:24:43.058: INFO: starting watch
    STEP: patching 01/29/23 03:24:43.06
    STEP: updating 01/29/23 03:24:43.069
    Jan 29 03:24:43.082: INFO: waiting for watch events with expected annotations
    Jan 29 03:24:43.082: INFO: saw patched and updated annotations
    STEP: patching /status 01/29/23 03:24:43.082
    STEP: updating /status 01/29/23 03:24:43.091
    STEP: get /status 01/29/23 03:24:43.113
    STEP: deleting 01/29/23 03:24:43.119
    STEP: deleting a collection 01/29/23 03:24:43.139
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:187
    Jan 29 03:24:43.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingress-4005" for this suite. 01/29/23 03:24:43.185
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:24:43.198
Jan 29 03:24:43.198: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename svcaccounts 01/29/23 03:24:43.2
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:24:43.23
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:24:43.235
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
Jan 29 03:24:43.270: INFO: Waiting up to 5m0s for pod "pod-service-account-bf88436e-78a0-446d-a7f7-bfdc3eae30b6" in namespace "svcaccounts-1088" to be "running"
Jan 29 03:24:43.277: INFO: Pod "pod-service-account-bf88436e-78a0-446d-a7f7-bfdc3eae30b6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.756027ms
Jan 29 03:24:45.284: INFO: Pod "pod-service-account-bf88436e-78a0-446d-a7f7-bfdc3eae30b6": Phase="Running", Reason="", readiness=true. Elapsed: 2.014274355s
Jan 29 03:24:45.284: INFO: Pod "pod-service-account-bf88436e-78a0-446d-a7f7-bfdc3eae30b6" satisfied condition "running"
STEP: reading a file in the container 01/29/23 03:24:45.284
Jan 29 03:24:45.284: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1088 pod-service-account-bf88436e-78a0-446d-a7f7-bfdc3eae30b6 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 01/29/23 03:24:45.515
Jan 29 03:24:45.515: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1088 pod-service-account-bf88436e-78a0-446d-a7f7-bfdc3eae30b6 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 01/29/23 03:24:45.764
Jan 29 03:24:45.764: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1088 pod-service-account-bf88436e-78a0-446d-a7f7-bfdc3eae30b6 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Jan 29 03:24:46.002: INFO: Got root ca configmap in namespace "svcaccounts-1088"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jan 29 03:24:46.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1088" for this suite. 01/29/23 03:24:46.035
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","completed":120,"skipped":2239,"failed":0}
------------------------------
• [2.853 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:24:43.198
    Jan 29 03:24:43.198: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename svcaccounts 01/29/23 03:24:43.2
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:24:43.23
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:24:43.235
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:75
    Jan 29 03:24:43.270: INFO: Waiting up to 5m0s for pod "pod-service-account-bf88436e-78a0-446d-a7f7-bfdc3eae30b6" in namespace "svcaccounts-1088" to be "running"
    Jan 29 03:24:43.277: INFO: Pod "pod-service-account-bf88436e-78a0-446d-a7f7-bfdc3eae30b6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.756027ms
    Jan 29 03:24:45.284: INFO: Pod "pod-service-account-bf88436e-78a0-446d-a7f7-bfdc3eae30b6": Phase="Running", Reason="", readiness=true. Elapsed: 2.014274355s
    Jan 29 03:24:45.284: INFO: Pod "pod-service-account-bf88436e-78a0-446d-a7f7-bfdc3eae30b6" satisfied condition "running"
    STEP: reading a file in the container 01/29/23 03:24:45.284
    Jan 29 03:24:45.284: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1088 pod-service-account-bf88436e-78a0-446d-a7f7-bfdc3eae30b6 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 01/29/23 03:24:45.515
    Jan 29 03:24:45.515: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1088 pod-service-account-bf88436e-78a0-446d-a7f7-bfdc3eae30b6 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 01/29/23 03:24:45.764
    Jan 29 03:24:45.764: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1088 pod-service-account-bf88436e-78a0-446d-a7f7-bfdc3eae30b6 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Jan 29 03:24:46.002: INFO: Got root ca configmap in namespace "svcaccounts-1088"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jan 29 03:24:46.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-1088" for this suite. 01/29/23 03:24:46.035
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:24:46.052
Jan 29 03:24:46.052: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename runtimeclass 01/29/23 03:24:46.054
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:24:46.13
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:24:46.135
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 01/29/23 03:24:46.141
STEP: getting /apis/node.k8s.io 01/29/23 03:24:46.145
STEP: getting /apis/node.k8s.io/v1 01/29/23 03:24:46.147
STEP: creating 01/29/23 03:24:46.149
STEP: watching 01/29/23 03:24:46.179
Jan 29 03:24:46.179: INFO: starting watch
STEP: getting 01/29/23 03:24:46.192
STEP: listing 01/29/23 03:24:46.202
STEP: patching 01/29/23 03:24:46.21
STEP: updating 01/29/23 03:24:46.217
Jan 29 03:24:46.224: INFO: waiting for watch events with expected annotations
STEP: deleting 01/29/23 03:24:46.225
STEP: deleting a collection 01/29/23 03:24:46.247
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jan 29 03:24:46.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-134" for this suite. 01/29/23 03:24:46.286
{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","completed":121,"skipped":2243,"failed":0}
------------------------------
• [0.250 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:24:46.052
    Jan 29 03:24:46.052: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename runtimeclass 01/29/23 03:24:46.054
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:24:46.13
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:24:46.135
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 01/29/23 03:24:46.141
    STEP: getting /apis/node.k8s.io 01/29/23 03:24:46.145
    STEP: getting /apis/node.k8s.io/v1 01/29/23 03:24:46.147
    STEP: creating 01/29/23 03:24:46.149
    STEP: watching 01/29/23 03:24:46.179
    Jan 29 03:24:46.179: INFO: starting watch
    STEP: getting 01/29/23 03:24:46.192
    STEP: listing 01/29/23 03:24:46.202
    STEP: patching 01/29/23 03:24:46.21
    STEP: updating 01/29/23 03:24:46.217
    Jan 29 03:24:46.224: INFO: waiting for watch events with expected annotations
    STEP: deleting 01/29/23 03:24:46.225
    STEP: deleting a collection 01/29/23 03:24:46.247
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jan 29 03:24:46.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-134" for this suite. 01/29/23 03:24:46.286
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:24:46.304
Jan 29 03:24:46.304: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename container-probe 01/29/23 03:24:46.305
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:24:46.338
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:24:46.344
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
STEP: Creating pod liveness-2b46d864-3e4b-4a7d-b6d5-1aaa2d27015d in namespace container-probe-9878 01/29/23 03:24:46.35
Jan 29 03:24:46.373: INFO: Waiting up to 5m0s for pod "liveness-2b46d864-3e4b-4a7d-b6d5-1aaa2d27015d" in namespace "container-probe-9878" to be "not pending"
Jan 29 03:24:46.380: INFO: Pod "liveness-2b46d864-3e4b-4a7d-b6d5-1aaa2d27015d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.05841ms
Jan 29 03:24:48.387: INFO: Pod "liveness-2b46d864-3e4b-4a7d-b6d5-1aaa2d27015d": Phase="Running", Reason="", readiness=true. Elapsed: 2.013953193s
Jan 29 03:24:48.387: INFO: Pod "liveness-2b46d864-3e4b-4a7d-b6d5-1aaa2d27015d" satisfied condition "not pending"
Jan 29 03:24:48.387: INFO: Started pod liveness-2b46d864-3e4b-4a7d-b6d5-1aaa2d27015d in namespace container-probe-9878
STEP: checking the pod's current state and verifying that restartCount is present 01/29/23 03:24:48.387
Jan 29 03:24:48.395: INFO: Initial restart count of pod liveness-2b46d864-3e4b-4a7d-b6d5-1aaa2d27015d is 0
STEP: deleting the pod 01/29/23 03:28:49.377
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan 29 03:28:49.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9878" for this suite. 01/29/23 03:28:49.487
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","completed":122,"skipped":2255,"failed":0}
------------------------------
• [SLOW TEST] [243.194 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:24:46.304
    Jan 29 03:24:46.304: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename container-probe 01/29/23 03:24:46.305
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:24:46.338
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:24:46.344
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:180
    STEP: Creating pod liveness-2b46d864-3e4b-4a7d-b6d5-1aaa2d27015d in namespace container-probe-9878 01/29/23 03:24:46.35
    Jan 29 03:24:46.373: INFO: Waiting up to 5m0s for pod "liveness-2b46d864-3e4b-4a7d-b6d5-1aaa2d27015d" in namespace "container-probe-9878" to be "not pending"
    Jan 29 03:24:46.380: INFO: Pod "liveness-2b46d864-3e4b-4a7d-b6d5-1aaa2d27015d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.05841ms
    Jan 29 03:24:48.387: INFO: Pod "liveness-2b46d864-3e4b-4a7d-b6d5-1aaa2d27015d": Phase="Running", Reason="", readiness=true. Elapsed: 2.013953193s
    Jan 29 03:24:48.387: INFO: Pod "liveness-2b46d864-3e4b-4a7d-b6d5-1aaa2d27015d" satisfied condition "not pending"
    Jan 29 03:24:48.387: INFO: Started pod liveness-2b46d864-3e4b-4a7d-b6d5-1aaa2d27015d in namespace container-probe-9878
    STEP: checking the pod's current state and verifying that restartCount is present 01/29/23 03:24:48.387
    Jan 29 03:24:48.395: INFO: Initial restart count of pod liveness-2b46d864-3e4b-4a7d-b6d5-1aaa2d27015d is 0
    STEP: deleting the pod 01/29/23 03:28:49.377
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan 29 03:28:49.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-9878" for this suite. 01/29/23 03:28:49.487
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:28:49.503
Jan 29 03:28:49.503: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename sysctl 01/29/23 03:28:49.504
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:28:49.543
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:28:49.549
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 01/29/23 03:28:49.555
STEP: Watching for error events or started pod 01/29/23 03:28:49.571
STEP: Waiting for pod completion 01/29/23 03:28:51.58
Jan 29 03:28:51.580: INFO: Waiting up to 3m0s for pod "sysctl-a8ca41be-ac86-4e53-b8df-3284ab935ab2" in namespace "sysctl-5582" to be "completed"
Jan 29 03:28:51.585: INFO: Pod "sysctl-a8ca41be-ac86-4e53-b8df-3284ab935ab2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.560759ms
Jan 29 03:28:53.592: INFO: Pod "sysctl-a8ca41be-ac86-4e53-b8df-3284ab935ab2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012609266s
Jan 29 03:28:53.593: INFO: Pod "sysctl-a8ca41be-ac86-4e53-b8df-3284ab935ab2" satisfied condition "completed"
STEP: Checking that the pod succeeded 01/29/23 03:28:53.599
STEP: Getting logs from the pod 01/29/23 03:28:53.599
STEP: Checking that the sysctl is actually updated 01/29/23 03:28:53.628
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
Jan 29 03:28:53.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-5582" for this suite. 01/29/23 03:28:53.639
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":123,"skipped":2335,"failed":0}
------------------------------
• [4.148 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:28:49.503
    Jan 29 03:28:49.503: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename sysctl 01/29/23 03:28:49.504
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:28:49.543
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:28:49.549
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 01/29/23 03:28:49.555
    STEP: Watching for error events or started pod 01/29/23 03:28:49.571
    STEP: Waiting for pod completion 01/29/23 03:28:51.58
    Jan 29 03:28:51.580: INFO: Waiting up to 3m0s for pod "sysctl-a8ca41be-ac86-4e53-b8df-3284ab935ab2" in namespace "sysctl-5582" to be "completed"
    Jan 29 03:28:51.585: INFO: Pod "sysctl-a8ca41be-ac86-4e53-b8df-3284ab935ab2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.560759ms
    Jan 29 03:28:53.592: INFO: Pod "sysctl-a8ca41be-ac86-4e53-b8df-3284ab935ab2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012609266s
    Jan 29 03:28:53.593: INFO: Pod "sysctl-a8ca41be-ac86-4e53-b8df-3284ab935ab2" satisfied condition "completed"
    STEP: Checking that the pod succeeded 01/29/23 03:28:53.599
    STEP: Getting logs from the pod 01/29/23 03:28:53.599
    STEP: Checking that the sysctl is actually updated 01/29/23 03:28:53.628
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan 29 03:28:53.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-5582" for this suite. 01/29/23 03:28:53.639
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:28:53.653
Jan 29 03:28:53.653: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename namespaces 01/29/23 03:28:53.654
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:28:53.682
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:28:53.687
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
STEP: Creating a test namespace 01/29/23 03:28:53.692
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:28:53.724
STEP: Creating a pod in the namespace 01/29/23 03:28:53.73
STEP: Waiting for the pod to have running status 01/29/23 03:28:53.746
Jan 29 03:28:53.747: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-42" to be "running"
Jan 29 03:28:53.753: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.266463ms
Jan 29 03:28:55.770: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.023384481s
Jan 29 03:28:55.770: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 01/29/23 03:28:55.77
STEP: Waiting for the namespace to be removed. 01/29/23 03:28:55.786
STEP: Recreating the namespace 01/29/23 03:29:07.794
STEP: Verifying there are no pods in the namespace 01/29/23 03:29:07.824
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Jan 29 03:29:07.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1998" for this suite. 01/29/23 03:29:07.842
STEP: Destroying namespace "nsdeletetest-42" for this suite. 01/29/23 03:29:07.858
Jan 29 03:29:07.865: INFO: Namespace nsdeletetest-42 was already deleted
STEP: Destroying namespace "nsdeletetest-8053" for this suite. 01/29/23 03:29:07.865
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","completed":124,"skipped":2347,"failed":0}
------------------------------
• [SLOW TEST] [14.224 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:28:53.653
    Jan 29 03:28:53.653: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename namespaces 01/29/23 03:28:53.654
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:28:53.682
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:28:53.687
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:242
    STEP: Creating a test namespace 01/29/23 03:28:53.692
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:28:53.724
    STEP: Creating a pod in the namespace 01/29/23 03:28:53.73
    STEP: Waiting for the pod to have running status 01/29/23 03:28:53.746
    Jan 29 03:28:53.747: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-42" to be "running"
    Jan 29 03:28:53.753: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.266463ms
    Jan 29 03:28:55.770: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.023384481s
    Jan 29 03:28:55.770: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 01/29/23 03:28:55.77
    STEP: Waiting for the namespace to be removed. 01/29/23 03:28:55.786
    STEP: Recreating the namespace 01/29/23 03:29:07.794
    STEP: Verifying there are no pods in the namespace 01/29/23 03:29:07.824
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Jan 29 03:29:07.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-1998" for this suite. 01/29/23 03:29:07.842
    STEP: Destroying namespace "nsdeletetest-42" for this suite. 01/29/23 03:29:07.858
    Jan 29 03:29:07.865: INFO: Namespace nsdeletetest-42 was already deleted
    STEP: Destroying namespace "nsdeletetest-8053" for this suite. 01/29/23 03:29:07.865
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:29:07.877
Jan 29 03:29:07.878: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename runtimeclass 01/29/23 03:29:07.879
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:29:07.908
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:29:07.916
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jan 29 03:29:07.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-9944" for this suite. 01/29/23 03:29:07.948
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]","completed":125,"skipped":2351,"failed":0}
------------------------------
• [0.087 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:29:07.877
    Jan 29 03:29:07.878: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename runtimeclass 01/29/23 03:29:07.879
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:29:07.908
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:29:07.916
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jan 29 03:29:07.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-9944" for this suite. 01/29/23 03:29:07.948
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:29:07.966
Jan 29 03:29:07.966: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename sched-preemption 01/29/23 03:29:07.968
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:29:08
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:29:08.006
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jan 29 03:29:08.036: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 29 03:30:08.097: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:30:08.104
Jan 29 03:30:08.104: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename sched-preemption-path 01/29/23 03:30:08.105
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:30:08.155
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:30:08.161
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:496
STEP: Finding an available node 01/29/23 03:30:08.166
STEP: Trying to launch a pod without a label to get a node which can launch it. 01/29/23 03:30:08.167
Jan 29 03:30:08.184: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-4180" to be "running"
Jan 29 03:30:08.190: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 6.263864ms
Jan 29 03:30:10.198: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013735013s
Jan 29 03:30:12.198: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.014108853s
Jan 29 03:30:12.198: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 01/29/23 03:30:12.209
Jan 29 03:30:12.310: INFO: found a healthy node: slave2
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
Jan 29 03:30:20.460: INFO: pods created so far: [1 1 1]
Jan 29 03:30:20.460: INFO: length of pods created so far: 3
Jan 29 03:30:24.479: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:187
Jan 29 03:30:31.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-4180" for this suite. 01/29/23 03:30:31.496
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:470
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Jan 29 03:30:31.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-7674" for this suite. 01/29/23 03:30:31.57
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","completed":126,"skipped":2370,"failed":0}
------------------------------
• [SLOW TEST] [83.718 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:458
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:543

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:29:07.966
    Jan 29 03:29:07.966: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename sched-preemption 01/29/23 03:29:07.968
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:29:08
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:29:08.006
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Jan 29 03:29:08.036: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 29 03:30:08.097: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:30:08.104
    Jan 29 03:30:08.104: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename sched-preemption-path 01/29/23 03:30:08.105
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:30:08.155
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:30:08.161
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:496
    STEP: Finding an available node 01/29/23 03:30:08.166
    STEP: Trying to launch a pod without a label to get a node which can launch it. 01/29/23 03:30:08.167
    Jan 29 03:30:08.184: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-4180" to be "running"
    Jan 29 03:30:08.190: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 6.263864ms
    Jan 29 03:30:10.198: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013735013s
    Jan 29 03:30:12.198: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.014108853s
    Jan 29 03:30:12.198: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 01/29/23 03:30:12.209
    Jan 29 03:30:12.310: INFO: found a healthy node: slave2
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:543
    Jan 29 03:30:20.460: INFO: pods created so far: [1 1 1]
    Jan 29 03:30:20.460: INFO: length of pods created so far: 3
    Jan 29 03:30:24.479: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:187
    Jan 29 03:30:31.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-4180" for this suite. 01/29/23 03:30:31.496
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:470
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Jan 29 03:30:31.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-7674" for this suite. 01/29/23 03:30:31.57
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:30:31.685
Jan 29 03:30:31.685: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename kubectl 01/29/23 03:30:31.687
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:30:31.719
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:30:31.726
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1492
STEP: creating the pod 01/29/23 03:30:31.732
Jan 29 03:30:31.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8460 create -f -'
Jan 29 03:30:32.793: INFO: stderr: ""
Jan 29 03:30:32.793: INFO: stdout: "pod/pause created\n"
Jan 29 03:30:32.793: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jan 29 03:30:32.793: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-8460" to be "running and ready"
Jan 29 03:30:32.801: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 7.801675ms
Jan 29 03:30:32.801: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'slave2' to be 'Running' but was 'Pending'
Jan 29 03:30:34.809: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01603249s
Jan 29 03:30:34.809: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'slave2' to be 'Running' but was 'Pending'
Jan 29 03:30:36.809: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.016266649s
Jan 29 03:30:36.809: INFO: Pod "pause" satisfied condition "running and ready"
Jan 29 03:30:36.809: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
STEP: adding the label testing-label with value testing-label-value to a pod 01/29/23 03:30:36.809
Jan 29 03:30:36.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8460 label pods pause testing-label=testing-label-value'
Jan 29 03:30:36.963: INFO: stderr: ""
Jan 29 03:30:36.963: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 01/29/23 03:30:36.964
Jan 29 03:30:36.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8460 get pod pause -L testing-label'
Jan 29 03:30:37.100: INFO: stderr: ""
Jan 29 03:30:37.100: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    testing-label-value\n"
STEP: removing the label testing-label of a pod 01/29/23 03:30:37.1
Jan 29 03:30:37.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8460 label pods pause testing-label-'
Jan 29 03:30:37.233: INFO: stderr: ""
Jan 29 03:30:37.233: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 01/29/23 03:30:37.233
Jan 29 03:30:37.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8460 get pod pause -L testing-label'
Jan 29 03:30:37.353: INFO: stderr: ""
Jan 29 03:30:37.353: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1498
STEP: using delete to clean up resources 01/29/23 03:30:37.353
Jan 29 03:30:37.353: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8460 delete --grace-period=0 --force -f -'
Jan 29 03:30:37.559: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 29 03:30:37.559: INFO: stdout: "pod \"pause\" force deleted\n"
Jan 29 03:30:37.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8460 get rc,svc -l name=pause --no-headers'
Jan 29 03:30:37.686: INFO: stderr: "No resources found in kubectl-8460 namespace.\n"
Jan 29 03:30:37.686: INFO: stdout: ""
Jan 29 03:30:37.687: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8460 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 29 03:30:37.793: INFO: stderr: ""
Jan 29 03:30:37.793: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 29 03:30:37.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8460" for this suite. 01/29/23 03:30:37.805
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","completed":127,"skipped":2373,"failed":0}
------------------------------
• [SLOW TEST] [6.134 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1490
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:30:31.685
    Jan 29 03:30:31.685: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename kubectl 01/29/23 03:30:31.687
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:30:31.719
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:30:31.726
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1492
    STEP: creating the pod 01/29/23 03:30:31.732
    Jan 29 03:30:31.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8460 create -f -'
    Jan 29 03:30:32.793: INFO: stderr: ""
    Jan 29 03:30:32.793: INFO: stdout: "pod/pause created\n"
    Jan 29 03:30:32.793: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Jan 29 03:30:32.793: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-8460" to be "running and ready"
    Jan 29 03:30:32.801: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 7.801675ms
    Jan 29 03:30:32.801: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'slave2' to be 'Running' but was 'Pending'
    Jan 29 03:30:34.809: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01603249s
    Jan 29 03:30:34.809: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'slave2' to be 'Running' but was 'Pending'
    Jan 29 03:30:36.809: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.016266649s
    Jan 29 03:30:36.809: INFO: Pod "pause" satisfied condition "running and ready"
    Jan 29 03:30:36.809: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1507
    STEP: adding the label testing-label with value testing-label-value to a pod 01/29/23 03:30:36.809
    Jan 29 03:30:36.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8460 label pods pause testing-label=testing-label-value'
    Jan 29 03:30:36.963: INFO: stderr: ""
    Jan 29 03:30:36.963: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 01/29/23 03:30:36.964
    Jan 29 03:30:36.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8460 get pod pause -L testing-label'
    Jan 29 03:30:37.100: INFO: stderr: ""
    Jan 29 03:30:37.100: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 01/29/23 03:30:37.1
    Jan 29 03:30:37.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8460 label pods pause testing-label-'
    Jan 29 03:30:37.233: INFO: stderr: ""
    Jan 29 03:30:37.233: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 01/29/23 03:30:37.233
    Jan 29 03:30:37.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8460 get pod pause -L testing-label'
    Jan 29 03:30:37.353: INFO: stderr: ""
    Jan 29 03:30:37.353: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1498
    STEP: using delete to clean up resources 01/29/23 03:30:37.353
    Jan 29 03:30:37.353: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8460 delete --grace-period=0 --force -f -'
    Jan 29 03:30:37.559: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 29 03:30:37.559: INFO: stdout: "pod \"pause\" force deleted\n"
    Jan 29 03:30:37.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8460 get rc,svc -l name=pause --no-headers'
    Jan 29 03:30:37.686: INFO: stderr: "No resources found in kubectl-8460 namespace.\n"
    Jan 29 03:30:37.686: INFO: stdout: ""
    Jan 29 03:30:37.687: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8460 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jan 29 03:30:37.793: INFO: stderr: ""
    Jan 29 03:30:37.793: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 29 03:30:37.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-8460" for this suite. 01/29/23 03:30:37.805
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:30:37.822
Jan 29 03:30:37.823: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename watch 01/29/23 03:30:37.824
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:30:37.86
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:30:37.875
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 01/29/23 03:30:37.882
STEP: modifying the configmap once 01/29/23 03:30:37.89
STEP: modifying the configmap a second time 01/29/23 03:30:37.909
STEP: deleting the configmap 01/29/23 03:30:37.923
STEP: creating a watch on configmaps from the resource version returned by the first update 01/29/23 03:30:37.936
STEP: Expecting to observe notifications for all changes to the configmap after the first update 01/29/23 03:30:37.939
Jan 29 03:30:37.939: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4712  c8ef65a7-5450-421f-983e-475fee784ead 5959201 0 2023-01-29 03:30:37 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 29 03:30:37.939: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4712  c8ef65a7-5450-421f-983e-475fee784ead 5959203 0 2023-01-29 03:30:37 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jan 29 03:30:37.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4712" for this suite. 01/29/23 03:30:37.949
{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","completed":128,"skipped":2409,"failed":0}
------------------------------
• [0.147 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:30:37.822
    Jan 29 03:30:37.823: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename watch 01/29/23 03:30:37.824
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:30:37.86
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:30:37.875
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 01/29/23 03:30:37.882
    STEP: modifying the configmap once 01/29/23 03:30:37.89
    STEP: modifying the configmap a second time 01/29/23 03:30:37.909
    STEP: deleting the configmap 01/29/23 03:30:37.923
    STEP: creating a watch on configmaps from the resource version returned by the first update 01/29/23 03:30:37.936
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 01/29/23 03:30:37.939
    Jan 29 03:30:37.939: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4712  c8ef65a7-5450-421f-983e-475fee784ead 5959201 0 2023-01-29 03:30:37 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 29 03:30:37.939: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4712  c8ef65a7-5450-421f-983e-475fee784ead 5959203 0 2023-01-29 03:30:37 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jan 29 03:30:37.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-4712" for this suite. 01/29/23 03:30:37.949
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:30:37.971
Jan 29 03:30:37.971: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename pods 01/29/23 03:30:37.972
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:30:38.005
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:30:38.01
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
STEP: creating the pod 01/29/23 03:30:38.02
STEP: setting up watch 01/29/23 03:30:38.02
STEP: submitting the pod to kubernetes 01/29/23 03:30:38.126
STEP: verifying the pod is in kubernetes 01/29/23 03:30:38.149
STEP: verifying pod creation was observed 01/29/23 03:30:38.155
Jan 29 03:30:38.155: INFO: Waiting up to 5m0s for pod "pod-submit-remove-449f58ff-3d34-4efd-9e6e-5309b00c18de" in namespace "pods-3885" to be "running"
Jan 29 03:30:38.161: INFO: Pod "pod-submit-remove-449f58ff-3d34-4efd-9e6e-5309b00c18de": Phase="Pending", Reason="", readiness=false. Elapsed: 5.493478ms
Jan 29 03:30:40.169: INFO: Pod "pod-submit-remove-449f58ff-3d34-4efd-9e6e-5309b00c18de": Phase="Running", Reason="", readiness=true. Elapsed: 2.01326229s
Jan 29 03:30:40.169: INFO: Pod "pod-submit-remove-449f58ff-3d34-4efd-9e6e-5309b00c18de" satisfied condition "running"
STEP: deleting the pod gracefully 01/29/23 03:30:40.175
STEP: verifying pod deletion was observed 01/29/23 03:30:40.209
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 29 03:30:42.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3885" for this suite. 01/29/23 03:30:42.919
{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","completed":129,"skipped":2415,"failed":0}
------------------------------
• [4.958 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:30:37.971
    Jan 29 03:30:37.971: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename pods 01/29/23 03:30:37.972
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:30:38.005
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:30:38.01
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:225
    STEP: creating the pod 01/29/23 03:30:38.02
    STEP: setting up watch 01/29/23 03:30:38.02
    STEP: submitting the pod to kubernetes 01/29/23 03:30:38.126
    STEP: verifying the pod is in kubernetes 01/29/23 03:30:38.149
    STEP: verifying pod creation was observed 01/29/23 03:30:38.155
    Jan 29 03:30:38.155: INFO: Waiting up to 5m0s for pod "pod-submit-remove-449f58ff-3d34-4efd-9e6e-5309b00c18de" in namespace "pods-3885" to be "running"
    Jan 29 03:30:38.161: INFO: Pod "pod-submit-remove-449f58ff-3d34-4efd-9e6e-5309b00c18de": Phase="Pending", Reason="", readiness=false. Elapsed: 5.493478ms
    Jan 29 03:30:40.169: INFO: Pod "pod-submit-remove-449f58ff-3d34-4efd-9e6e-5309b00c18de": Phase="Running", Reason="", readiness=true. Elapsed: 2.01326229s
    Jan 29 03:30:40.169: INFO: Pod "pod-submit-remove-449f58ff-3d34-4efd-9e6e-5309b00c18de" satisfied condition "running"
    STEP: deleting the pod gracefully 01/29/23 03:30:40.175
    STEP: verifying pod deletion was observed 01/29/23 03:30:40.209
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 29 03:30:42.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-3885" for this suite. 01/29/23 03:30:42.919
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:30:42.932
Jan 29 03:30:42.932: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename svcaccounts 01/29/23 03:30:42.934
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:30:42.969
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:30:42.977
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
STEP: Creating a pod to test service account token:  01/29/23 03:30:42.984
Jan 29 03:30:43.011: INFO: Waiting up to 5m0s for pod "test-pod-7f80d750-0c93-4f50-9371-14b9c6142647" in namespace "svcaccounts-1501" to be "Succeeded or Failed"
Jan 29 03:30:43.017: INFO: Pod "test-pod-7f80d750-0c93-4f50-9371-14b9c6142647": Phase="Pending", Reason="", readiness=false. Elapsed: 5.947922ms
Jan 29 03:30:45.024: INFO: Pod "test-pod-7f80d750-0c93-4f50-9371-14b9c6142647": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012807967s
Jan 29 03:30:47.025: INFO: Pod "test-pod-7f80d750-0c93-4f50-9371-14b9c6142647": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013863732s
STEP: Saw pod success 01/29/23 03:30:47.025
Jan 29 03:30:47.025: INFO: Pod "test-pod-7f80d750-0c93-4f50-9371-14b9c6142647" satisfied condition "Succeeded or Failed"
Jan 29 03:30:47.031: INFO: Trying to get logs from node slave2 pod test-pod-7f80d750-0c93-4f50-9371-14b9c6142647 container agnhost-container: <nil>
STEP: delete the pod 01/29/23 03:30:47.075
Jan 29 03:30:47.182: INFO: Waiting for pod test-pod-7f80d750-0c93-4f50-9371-14b9c6142647 to disappear
Jan 29 03:30:47.189: INFO: Pod test-pod-7f80d750-0c93-4f50-9371-14b9c6142647 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jan 29 03:30:47.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1501" for this suite. 01/29/23 03:30:47.198
{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","completed":130,"skipped":2454,"failed":0}
------------------------------
• [4.287 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:30:42.932
    Jan 29 03:30:42.932: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename svcaccounts 01/29/23 03:30:42.934
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:30:42.969
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:30:42.977
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:272
    STEP: Creating a pod to test service account token:  01/29/23 03:30:42.984
    Jan 29 03:30:43.011: INFO: Waiting up to 5m0s for pod "test-pod-7f80d750-0c93-4f50-9371-14b9c6142647" in namespace "svcaccounts-1501" to be "Succeeded or Failed"
    Jan 29 03:30:43.017: INFO: Pod "test-pod-7f80d750-0c93-4f50-9371-14b9c6142647": Phase="Pending", Reason="", readiness=false. Elapsed: 5.947922ms
    Jan 29 03:30:45.024: INFO: Pod "test-pod-7f80d750-0c93-4f50-9371-14b9c6142647": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012807967s
    Jan 29 03:30:47.025: INFO: Pod "test-pod-7f80d750-0c93-4f50-9371-14b9c6142647": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013863732s
    STEP: Saw pod success 01/29/23 03:30:47.025
    Jan 29 03:30:47.025: INFO: Pod "test-pod-7f80d750-0c93-4f50-9371-14b9c6142647" satisfied condition "Succeeded or Failed"
    Jan 29 03:30:47.031: INFO: Trying to get logs from node slave2 pod test-pod-7f80d750-0c93-4f50-9371-14b9c6142647 container agnhost-container: <nil>
    STEP: delete the pod 01/29/23 03:30:47.075
    Jan 29 03:30:47.182: INFO: Waiting for pod test-pod-7f80d750-0c93-4f50-9371-14b9c6142647 to disappear
    Jan 29 03:30:47.189: INFO: Pod test-pod-7f80d750-0c93-4f50-9371-14b9c6142647 no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jan 29 03:30:47.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-1501" for this suite. 01/29/23 03:30:47.198
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:30:47.222
Jan 29 03:30:47.222: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename custom-resource-definition 01/29/23 03:30:47.223
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:30:47.257
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:30:47.263
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Jan 29 03:30:47.268: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 29 03:30:50.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-382" for this suite. 01/29/23 03:30:50.444
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","completed":131,"skipped":2485,"failed":0}
------------------------------
• [3.234 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:30:47.222
    Jan 29 03:30:47.222: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename custom-resource-definition 01/29/23 03:30:47.223
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:30:47.257
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:30:47.263
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Jan 29 03:30:47.268: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 29 03:30:50.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-382" for this suite. 01/29/23 03:30:50.444
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:30:50.458
Jan 29 03:30:50.458: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename container-lifecycle-hook 01/29/23 03:30:50.459
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:30:50.504
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:30:50.51
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 01/29/23 03:30:50.527
Jan 29 03:30:50.545: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6281" to be "running and ready"
Jan 29 03:30:50.554: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 9.385345ms
Jan 29 03:30:50.554: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 29 03:30:52.561: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.01612767s
Jan 29 03:30:52.561: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan 29 03:30:52.561: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
STEP: create the pod with lifecycle hook 01/29/23 03:30:52.567
Jan 29 03:30:52.582: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-6281" to be "running and ready"
Jan 29 03:30:52.589: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 7.068249ms
Jan 29 03:30:52.589: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 29 03:30:54.597: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.015463266s
Jan 29 03:30:54.597: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Jan 29 03:30:54.597: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 01/29/23 03:30:54.603
STEP: delete the pod with lifecycle hook 01/29/23 03:30:54.635
Jan 29 03:30:54.671: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 29 03:30:54.677: INFO: Pod pod-with-poststart-http-hook still exists
Jan 29 03:30:56.678: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 29 03:30:56.686: INFO: Pod pod-with-poststart-http-hook still exists
Jan 29 03:30:58.678: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 29 03:30:58.685: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Jan 29 03:30:58.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6281" for this suite. 01/29/23 03:30:58.694
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","completed":132,"skipped":2508,"failed":0}
------------------------------
• [SLOW TEST] [8.246 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:30:50.458
    Jan 29 03:30:50.458: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/29/23 03:30:50.459
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:30:50.504
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:30:50.51
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 01/29/23 03:30:50.527
    Jan 29 03:30:50.545: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6281" to be "running and ready"
    Jan 29 03:30:50.554: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 9.385345ms
    Jan 29 03:30:50.554: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 29 03:30:52.561: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.01612767s
    Jan 29 03:30:52.561: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan 29 03:30:52.561: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:130
    STEP: create the pod with lifecycle hook 01/29/23 03:30:52.567
    Jan 29 03:30:52.582: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-6281" to be "running and ready"
    Jan 29 03:30:52.589: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 7.068249ms
    Jan 29 03:30:52.589: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 29 03:30:54.597: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.015463266s
    Jan 29 03:30:54.597: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Jan 29 03:30:54.597: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 01/29/23 03:30:54.603
    STEP: delete the pod with lifecycle hook 01/29/23 03:30:54.635
    Jan 29 03:30:54.671: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jan 29 03:30:54.677: INFO: Pod pod-with-poststart-http-hook still exists
    Jan 29 03:30:56.678: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jan 29 03:30:56.686: INFO: Pod pod-with-poststart-http-hook still exists
    Jan 29 03:30:58.678: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jan 29 03:30:58.685: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Jan 29 03:30:58.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-6281" for this suite. 01/29/23 03:30:58.694
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:30:58.705
Jan 29 03:30:58.705: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename deployment 01/29/23 03:30:58.707
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:30:58.735
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:30:58.741
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Jan 29 03:30:58.747: INFO: Creating simple deployment test-new-deployment
Jan 29 03:30:58.777: INFO: new replicaset for deployment "test-new-deployment" is yet to be created
STEP: getting scale subresource 01/29/23 03:31:00.819
STEP: updating a scale subresource 01/29/23 03:31:00.828
STEP: verifying the deployment Spec.Replicas was modified 01/29/23 03:31:00.839
STEP: Patch a scale subresource 01/29/23 03:31:00.85
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 29 03:31:00.908: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-188  f6fe4801-6d67-4cb8-8ce9-8e7bf5445d64 5959491 3 2023-01-29 03:30:58 +0000 UTC <nil> <nil> map[name:httpd] map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/revision:1] [] [] []},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0x4003085278 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-845c8977d9" has successfully progressed.,LastUpdateTime:2023-01-29 03:31:00 +0000 UTC,LastTransitionTime:2023-01-29 03:30:58 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-29 03:31:00 +0000 UTC,LastTransitionTime:2023-01-29 03:31:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 29 03:31:00.920: INFO: New ReplicaSet "test-new-deployment-845c8977d9" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-845c8977d9  deployment-188  1b32d95a-b71a-442e-85a2-612445e8f80f 5959496 3 2023-01-29 03:30:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment f6fe4801-6d67-4cb8-8ce9-8e7bf5445d64 0x40030856e7 0x40030856e8}] [] []},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0x4003085748 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 29 03:31:00.945: INFO: Pod "test-new-deployment-845c8977d9-h5m5j" is available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-h5m5j test-new-deployment-845c8977d9- deployment-188  ff4b795a-5cd2-4142-a1e9-c09824bc7ab8 5959480 0 2023-01-29 03:30:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "100.101.49.6"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "100.101.49.6"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 1b32d95a-b71a-442e-85a2-612445e8f80f 0x40042d7f47 0x40042d7f48}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-98wzc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-98wzc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:30:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:31:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:31:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:30:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.245,PodIP:100.101.49.6,StartTime:2023-01-29 03:30:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-29 03:30:59 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry-jinan-lab.inspurcloud.cn/library/cke/httpd-arm64:2.4.38-2,ImageID:docker-pullable://registry-jinan-lab.inspurcloud.cn/library/cke/httpd-arm64@sha256:a4f0279e65f8d239ea154d08f0572c5419f43e4ed748bee8a1a1b762d0d36893,ContainerID:docker://a3a6c207803f268269df615b2450097c7f622ac2fb6a4876a001808d78c1edbe,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.101.49.6,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 03:31:00.946: INFO: Pod "test-new-deployment-845c8977d9-sch2n" is not available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-sch2n test-new-deployment-845c8977d9- deployment-188  1b3e2bfa-112a-45cd-8b2c-d3310b551f6d 5959497 0 2023-01-29 03:31:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 1b32d95a-b71a-442e-85a2-612445e8f80f 0x4004402107 0x4004402108}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8gm8t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8gm8t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:31:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:31:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:31:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:31:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.244,PodIP:,StartTime:2023-01-29 03:31:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan 29 03:31:00.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-188" for this suite. 01/29/23 03:31:00.961
{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","completed":133,"skipped":2526,"failed":0}
------------------------------
• [2.284 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:30:58.705
    Jan 29 03:30:58.705: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename deployment 01/29/23 03:30:58.707
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:30:58.735
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:30:58.741
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Jan 29 03:30:58.747: INFO: Creating simple deployment test-new-deployment
    Jan 29 03:30:58.777: INFO: new replicaset for deployment "test-new-deployment" is yet to be created
    STEP: getting scale subresource 01/29/23 03:31:00.819
    STEP: updating a scale subresource 01/29/23 03:31:00.828
    STEP: verifying the deployment Spec.Replicas was modified 01/29/23 03:31:00.839
    STEP: Patch a scale subresource 01/29/23 03:31:00.85
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 29 03:31:00.908: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-188  f6fe4801-6d67-4cb8-8ce9-8e7bf5445d64 5959491 3 2023-01-29 03:30:58 +0000 UTC <nil> <nil> map[name:httpd] map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/revision:1] [] [] []},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0x4003085278 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-845c8977d9" has successfully progressed.,LastUpdateTime:2023-01-29 03:31:00 +0000 UTC,LastTransitionTime:2023-01-29 03:30:58 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-29 03:31:00 +0000 UTC,LastTransitionTime:2023-01-29 03:31:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan 29 03:31:00.920: INFO: New ReplicaSet "test-new-deployment-845c8977d9" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-845c8977d9  deployment-188  1b32d95a-b71a-442e-85a2-612445e8f80f 5959496 3 2023-01-29 03:30:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment f6fe4801-6d67-4cb8-8ce9-8e7bf5445d64 0x40030856e7 0x40030856e8}] [] []},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0x4003085748 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 29 03:31:00.945: INFO: Pod "test-new-deployment-845c8977d9-h5m5j" is available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-h5m5j test-new-deployment-845c8977d9- deployment-188  ff4b795a-5cd2-4142-a1e9-c09824bc7ab8 5959480 0 2023-01-29 03:30:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "",
        "ips": [
            "100.101.49.6"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "",
        "ips": [
            "100.101.49.6"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 1b32d95a-b71a-442e-85a2-612445e8f80f 0x40042d7f47 0x40042d7f48}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-98wzc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-98wzc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:30:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:31:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:31:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:30:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.245,PodIP:100.101.49.6,StartTime:2023-01-29 03:30:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-29 03:30:59 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry-jinan-lab.inspurcloud.cn/library/cke/httpd-arm64:2.4.38-2,ImageID:docker-pullable://registry-jinan-lab.inspurcloud.cn/library/cke/httpd-arm64@sha256:a4f0279e65f8d239ea154d08f0572c5419f43e4ed748bee8a1a1b762d0d36893,ContainerID:docker://a3a6c207803f268269df615b2450097c7f622ac2fb6a4876a001808d78c1edbe,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.101.49.6,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 29 03:31:00.946: INFO: Pod "test-new-deployment-845c8977d9-sch2n" is not available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-sch2n test-new-deployment-845c8977d9- deployment-188  1b3e2bfa-112a-45cd-8b2c-d3310b551f6d 5959497 0 2023-01-29 03:31:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 1b32d95a-b71a-442e-85a2-612445e8f80f 0x4004402107 0x4004402108}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8gm8t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8gm8t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:31:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:31:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:31:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:31:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.244,PodIP:,StartTime:2023-01-29 03:31:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan 29 03:31:00.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-188" for this suite. 01/29/23 03:31:00.961
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:31:00.99
Jan 29 03:31:00.990: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename kubectl 01/29/23 03:31:00.992
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:31:01.07
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:31:01.079
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
STEP: validating cluster-info 01/29/23 03:31:01.087
Jan 29 03:31:01.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-1392 cluster-info'
Jan 29 03:31:01.200: INFO: stderr: ""
Jan 29 03:31:01.200: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://100.105.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 29 03:31:01.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1392" for this suite. 01/29/23 03:31:01.21
{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","completed":134,"skipped":2527,"failed":0}
------------------------------
• [0.232 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1242
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:31:00.99
    Jan 29 03:31:00.990: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename kubectl 01/29/23 03:31:00.992
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:31:01.07
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:31:01.079
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1248
    STEP: validating cluster-info 01/29/23 03:31:01.087
    Jan 29 03:31:01.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-1392 cluster-info'
    Jan 29 03:31:01.200: INFO: stderr: ""
    Jan 29 03:31:01.200: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://100.105.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 29 03:31:01.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-1392" for this suite. 01/29/23 03:31:01.21
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:31:01.229
Jan 29 03:31:01.229: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename secrets 01/29/23 03:31:01.23
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:31:01.261
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:31:01.267
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 29 03:31:01.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5238" for this suite. 01/29/23 03:31:01.344
{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","completed":135,"skipped":2632,"failed":0}
------------------------------
• [0.125 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:31:01.229
    Jan 29 03:31:01.229: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename secrets 01/29/23 03:31:01.23
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:31:01.261
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:31:01.267
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:385
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 29 03:31:01.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-5238" for this suite. 01/29/23 03:31:01.344
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:31:01.356
Jan 29 03:31:01.356: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename cronjob 01/29/23 03:31:01.357
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:31:01.383
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:31:01.389
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 01/29/23 03:31:01.395
STEP: Ensuring a job is scheduled 01/29/23 03:31:01.408
STEP: Ensuring exactly one is scheduled 01/29/23 03:32:01.417
STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/29/23 03:32:01.424
STEP: Ensuring the job is replaced with a new one 01/29/23 03:32:01.432
STEP: Removing cronjob 01/29/23 03:33:01.441
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jan 29 03:33:01.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-3299" for this suite. 01/29/23 03:33:01.462
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","completed":136,"skipped":2644,"failed":0}
------------------------------
• [SLOW TEST] [120.118 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:31:01.356
    Jan 29 03:31:01.356: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename cronjob 01/29/23 03:31:01.357
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:31:01.383
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:31:01.389
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 01/29/23 03:31:01.395
    STEP: Ensuring a job is scheduled 01/29/23 03:31:01.408
    STEP: Ensuring exactly one is scheduled 01/29/23 03:32:01.417
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/29/23 03:32:01.424
    STEP: Ensuring the job is replaced with a new one 01/29/23 03:32:01.432
    STEP: Removing cronjob 01/29/23 03:33:01.441
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jan 29 03:33:01.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-3299" for this suite. 01/29/23 03:33:01.462
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:33:01.475
Jan 29 03:33:01.475: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename configmap 01/29/23 03:33:01.477
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:33:01.516
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:33:01.523
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
STEP: Creating configMap with name configmap-test-volume-1bb180d0-260c-45db-b665-41bd3c1760a4 01/29/23 03:33:01.529
STEP: Creating a pod to test consume configMaps 01/29/23 03:33:01.538
Jan 29 03:33:01.566: INFO: Waiting up to 5m0s for pod "pod-configmaps-8bfd1d9a-5a9b-456b-b5d7-53d71479a8d9" in namespace "configmap-2663" to be "Succeeded or Failed"
Jan 29 03:33:01.571: INFO: Pod "pod-configmaps-8bfd1d9a-5a9b-456b-b5d7-53d71479a8d9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.782521ms
Jan 29 03:33:03.581: INFO: Pod "pod-configmaps-8bfd1d9a-5a9b-456b-b5d7-53d71479a8d9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015742268s
Jan 29 03:33:05.581: INFO: Pod "pod-configmaps-8bfd1d9a-5a9b-456b-b5d7-53d71479a8d9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015338304s
STEP: Saw pod success 01/29/23 03:33:05.581
Jan 29 03:33:05.581: INFO: Pod "pod-configmaps-8bfd1d9a-5a9b-456b-b5d7-53d71479a8d9" satisfied condition "Succeeded or Failed"
Jan 29 03:33:05.587: INFO: Trying to get logs from node slave2 pod pod-configmaps-8bfd1d9a-5a9b-456b-b5d7-53d71479a8d9 container agnhost-container: <nil>
STEP: delete the pod 01/29/23 03:33:05.616
Jan 29 03:33:05.706: INFO: Waiting for pod pod-configmaps-8bfd1d9a-5a9b-456b-b5d7-53d71479a8d9 to disappear
Jan 29 03:33:05.711: INFO: Pod pod-configmaps-8bfd1d9a-5a9b-456b-b5d7-53d71479a8d9 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 29 03:33:05.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2663" for this suite. 01/29/23 03:33:05.721
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":137,"skipped":2653,"failed":0}
------------------------------
• [4.255 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:33:01.475
    Jan 29 03:33:01.475: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename configmap 01/29/23 03:33:01.477
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:33:01.516
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:33:01.523
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:56
    STEP: Creating configMap with name configmap-test-volume-1bb180d0-260c-45db-b665-41bd3c1760a4 01/29/23 03:33:01.529
    STEP: Creating a pod to test consume configMaps 01/29/23 03:33:01.538
    Jan 29 03:33:01.566: INFO: Waiting up to 5m0s for pod "pod-configmaps-8bfd1d9a-5a9b-456b-b5d7-53d71479a8d9" in namespace "configmap-2663" to be "Succeeded or Failed"
    Jan 29 03:33:01.571: INFO: Pod "pod-configmaps-8bfd1d9a-5a9b-456b-b5d7-53d71479a8d9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.782521ms
    Jan 29 03:33:03.581: INFO: Pod "pod-configmaps-8bfd1d9a-5a9b-456b-b5d7-53d71479a8d9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015742268s
    Jan 29 03:33:05.581: INFO: Pod "pod-configmaps-8bfd1d9a-5a9b-456b-b5d7-53d71479a8d9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015338304s
    STEP: Saw pod success 01/29/23 03:33:05.581
    Jan 29 03:33:05.581: INFO: Pod "pod-configmaps-8bfd1d9a-5a9b-456b-b5d7-53d71479a8d9" satisfied condition "Succeeded or Failed"
    Jan 29 03:33:05.587: INFO: Trying to get logs from node slave2 pod pod-configmaps-8bfd1d9a-5a9b-456b-b5d7-53d71479a8d9 container agnhost-container: <nil>
    STEP: delete the pod 01/29/23 03:33:05.616
    Jan 29 03:33:05.706: INFO: Waiting for pod pod-configmaps-8bfd1d9a-5a9b-456b-b5d7-53d71479a8d9 to disappear
    Jan 29 03:33:05.711: INFO: Pod pod-configmaps-8bfd1d9a-5a9b-456b-b5d7-53d71479a8d9 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 29 03:33:05.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-2663" for this suite. 01/29/23 03:33:05.721
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:33:05.732
Jan 29 03:33:05.732: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename kubectl 01/29/23 03:33:05.733
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:33:05.768
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:33:05.773
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
Jan 29 03:33:05.779: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-4578 create -f -'
Jan 29 03:33:07.590: INFO: stderr: ""
Jan 29 03:33:07.590: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Jan 29 03:33:07.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-4578 create -f -'
Jan 29 03:33:07.972: INFO: stderr: ""
Jan 29 03:33:07.972: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 01/29/23 03:33:07.972
Jan 29 03:33:08.980: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 29 03:33:08.980: INFO: Found 0 / 1
Jan 29 03:33:09.982: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 29 03:33:09.982: INFO: Found 1 / 1
Jan 29 03:33:09.982: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 29 03:33:09.988: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 29 03:33:09.988: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 29 03:33:09.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-4578 describe pod agnhost-primary-rbs9j'
Jan 29 03:33:10.119: INFO: stderr: ""
Jan 29 03:33:10.119: INFO: stdout: "Name:                 agnhost-primary-rbs9j\nNamespace:            kubectl-4578\nPriority:             10000000\nPriority Class Name:  priority-class-apps\nService Account:      default\nNode:                 slave2/192.168.122.245\nStart Time:           Sun, 29 Jan 2023 03:33:07 +0000\nLabels:               app=agnhost\n                      role=primary\nAnnotations:          k8s.v1.cni.cncf.io/network-status:\n                        [{\n                            \"name\": \"\",\n                            \"ips\": [\n                                \"100.101.49.9\"\n                            ],\n                            \"default\": true,\n                            \"dns\": {}\n                        }]\n                      k8s.v1.cni.cncf.io/networks-status:\n                        [{\n                            \"name\": \"\",\n                            \"ips\": [\n                                \"100.101.49.9\"\n                            ],\n                            \"default\": true,\n                            \"dns\": {}\n                        }]\nStatus:               Running\nIP:                   100.101.49.9\nIPs:\n  IP:           100.101.49.9\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   docker://4c037cdb6467de16b3943bb3f8369854b5280d56cbf8dd09eca26bf593351510\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.40\n    Image ID:       docker-pullable://registry-jinan-lab.inspurcloud.cn/library/cke/agnhost-arm64@sha256:3df3d52919cdbe24d42667d1be02605b720b80c8133f2203c4a78f77e3f2429e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Sun, 29 Jan 2023 03:33:08 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-wf4pz (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-wf4pz:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-4578/agnhost-primary-rbs9j to slave2\n  Normal  Pulled     2s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.40\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    2s    kubelet            Started container agnhost-primary\n"
Jan 29 03:33:10.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-4578 describe rc agnhost-primary'
Jan 29 03:33:10.246: INFO: stderr: ""
Jan 29 03:33:10.246: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-4578\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-rbs9j\n"
Jan 29 03:33:10.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-4578 describe service agnhost-primary'
Jan 29 03:33:10.369: INFO: stderr: ""
Jan 29 03:33:10.369: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-4578\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                100.105.101.93\nIPs:               100.105.101.93\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         100.101.49.9:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jan 29 03:33:10.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-4578 describe node master1'
Jan 29 03:33:10.549: INFO: stderr: ""
Jan 29 03:33:10.549: INFO: stdout: "Name:               master1\nRoles:              master,node\nLabels:             beta.kubernetes.io/arch=arm64\n                    beta.kubernetes.io/os=linux\n                    cie.inspur.com/cluster=true\n                    icp.cke-cluster=cke-v6-test-cluster\n                    kubernetes.io/arch=arm64\n                    kubernetes.io/hostname=master1\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=true\n                    node-role.kubernetes.io/node=true\n                    node.kubernetes.io/master=true\n                    node.kubernetes.io/node=true\n                    role.cke.inspur.com/nfs=pre\nAnnotations:        install_net_address: 192.168.122.241\n                    install_net_port: 6233\n                    instanceID: 6f4f6c99-b6dd-4ed6-92c7-30aaca4ac197\n                    node.alpha.kubernetes.io/ttl: 0\n                    resources: 4C/8G/100G\nCreationTimestamp:  Wed, 11 Jan 2023 07:48:39 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  master1\n  AcquireTime:     <unset>\n  RenewTime:       Sun, 29 Jan 2023 03:33:02 +0000\nConditions:\n  Type                        Status  LastHeartbeatTime                 LastTransitionTime                Reason                        Message\n  ----                        ------  -----------------                 ------------------                ------                        -------\n  ChronyProblem               False   Sun, 29 Jan 2023 03:32:21 +0000   Wed, 11 Jan 2023 07:52:17 +0000   ChronyIsUp                    chrony service is up\n  CorruptDockerOverlay2       False   Sun, 29 Jan 2023 03:32:21 +0000   Wed, 11 Jan 2023 07:52:17 +0000   NoCorruptDockerOverlay2       docker overlay2 is functioning properly\n  FrequentDockerRestart       False   Sun, 29 Jan 2023 03:32:21 +0000   Wed, 11 Jan 2023 08:02:33 +0000   NoFrequentDockerRestart       docker is functioning properly\n  KubeletUnhealthy            False   Sun, 29 Jan 2023 03:32:21 +0000   Wed, 11 Jan 2023 07:52:17 +0000   KubeletIsHealthy              kubelet on the node is functioning properly\n  ContainerRuntimeUnhealthy   False   Sun, 29 Jan 2023 03:32:21 +0000   Wed, 11 Jan 2023 07:52:17 +0000   ContainerRuntimeIsHealthy     Container runtime on the node is functioning properly\n  FrequentKubeletRestart      False   Sun, 29 Jan 2023 03:32:21 +0000   Wed, 11 Jan 2023 07:52:17 +0000   NoFrequentKubeletRestart      kubelet is functioning properly\n  ReadonlyFilesystem          False   Sun, 29 Jan 2023 03:32:21 +0000   Wed, 11 Jan 2023 07:52:17 +0000   FilesystemIsNotReadOnly       Filesystem is not read-only\n  DockerHung                  False   Sun, 29 Jan 2023 03:32:21 +0000   Wed, 11 Jan 2023 07:52:17 +0000   NoDockerHung                  docker is functioning properly\n  FrequentContainerdRestart   False   Sun, 29 Jan 2023 03:32:21 +0000   Wed, 11 Jan 2023 07:52:17 +0000   NoFrequentContainerdRestart   containerd is functioning properly\n  KernelDeadlock              False   Sun, 29 Jan 2023 03:32:21 +0000   Wed, 11 Jan 2023 07:52:17 +0000   KernelHasNoDeadlock           kernel has no deadlock\n  NetworkUnavailable          False   Wed, 11 Jan 2023 07:49:29 +0000   Wed, 11 Jan 2023 07:49:29 +0000   CalicoIsUp                    Calico is running on this node\n  MemoryPressure              False   Sun, 29 Jan 2023 03:33:08 +0000   Wed, 11 Jan 2023 07:48:39 +0000   KubeletHasSufficientMemory    kubelet has sufficient memory available\n  DiskPressure                False   Sun, 29 Jan 2023 03:33:08 +0000   Wed, 11 Jan 2023 07:48:39 +0000   KubeletHasNoDiskPressure      kubelet has no disk pressure\n  PIDPressure                 False   Sun, 29 Jan 2023 03:33:08 +0000   Wed, 11 Jan 2023 07:48:39 +0000   KubeletHasSufficientPID       kubelet has sufficient PID available\n  Ready                       True    Sun, 29 Jan 2023 03:33:08 +0000   Wed, 11 Jan 2023 07:48:50 +0000   KubeletReady                  kubelet is posting ready status\nAddresses:\n  InternalIP:  192.168.122.241\n  Hostname:    master1\nCapacity:\n  cpu:                4\n  ephemeral-storage:  92223720Ki\n  hugepages-2Mi:      0\n  hugepages-512Mi:    0\n  memory:             9065280Ki\n  pods:               70\nAllocatable:\n  cpu:                3\n  ephemeral-storage:  86980840Ki\n  hugepages-2Mi:      0\n  hugepages-512Mi:    0\n  memory:             7540992Ki\n  pods:               70\nSystem Info:\n  Machine ID:                 7d3268836fc841638a72d6b6bb43270a\n  System UUID:                7d326883-6fc8-4163-8a72-d6b6bb43270a\n  Boot ID:                    ab79621d-4985-4077-84ff-99ba80594a2d\n  Kernel Version:             4.19.90-25.9.v2101.ky10.aarch64\n  OS Image:                   Kylin Linux Advanced Server V10 (Sword)\n  Operating System:           linux\n  Architecture:               arm64\n  Container Runtime Version:  docker://19.3.14\n  Kubelet Version:            v1.25.4-1\n  Kube-Proxy Version:         v1.25.4-1\nNon-terminated Pods:          (14 in total)\n  Namespace                   Name                                        CPU Requests  CPU Limits   Memory Requests  Memory Limits  Age\n  ---------                   ----                                        ------------  ----------   ---------------  -------------  ---\n  kube-system                 calico-kube-controllers-5c6d4b68d6-6p9cm    30m (1%)      1 (33%)      64Mi (0%)        4Gi (55%)      17d\n  kube-system                 calico-node-j4qnr                           300m (10%)    1 (33%)      512Mi (6%)       1Gi (13%)      17d\n  kube-system                 cke-admission-daemonset-g5mvj               0 (0%)        0 (0%)       0 (0%)           0 (0%)         17d\n  kube-system                 cke-controller-manager-master1              100m (3%)     2 (66%)      100Mi (1%)       6Gi (83%)      17d\n  kube-system                 component-controller-manager-master1        100m (3%)     2 (66%)      100Mi (1%)       6Gi (83%)      17d\n  kube-system                 coredns-tntlt                               300m (10%)    1 (33%)      256Mi (3%)       512Mi (6%)     17d\n  kube-system                 keepalived-master1                          500m (16%)    1 (33%)      256Mi (3%)       1Gi (13%)      17d\n  kube-system                 kube-apiserver-master1                      500m (16%)    2 (66%)      1Gi (13%)        6Gi (83%)      17d\n  kube-system                 kube-controller-manager-master1             100m (3%)     2 (66%)      100Mi (1%)       6Gi (83%)      17d\n  kube-system                 kube-multus-ds-8b89r                        100m (3%)     500m (16%)   50Mi (0%)        300Mi (4%)     17d\n  kube-system                 kube-proxy-master1                          500m (16%)    2 (66%)      512M (6%)        2G (25%)       17d\n  kube-system                 kube-scheduler-master1                      80m (2%)      2 (66%)      170Mi (2%)       6Gi (83%)      17d\n  kube-system                 nginx-proxy-master1                         150m (5%)     1500m (50%)  192Mi (2%)       4Gi (55%)      17d\n  kube-system                 node-problem-detector-dgrmp                 50m (1%)      500m (16%)   80Mi (1%)        1Gi (13%)      17d\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests         Limits\n  --------           --------         ------\n  cpu                2810m (93%)      18500m (616%)\n  memory             3473696Ki (46%)  45776229Ki (607%)\n  ephemeral-storage  0 (0%)           0 (0%)\n  hugepages-2Mi      0 (0%)           0 (0%)\n  hugepages-512Mi    0 (0%)           0 (0%)\nEvents:              <none>\n"
Jan 29 03:33:10.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-4578 describe namespace kubectl-4578'
Jan 29 03:33:10.685: INFO: stderr: ""
Jan 29 03:33:10.685: INFO: stdout: "Name:         kubectl-4578\nLabels:       e2e-framework=kubectl\n              e2e-run=28d95024-e2ae-4f1a-8336-248a37063b71\n              kubernetes.io/metadata.name=kubectl-4578\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 29 03:33:10.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4578" for this suite. 01/29/23 03:33:10.696
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","completed":138,"skipped":2668,"failed":0}
------------------------------
• [4.974 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1268
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1274

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:33:05.732
    Jan 29 03:33:05.732: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename kubectl 01/29/23 03:33:05.733
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:33:05.768
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:33:05.773
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1274
    Jan 29 03:33:05.779: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-4578 create -f -'
    Jan 29 03:33:07.590: INFO: stderr: ""
    Jan 29 03:33:07.590: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Jan 29 03:33:07.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-4578 create -f -'
    Jan 29 03:33:07.972: INFO: stderr: ""
    Jan 29 03:33:07.972: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 01/29/23 03:33:07.972
    Jan 29 03:33:08.980: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 29 03:33:08.980: INFO: Found 0 / 1
    Jan 29 03:33:09.982: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 29 03:33:09.982: INFO: Found 1 / 1
    Jan 29 03:33:09.982: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Jan 29 03:33:09.988: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 29 03:33:09.988: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jan 29 03:33:09.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-4578 describe pod agnhost-primary-rbs9j'
    Jan 29 03:33:10.119: INFO: stderr: ""
    Jan 29 03:33:10.119: INFO: stdout: "Name:                 agnhost-primary-rbs9j\nNamespace:            kubectl-4578\nPriority:             10000000\nPriority Class Name:  priority-class-apps\nService Account:      default\nNode:                 slave2/192.168.122.245\nStart Time:           Sun, 29 Jan 2023 03:33:07 +0000\nLabels:               app=agnhost\n                      role=primary\nAnnotations:          k8s.v1.cni.cncf.io/network-status:\n                        [{\n                            \"name\": \"\",\n                            \"ips\": [\n                                \"100.101.49.9\"\n                            ],\n                            \"default\": true,\n                            \"dns\": {}\n                        }]\n                      k8s.v1.cni.cncf.io/networks-status:\n                        [{\n                            \"name\": \"\",\n                            \"ips\": [\n                                \"100.101.49.9\"\n                            ],\n                            \"default\": true,\n                            \"dns\": {}\n                        }]\nStatus:               Running\nIP:                   100.101.49.9\nIPs:\n  IP:           100.101.49.9\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   docker://4c037cdb6467de16b3943bb3f8369854b5280d56cbf8dd09eca26bf593351510\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.40\n    Image ID:       docker-pullable://registry-jinan-lab.inspurcloud.cn/library/cke/agnhost-arm64@sha256:3df3d52919cdbe24d42667d1be02605b720b80c8133f2203c4a78f77e3f2429e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Sun, 29 Jan 2023 03:33:08 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-wf4pz (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-wf4pz:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-4578/agnhost-primary-rbs9j to slave2\n  Normal  Pulled     2s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.40\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    2s    kubelet            Started container agnhost-primary\n"
    Jan 29 03:33:10.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-4578 describe rc agnhost-primary'
    Jan 29 03:33:10.246: INFO: stderr: ""
    Jan 29 03:33:10.246: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-4578\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-rbs9j\n"
    Jan 29 03:33:10.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-4578 describe service agnhost-primary'
    Jan 29 03:33:10.369: INFO: stderr: ""
    Jan 29 03:33:10.369: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-4578\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                100.105.101.93\nIPs:               100.105.101.93\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         100.101.49.9:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Jan 29 03:33:10.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-4578 describe node master1'
    Jan 29 03:33:10.549: INFO: stderr: ""
    Jan 29 03:33:10.549: INFO: stdout: "Name:               master1\nRoles:              master,node\nLabels:             beta.kubernetes.io/arch=arm64\n                    beta.kubernetes.io/os=linux\n                    cie.inspur.com/cluster=true\n                    icp.cke-cluster=cke-v6-test-cluster\n                    kubernetes.io/arch=arm64\n                    kubernetes.io/hostname=master1\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=true\n                    node-role.kubernetes.io/node=true\n                    node.kubernetes.io/master=true\n                    node.kubernetes.io/node=true\n                    role.cke.inspur.com/nfs=pre\nAnnotations:        install_net_address: 192.168.122.241\n                    install_net_port: 6233\n                    instanceID: 6f4f6c99-b6dd-4ed6-92c7-30aaca4ac197\n                    node.alpha.kubernetes.io/ttl: 0\n                    resources: 4C/8G/100G\nCreationTimestamp:  Wed, 11 Jan 2023 07:48:39 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  master1\n  AcquireTime:     <unset>\n  RenewTime:       Sun, 29 Jan 2023 03:33:02 +0000\nConditions:\n  Type                        Status  LastHeartbeatTime                 LastTransitionTime                Reason                        Message\n  ----                        ------  -----------------                 ------------------                ------                        -------\n  ChronyProblem               False   Sun, 29 Jan 2023 03:32:21 +0000   Wed, 11 Jan 2023 07:52:17 +0000   ChronyIsUp                    chrony service is up\n  CorruptDockerOverlay2       False   Sun, 29 Jan 2023 03:32:21 +0000   Wed, 11 Jan 2023 07:52:17 +0000   NoCorruptDockerOverlay2       docker overlay2 is functioning properly\n  FrequentDockerRestart       False   Sun, 29 Jan 2023 03:32:21 +0000   Wed, 11 Jan 2023 08:02:33 +0000   NoFrequentDockerRestart       docker is functioning properly\n  KubeletUnhealthy            False   Sun, 29 Jan 2023 03:32:21 +0000   Wed, 11 Jan 2023 07:52:17 +0000   KubeletIsHealthy              kubelet on the node is functioning properly\n  ContainerRuntimeUnhealthy   False   Sun, 29 Jan 2023 03:32:21 +0000   Wed, 11 Jan 2023 07:52:17 +0000   ContainerRuntimeIsHealthy     Container runtime on the node is functioning properly\n  FrequentKubeletRestart      False   Sun, 29 Jan 2023 03:32:21 +0000   Wed, 11 Jan 2023 07:52:17 +0000   NoFrequentKubeletRestart      kubelet is functioning properly\n  ReadonlyFilesystem          False   Sun, 29 Jan 2023 03:32:21 +0000   Wed, 11 Jan 2023 07:52:17 +0000   FilesystemIsNotReadOnly       Filesystem is not read-only\n  DockerHung                  False   Sun, 29 Jan 2023 03:32:21 +0000   Wed, 11 Jan 2023 07:52:17 +0000   NoDockerHung                  docker is functioning properly\n  FrequentContainerdRestart   False   Sun, 29 Jan 2023 03:32:21 +0000   Wed, 11 Jan 2023 07:52:17 +0000   NoFrequentContainerdRestart   containerd is functioning properly\n  KernelDeadlock              False   Sun, 29 Jan 2023 03:32:21 +0000   Wed, 11 Jan 2023 07:52:17 +0000   KernelHasNoDeadlock           kernel has no deadlock\n  NetworkUnavailable          False   Wed, 11 Jan 2023 07:49:29 +0000   Wed, 11 Jan 2023 07:49:29 +0000   CalicoIsUp                    Calico is running on this node\n  MemoryPressure              False   Sun, 29 Jan 2023 03:33:08 +0000   Wed, 11 Jan 2023 07:48:39 +0000   KubeletHasSufficientMemory    kubelet has sufficient memory available\n  DiskPressure                False   Sun, 29 Jan 2023 03:33:08 +0000   Wed, 11 Jan 2023 07:48:39 +0000   KubeletHasNoDiskPressure      kubelet has no disk pressure\n  PIDPressure                 False   Sun, 29 Jan 2023 03:33:08 +0000   Wed, 11 Jan 2023 07:48:39 +0000   KubeletHasSufficientPID       kubelet has sufficient PID available\n  Ready                       True    Sun, 29 Jan 2023 03:33:08 +0000   Wed, 11 Jan 2023 07:48:50 +0000   KubeletReady                  kubelet is posting ready status\nAddresses:\n  InternalIP:  192.168.122.241\n  Hostname:    master1\nCapacity:\n  cpu:                4\n  ephemeral-storage:  92223720Ki\n  hugepages-2Mi:      0\n  hugepages-512Mi:    0\n  memory:             9065280Ki\n  pods:               70\nAllocatable:\n  cpu:                3\n  ephemeral-storage:  86980840Ki\n  hugepages-2Mi:      0\n  hugepages-512Mi:    0\n  memory:             7540992Ki\n  pods:               70\nSystem Info:\n  Machine ID:                 7d3268836fc841638a72d6b6bb43270a\n  System UUID:                7d326883-6fc8-4163-8a72-d6b6bb43270a\n  Boot ID:                    ab79621d-4985-4077-84ff-99ba80594a2d\n  Kernel Version:             4.19.90-25.9.v2101.ky10.aarch64\n  OS Image:                   Kylin Linux Advanced Server V10 (Sword)\n  Operating System:           linux\n  Architecture:               arm64\n  Container Runtime Version:  docker://19.3.14\n  Kubelet Version:            v1.25.4-1\n  Kube-Proxy Version:         v1.25.4-1\nNon-terminated Pods:          (14 in total)\n  Namespace                   Name                                        CPU Requests  CPU Limits   Memory Requests  Memory Limits  Age\n  ---------                   ----                                        ------------  ----------   ---------------  -------------  ---\n  kube-system                 calico-kube-controllers-5c6d4b68d6-6p9cm    30m (1%)      1 (33%)      64Mi (0%)        4Gi (55%)      17d\n  kube-system                 calico-node-j4qnr                           300m (10%)    1 (33%)      512Mi (6%)       1Gi (13%)      17d\n  kube-system                 cke-admission-daemonset-g5mvj               0 (0%)        0 (0%)       0 (0%)           0 (0%)         17d\n  kube-system                 cke-controller-manager-master1              100m (3%)     2 (66%)      100Mi (1%)       6Gi (83%)      17d\n  kube-system                 component-controller-manager-master1        100m (3%)     2 (66%)      100Mi (1%)       6Gi (83%)      17d\n  kube-system                 coredns-tntlt                               300m (10%)    1 (33%)      256Mi (3%)       512Mi (6%)     17d\n  kube-system                 keepalived-master1                          500m (16%)    1 (33%)      256Mi (3%)       1Gi (13%)      17d\n  kube-system                 kube-apiserver-master1                      500m (16%)    2 (66%)      1Gi (13%)        6Gi (83%)      17d\n  kube-system                 kube-controller-manager-master1             100m (3%)     2 (66%)      100Mi (1%)       6Gi (83%)      17d\n  kube-system                 kube-multus-ds-8b89r                        100m (3%)     500m (16%)   50Mi (0%)        300Mi (4%)     17d\n  kube-system                 kube-proxy-master1                          500m (16%)    2 (66%)      512M (6%)        2G (25%)       17d\n  kube-system                 kube-scheduler-master1                      80m (2%)      2 (66%)      170Mi (2%)       6Gi (83%)      17d\n  kube-system                 nginx-proxy-master1                         150m (5%)     1500m (50%)  192Mi (2%)       4Gi (55%)      17d\n  kube-system                 node-problem-detector-dgrmp                 50m (1%)      500m (16%)   80Mi (1%)        1Gi (13%)      17d\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests         Limits\n  --------           --------         ------\n  cpu                2810m (93%)      18500m (616%)\n  memory             3473696Ki (46%)  45776229Ki (607%)\n  ephemeral-storage  0 (0%)           0 (0%)\n  hugepages-2Mi      0 (0%)           0 (0%)\n  hugepages-512Mi    0 (0%)           0 (0%)\nEvents:              <none>\n"
    Jan 29 03:33:10.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-4578 describe namespace kubectl-4578'
    Jan 29 03:33:10.685: INFO: stderr: ""
    Jan 29 03:33:10.685: INFO: stdout: "Name:         kubectl-4578\nLabels:       e2e-framework=kubectl\n              e2e-run=28d95024-e2ae-4f1a-8336-248a37063b71\n              kubernetes.io/metadata.name=kubectl-4578\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 29 03:33:10.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-4578" for this suite. 01/29/23 03:33:10.696
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:33:10.709
Jan 29 03:33:10.709: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename configmap 01/29/23 03:33:10.71
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:33:10.742
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:33:10.748
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
STEP: Creating configMap with name configmap-test-upd-0aea1027-509a-4c47-b0e6-59c87605f727 01/29/23 03:33:10.763
STEP: Creating the pod 01/29/23 03:33:10.773
Jan 29 03:33:10.797: INFO: Waiting up to 5m0s for pod "pod-configmaps-a2dcd92d-a48d-42a3-9350-0a4eb4145963" in namespace "configmap-4090" to be "running and ready"
Jan 29 03:33:10.805: INFO: Pod "pod-configmaps-a2dcd92d-a48d-42a3-9350-0a4eb4145963": Phase="Pending", Reason="", readiness=false. Elapsed: 7.734014ms
Jan 29 03:33:10.805: INFO: The phase of Pod pod-configmaps-a2dcd92d-a48d-42a3-9350-0a4eb4145963 is Pending, waiting for it to be Running (with Ready = true)
Jan 29 03:33:12.811: INFO: Pod "pod-configmaps-a2dcd92d-a48d-42a3-9350-0a4eb4145963": Phase="Running", Reason="", readiness=true. Elapsed: 2.014429979s
Jan 29 03:33:12.811: INFO: The phase of Pod pod-configmaps-a2dcd92d-a48d-42a3-9350-0a4eb4145963 is Running (Ready = true)
Jan 29 03:33:12.811: INFO: Pod "pod-configmaps-a2dcd92d-a48d-42a3-9350-0a4eb4145963" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-0aea1027-509a-4c47-b0e6-59c87605f727 01/29/23 03:33:12.833
STEP: waiting to observe update in volume 01/29/23 03:33:12.842
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 29 03:33:14.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4090" for this suite. 01/29/23 03:33:14.883
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":139,"skipped":2701,"failed":0}
------------------------------
• [4.191 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:33:10.709
    Jan 29 03:33:10.709: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename configmap 01/29/23 03:33:10.71
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:33:10.742
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:33:10.748
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:123
    STEP: Creating configMap with name configmap-test-upd-0aea1027-509a-4c47-b0e6-59c87605f727 01/29/23 03:33:10.763
    STEP: Creating the pod 01/29/23 03:33:10.773
    Jan 29 03:33:10.797: INFO: Waiting up to 5m0s for pod "pod-configmaps-a2dcd92d-a48d-42a3-9350-0a4eb4145963" in namespace "configmap-4090" to be "running and ready"
    Jan 29 03:33:10.805: INFO: Pod "pod-configmaps-a2dcd92d-a48d-42a3-9350-0a4eb4145963": Phase="Pending", Reason="", readiness=false. Elapsed: 7.734014ms
    Jan 29 03:33:10.805: INFO: The phase of Pod pod-configmaps-a2dcd92d-a48d-42a3-9350-0a4eb4145963 is Pending, waiting for it to be Running (with Ready = true)
    Jan 29 03:33:12.811: INFO: Pod "pod-configmaps-a2dcd92d-a48d-42a3-9350-0a4eb4145963": Phase="Running", Reason="", readiness=true. Elapsed: 2.014429979s
    Jan 29 03:33:12.811: INFO: The phase of Pod pod-configmaps-a2dcd92d-a48d-42a3-9350-0a4eb4145963 is Running (Ready = true)
    Jan 29 03:33:12.811: INFO: Pod "pod-configmaps-a2dcd92d-a48d-42a3-9350-0a4eb4145963" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-0aea1027-509a-4c47-b0e6-59c87605f727 01/29/23 03:33:12.833
    STEP: waiting to observe update in volume 01/29/23 03:33:12.842
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 29 03:33:14.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-4090" for this suite. 01/29/23 03:33:14.883
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:33:14.9
Jan 29 03:33:14.900: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename controllerrevisions 01/29/23 03:33:14.901
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:33:14.93
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:33:14.936
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-vphzj-daemon-set" 01/29/23 03:33:15.004
STEP: Check that daemon pods launch on every node of the cluster. 01/29/23 03:33:15.016
Jan 29 03:33:15.032: INFO: Number of nodes with available pods controlled by daemonset e2e-vphzj-daemon-set: 0
Jan 29 03:33:15.032: INFO: Node master1 is running 0 daemon pod, expected 1
Jan 29 03:33:16.060: INFO: Number of nodes with available pods controlled by daemonset e2e-vphzj-daemon-set: 0
Jan 29 03:33:16.060: INFO: Node master1 is running 0 daemon pod, expected 1
Jan 29 03:33:17.052: INFO: Number of nodes with available pods controlled by daemonset e2e-vphzj-daemon-set: 1
Jan 29 03:33:17.052: INFO: Node master1 is running 0 daemon pod, expected 1
Jan 29 03:33:18.055: INFO: Number of nodes with available pods controlled by daemonset e2e-vphzj-daemon-set: 5
Jan 29 03:33:18.055: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset e2e-vphzj-daemon-set
STEP: Confirm DaemonSet "e2e-vphzj-daemon-set" successfully created with "daemonset-name=e2e-vphzj-daemon-set" label 01/29/23 03:33:18.065
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-vphzj-daemon-set" 01/29/23 03:33:18.085
Jan 29 03:33:18.098: INFO: Located ControllerRevision: "e2e-vphzj-daemon-set-5fd557bbc7"
STEP: Patching ControllerRevision "e2e-vphzj-daemon-set-5fd557bbc7" 01/29/23 03:33:18.11
Jan 29 03:33:18.123: INFO: e2e-vphzj-daemon-set-5fd557bbc7 has been patched
STEP: Create a new ControllerRevision 01/29/23 03:33:18.124
Jan 29 03:33:18.134: INFO: Created ControllerRevision: e2e-vphzj-daemon-set-964587b5c
STEP: Confirm that there are two ControllerRevisions 01/29/23 03:33:18.135
Jan 29 03:33:18.135: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 29 03:33:18.145: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-vphzj-daemon-set-5fd557bbc7" 01/29/23 03:33:18.145
STEP: Confirm that there is only one ControllerRevision 01/29/23 03:33:18.161
Jan 29 03:33:18.162: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 29 03:33:18.171: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-vphzj-daemon-set-964587b5c" 01/29/23 03:33:18.18
Jan 29 03:33:18.196: INFO: e2e-vphzj-daemon-set-964587b5c has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 01/29/23 03:33:18.196
W0129 03:33:18.209740      22 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 01/29/23 03:33:18.209
Jan 29 03:33:18.210: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 29 03:33:19.220: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 29 03:33:19.228: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-vphzj-daemon-set-964587b5c=updated" 01/29/23 03:33:19.228
STEP: Confirm that there is only one ControllerRevision 01/29/23 03:33:19.245
Jan 29 03:33:19.245: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 29 03:33:19.252: INFO: Found 1 ControllerRevisions
Jan 29 03:33:19.266: INFO: ControllerRevision "e2e-vphzj-daemon-set-c567d96d4" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-vphzj-daemon-set" 01/29/23 03:33:19.272
STEP: deleting DaemonSet.extensions e2e-vphzj-daemon-set in namespace controllerrevisions-73, will wait for the garbage collector to delete the pods 01/29/23 03:33:19.272
Jan 29 03:33:19.339: INFO: Deleting DaemonSet.extensions e2e-vphzj-daemon-set took: 9.99983ms
Jan 29 03:33:19.440: INFO: Terminating DaemonSet.extensions e2e-vphzj-daemon-set pods took: 100.814985ms
Jan 29 03:33:22.247: INFO: Number of nodes with available pods controlled by daemonset e2e-vphzj-daemon-set: 0
Jan 29 03:33:22.247: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-vphzj-daemon-set
Jan 29 03:33:22.253: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"5960459"},"items":null}

Jan 29 03:33:22.258: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"5960459"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:187
Jan 29 03:33:22.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "controllerrevisions-73" for this suite. 01/29/23 03:33:22.309
{"msg":"PASSED [sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]","completed":140,"skipped":2701,"failed":0}
------------------------------
• [SLOW TEST] [7.423 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:33:14.9
    Jan 29 03:33:14.900: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename controllerrevisions 01/29/23 03:33:14.901
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:33:14.93
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:33:14.936
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-vphzj-daemon-set" 01/29/23 03:33:15.004
    STEP: Check that daemon pods launch on every node of the cluster. 01/29/23 03:33:15.016
    Jan 29 03:33:15.032: INFO: Number of nodes with available pods controlled by daemonset e2e-vphzj-daemon-set: 0
    Jan 29 03:33:15.032: INFO: Node master1 is running 0 daemon pod, expected 1
    Jan 29 03:33:16.060: INFO: Number of nodes with available pods controlled by daemonset e2e-vphzj-daemon-set: 0
    Jan 29 03:33:16.060: INFO: Node master1 is running 0 daemon pod, expected 1
    Jan 29 03:33:17.052: INFO: Number of nodes with available pods controlled by daemonset e2e-vphzj-daemon-set: 1
    Jan 29 03:33:17.052: INFO: Node master1 is running 0 daemon pod, expected 1
    Jan 29 03:33:18.055: INFO: Number of nodes with available pods controlled by daemonset e2e-vphzj-daemon-set: 5
    Jan 29 03:33:18.055: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset e2e-vphzj-daemon-set
    STEP: Confirm DaemonSet "e2e-vphzj-daemon-set" successfully created with "daemonset-name=e2e-vphzj-daemon-set" label 01/29/23 03:33:18.065
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-vphzj-daemon-set" 01/29/23 03:33:18.085
    Jan 29 03:33:18.098: INFO: Located ControllerRevision: "e2e-vphzj-daemon-set-5fd557bbc7"
    STEP: Patching ControllerRevision "e2e-vphzj-daemon-set-5fd557bbc7" 01/29/23 03:33:18.11
    Jan 29 03:33:18.123: INFO: e2e-vphzj-daemon-set-5fd557bbc7 has been patched
    STEP: Create a new ControllerRevision 01/29/23 03:33:18.124
    Jan 29 03:33:18.134: INFO: Created ControllerRevision: e2e-vphzj-daemon-set-964587b5c
    STEP: Confirm that there are two ControllerRevisions 01/29/23 03:33:18.135
    Jan 29 03:33:18.135: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 29 03:33:18.145: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-vphzj-daemon-set-5fd557bbc7" 01/29/23 03:33:18.145
    STEP: Confirm that there is only one ControllerRevision 01/29/23 03:33:18.161
    Jan 29 03:33:18.162: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 29 03:33:18.171: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-vphzj-daemon-set-964587b5c" 01/29/23 03:33:18.18
    Jan 29 03:33:18.196: INFO: e2e-vphzj-daemon-set-964587b5c has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 01/29/23 03:33:18.196
    W0129 03:33:18.209740      22 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 01/29/23 03:33:18.209
    Jan 29 03:33:18.210: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 29 03:33:19.220: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 29 03:33:19.228: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-vphzj-daemon-set-964587b5c=updated" 01/29/23 03:33:19.228
    STEP: Confirm that there is only one ControllerRevision 01/29/23 03:33:19.245
    Jan 29 03:33:19.245: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 29 03:33:19.252: INFO: Found 1 ControllerRevisions
    Jan 29 03:33:19.266: INFO: ControllerRevision "e2e-vphzj-daemon-set-c567d96d4" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-vphzj-daemon-set" 01/29/23 03:33:19.272
    STEP: deleting DaemonSet.extensions e2e-vphzj-daemon-set in namespace controllerrevisions-73, will wait for the garbage collector to delete the pods 01/29/23 03:33:19.272
    Jan 29 03:33:19.339: INFO: Deleting DaemonSet.extensions e2e-vphzj-daemon-set took: 9.99983ms
    Jan 29 03:33:19.440: INFO: Terminating DaemonSet.extensions e2e-vphzj-daemon-set pods took: 100.814985ms
    Jan 29 03:33:22.247: INFO: Number of nodes with available pods controlled by daemonset e2e-vphzj-daemon-set: 0
    Jan 29 03:33:22.247: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-vphzj-daemon-set
    Jan 29 03:33:22.253: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"5960459"},"items":null}

    Jan 29 03:33:22.258: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"5960459"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:187
    Jan 29 03:33:22.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "controllerrevisions-73" for this suite. 01/29/23 03:33:22.309
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:33:22.323
Jan 29 03:33:22.324: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename resourcequota 01/29/23 03:33:22.325
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:33:22.355
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:33:22.362
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
STEP: Creating a ResourceQuota with best effort scope 01/29/23 03:33:22.368
STEP: Ensuring ResourceQuota status is calculated 01/29/23 03:33:22.377
STEP: Creating a ResourceQuota with not best effort scope 01/29/23 03:33:24.385
STEP: Ensuring ResourceQuota status is calculated 01/29/23 03:33:24.393
STEP: Creating a best-effort pod 01/29/23 03:33:26.402
STEP: Ensuring resource quota with best effort scope captures the pod usage 01/29/23 03:33:26.428
STEP: Ensuring resource quota with not best effort ignored the pod usage 01/29/23 03:33:28.436
STEP: Deleting the pod 01/29/23 03:33:30.445
STEP: Ensuring resource quota status released the pod usage 01/29/23 03:33:30.524
STEP: Creating a not best-effort pod 01/29/23 03:33:32.533
STEP: Ensuring resource quota with not best effort scope captures the pod usage 01/29/23 03:33:32.553
STEP: Ensuring resource quota with best effort scope ignored the pod usage 01/29/23 03:33:34.567
STEP: Deleting the pod 01/29/23 03:33:36.576
STEP: Ensuring resource quota status released the pod usage 01/29/23 03:33:36.645
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 29 03:33:38.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5668" for this suite. 01/29/23 03:33:38.665
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","completed":141,"skipped":2710,"failed":0}
------------------------------
• [SLOW TEST] [16.353 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:33:22.323
    Jan 29 03:33:22.324: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename resourcequota 01/29/23 03:33:22.325
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:33:22.355
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:33:22.362
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:793
    STEP: Creating a ResourceQuota with best effort scope 01/29/23 03:33:22.368
    STEP: Ensuring ResourceQuota status is calculated 01/29/23 03:33:22.377
    STEP: Creating a ResourceQuota with not best effort scope 01/29/23 03:33:24.385
    STEP: Ensuring ResourceQuota status is calculated 01/29/23 03:33:24.393
    STEP: Creating a best-effort pod 01/29/23 03:33:26.402
    STEP: Ensuring resource quota with best effort scope captures the pod usage 01/29/23 03:33:26.428
    STEP: Ensuring resource quota with not best effort ignored the pod usage 01/29/23 03:33:28.436
    STEP: Deleting the pod 01/29/23 03:33:30.445
    STEP: Ensuring resource quota status released the pod usage 01/29/23 03:33:30.524
    STEP: Creating a not best-effort pod 01/29/23 03:33:32.533
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 01/29/23 03:33:32.553
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 01/29/23 03:33:34.567
    STEP: Deleting the pod 01/29/23 03:33:36.576
    STEP: Ensuring resource quota status released the pod usage 01/29/23 03:33:36.645
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 29 03:33:38.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-5668" for this suite. 01/29/23 03:33:38.665
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:33:38.677
Jan 29 03:33:38.677: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename downward-api 01/29/23 03:33:38.678
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:33:38.713
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:33:38.719
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
STEP: Creating a pod to test downward API volume plugin 01/29/23 03:33:38.725
Jan 29 03:33:38.746: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ef71fb00-2a81-415d-a4d7-f204dbcb3f2b" in namespace "downward-api-2780" to be "Succeeded or Failed"
Jan 29 03:33:38.758: INFO: Pod "downwardapi-volume-ef71fb00-2a81-415d-a4d7-f204dbcb3f2b": Phase="Pending", Reason="", readiness=false. Elapsed: 11.880483ms
Jan 29 03:33:40.767: INFO: Pod "downwardapi-volume-ef71fb00-2a81-415d-a4d7-f204dbcb3f2b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020186479s
Jan 29 03:33:42.765: INFO: Pod "downwardapi-volume-ef71fb00-2a81-415d-a4d7-f204dbcb3f2b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019065026s
STEP: Saw pod success 01/29/23 03:33:42.766
Jan 29 03:33:42.766: INFO: Pod "downwardapi-volume-ef71fb00-2a81-415d-a4d7-f204dbcb3f2b" satisfied condition "Succeeded or Failed"
Jan 29 03:33:42.771: INFO: Trying to get logs from node slave2 pod downwardapi-volume-ef71fb00-2a81-415d-a4d7-f204dbcb3f2b container client-container: <nil>
STEP: delete the pod 01/29/23 03:33:42.784
Jan 29 03:33:42.883: INFO: Waiting for pod downwardapi-volume-ef71fb00-2a81-415d-a4d7-f204dbcb3f2b to disappear
Jan 29 03:33:42.889: INFO: Pod downwardapi-volume-ef71fb00-2a81-415d-a4d7-f204dbcb3f2b no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 29 03:33:42.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2780" for this suite. 01/29/23 03:33:42.898
{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","completed":142,"skipped":2712,"failed":0}
------------------------------
• [4.233 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:33:38.677
    Jan 29 03:33:38.677: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename downward-api 01/29/23 03:33:38.678
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:33:38.713
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:33:38.719
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:52
    STEP: Creating a pod to test downward API volume plugin 01/29/23 03:33:38.725
    Jan 29 03:33:38.746: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ef71fb00-2a81-415d-a4d7-f204dbcb3f2b" in namespace "downward-api-2780" to be "Succeeded or Failed"
    Jan 29 03:33:38.758: INFO: Pod "downwardapi-volume-ef71fb00-2a81-415d-a4d7-f204dbcb3f2b": Phase="Pending", Reason="", readiness=false. Elapsed: 11.880483ms
    Jan 29 03:33:40.767: INFO: Pod "downwardapi-volume-ef71fb00-2a81-415d-a4d7-f204dbcb3f2b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020186479s
    Jan 29 03:33:42.765: INFO: Pod "downwardapi-volume-ef71fb00-2a81-415d-a4d7-f204dbcb3f2b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019065026s
    STEP: Saw pod success 01/29/23 03:33:42.766
    Jan 29 03:33:42.766: INFO: Pod "downwardapi-volume-ef71fb00-2a81-415d-a4d7-f204dbcb3f2b" satisfied condition "Succeeded or Failed"
    Jan 29 03:33:42.771: INFO: Trying to get logs from node slave2 pod downwardapi-volume-ef71fb00-2a81-415d-a4d7-f204dbcb3f2b container client-container: <nil>
    STEP: delete the pod 01/29/23 03:33:42.784
    Jan 29 03:33:42.883: INFO: Waiting for pod downwardapi-volume-ef71fb00-2a81-415d-a4d7-f204dbcb3f2b to disappear
    Jan 29 03:33:42.889: INFO: Pod downwardapi-volume-ef71fb00-2a81-415d-a4d7-f204dbcb3f2b no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 29 03:33:42.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-2780" for this suite. 01/29/23 03:33:42.898
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:33:42.911
Jan 29 03:33:42.912: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename hostport 01/29/23 03:33:42.913
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:33:42.942
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:33:42.95
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 01/29/23 03:33:42.969
Jan 29 03:33:42.989: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-8987" to be "running and ready"
Jan 29 03:33:42.995: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.570706ms
Jan 29 03:33:42.995: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 29 03:33:45.003: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014445764s
Jan 29 03:33:45.003: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 29 03:33:47.004: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.015250273s
Jan 29 03:33:47.004: INFO: The phase of Pod pod1 is Running (Ready = true)
Jan 29 03:33:47.004: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 192.168.122.243 on the node which pod1 resides and expect scheduled 01/29/23 03:33:47.004
Jan 29 03:33:47.018: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-8987" to be "running and ready"
Jan 29 03:33:47.024: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.283344ms
Jan 29 03:33:47.024: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 29 03:33:49.032: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014585805s
Jan 29 03:33:49.032: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 29 03:33:51.032: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.014274026s
Jan 29 03:33:51.032: INFO: The phase of Pod pod2 is Running (Ready = true)
Jan 29 03:33:51.032: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 192.168.122.243 but use UDP protocol on the node which pod2 resides 01/29/23 03:33:51.032
Jan 29 03:33:51.045: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-8987" to be "running and ready"
Jan 29 03:33:51.052: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.399225ms
Jan 29 03:33:51.052: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Jan 29 03:33:53.060: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.014537145s
Jan 29 03:33:53.060: INFO: The phase of Pod pod3 is Running (Ready = true)
Jan 29 03:33:53.060: INFO: Pod "pod3" satisfied condition "running and ready"
Jan 29 03:33:53.074: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-8987" to be "running and ready"
Jan 29 03:33:53.080: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 5.635259ms
Jan 29 03:33:53.080: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Jan 29 03:33:55.088: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.014236463s
Jan 29 03:33:55.088: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Jan 29 03:33:55.088: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 01/29/23 03:33:55.094
Jan 29 03:33:55.095: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 192.168.122.243 http://127.0.0.1:54323/hostname] Namespace:hostport-8987 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 29 03:33:55.095: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
Jan 29 03:33:55.095: INFO: ExecWithOptions: Clientset creation
Jan 29 03:33:55.095: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/hostport-8987/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+192.168.122.243+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.122.243, port: 54323 01/29/23 03:33:55.253
Jan 29 03:33:55.253: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://192.168.122.243:54323/hostname] Namespace:hostport-8987 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 29 03:33:55.253: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
Jan 29 03:33:55.254: INFO: ExecWithOptions: Clientset creation
Jan 29 03:33:55.254: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/hostport-8987/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F192.168.122.243%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.122.243, port: 54323 UDP 01/29/23 03:33:55.421
Jan 29 03:33:55.421: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 192.168.122.243 54323] Namespace:hostport-8987 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 29 03:33:55.421: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
Jan 29 03:33:55.422: INFO: ExecWithOptions: Clientset creation
Jan 29 03:33:55.422: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/hostport-8987/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+192.168.122.243+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/framework.go:187
Jan 29 03:34:00.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-8987" for this suite. 01/29/23 03:34:00.602
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","completed":143,"skipped":2732,"failed":0}
------------------------------
• [SLOW TEST] [17.704 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:33:42.911
    Jan 29 03:33:42.912: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename hostport 01/29/23 03:33:42.913
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:33:42.942
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:33:42.95
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 01/29/23 03:33:42.969
    Jan 29 03:33:42.989: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-8987" to be "running and ready"
    Jan 29 03:33:42.995: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.570706ms
    Jan 29 03:33:42.995: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan 29 03:33:45.003: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014445764s
    Jan 29 03:33:45.003: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan 29 03:33:47.004: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.015250273s
    Jan 29 03:33:47.004: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jan 29 03:33:47.004: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 192.168.122.243 on the node which pod1 resides and expect scheduled 01/29/23 03:33:47.004
    Jan 29 03:33:47.018: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-8987" to be "running and ready"
    Jan 29 03:33:47.024: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.283344ms
    Jan 29 03:33:47.024: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 29 03:33:49.032: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014585805s
    Jan 29 03:33:49.032: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 29 03:33:51.032: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.014274026s
    Jan 29 03:33:51.032: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jan 29 03:33:51.032: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 192.168.122.243 but use UDP protocol on the node which pod2 resides 01/29/23 03:33:51.032
    Jan 29 03:33:51.045: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-8987" to be "running and ready"
    Jan 29 03:33:51.052: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.399225ms
    Jan 29 03:33:51.052: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Jan 29 03:33:53.060: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.014537145s
    Jan 29 03:33:53.060: INFO: The phase of Pod pod3 is Running (Ready = true)
    Jan 29 03:33:53.060: INFO: Pod "pod3" satisfied condition "running and ready"
    Jan 29 03:33:53.074: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-8987" to be "running and ready"
    Jan 29 03:33:53.080: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 5.635259ms
    Jan 29 03:33:53.080: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Jan 29 03:33:55.088: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.014236463s
    Jan 29 03:33:55.088: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Jan 29 03:33:55.088: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 01/29/23 03:33:55.094
    Jan 29 03:33:55.095: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 192.168.122.243 http://127.0.0.1:54323/hostname] Namespace:hostport-8987 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 29 03:33:55.095: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    Jan 29 03:33:55.095: INFO: ExecWithOptions: Clientset creation
    Jan 29 03:33:55.095: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/hostport-8987/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+192.168.122.243+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.122.243, port: 54323 01/29/23 03:33:55.253
    Jan 29 03:33:55.253: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://192.168.122.243:54323/hostname] Namespace:hostport-8987 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 29 03:33:55.253: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    Jan 29 03:33:55.254: INFO: ExecWithOptions: Clientset creation
    Jan 29 03:33:55.254: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/hostport-8987/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F192.168.122.243%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.122.243, port: 54323 UDP 01/29/23 03:33:55.421
    Jan 29 03:33:55.421: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 192.168.122.243 54323] Namespace:hostport-8987 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 29 03:33:55.421: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    Jan 29 03:33:55.422: INFO: ExecWithOptions: Clientset creation
    Jan 29 03:33:55.422: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/hostport-8987/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+192.168.122.243+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/framework.go:187
    Jan 29 03:34:00.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "hostport-8987" for this suite. 01/29/23 03:34:00.602
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:34:00.616
Jan 29 03:34:00.617: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename emptydir 01/29/23 03:34:00.618
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:34:00.648
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:34:00.655
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
STEP: Creating a pod to test emptydir 0644 on tmpfs 01/29/23 03:34:00.66
Jan 29 03:34:00.685: INFO: Waiting up to 5m0s for pod "pod-9120977a-15ee-4180-9d4c-40a233cbe313" in namespace "emptydir-312" to be "Succeeded or Failed"
Jan 29 03:34:00.691: INFO: Pod "pod-9120977a-15ee-4180-9d4c-40a233cbe313": Phase="Pending", Reason="", readiness=false. Elapsed: 6.114642ms
Jan 29 03:34:02.699: INFO: Pod "pod-9120977a-15ee-4180-9d4c-40a233cbe313": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014218142s
Jan 29 03:34:04.701: INFO: Pod "pod-9120977a-15ee-4180-9d4c-40a233cbe313": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016516941s
STEP: Saw pod success 01/29/23 03:34:04.702
Jan 29 03:34:04.702: INFO: Pod "pod-9120977a-15ee-4180-9d4c-40a233cbe313" satisfied condition "Succeeded or Failed"
Jan 29 03:34:04.708: INFO: Trying to get logs from node slave2 pod pod-9120977a-15ee-4180-9d4c-40a233cbe313 container test-container: <nil>
STEP: delete the pod 01/29/23 03:34:04.724
Jan 29 03:34:04.817: INFO: Waiting for pod pod-9120977a-15ee-4180-9d4c-40a233cbe313 to disappear
Jan 29 03:34:04.823: INFO: Pod pod-9120977a-15ee-4180-9d4c-40a233cbe313 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 29 03:34:04.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-312" for this suite. 01/29/23 03:34:04.833
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":144,"skipped":2737,"failed":0}
------------------------------
• [4.228 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:34:00.616
    Jan 29 03:34:00.617: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename emptydir 01/29/23 03:34:00.618
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:34:00.648
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:34:00.655
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:126
    STEP: Creating a pod to test emptydir 0644 on tmpfs 01/29/23 03:34:00.66
    Jan 29 03:34:00.685: INFO: Waiting up to 5m0s for pod "pod-9120977a-15ee-4180-9d4c-40a233cbe313" in namespace "emptydir-312" to be "Succeeded or Failed"
    Jan 29 03:34:00.691: INFO: Pod "pod-9120977a-15ee-4180-9d4c-40a233cbe313": Phase="Pending", Reason="", readiness=false. Elapsed: 6.114642ms
    Jan 29 03:34:02.699: INFO: Pod "pod-9120977a-15ee-4180-9d4c-40a233cbe313": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014218142s
    Jan 29 03:34:04.701: INFO: Pod "pod-9120977a-15ee-4180-9d4c-40a233cbe313": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016516941s
    STEP: Saw pod success 01/29/23 03:34:04.702
    Jan 29 03:34:04.702: INFO: Pod "pod-9120977a-15ee-4180-9d4c-40a233cbe313" satisfied condition "Succeeded or Failed"
    Jan 29 03:34:04.708: INFO: Trying to get logs from node slave2 pod pod-9120977a-15ee-4180-9d4c-40a233cbe313 container test-container: <nil>
    STEP: delete the pod 01/29/23 03:34:04.724
    Jan 29 03:34:04.817: INFO: Waiting for pod pod-9120977a-15ee-4180-9d4c-40a233cbe313 to disappear
    Jan 29 03:34:04.823: INFO: Pod pod-9120977a-15ee-4180-9d4c-40a233cbe313 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 29 03:34:04.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-312" for this suite. 01/29/23 03:34:04.833
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:34:04.845
Jan 29 03:34:04.846: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename projected 01/29/23 03:34:04.847
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:34:04.883
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:34:04.889
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
STEP: Creating projection with configMap that has name projected-configmap-test-upd-f4d4eaf6-fb78-40aa-b65f-96e2e221a2ce 01/29/23 03:34:04.903
STEP: Creating the pod 01/29/23 03:34:04.91
Jan 29 03:34:04.926: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c7343eae-a6b8-4791-8af1-c1fbcaccb3b8" in namespace "projected-1262" to be "running and ready"
Jan 29 03:34:04.931: INFO: Pod "pod-projected-configmaps-c7343eae-a6b8-4791-8af1-c1fbcaccb3b8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.574679ms
Jan 29 03:34:04.931: INFO: The phase of Pod pod-projected-configmaps-c7343eae-a6b8-4791-8af1-c1fbcaccb3b8 is Pending, waiting for it to be Running (with Ready = true)
Jan 29 03:34:06.939: INFO: Pod "pod-projected-configmaps-c7343eae-a6b8-4791-8af1-c1fbcaccb3b8": Phase="Running", Reason="", readiness=true. Elapsed: 2.013253936s
Jan 29 03:34:06.939: INFO: The phase of Pod pod-projected-configmaps-c7343eae-a6b8-4791-8af1-c1fbcaccb3b8 is Running (Ready = true)
Jan 29 03:34:06.939: INFO: Pod "pod-projected-configmaps-c7343eae-a6b8-4791-8af1-c1fbcaccb3b8" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-f4d4eaf6-fb78-40aa-b65f-96e2e221a2ce 01/29/23 03:34:06.962
STEP: waiting to observe update in volume 01/29/23 03:34:06.981
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 29 03:34:09.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1262" for this suite. 01/29/23 03:34:09.026
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":145,"skipped":2743,"failed":0}
------------------------------
• [4.191 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:34:04.845
    Jan 29 03:34:04.846: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename projected 01/29/23 03:34:04.847
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:34:04.883
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:34:04.889
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:123
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-f4d4eaf6-fb78-40aa-b65f-96e2e221a2ce 01/29/23 03:34:04.903
    STEP: Creating the pod 01/29/23 03:34:04.91
    Jan 29 03:34:04.926: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c7343eae-a6b8-4791-8af1-c1fbcaccb3b8" in namespace "projected-1262" to be "running and ready"
    Jan 29 03:34:04.931: INFO: Pod "pod-projected-configmaps-c7343eae-a6b8-4791-8af1-c1fbcaccb3b8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.574679ms
    Jan 29 03:34:04.931: INFO: The phase of Pod pod-projected-configmaps-c7343eae-a6b8-4791-8af1-c1fbcaccb3b8 is Pending, waiting for it to be Running (with Ready = true)
    Jan 29 03:34:06.939: INFO: Pod "pod-projected-configmaps-c7343eae-a6b8-4791-8af1-c1fbcaccb3b8": Phase="Running", Reason="", readiness=true. Elapsed: 2.013253936s
    Jan 29 03:34:06.939: INFO: The phase of Pod pod-projected-configmaps-c7343eae-a6b8-4791-8af1-c1fbcaccb3b8 is Running (Ready = true)
    Jan 29 03:34:06.939: INFO: Pod "pod-projected-configmaps-c7343eae-a6b8-4791-8af1-c1fbcaccb3b8" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-f4d4eaf6-fb78-40aa-b65f-96e2e221a2ce 01/29/23 03:34:06.962
    STEP: waiting to observe update in volume 01/29/23 03:34:06.981
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 29 03:34:09.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1262" for this suite. 01/29/23 03:34:09.026
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:34:09.037
Jan 29 03:34:09.038: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename resourcequota 01/29/23 03:34:09.039
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:34:09.067
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:34:09.072
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
STEP: Counting existing ResourceQuota 01/29/23 03:34:09.078
STEP: Creating a ResourceQuota 01/29/23 03:34:14.085
STEP: Ensuring resource quota status is calculated 01/29/23 03:34:14.094
STEP: Creating a ReplicationController 01/29/23 03:34:16.102
STEP: Ensuring resource quota status captures replication controller creation 01/29/23 03:34:16.117
STEP: Deleting a ReplicationController 01/29/23 03:34:18.126
STEP: Ensuring resource quota status released usage 01/29/23 03:34:18.137
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 29 03:34:20.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6204" for this suite. 01/29/23 03:34:20.155
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","completed":146,"skipped":2743,"failed":0}
------------------------------
• [SLOW TEST] [11.129 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:34:09.037
    Jan 29 03:34:09.038: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename resourcequota 01/29/23 03:34:09.039
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:34:09.067
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:34:09.072
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:382
    STEP: Counting existing ResourceQuota 01/29/23 03:34:09.078
    STEP: Creating a ResourceQuota 01/29/23 03:34:14.085
    STEP: Ensuring resource quota status is calculated 01/29/23 03:34:14.094
    STEP: Creating a ReplicationController 01/29/23 03:34:16.102
    STEP: Ensuring resource quota status captures replication controller creation 01/29/23 03:34:16.117
    STEP: Deleting a ReplicationController 01/29/23 03:34:18.126
    STEP: Ensuring resource quota status released usage 01/29/23 03:34:18.137
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 29 03:34:20.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-6204" for this suite. 01/29/23 03:34:20.155
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:34:20.169
Jan 29 03:34:20.169: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename gc 01/29/23 03:34:20.17
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:34:20.198
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:34:20.204
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 01/29/23 03:34:20.209
STEP: delete the rc 01/29/23 03:34:25.224
STEP: wait for all pods to be garbage collected 01/29/23 03:34:25.238
STEP: Gathering metrics 01/29/23 03:34:30.255
Jan 29 03:34:30.307: INFO: Waiting up to 5m0s for pod "kube-controller-manager-master3" in namespace "kube-system" to be "running and ready"
Jan 29 03:34:30.313: INFO: Pod "kube-controller-manager-master3": Phase="Running", Reason="", readiness=true. Elapsed: 6.232743ms
Jan 29 03:34:30.313: INFO: The phase of Pod kube-controller-manager-master3 is Running (Ready = true)
Jan 29 03:34:30.313: INFO: Pod "kube-controller-manager-master3" satisfied condition "running and ready"
E0129 03:34:30.381210      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:34:30.381210      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:34:32.522854      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:34:32.522854      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:34:33.590971      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:34:33.590971      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:34:34.681491      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:34:34.681491      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:34:35.756413      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:34:35.756413      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:34:36.830694      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:34:36.830694      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:34:37.899316      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:34:37.899316      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:34:38.966707      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:34:38.966707      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:34:40.034970      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:34:40.034970      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:34:41.108539      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:34:41.108539      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:34:42.178390      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:34:42.178390      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:34:43.246364      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:34:43.246364      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:34:44.337120      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:34:44.337120      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:34:45.402938      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:34:45.402938      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:34:46.502876      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:34:46.502876      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:34:47.586688      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:34:47.586688      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:34:48.655444      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:34:48.655444      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:34:49.729311      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:34:49.729311      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:34:50.802430      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:34:50.802430      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:34:51.883424      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:34:51.883424      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:34:52.954788      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:34:52.954788      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:34:53.178659      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:34:53.178659      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:34:54.251497      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:34:54.251497      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:34:55.324893      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:34:55.324893      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:34:56.408354      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:34:56.408354      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:34:57.483487      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:34:57.483487      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:34:58.561461      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:34:58.561461      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:34:59.634523      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:34:59.634523      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:00.711142      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:00.711142      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:01.803842      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:01.803842      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:02.908678      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:02.908678      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:03.975763      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:03.975763      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:04.177628      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:04.177628      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:05.248900      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:05.248900      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:07.395729      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:07.395729      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:08.463678      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:08.463678      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:09.538716      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:09.538716      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:10.614889      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:10.614889      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:11.687707      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:11.687707      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:12.761428      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:12.761428      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:13.842974      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:13.842974      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:14.922803      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:14.922803      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:15.177319      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:15.177319      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:16.244922      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:16.244922      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:17.313705      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:17.313705      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:18.383114      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:18.383114      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:19.449960      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:19.449960      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:20.518139      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:20.518139      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:21.589198      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:21.589198      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:22.667874      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:22.667874      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:23.743568      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:23.743568      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:24.815023      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:24.815023      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:25.881457      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:25.881457      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:26.180697      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:26.180697      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:27.248875      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:27.248875      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:28.326600      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:28.326600      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:29.401579      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:29.401579      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:30.469384      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:30.469384      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:31.541068      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:31.541068      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:32.621835      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:32.621835      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:34.792780      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:34.792780      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:35.871292      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:35.871292      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:36.954744      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:36.954744      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:37.177209      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:37.177209      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:38.261601      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:38.261601      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:39.331584      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:39.331584      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:40.400700      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:40.400700      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:41.484707      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:41.484707      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:42.555479      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:42.555479      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:43.635629      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:43.635629      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:44.707908      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:44.707908      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:45.775681      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:45.775681      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:48.147044      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:48.147044      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:48.224763      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:48.224763      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:49.308516      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:49.308516      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:50.381887      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:50.381887      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:51.452473      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:51.452473      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:52.523862      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:52.523862      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:53.596744      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:53.596744      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:54.667628      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:54.667628      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:55.743623      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:55.743623      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:56.815676      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:56.815676      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:57.890993      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:57.890993      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:58.963581      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 03:35:58.963581      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
Jan 29 03:35:58.964: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan 29 03:35:58.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2436" for this suite. 01/29/23 03:35:58.976
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","completed":147,"skipped":2768,"failed":0}
------------------------------
• [SLOW TEST] [98.817 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:34:20.169
    Jan 29 03:34:20.169: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename gc 01/29/23 03:34:20.17
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:34:20.198
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:34:20.204
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 01/29/23 03:34:20.209
    STEP: delete the rc 01/29/23 03:34:25.224
    STEP: wait for all pods to be garbage collected 01/29/23 03:34:25.238
    STEP: Gathering metrics 01/29/23 03:34:30.255
    Jan 29 03:34:30.307: INFO: Waiting up to 5m0s for pod "kube-controller-manager-master3" in namespace "kube-system" to be "running and ready"
    Jan 29 03:34:30.313: INFO: Pod "kube-controller-manager-master3": Phase="Running", Reason="", readiness=true. Elapsed: 6.232743ms
    Jan 29 03:34:30.313: INFO: The phase of Pod kube-controller-manager-master3 is Running (Ready = true)
    Jan 29 03:34:30.313: INFO: Pod "kube-controller-manager-master3" satisfied condition "running and ready"
    E0129 03:34:30.381210      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:34:32.522854      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:34:33.590971      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:34:34.681491      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:34:35.756413      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:34:36.830694      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:34:37.899316      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:34:38.966707      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:34:40.034970      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:34:41.108539      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:34:42.178390      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:34:43.246364      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:34:44.337120      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:34:45.402938      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:34:46.502876      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:34:47.586688      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:34:48.655444      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:34:49.729311      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:34:50.802430      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:34:51.883424      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:34:52.954788      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:34:53.178659      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:34:54.251497      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:34:55.324893      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:34:56.408354      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:34:57.483487      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:34:58.561461      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:34:59.634523      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:35:00.711142      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:35:01.803842      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:35:02.908678      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:35:03.975763      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:35:04.177628      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:35:05.248900      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:35:07.395729      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:35:08.463678      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:35:09.538716      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:35:10.614889      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:35:11.687707      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:35:12.761428      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:35:13.842974      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:35:14.922803      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:35:15.177319      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:35:16.244922      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:35:17.313705      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:35:18.383114      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:35:19.449960      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:35:20.518139      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:35:21.589198      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:35:22.667874      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:35:23.743568      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:35:24.815023      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:35:25.881457      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:35:26.180697      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:35:27.248875      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:35:28.326600      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:35:29.401579      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:35:30.469384      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:35:31.541068      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:35:32.621835      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:35:34.792780      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:35:35.871292      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:35:36.954744      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:35:37.177209      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:35:38.261601      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:35:39.331584      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:35:40.400700      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:35:41.484707      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:35:42.555479      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:35:43.635629      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:35:44.707908      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:35:45.775681      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:35:48.147044      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:35:48.224763      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:35:49.308516      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:35:50.381887      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:35:51.452473      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:35:52.523862      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:35:53.596744      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:35:54.667628      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:35:55.743623      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:35:56.815676      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:35:57.890993      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 03:35:58.963581      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    Jan 29 03:35:58.964: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan 29 03:35:58.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-2436" for this suite. 01/29/23 03:35:58.976
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:35:58.991
Jan 29 03:35:58.991: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename configmap 01/29/23 03:35:58.992
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:35:59.035
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:35:59.042
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
STEP: Creating configMap configmap-6042/configmap-test-ee3e633e-df49-441b-8f81-f9ff02055897 01/29/23 03:35:59.048
STEP: Creating a pod to test consume configMaps 01/29/23 03:35:59.055
Jan 29 03:35:59.084: INFO: Waiting up to 5m0s for pod "pod-configmaps-8764c161-f979-4f1b-ab5d-198718c53104" in namespace "configmap-6042" to be "Succeeded or Failed"
Jan 29 03:35:59.092: INFO: Pod "pod-configmaps-8764c161-f979-4f1b-ab5d-198718c53104": Phase="Pending", Reason="", readiness=false. Elapsed: 7.991236ms
Jan 29 03:36:01.100: INFO: Pod "pod-configmaps-8764c161-f979-4f1b-ab5d-198718c53104": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016228671s
Jan 29 03:36:03.099: INFO: Pod "pod-configmaps-8764c161-f979-4f1b-ab5d-198718c53104": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015302282s
STEP: Saw pod success 01/29/23 03:36:03.099
Jan 29 03:36:03.099: INFO: Pod "pod-configmaps-8764c161-f979-4f1b-ab5d-198718c53104" satisfied condition "Succeeded or Failed"
Jan 29 03:36:03.105: INFO: Trying to get logs from node slave2 pod pod-configmaps-8764c161-f979-4f1b-ab5d-198718c53104 container env-test: <nil>
STEP: delete the pod 01/29/23 03:36:03.137
Jan 29 03:36:03.247: INFO: Waiting for pod pod-configmaps-8764c161-f979-4f1b-ab5d-198718c53104 to disappear
Jan 29 03:36:03.253: INFO: Pod pod-configmaps-8764c161-f979-4f1b-ab5d-198718c53104 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Jan 29 03:36:03.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6042" for this suite. 01/29/23 03:36:03.262
{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","completed":148,"skipped":2820,"failed":0}
------------------------------
• [4.285 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:35:58.991
    Jan 29 03:35:58.991: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename configmap 01/29/23 03:35:58.992
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:35:59.035
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:35:59.042
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:92
    STEP: Creating configMap configmap-6042/configmap-test-ee3e633e-df49-441b-8f81-f9ff02055897 01/29/23 03:35:59.048
    STEP: Creating a pod to test consume configMaps 01/29/23 03:35:59.055
    Jan 29 03:35:59.084: INFO: Waiting up to 5m0s for pod "pod-configmaps-8764c161-f979-4f1b-ab5d-198718c53104" in namespace "configmap-6042" to be "Succeeded or Failed"
    Jan 29 03:35:59.092: INFO: Pod "pod-configmaps-8764c161-f979-4f1b-ab5d-198718c53104": Phase="Pending", Reason="", readiness=false. Elapsed: 7.991236ms
    Jan 29 03:36:01.100: INFO: Pod "pod-configmaps-8764c161-f979-4f1b-ab5d-198718c53104": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016228671s
    Jan 29 03:36:03.099: INFO: Pod "pod-configmaps-8764c161-f979-4f1b-ab5d-198718c53104": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015302282s
    STEP: Saw pod success 01/29/23 03:36:03.099
    Jan 29 03:36:03.099: INFO: Pod "pod-configmaps-8764c161-f979-4f1b-ab5d-198718c53104" satisfied condition "Succeeded or Failed"
    Jan 29 03:36:03.105: INFO: Trying to get logs from node slave2 pod pod-configmaps-8764c161-f979-4f1b-ab5d-198718c53104 container env-test: <nil>
    STEP: delete the pod 01/29/23 03:36:03.137
    Jan 29 03:36:03.247: INFO: Waiting for pod pod-configmaps-8764c161-f979-4f1b-ab5d-198718c53104 to disappear
    Jan 29 03:36:03.253: INFO: Pod pod-configmaps-8764c161-f979-4f1b-ab5d-198718c53104 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 29 03:36:03.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-6042" for this suite. 01/29/23 03:36:03.262
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:36:03.277
Jan 29 03:36:03.277: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename webhook 01/29/23 03:36:03.279
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:36:03.31
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:36:03.316
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/29/23 03:36:03.345
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/29/23 03:36:05.387
STEP: Deploying the webhook pod 01/29/23 03:36:05.404
STEP: Wait for the deployment to be ready 01/29/23 03:36:05.434
Jan 29 03:36:05.448: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/29/23 03:36:07.469
STEP: Verifying the service has paired with the endpoint 01/29/23 03:36:07.486
Jan 29 03:36:08.487: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 01/29/23 03:36:08.494
STEP: create a namespace for the webhook 01/29/23 03:36:08.523
STEP: create a configmap should be unconditionally rejected by the webhook 01/29/23 03:36:08.538
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 29 03:36:08.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-659" for this suite. 01/29/23 03:36:08.586
STEP: Destroying namespace "webhook-659-markers" for this suite. 01/29/23 03:36:08.597
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","completed":149,"skipped":2843,"failed":0}
------------------------------
• [SLOW TEST] [5.429 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:36:03.277
    Jan 29 03:36:03.277: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename webhook 01/29/23 03:36:03.279
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:36:03.31
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:36:03.316
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/29/23 03:36:03.345
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/29/23 03:36:05.387
    STEP: Deploying the webhook pod 01/29/23 03:36:05.404
    STEP: Wait for the deployment to be ready 01/29/23 03:36:05.434
    Jan 29 03:36:05.448: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/29/23 03:36:07.469
    STEP: Verifying the service has paired with the endpoint 01/29/23 03:36:07.486
    Jan 29 03:36:08.487: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:238
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 01/29/23 03:36:08.494
    STEP: create a namespace for the webhook 01/29/23 03:36:08.523
    STEP: create a configmap should be unconditionally rejected by the webhook 01/29/23 03:36:08.538
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 29 03:36:08.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-659" for this suite. 01/29/23 03:36:08.586
    STEP: Destroying namespace "webhook-659-markers" for this suite. 01/29/23 03:36:08.597
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:36:08.707
Jan 29 03:36:08.707: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename kubelet-test 01/29/23 03:36:08.709
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:36:08.742
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:36:08.748
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jan 29 03:36:08.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9755" for this suite. 01/29/23 03:36:08.903
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","completed":150,"skipped":2851,"failed":0}
------------------------------
• [0.208 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:36:08.707
    Jan 29 03:36:08.707: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename kubelet-test 01/29/23 03:36:08.709
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:36:08.742
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:36:08.748
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jan 29 03:36:08.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-9755" for this suite. 01/29/23 03:36:08.903
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:36:08.917
Jan 29 03:36:08.917: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename container-lifecycle-hook 01/29/23 03:36:08.919
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:36:08.957
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:36:08.964
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 01/29/23 03:36:08.981
Jan 29 03:36:08.999: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6117" to be "running and ready"
Jan 29 03:36:09.006: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 6.701627ms
Jan 29 03:36:09.006: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 29 03:36:11.012: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.013405731s
Jan 29 03:36:11.013: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan 29 03:36:11.013: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
STEP: create the pod with lifecycle hook 01/29/23 03:36:11.018
Jan 29 03:36:11.029: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-6117" to be "running and ready"
Jan 29 03:36:11.035: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 5.859041ms
Jan 29 03:36:11.035: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 29 03:36:13.045: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.015532726s
Jan 29 03:36:13.045: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Jan 29 03:36:13.045: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 01/29/23 03:36:13.053
STEP: delete the pod with lifecycle hook 01/29/23 03:36:13.071
Jan 29 03:36:13.111: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 29 03:36:13.121: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 29 03:36:15.122: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 29 03:36:15.129: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 29 03:36:17.122: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 29 03:36:17.129: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Jan 29 03:36:17.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6117" for this suite. 01/29/23 03:36:17.138
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","completed":151,"skipped":2866,"failed":0}
------------------------------
• [SLOW TEST] [8.233 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:36:08.917
    Jan 29 03:36:08.917: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/29/23 03:36:08.919
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:36:08.957
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:36:08.964
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 01/29/23 03:36:08.981
    Jan 29 03:36:08.999: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6117" to be "running and ready"
    Jan 29 03:36:09.006: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 6.701627ms
    Jan 29 03:36:09.006: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 29 03:36:11.012: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.013405731s
    Jan 29 03:36:11.013: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan 29 03:36:11.013: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:97
    STEP: create the pod with lifecycle hook 01/29/23 03:36:11.018
    Jan 29 03:36:11.029: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-6117" to be "running and ready"
    Jan 29 03:36:11.035: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 5.859041ms
    Jan 29 03:36:11.035: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 29 03:36:13.045: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.015532726s
    Jan 29 03:36:13.045: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Jan 29 03:36:13.045: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 01/29/23 03:36:13.053
    STEP: delete the pod with lifecycle hook 01/29/23 03:36:13.071
    Jan 29 03:36:13.111: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jan 29 03:36:13.121: INFO: Pod pod-with-poststart-exec-hook still exists
    Jan 29 03:36:15.122: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jan 29 03:36:15.129: INFO: Pod pod-with-poststart-exec-hook still exists
    Jan 29 03:36:17.122: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jan 29 03:36:17.129: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Jan 29 03:36:17.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-6117" for this suite. 01/29/23 03:36:17.138
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:36:17.151
Jan 29 03:36:17.151: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename endpointslicemirroring 01/29/23 03:36:17.153
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:36:17.186
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:36:17.192
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 01/29/23 03:36:17.214
Jan 29 03:36:17.235: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 01/29/23 03:36:19.242
Jan 29 03:36:19.256: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint 01/29/23 03:36:21.265
Jan 29 03:36:21.281: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:187
Jan 29 03:36:23.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-5955" for this suite. 01/29/23 03:36:23.296
{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","completed":152,"skipped":2869,"failed":0}
------------------------------
• [SLOW TEST] [6.156 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:36:17.151
    Jan 29 03:36:17.151: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename endpointslicemirroring 01/29/23 03:36:17.153
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:36:17.186
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:36:17.192
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 01/29/23 03:36:17.214
    Jan 29 03:36:17.235: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 01/29/23 03:36:19.242
    Jan 29 03:36:19.256: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
    STEP: mirroring deletion of a custom Endpoint 01/29/23 03:36:21.265
    Jan 29 03:36:21.281: INFO: Waiting for 0 EndpointSlices to exist, got 1
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:187
    Jan 29 03:36:23.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslicemirroring-5955" for this suite. 01/29/23 03:36:23.296
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:36:23.308
Jan 29 03:36:23.308: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename downward-api 01/29/23 03:36:23.31
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:36:23.339
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:36:23.345
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
STEP: Creating a pod to test downward API volume plugin 01/29/23 03:36:23.351
Jan 29 03:36:23.368: INFO: Waiting up to 5m0s for pod "downwardapi-volume-87b29047-2052-45e7-8bdb-bde068ec535a" in namespace "downward-api-2256" to be "Succeeded or Failed"
Jan 29 03:36:23.375: INFO: Pod "downwardapi-volume-87b29047-2052-45e7-8bdb-bde068ec535a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.368905ms
Jan 29 03:36:25.382: INFO: Pod "downwardapi-volume-87b29047-2052-45e7-8bdb-bde068ec535a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013103249s
Jan 29 03:36:27.382: INFO: Pod "downwardapi-volume-87b29047-2052-45e7-8bdb-bde068ec535a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01358321s
Jan 29 03:36:29.383: INFO: Pod "downwardapi-volume-87b29047-2052-45e7-8bdb-bde068ec535a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014706535s
STEP: Saw pod success 01/29/23 03:36:29.383
Jan 29 03:36:29.383: INFO: Pod "downwardapi-volume-87b29047-2052-45e7-8bdb-bde068ec535a" satisfied condition "Succeeded or Failed"
Jan 29 03:36:29.390: INFO: Trying to get logs from node slave2 pod downwardapi-volume-87b29047-2052-45e7-8bdb-bde068ec535a container client-container: <nil>
STEP: delete the pod 01/29/23 03:36:29.405
Jan 29 03:36:29.501: INFO: Waiting for pod downwardapi-volume-87b29047-2052-45e7-8bdb-bde068ec535a to disappear
Jan 29 03:36:29.507: INFO: Pod downwardapi-volume-87b29047-2052-45e7-8bdb-bde068ec535a no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 29 03:36:29.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2256" for this suite. 01/29/23 03:36:29.517
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":153,"skipped":2878,"failed":0}
------------------------------
• [SLOW TEST] [6.220 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:36:23.308
    Jan 29 03:36:23.308: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename downward-api 01/29/23 03:36:23.31
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:36:23.339
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:36:23.345
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:260
    STEP: Creating a pod to test downward API volume plugin 01/29/23 03:36:23.351
    Jan 29 03:36:23.368: INFO: Waiting up to 5m0s for pod "downwardapi-volume-87b29047-2052-45e7-8bdb-bde068ec535a" in namespace "downward-api-2256" to be "Succeeded or Failed"
    Jan 29 03:36:23.375: INFO: Pod "downwardapi-volume-87b29047-2052-45e7-8bdb-bde068ec535a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.368905ms
    Jan 29 03:36:25.382: INFO: Pod "downwardapi-volume-87b29047-2052-45e7-8bdb-bde068ec535a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013103249s
    Jan 29 03:36:27.382: INFO: Pod "downwardapi-volume-87b29047-2052-45e7-8bdb-bde068ec535a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01358321s
    Jan 29 03:36:29.383: INFO: Pod "downwardapi-volume-87b29047-2052-45e7-8bdb-bde068ec535a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014706535s
    STEP: Saw pod success 01/29/23 03:36:29.383
    Jan 29 03:36:29.383: INFO: Pod "downwardapi-volume-87b29047-2052-45e7-8bdb-bde068ec535a" satisfied condition "Succeeded or Failed"
    Jan 29 03:36:29.390: INFO: Trying to get logs from node slave2 pod downwardapi-volume-87b29047-2052-45e7-8bdb-bde068ec535a container client-container: <nil>
    STEP: delete the pod 01/29/23 03:36:29.405
    Jan 29 03:36:29.501: INFO: Waiting for pod downwardapi-volume-87b29047-2052-45e7-8bdb-bde068ec535a to disappear
    Jan 29 03:36:29.507: INFO: Pod downwardapi-volume-87b29047-2052-45e7-8bdb-bde068ec535a no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 29 03:36:29.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-2256" for this suite. 01/29/23 03:36:29.517
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:36:29.529
Jan 29 03:36:29.530: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename emptydir 01/29/23 03:36:29.531
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:36:29.564
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:36:29.57
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
STEP: Creating a pod to test emptydir 0666 on node default medium 01/29/23 03:36:29.575
Jan 29 03:36:29.594: INFO: Waiting up to 5m0s for pod "pod-96d67577-61af-4362-a79e-ce2bad6f1647" in namespace "emptydir-8763" to be "Succeeded or Failed"
Jan 29 03:36:29.601: INFO: Pod "pod-96d67577-61af-4362-a79e-ce2bad6f1647": Phase="Pending", Reason="", readiness=false. Elapsed: 7.18549ms
Jan 29 03:36:31.609: INFO: Pod "pod-96d67577-61af-4362-a79e-ce2bad6f1647": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015017962s
Jan 29 03:36:33.609: INFO: Pod "pod-96d67577-61af-4362-a79e-ce2bad6f1647": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015231481s
Jan 29 03:36:35.609: INFO: Pod "pod-96d67577-61af-4362-a79e-ce2bad6f1647": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01541014s
STEP: Saw pod success 01/29/23 03:36:35.609
Jan 29 03:36:35.610: INFO: Pod "pod-96d67577-61af-4362-a79e-ce2bad6f1647" satisfied condition "Succeeded or Failed"
Jan 29 03:36:35.616: INFO: Trying to get logs from node slave2 pod pod-96d67577-61af-4362-a79e-ce2bad6f1647 container test-container: <nil>
STEP: delete the pod 01/29/23 03:36:35.633
Jan 29 03:36:35.726: INFO: Waiting for pod pod-96d67577-61af-4362-a79e-ce2bad6f1647 to disappear
Jan 29 03:36:35.731: INFO: Pod pod-96d67577-61af-4362-a79e-ce2bad6f1647 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 29 03:36:35.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8763" for this suite. 01/29/23 03:36:35.74
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":154,"skipped":2896,"failed":0}
------------------------------
• [SLOW TEST] [6.225 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:36:29.529
    Jan 29 03:36:29.530: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename emptydir 01/29/23 03:36:29.531
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:36:29.564
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:36:29.57
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:206
    STEP: Creating a pod to test emptydir 0666 on node default medium 01/29/23 03:36:29.575
    Jan 29 03:36:29.594: INFO: Waiting up to 5m0s for pod "pod-96d67577-61af-4362-a79e-ce2bad6f1647" in namespace "emptydir-8763" to be "Succeeded or Failed"
    Jan 29 03:36:29.601: INFO: Pod "pod-96d67577-61af-4362-a79e-ce2bad6f1647": Phase="Pending", Reason="", readiness=false. Elapsed: 7.18549ms
    Jan 29 03:36:31.609: INFO: Pod "pod-96d67577-61af-4362-a79e-ce2bad6f1647": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015017962s
    Jan 29 03:36:33.609: INFO: Pod "pod-96d67577-61af-4362-a79e-ce2bad6f1647": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015231481s
    Jan 29 03:36:35.609: INFO: Pod "pod-96d67577-61af-4362-a79e-ce2bad6f1647": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01541014s
    STEP: Saw pod success 01/29/23 03:36:35.609
    Jan 29 03:36:35.610: INFO: Pod "pod-96d67577-61af-4362-a79e-ce2bad6f1647" satisfied condition "Succeeded or Failed"
    Jan 29 03:36:35.616: INFO: Trying to get logs from node slave2 pod pod-96d67577-61af-4362-a79e-ce2bad6f1647 container test-container: <nil>
    STEP: delete the pod 01/29/23 03:36:35.633
    Jan 29 03:36:35.726: INFO: Waiting for pod pod-96d67577-61af-4362-a79e-ce2bad6f1647 to disappear
    Jan 29 03:36:35.731: INFO: Pod pod-96d67577-61af-4362-a79e-ce2bad6f1647 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 29 03:36:35.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-8763" for this suite. 01/29/23 03:36:35.74
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:36:35.755
Jan 29 03:36:35.756: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename deployment 01/29/23 03:36:35.757
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:36:35.789
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:36:35.794
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Jan 29 03:36:35.800: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jan 29 03:36:35.819: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 29 03:36:40.827: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/29/23 03:36:40.827
Jan 29 03:36:40.828: INFO: Creating deployment "test-rolling-update-deployment"
Jan 29 03:36:40.842: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jan 29 03:36:40.856: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jan 29 03:36:42.870: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jan 29 03:36:42.877: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 29 03:36:42.897: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-6272  d8355c71-b116-4e36-995d-9f33a8b41636 5961949 1 2023-01-29 03:36:40 +0000 UTC <nil> <nil> map[name:sample-pod] map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/revision:3546343826724305833] [] [] []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0x4005cd8668 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-29 03:36:40 +0000 UTC,LastTransitionTime:2023-01-29 03:36:40 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-78f575d8ff" has successfully progressed.,LastUpdateTime:2023-01-29 03:36:42 +0000 UTC,LastTransitionTime:2023-01-29 03:36:40 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 29 03:36:42.905: INFO: New ReplicaSet "test-rolling-update-deployment-78f575d8ff" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-78f575d8ff  deployment-6272  2034e762-39f9-4558-a04f-9edf65d84c86 5961939 1 2023-01-29 03:36:40 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment d8355c71-b116-4e36-995d-9f33a8b41636 0x4005cd8b1e 0x4005cd8b1f}] [] []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 78f575d8ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0x4005cd8ba8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 29 03:36:42.905: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jan 29 03:36:42.905: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-6272  f4ee0708-b5a0-49b7-8c14-ce02ba8740c7 5961948 2 2023-01-29 03:36:35 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment d8355c71-b116-4e36-995d-9f33a8b41636 0x4005cd8a4e 0x4005cd8a4f}] [] []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0x4005cd8ab8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 29 03:36:42.914: INFO: Pod "test-rolling-update-deployment-78f575d8ff-xkwll" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-78f575d8ff-xkwll test-rolling-update-deployment-78f575d8ff- deployment-6272  680e0ce6-c807-4d3b-bb10-aae4ff9d9d74 5961938 0 2023-01-29 03:36:40 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "100.101.49.29"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "100.101.49.29"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet test-rolling-update-deployment-78f575d8ff 2034e762-39f9-4558-a04f-9edf65d84c86 0x4005cd9007 0x4005cd9008}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6b4gm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6b4gm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:36:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:36:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:36:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:36:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.245,PodIP:100.101.49.29,StartTime:2023-01-29 03:36:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-29 03:36:42 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry-jinan-lab.inspurcloud.cn/library/cke/agnhost-arm64:2.40,ImageID:docker-pullable://registry-jinan-lab.inspurcloud.cn/library/cke/agnhost-arm64@sha256:3df3d52919cdbe24d42667d1be02605b720b80c8133f2203c4a78f77e3f2429e,ContainerID:docker://1e7707ee53923bc15a8baa4b601f5707f13748b334bd13268dafd395aecf14ed,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.101.49.29,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan 29 03:36:42.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6272" for this suite. 01/29/23 03:36:42.924
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","completed":155,"skipped":2911,"failed":0}
------------------------------
• [SLOW TEST] [7.180 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:36:35.755
    Jan 29 03:36:35.756: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename deployment 01/29/23 03:36:35.757
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:36:35.789
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:36:35.794
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Jan 29 03:36:35.800: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    Jan 29 03:36:35.819: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan 29 03:36:40.827: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/29/23 03:36:40.827
    Jan 29 03:36:40.828: INFO: Creating deployment "test-rolling-update-deployment"
    Jan 29 03:36:40.842: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Jan 29 03:36:40.856: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    Jan 29 03:36:42.870: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Jan 29 03:36:42.877: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 29 03:36:42.897: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-6272  d8355c71-b116-4e36-995d-9f33a8b41636 5961949 1 2023-01-29 03:36:40 +0000 UTC <nil> <nil> map[name:sample-pod] map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/revision:3546343826724305833] [] [] []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0x4005cd8668 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-29 03:36:40 +0000 UTC,LastTransitionTime:2023-01-29 03:36:40 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-78f575d8ff" has successfully progressed.,LastUpdateTime:2023-01-29 03:36:42 +0000 UTC,LastTransitionTime:2023-01-29 03:36:40 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan 29 03:36:42.905: INFO: New ReplicaSet "test-rolling-update-deployment-78f575d8ff" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-78f575d8ff  deployment-6272  2034e762-39f9-4558-a04f-9edf65d84c86 5961939 1 2023-01-29 03:36:40 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment d8355c71-b116-4e36-995d-9f33a8b41636 0x4005cd8b1e 0x4005cd8b1f}] [] []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 78f575d8ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0x4005cd8ba8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 29 03:36:42.905: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Jan 29 03:36:42.905: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-6272  f4ee0708-b5a0-49b7-8c14-ce02ba8740c7 5961948 2 2023-01-29 03:36:35 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment d8355c71-b116-4e36-995d-9f33a8b41636 0x4005cd8a4e 0x4005cd8a4f}] [] []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0x4005cd8ab8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 29 03:36:42.914: INFO: Pod "test-rolling-update-deployment-78f575d8ff-xkwll" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-78f575d8ff-xkwll test-rolling-update-deployment-78f575d8ff- deployment-6272  680e0ce6-c807-4d3b-bb10-aae4ff9d9d74 5961938 0 2023-01-29 03:36:40 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "",
        "ips": [
            "100.101.49.29"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "",
        "ips": [
            "100.101.49.29"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet test-rolling-update-deployment-78f575d8ff 2034e762-39f9-4558-a04f-9edf65d84c86 0x4005cd9007 0x4005cd9008}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6b4gm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6b4gm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:36:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:36:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:36:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:36:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.245,PodIP:100.101.49.29,StartTime:2023-01-29 03:36:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-29 03:36:42 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry-jinan-lab.inspurcloud.cn/library/cke/agnhost-arm64:2.40,ImageID:docker-pullable://registry-jinan-lab.inspurcloud.cn/library/cke/agnhost-arm64@sha256:3df3d52919cdbe24d42667d1be02605b720b80c8133f2203c4a78f77e3f2429e,ContainerID:docker://1e7707ee53923bc15a8baa4b601f5707f13748b334bd13268dafd395aecf14ed,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.101.49.29,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan 29 03:36:42.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-6272" for this suite. 01/29/23 03:36:42.924
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:36:42.938
Jan 29 03:36:42.938: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename dns 01/29/23 03:36:42.939
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:36:42.974
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:36:42.984
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 01/29/23 03:36:42.99
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6851 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6851;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6851 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6851;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6851.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6851.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6851.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6851.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6851.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6851.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6851.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6851.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6851.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6851.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6851.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6851.svc;check="$$(dig +notcp +noall +answer +search 84.122.105.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.105.122.84_udp@PTR;check="$$(dig +tcp +noall +answer +search 84.122.105.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.105.122.84_tcp@PTR;sleep 1; done
 01/29/23 03:36:43.017
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6851 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6851;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6851 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6851;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6851.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6851.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6851.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6851.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6851.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6851.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6851.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6851.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6851.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6851.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6851.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6851.svc;check="$$(dig +notcp +noall +answer +search 84.122.105.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.105.122.84_udp@PTR;check="$$(dig +tcp +noall +answer +search 84.122.105.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.105.122.84_tcp@PTR;sleep 1; done
 01/29/23 03:36:43.017
STEP: creating a pod to probe DNS 01/29/23 03:36:43.017
STEP: submitting the pod to kubernetes 01/29/23 03:36:43.018
Jan 29 03:36:43.059: INFO: Waiting up to 15m0s for pod "dns-test-726e02c9-df7d-4935-a702-e8233aab1572" in namespace "dns-6851" to be "running"
Jan 29 03:36:43.070: INFO: Pod "dns-test-726e02c9-df7d-4935-a702-e8233aab1572": Phase="Pending", Reason="", readiness=false. Elapsed: 10.340052ms
Jan 29 03:36:45.078: INFO: Pod "dns-test-726e02c9-df7d-4935-a702-e8233aab1572": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018593287s
Jan 29 03:36:47.078: INFO: Pod "dns-test-726e02c9-df7d-4935-a702-e8233aab1572": Phase="Running", Reason="", readiness=true. Elapsed: 4.018446423s
Jan 29 03:36:47.078: INFO: Pod "dns-test-726e02c9-df7d-4935-a702-e8233aab1572" satisfied condition "running"
STEP: retrieving the pod 01/29/23 03:36:47.079
STEP: looking for the results for each expected name from probers 01/29/23 03:36:47.086
Jan 29 03:36:47.094: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6851/dns-test-726e02c9-df7d-4935-a702-e8233aab1572: the server could not find the requested resource (get pods dns-test-726e02c9-df7d-4935-a702-e8233aab1572)
Jan 29 03:36:47.100: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6851/dns-test-726e02c9-df7d-4935-a702-e8233aab1572: the server could not find the requested resource (get pods dns-test-726e02c9-df7d-4935-a702-e8233aab1572)
Jan 29 03:36:47.107: INFO: Unable to read wheezy_udp@dns-test-service.dns-6851 from pod dns-6851/dns-test-726e02c9-df7d-4935-a702-e8233aab1572: the server could not find the requested resource (get pods dns-test-726e02c9-df7d-4935-a702-e8233aab1572)
Jan 29 03:36:47.113: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6851 from pod dns-6851/dns-test-726e02c9-df7d-4935-a702-e8233aab1572: the server could not find the requested resource (get pods dns-test-726e02c9-df7d-4935-a702-e8233aab1572)
Jan 29 03:36:47.120: INFO: Unable to read wheezy_udp@dns-test-service.dns-6851.svc from pod dns-6851/dns-test-726e02c9-df7d-4935-a702-e8233aab1572: the server could not find the requested resource (get pods dns-test-726e02c9-df7d-4935-a702-e8233aab1572)
Jan 29 03:36:47.126: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6851.svc from pod dns-6851/dns-test-726e02c9-df7d-4935-a702-e8233aab1572: the server could not find the requested resource (get pods dns-test-726e02c9-df7d-4935-a702-e8233aab1572)
Jan 29 03:36:47.168: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6851/dns-test-726e02c9-df7d-4935-a702-e8233aab1572: the server could not find the requested resource (get pods dns-test-726e02c9-df7d-4935-a702-e8233aab1572)
Jan 29 03:36:47.174: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6851/dns-test-726e02c9-df7d-4935-a702-e8233aab1572: the server could not find the requested resource (get pods dns-test-726e02c9-df7d-4935-a702-e8233aab1572)
Jan 29 03:36:47.181: INFO: Unable to read jessie_udp@dns-test-service.dns-6851 from pod dns-6851/dns-test-726e02c9-df7d-4935-a702-e8233aab1572: the server could not find the requested resource (get pods dns-test-726e02c9-df7d-4935-a702-e8233aab1572)
Jan 29 03:36:47.188: INFO: Unable to read jessie_tcp@dns-test-service.dns-6851 from pod dns-6851/dns-test-726e02c9-df7d-4935-a702-e8233aab1572: the server could not find the requested resource (get pods dns-test-726e02c9-df7d-4935-a702-e8233aab1572)
Jan 29 03:36:47.194: INFO: Unable to read jessie_udp@dns-test-service.dns-6851.svc from pod dns-6851/dns-test-726e02c9-df7d-4935-a702-e8233aab1572: the server could not find the requested resource (get pods dns-test-726e02c9-df7d-4935-a702-e8233aab1572)
Jan 29 03:36:47.201: INFO: Unable to read jessie_tcp@dns-test-service.dns-6851.svc from pod dns-6851/dns-test-726e02c9-df7d-4935-a702-e8233aab1572: the server could not find the requested resource (get pods dns-test-726e02c9-df7d-4935-a702-e8233aab1572)
Jan 29 03:36:47.207: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6851.svc from pod dns-6851/dns-test-726e02c9-df7d-4935-a702-e8233aab1572: the server could not find the requested resource (get pods dns-test-726e02c9-df7d-4935-a702-e8233aab1572)
Jan 29 03:36:47.215: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6851.svc from pod dns-6851/dns-test-726e02c9-df7d-4935-a702-e8233aab1572: the server could not find the requested resource (get pods dns-test-726e02c9-df7d-4935-a702-e8233aab1572)
Jan 29 03:36:47.241: INFO: Lookups using dns-6851/dns-test-726e02c9-df7d-4935-a702-e8233aab1572 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6851 wheezy_tcp@dns-test-service.dns-6851 wheezy_udp@dns-test-service.dns-6851.svc wheezy_tcp@dns-test-service.dns-6851.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6851 jessie_tcp@dns-test-service.dns-6851 jessie_udp@dns-test-service.dns-6851.svc jessie_tcp@dns-test-service.dns-6851.svc jessie_udp@_http._tcp.dns-test-service.dns-6851.svc jessie_tcp@_http._tcp.dns-test-service.dns-6851.svc]

Jan 29 03:36:52.403: INFO: DNS probes using dns-6851/dns-test-726e02c9-df7d-4935-a702-e8233aab1572 succeeded

STEP: deleting the pod 01/29/23 03:36:52.403
STEP: deleting the test service 01/29/23 03:36:52.518
STEP: deleting the test headless service 01/29/23 03:36:52.565
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan 29 03:36:52.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6851" for this suite. 01/29/23 03:36:52.608
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","completed":156,"skipped":2931,"failed":0}
------------------------------
• [SLOW TEST] [9.684 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:36:42.938
    Jan 29 03:36:42.938: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename dns 01/29/23 03:36:42.939
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:36:42.974
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:36:42.984
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 01/29/23 03:36:42.99
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6851 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6851;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6851 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6851;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6851.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6851.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6851.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6851.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6851.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6851.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6851.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6851.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6851.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6851.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6851.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6851.svc;check="$$(dig +notcp +noall +answer +search 84.122.105.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.105.122.84_udp@PTR;check="$$(dig +tcp +noall +answer +search 84.122.105.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.105.122.84_tcp@PTR;sleep 1; done
     01/29/23 03:36:43.017
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6851 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6851;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6851 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6851;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6851.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6851.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6851.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6851.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6851.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6851.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6851.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6851.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6851.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6851.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6851.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6851.svc;check="$$(dig +notcp +noall +answer +search 84.122.105.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.105.122.84_udp@PTR;check="$$(dig +tcp +noall +answer +search 84.122.105.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.105.122.84_tcp@PTR;sleep 1; done
     01/29/23 03:36:43.017
    STEP: creating a pod to probe DNS 01/29/23 03:36:43.017
    STEP: submitting the pod to kubernetes 01/29/23 03:36:43.018
    Jan 29 03:36:43.059: INFO: Waiting up to 15m0s for pod "dns-test-726e02c9-df7d-4935-a702-e8233aab1572" in namespace "dns-6851" to be "running"
    Jan 29 03:36:43.070: INFO: Pod "dns-test-726e02c9-df7d-4935-a702-e8233aab1572": Phase="Pending", Reason="", readiness=false. Elapsed: 10.340052ms
    Jan 29 03:36:45.078: INFO: Pod "dns-test-726e02c9-df7d-4935-a702-e8233aab1572": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018593287s
    Jan 29 03:36:47.078: INFO: Pod "dns-test-726e02c9-df7d-4935-a702-e8233aab1572": Phase="Running", Reason="", readiness=true. Elapsed: 4.018446423s
    Jan 29 03:36:47.078: INFO: Pod "dns-test-726e02c9-df7d-4935-a702-e8233aab1572" satisfied condition "running"
    STEP: retrieving the pod 01/29/23 03:36:47.079
    STEP: looking for the results for each expected name from probers 01/29/23 03:36:47.086
    Jan 29 03:36:47.094: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6851/dns-test-726e02c9-df7d-4935-a702-e8233aab1572: the server could not find the requested resource (get pods dns-test-726e02c9-df7d-4935-a702-e8233aab1572)
    Jan 29 03:36:47.100: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6851/dns-test-726e02c9-df7d-4935-a702-e8233aab1572: the server could not find the requested resource (get pods dns-test-726e02c9-df7d-4935-a702-e8233aab1572)
    Jan 29 03:36:47.107: INFO: Unable to read wheezy_udp@dns-test-service.dns-6851 from pod dns-6851/dns-test-726e02c9-df7d-4935-a702-e8233aab1572: the server could not find the requested resource (get pods dns-test-726e02c9-df7d-4935-a702-e8233aab1572)
    Jan 29 03:36:47.113: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6851 from pod dns-6851/dns-test-726e02c9-df7d-4935-a702-e8233aab1572: the server could not find the requested resource (get pods dns-test-726e02c9-df7d-4935-a702-e8233aab1572)
    Jan 29 03:36:47.120: INFO: Unable to read wheezy_udp@dns-test-service.dns-6851.svc from pod dns-6851/dns-test-726e02c9-df7d-4935-a702-e8233aab1572: the server could not find the requested resource (get pods dns-test-726e02c9-df7d-4935-a702-e8233aab1572)
    Jan 29 03:36:47.126: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6851.svc from pod dns-6851/dns-test-726e02c9-df7d-4935-a702-e8233aab1572: the server could not find the requested resource (get pods dns-test-726e02c9-df7d-4935-a702-e8233aab1572)
    Jan 29 03:36:47.168: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6851/dns-test-726e02c9-df7d-4935-a702-e8233aab1572: the server could not find the requested resource (get pods dns-test-726e02c9-df7d-4935-a702-e8233aab1572)
    Jan 29 03:36:47.174: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6851/dns-test-726e02c9-df7d-4935-a702-e8233aab1572: the server could not find the requested resource (get pods dns-test-726e02c9-df7d-4935-a702-e8233aab1572)
    Jan 29 03:36:47.181: INFO: Unable to read jessie_udp@dns-test-service.dns-6851 from pod dns-6851/dns-test-726e02c9-df7d-4935-a702-e8233aab1572: the server could not find the requested resource (get pods dns-test-726e02c9-df7d-4935-a702-e8233aab1572)
    Jan 29 03:36:47.188: INFO: Unable to read jessie_tcp@dns-test-service.dns-6851 from pod dns-6851/dns-test-726e02c9-df7d-4935-a702-e8233aab1572: the server could not find the requested resource (get pods dns-test-726e02c9-df7d-4935-a702-e8233aab1572)
    Jan 29 03:36:47.194: INFO: Unable to read jessie_udp@dns-test-service.dns-6851.svc from pod dns-6851/dns-test-726e02c9-df7d-4935-a702-e8233aab1572: the server could not find the requested resource (get pods dns-test-726e02c9-df7d-4935-a702-e8233aab1572)
    Jan 29 03:36:47.201: INFO: Unable to read jessie_tcp@dns-test-service.dns-6851.svc from pod dns-6851/dns-test-726e02c9-df7d-4935-a702-e8233aab1572: the server could not find the requested resource (get pods dns-test-726e02c9-df7d-4935-a702-e8233aab1572)
    Jan 29 03:36:47.207: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6851.svc from pod dns-6851/dns-test-726e02c9-df7d-4935-a702-e8233aab1572: the server could not find the requested resource (get pods dns-test-726e02c9-df7d-4935-a702-e8233aab1572)
    Jan 29 03:36:47.215: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6851.svc from pod dns-6851/dns-test-726e02c9-df7d-4935-a702-e8233aab1572: the server could not find the requested resource (get pods dns-test-726e02c9-df7d-4935-a702-e8233aab1572)
    Jan 29 03:36:47.241: INFO: Lookups using dns-6851/dns-test-726e02c9-df7d-4935-a702-e8233aab1572 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6851 wheezy_tcp@dns-test-service.dns-6851 wheezy_udp@dns-test-service.dns-6851.svc wheezy_tcp@dns-test-service.dns-6851.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6851 jessie_tcp@dns-test-service.dns-6851 jessie_udp@dns-test-service.dns-6851.svc jessie_tcp@dns-test-service.dns-6851.svc jessie_udp@_http._tcp.dns-test-service.dns-6851.svc jessie_tcp@_http._tcp.dns-test-service.dns-6851.svc]

    Jan 29 03:36:52.403: INFO: DNS probes using dns-6851/dns-test-726e02c9-df7d-4935-a702-e8233aab1572 succeeded

    STEP: deleting the pod 01/29/23 03:36:52.403
    STEP: deleting the test service 01/29/23 03:36:52.518
    STEP: deleting the test headless service 01/29/23 03:36:52.565
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan 29 03:36:52.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-6851" for this suite. 01/29/23 03:36:52.608
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:36:52.624
Jan 29 03:36:52.624: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename events 01/29/23 03:36:52.626
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:36:52.666
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:36:52.671
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 01/29/23 03:36:52.676
STEP: get a list of Events with a label in the current namespace 01/29/23 03:36:52.697
STEP: delete a list of events 01/29/23 03:36:52.704
Jan 29 03:36:52.704: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 01/29/23 03:36:52.737
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
Jan 29 03:36:52.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9630" for this suite. 01/29/23 03:36:52.752
{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","completed":157,"skipped":2970,"failed":0}
------------------------------
• [0.139 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:36:52.624
    Jan 29 03:36:52.624: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename events 01/29/23 03:36:52.626
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:36:52.666
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:36:52.671
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 01/29/23 03:36:52.676
    STEP: get a list of Events with a label in the current namespace 01/29/23 03:36:52.697
    STEP: delete a list of events 01/29/23 03:36:52.704
    Jan 29 03:36:52.704: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 01/29/23 03:36:52.737
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    Jan 29 03:36:52.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-9630" for this suite. 01/29/23 03:36:52.752
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:36:52.765
Jan 29 03:36:52.766: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename pod-network-test 01/29/23 03:36:52.767
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:36:52.794
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:36:52.8
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-2614 01/29/23 03:36:52.806
STEP: creating a selector 01/29/23 03:36:52.806
STEP: Creating the service pods in kubernetes 01/29/23 03:36:52.806
Jan 29 03:36:52.806: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 29 03:36:52.913: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-2614" to be "running and ready"
Jan 29 03:36:52.924: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.443933ms
Jan 29 03:36:52.924: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 29 03:36:54.931: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.018120964s
Jan 29 03:36:54.931: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 29 03:36:56.934: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.020727179s
Jan 29 03:36:56.934: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 29 03:36:58.933: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.020206452s
Jan 29 03:36:58.934: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 29 03:37:00.934: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.021207457s
Jan 29 03:37:00.935: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 29 03:37:02.931: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.017641989s
Jan 29 03:37:02.931: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 29 03:37:04.934: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.020944569s
Jan 29 03:37:04.934: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 29 03:37:06.931: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.017625183s
Jan 29 03:37:06.931: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 29 03:37:08.934: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.021154545s
Jan 29 03:37:08.935: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 29 03:37:10.932: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.018670305s
Jan 29 03:37:10.932: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 29 03:37:12.931: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.017953477s
Jan 29 03:37:12.931: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 29 03:37:14.931: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.017921034s
Jan 29 03:37:14.931: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan 29 03:37:14.931: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan 29 03:37:14.937: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-2614" to be "running and ready"
Jan 29 03:37:14.943: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 5.913281ms
Jan 29 03:37:14.943: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan 29 03:37:14.943: INFO: Pod "netserver-1" satisfied condition "running and ready"
Jan 29 03:37:14.949: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-2614" to be "running and ready"
Jan 29 03:37:14.955: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 6.108342ms
Jan 29 03:37:14.955: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Jan 29 03:37:14.955: INFO: Pod "netserver-2" satisfied condition "running and ready"
Jan 29 03:37:14.961: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-2614" to be "running and ready"
Jan 29 03:37:14.968: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 7.046389ms
Jan 29 03:37:14.968: INFO: The phase of Pod netserver-3 is Running (Ready = true)
Jan 29 03:37:14.968: INFO: Pod "netserver-3" satisfied condition "running and ready"
Jan 29 03:37:14.974: INFO: Waiting up to 5m0s for pod "netserver-4" in namespace "pod-network-test-2614" to be "running and ready"
Jan 29 03:37:14.980: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=true. Elapsed: 6.070722ms
Jan 29 03:37:14.980: INFO: The phase of Pod netserver-4 is Running (Ready = true)
Jan 29 03:37:14.980: INFO: Pod "netserver-4" satisfied condition "running and ready"
STEP: Creating test pods 01/29/23 03:37:14.986
Jan 29 03:37:15.000: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-2614" to be "running"
Jan 29 03:37:15.006: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.128723ms
Jan 29 03:37:17.013: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.013304451s
Jan 29 03:37:17.014: INFO: Pod "test-container-pod" satisfied condition "running"
Jan 29 03:37:17.020: INFO: Setting MaxTries for pod polling to 55 for networking test based on endpoint count 5
Jan 29 03:37:17.020: INFO: Breadth first check of 100.101.161.213 on host 192.168.122.241...
Jan 29 03:37:17.025: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.101.49.46:9080/dial?request=hostname&protocol=udp&host=100.101.161.213&port=8081&tries=1'] Namespace:pod-network-test-2614 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 29 03:37:17.025: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
Jan 29 03:37:17.026: INFO: ExecWithOptions: Clientset creation
Jan 29 03:37:17.026: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/pod-network-test-2614/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F100.101.49.46%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D100.101.161.213%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 29 03:37:17.154: INFO: Waiting for responses: map[]
Jan 29 03:37:17.154: INFO: reached 100.101.161.213 after 0/1 tries
Jan 29 03:37:17.154: INFO: Breadth first check of 100.101.208.231 on host 192.168.122.242...
Jan 29 03:37:17.161: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.101.49.46:9080/dial?request=hostname&protocol=udp&host=100.101.208.231&port=8081&tries=1'] Namespace:pod-network-test-2614 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 29 03:37:17.161: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
Jan 29 03:37:17.162: INFO: ExecWithOptions: Clientset creation
Jan 29 03:37:17.162: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/pod-network-test-2614/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F100.101.49.46%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D100.101.208.231%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 29 03:37:17.301: INFO: Waiting for responses: map[]
Jan 29 03:37:17.301: INFO: reached 100.101.208.231 after 0/1 tries
Jan 29 03:37:17.301: INFO: Breadth first check of 100.101.32.125 on host 192.168.122.243...
Jan 29 03:37:17.307: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.101.49.46:9080/dial?request=hostname&protocol=udp&host=100.101.32.125&port=8081&tries=1'] Namespace:pod-network-test-2614 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 29 03:37:17.307: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
Jan 29 03:37:17.308: INFO: ExecWithOptions: Clientset creation
Jan 29 03:37:17.308: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/pod-network-test-2614/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F100.101.49.46%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D100.101.32.125%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 29 03:37:17.429: INFO: Waiting for responses: map[]
Jan 29 03:37:17.429: INFO: reached 100.101.32.125 after 0/1 tries
Jan 29 03:37:17.429: INFO: Breadth first check of 100.101.51.61 on host 192.168.122.244...
Jan 29 03:37:17.436: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.101.49.46:9080/dial?request=hostname&protocol=udp&host=100.101.51.61&port=8081&tries=1'] Namespace:pod-network-test-2614 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 29 03:37:17.436: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
Jan 29 03:37:17.436: INFO: ExecWithOptions: Clientset creation
Jan 29 03:37:17.437: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/pod-network-test-2614/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F100.101.49.46%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D100.101.51.61%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 29 03:37:17.563: INFO: Waiting for responses: map[]
Jan 29 03:37:17.563: INFO: reached 100.101.51.61 after 0/1 tries
Jan 29 03:37:17.563: INFO: Breadth first check of 100.101.49.40 on host 192.168.122.245...
Jan 29 03:37:17.569: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.101.49.46:9080/dial?request=hostname&protocol=udp&host=100.101.49.40&port=8081&tries=1'] Namespace:pod-network-test-2614 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 29 03:37:17.569: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
Jan 29 03:37:17.570: INFO: ExecWithOptions: Clientset creation
Jan 29 03:37:17.570: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/pod-network-test-2614/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F100.101.49.46%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D100.101.49.40%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 29 03:37:17.693: INFO: Waiting for responses: map[]
Jan 29 03:37:17.693: INFO: reached 100.101.49.40 after 0/1 tries
Jan 29 03:37:17.693: INFO: Going to retry 0 out of 5 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Jan 29 03:37:17.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2614" for this suite. 01/29/23 03:37:17.704
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","completed":158,"skipped":2996,"failed":0}
------------------------------
• [SLOW TEST] [24.954 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:36:52.765
    Jan 29 03:36:52.766: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename pod-network-test 01/29/23 03:36:52.767
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:36:52.794
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:36:52.8
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-2614 01/29/23 03:36:52.806
    STEP: creating a selector 01/29/23 03:36:52.806
    STEP: Creating the service pods in kubernetes 01/29/23 03:36:52.806
    Jan 29 03:36:52.806: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan 29 03:36:52.913: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-2614" to be "running and ready"
    Jan 29 03:36:52.924: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.443933ms
    Jan 29 03:36:52.924: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 29 03:36:54.931: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.018120964s
    Jan 29 03:36:54.931: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 29 03:36:56.934: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.020727179s
    Jan 29 03:36:56.934: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 29 03:36:58.933: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.020206452s
    Jan 29 03:36:58.934: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 29 03:37:00.934: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.021207457s
    Jan 29 03:37:00.935: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 29 03:37:02.931: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.017641989s
    Jan 29 03:37:02.931: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 29 03:37:04.934: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.020944569s
    Jan 29 03:37:04.934: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 29 03:37:06.931: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.017625183s
    Jan 29 03:37:06.931: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 29 03:37:08.934: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.021154545s
    Jan 29 03:37:08.935: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 29 03:37:10.932: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.018670305s
    Jan 29 03:37:10.932: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 29 03:37:12.931: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.017953477s
    Jan 29 03:37:12.931: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 29 03:37:14.931: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.017921034s
    Jan 29 03:37:14.931: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan 29 03:37:14.931: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan 29 03:37:14.937: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-2614" to be "running and ready"
    Jan 29 03:37:14.943: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 5.913281ms
    Jan 29 03:37:14.943: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan 29 03:37:14.943: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Jan 29 03:37:14.949: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-2614" to be "running and ready"
    Jan 29 03:37:14.955: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 6.108342ms
    Jan 29 03:37:14.955: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Jan 29 03:37:14.955: INFO: Pod "netserver-2" satisfied condition "running and ready"
    Jan 29 03:37:14.961: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-2614" to be "running and ready"
    Jan 29 03:37:14.968: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 7.046389ms
    Jan 29 03:37:14.968: INFO: The phase of Pod netserver-3 is Running (Ready = true)
    Jan 29 03:37:14.968: INFO: Pod "netserver-3" satisfied condition "running and ready"
    Jan 29 03:37:14.974: INFO: Waiting up to 5m0s for pod "netserver-4" in namespace "pod-network-test-2614" to be "running and ready"
    Jan 29 03:37:14.980: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=true. Elapsed: 6.070722ms
    Jan 29 03:37:14.980: INFO: The phase of Pod netserver-4 is Running (Ready = true)
    Jan 29 03:37:14.980: INFO: Pod "netserver-4" satisfied condition "running and ready"
    STEP: Creating test pods 01/29/23 03:37:14.986
    Jan 29 03:37:15.000: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-2614" to be "running"
    Jan 29 03:37:15.006: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.128723ms
    Jan 29 03:37:17.013: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.013304451s
    Jan 29 03:37:17.014: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan 29 03:37:17.020: INFO: Setting MaxTries for pod polling to 55 for networking test based on endpoint count 5
    Jan 29 03:37:17.020: INFO: Breadth first check of 100.101.161.213 on host 192.168.122.241...
    Jan 29 03:37:17.025: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.101.49.46:9080/dial?request=hostname&protocol=udp&host=100.101.161.213&port=8081&tries=1'] Namespace:pod-network-test-2614 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 29 03:37:17.025: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    Jan 29 03:37:17.026: INFO: ExecWithOptions: Clientset creation
    Jan 29 03:37:17.026: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/pod-network-test-2614/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F100.101.49.46%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D100.101.161.213%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 29 03:37:17.154: INFO: Waiting for responses: map[]
    Jan 29 03:37:17.154: INFO: reached 100.101.161.213 after 0/1 tries
    Jan 29 03:37:17.154: INFO: Breadth first check of 100.101.208.231 on host 192.168.122.242...
    Jan 29 03:37:17.161: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.101.49.46:9080/dial?request=hostname&protocol=udp&host=100.101.208.231&port=8081&tries=1'] Namespace:pod-network-test-2614 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 29 03:37:17.161: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    Jan 29 03:37:17.162: INFO: ExecWithOptions: Clientset creation
    Jan 29 03:37:17.162: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/pod-network-test-2614/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F100.101.49.46%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D100.101.208.231%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 29 03:37:17.301: INFO: Waiting for responses: map[]
    Jan 29 03:37:17.301: INFO: reached 100.101.208.231 after 0/1 tries
    Jan 29 03:37:17.301: INFO: Breadth first check of 100.101.32.125 on host 192.168.122.243...
    Jan 29 03:37:17.307: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.101.49.46:9080/dial?request=hostname&protocol=udp&host=100.101.32.125&port=8081&tries=1'] Namespace:pod-network-test-2614 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 29 03:37:17.307: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    Jan 29 03:37:17.308: INFO: ExecWithOptions: Clientset creation
    Jan 29 03:37:17.308: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/pod-network-test-2614/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F100.101.49.46%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D100.101.32.125%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 29 03:37:17.429: INFO: Waiting for responses: map[]
    Jan 29 03:37:17.429: INFO: reached 100.101.32.125 after 0/1 tries
    Jan 29 03:37:17.429: INFO: Breadth first check of 100.101.51.61 on host 192.168.122.244...
    Jan 29 03:37:17.436: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.101.49.46:9080/dial?request=hostname&protocol=udp&host=100.101.51.61&port=8081&tries=1'] Namespace:pod-network-test-2614 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 29 03:37:17.436: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    Jan 29 03:37:17.436: INFO: ExecWithOptions: Clientset creation
    Jan 29 03:37:17.437: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/pod-network-test-2614/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F100.101.49.46%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D100.101.51.61%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 29 03:37:17.563: INFO: Waiting for responses: map[]
    Jan 29 03:37:17.563: INFO: reached 100.101.51.61 after 0/1 tries
    Jan 29 03:37:17.563: INFO: Breadth first check of 100.101.49.40 on host 192.168.122.245...
    Jan 29 03:37:17.569: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.101.49.46:9080/dial?request=hostname&protocol=udp&host=100.101.49.40&port=8081&tries=1'] Namespace:pod-network-test-2614 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 29 03:37:17.569: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    Jan 29 03:37:17.570: INFO: ExecWithOptions: Clientset creation
    Jan 29 03:37:17.570: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/pod-network-test-2614/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F100.101.49.46%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D100.101.49.40%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 29 03:37:17.693: INFO: Waiting for responses: map[]
    Jan 29 03:37:17.693: INFO: reached 100.101.49.40 after 0/1 tries
    Jan 29 03:37:17.693: INFO: Going to retry 0 out of 5 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Jan 29 03:37:17.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-2614" for this suite. 01/29/23 03:37:17.704
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:37:17.72
Jan 29 03:37:17.721: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename security-context-test 01/29/23 03:37:17.722
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:37:17.752
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:37:17.758
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
Jan 29 03:37:17.783: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-a885d2b4-956b-42ab-9396-a2b29ddea4eb" in namespace "security-context-test-5163" to be "Succeeded or Failed"
Jan 29 03:37:17.789: INFO: Pod "alpine-nnp-false-a885d2b4-956b-42ab-9396-a2b29ddea4eb": Phase="Pending", Reason="", readiness=false. Elapsed: 5.924921ms
Jan 29 03:37:19.797: INFO: Pod "alpine-nnp-false-a885d2b4-956b-42ab-9396-a2b29ddea4eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013878834s
Jan 29 03:37:21.798: INFO: Pod "alpine-nnp-false-a885d2b4-956b-42ab-9396-a2b29ddea4eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014082653s
Jan 29 03:37:21.798: INFO: Pod "alpine-nnp-false-a885d2b4-956b-42ab-9396-a2b29ddea4eb" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jan 29 03:37:21.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5163" for this suite. 01/29/23 03:37:21.822
{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","completed":159,"skipped":2999,"failed":0}
------------------------------
• [4.115 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:554
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:608

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:37:17.72
    Jan 29 03:37:17.721: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename security-context-test 01/29/23 03:37:17.722
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:37:17.752
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:37:17.758
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:608
    Jan 29 03:37:17.783: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-a885d2b4-956b-42ab-9396-a2b29ddea4eb" in namespace "security-context-test-5163" to be "Succeeded or Failed"
    Jan 29 03:37:17.789: INFO: Pod "alpine-nnp-false-a885d2b4-956b-42ab-9396-a2b29ddea4eb": Phase="Pending", Reason="", readiness=false. Elapsed: 5.924921ms
    Jan 29 03:37:19.797: INFO: Pod "alpine-nnp-false-a885d2b4-956b-42ab-9396-a2b29ddea4eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013878834s
    Jan 29 03:37:21.798: INFO: Pod "alpine-nnp-false-a885d2b4-956b-42ab-9396-a2b29ddea4eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014082653s
    Jan 29 03:37:21.798: INFO: Pod "alpine-nnp-false-a885d2b4-956b-42ab-9396-a2b29ddea4eb" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jan 29 03:37:21.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-5163" for this suite. 01/29/23 03:37:21.822
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:37:21.839
Jan 29 03:37:21.839: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename configmap 01/29/23 03:37:21.841
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:37:21.87
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:37:21.876
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
STEP: creating a ConfigMap 01/29/23 03:37:21.882
STEP: fetching the ConfigMap 01/29/23 03:37:21.889
STEP: patching the ConfigMap 01/29/23 03:37:21.895
STEP: listing all ConfigMaps in all namespaces with a label selector 01/29/23 03:37:21.905
STEP: deleting the ConfigMap by collection with a label selector 01/29/23 03:37:21.912
STEP: listing all ConfigMaps in test namespace 01/29/23 03:37:21.925
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Jan 29 03:37:21.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7832" for this suite. 01/29/23 03:37:21.942
{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","completed":160,"skipped":3045,"failed":0}
------------------------------
• [0.113 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:37:21.839
    Jan 29 03:37:21.839: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename configmap 01/29/23 03:37:21.841
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:37:21.87
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:37:21.876
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:168
    STEP: creating a ConfigMap 01/29/23 03:37:21.882
    STEP: fetching the ConfigMap 01/29/23 03:37:21.889
    STEP: patching the ConfigMap 01/29/23 03:37:21.895
    STEP: listing all ConfigMaps in all namespaces with a label selector 01/29/23 03:37:21.905
    STEP: deleting the ConfigMap by collection with a label selector 01/29/23 03:37:21.912
    STEP: listing all ConfigMaps in test namespace 01/29/23 03:37:21.925
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 29 03:37:21.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-7832" for this suite. 01/29/23 03:37:21.942
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:37:21.954
Jan 29 03:37:21.954: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename security-context-test 01/29/23 03:37:21.955
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:37:21.987
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:37:21.994
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
Jan 29 03:37:22.018: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-6d236604-0890-4de6-995c-5dabdb1d1bff" in namespace "security-context-test-8372" to be "Succeeded or Failed"
Jan 29 03:37:22.024: INFO: Pod "busybox-privileged-false-6d236604-0890-4de6-995c-5dabdb1d1bff": Phase="Pending", Reason="", readiness=false. Elapsed: 5.994562ms
Jan 29 03:37:24.038: INFO: Pod "busybox-privileged-false-6d236604-0890-4de6-995c-5dabdb1d1bff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019954357s
Jan 29 03:37:26.032: INFO: Pod "busybox-privileged-false-6d236604-0890-4de6-995c-5dabdb1d1bff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014224214s
Jan 29 03:37:26.032: INFO: Pod "busybox-privileged-false-6d236604-0890-4de6-995c-5dabdb1d1bff" satisfied condition "Succeeded or Failed"
Jan 29 03:37:26.046: INFO: Got logs for pod "busybox-privileged-false-6d236604-0890-4de6-995c-5dabdb1d1bff": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jan 29 03:37:26.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-8372" for this suite. 01/29/23 03:37:26.056
{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","completed":161,"skipped":3059,"failed":0}
------------------------------
• [4.114 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:490
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:527

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:37:21.954
    Jan 29 03:37:21.954: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename security-context-test 01/29/23 03:37:21.955
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:37:21.987
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:37:21.994
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:527
    Jan 29 03:37:22.018: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-6d236604-0890-4de6-995c-5dabdb1d1bff" in namespace "security-context-test-8372" to be "Succeeded or Failed"
    Jan 29 03:37:22.024: INFO: Pod "busybox-privileged-false-6d236604-0890-4de6-995c-5dabdb1d1bff": Phase="Pending", Reason="", readiness=false. Elapsed: 5.994562ms
    Jan 29 03:37:24.038: INFO: Pod "busybox-privileged-false-6d236604-0890-4de6-995c-5dabdb1d1bff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019954357s
    Jan 29 03:37:26.032: INFO: Pod "busybox-privileged-false-6d236604-0890-4de6-995c-5dabdb1d1bff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014224214s
    Jan 29 03:37:26.032: INFO: Pod "busybox-privileged-false-6d236604-0890-4de6-995c-5dabdb1d1bff" satisfied condition "Succeeded or Failed"
    Jan 29 03:37:26.046: INFO: Got logs for pod "busybox-privileged-false-6d236604-0890-4de6-995c-5dabdb1d1bff": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jan 29 03:37:26.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-8372" for this suite. 01/29/23 03:37:26.056
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:37:26.068
Jan 29 03:37:26.068: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename daemonsets 01/29/23 03:37:26.069
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:37:26.1
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:37:26.105
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
STEP: Creating simple DaemonSet "daemon-set" 01/29/23 03:37:26.164
STEP: Check that daemon pods launch on every node of the cluster. 01/29/23 03:37:26.177
Jan 29 03:37:26.193: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 29 03:37:26.193: INFO: Node master1 is running 0 daemon pod, expected 1
Jan 29 03:37:27.225: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 29 03:37:27.225: INFO: Node master1 is running 0 daemon pod, expected 1
Jan 29 03:37:28.211: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 29 03:37:28.211: INFO: Node master1 is running 0 daemon pod, expected 1
Jan 29 03:37:29.211: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Jan 29 03:37:29.211: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
STEP: Getting /status 01/29/23 03:37:29.217
Jan 29 03:37:29.224: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 01/29/23 03:37:29.224
Jan 29 03:37:29.238: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 01/29/23 03:37:29.238
Jan 29 03:37:29.242: INFO: Observed &DaemonSet event: ADDED
Jan 29 03:37:29.242: INFO: Observed &DaemonSet event: MODIFIED
Jan 29 03:37:29.243: INFO: Observed &DaemonSet event: MODIFIED
Jan 29 03:37:29.244: INFO: Observed &DaemonSet event: MODIFIED
Jan 29 03:37:29.244: INFO: Observed &DaemonSet event: MODIFIED
Jan 29 03:37:29.245: INFO: Observed &DaemonSet event: MODIFIED
Jan 29 03:37:29.245: INFO: Observed &DaemonSet event: MODIFIED
Jan 29 03:37:29.245: INFO: Found daemon set daemon-set in namespace daemonsets-3950 with labels: map[daemonset-name:daemon-set] annotations: map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 29 03:37:29.245: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 01/29/23 03:37:29.245
STEP: watching for the daemon set status to be patched 01/29/23 03:37:29.254
Jan 29 03:37:29.258: INFO: Observed &DaemonSet event: ADDED
Jan 29 03:37:29.258: INFO: Observed &DaemonSet event: MODIFIED
Jan 29 03:37:29.258: INFO: Observed &DaemonSet event: MODIFIED
Jan 29 03:37:29.259: INFO: Observed &DaemonSet event: MODIFIED
Jan 29 03:37:29.259: INFO: Observed &DaemonSet event: MODIFIED
Jan 29 03:37:29.259: INFO: Observed &DaemonSet event: MODIFIED
Jan 29 03:37:29.259: INFO: Observed &DaemonSet event: MODIFIED
Jan 29 03:37:29.259: INFO: Observed daemon set daemon-set in namespace daemonsets-3950 with annotations: map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 29 03:37:29.259: INFO: Observed &DaemonSet event: MODIFIED
Jan 29 03:37:29.259: INFO: Found daemon set daemon-set in namespace daemonsets-3950 with labels: map[daemonset-name:daemon-set] annotations: map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Jan 29 03:37:29.259: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 01/29/23 03:37:29.267
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3950, will wait for the garbage collector to delete the pods 01/29/23 03:37:29.267
Jan 29 03:37:29.338: INFO: Deleting DaemonSet.extensions daemon-set took: 13.884277ms
Jan 29 03:37:29.439: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.788706ms
Jan 29 03:37:32.146: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 29 03:37:32.146: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 29 03:37:32.152: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"5962628"},"items":null}

Jan 29 03:37:32.158: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"5962628"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan 29 03:37:32.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3950" for this suite. 01/29/23 03:37:32.212
{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","completed":162,"skipped":3066,"failed":0}
------------------------------
• [SLOW TEST] [6.162 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:37:26.068
    Jan 29 03:37:26.068: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename daemonsets 01/29/23 03:37:26.069
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:37:26.1
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:37:26.105
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:861
    STEP: Creating simple DaemonSet "daemon-set" 01/29/23 03:37:26.164
    STEP: Check that daemon pods launch on every node of the cluster. 01/29/23 03:37:26.177
    Jan 29 03:37:26.193: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 29 03:37:26.193: INFO: Node master1 is running 0 daemon pod, expected 1
    Jan 29 03:37:27.225: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 29 03:37:27.225: INFO: Node master1 is running 0 daemon pod, expected 1
    Jan 29 03:37:28.211: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 29 03:37:28.211: INFO: Node master1 is running 0 daemon pod, expected 1
    Jan 29 03:37:29.211: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
    Jan 29 03:37:29.211: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
    STEP: Getting /status 01/29/23 03:37:29.217
    Jan 29 03:37:29.224: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 01/29/23 03:37:29.224
    Jan 29 03:37:29.238: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 01/29/23 03:37:29.238
    Jan 29 03:37:29.242: INFO: Observed &DaemonSet event: ADDED
    Jan 29 03:37:29.242: INFO: Observed &DaemonSet event: MODIFIED
    Jan 29 03:37:29.243: INFO: Observed &DaemonSet event: MODIFIED
    Jan 29 03:37:29.244: INFO: Observed &DaemonSet event: MODIFIED
    Jan 29 03:37:29.244: INFO: Observed &DaemonSet event: MODIFIED
    Jan 29 03:37:29.245: INFO: Observed &DaemonSet event: MODIFIED
    Jan 29 03:37:29.245: INFO: Observed &DaemonSet event: MODIFIED
    Jan 29 03:37:29.245: INFO: Found daemon set daemon-set in namespace daemonsets-3950 with labels: map[daemonset-name:daemon-set] annotations: map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan 29 03:37:29.245: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 01/29/23 03:37:29.245
    STEP: watching for the daemon set status to be patched 01/29/23 03:37:29.254
    Jan 29 03:37:29.258: INFO: Observed &DaemonSet event: ADDED
    Jan 29 03:37:29.258: INFO: Observed &DaemonSet event: MODIFIED
    Jan 29 03:37:29.258: INFO: Observed &DaemonSet event: MODIFIED
    Jan 29 03:37:29.259: INFO: Observed &DaemonSet event: MODIFIED
    Jan 29 03:37:29.259: INFO: Observed &DaemonSet event: MODIFIED
    Jan 29 03:37:29.259: INFO: Observed &DaemonSet event: MODIFIED
    Jan 29 03:37:29.259: INFO: Observed &DaemonSet event: MODIFIED
    Jan 29 03:37:29.259: INFO: Observed daemon set daemon-set in namespace daemonsets-3950 with annotations: map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan 29 03:37:29.259: INFO: Observed &DaemonSet event: MODIFIED
    Jan 29 03:37:29.259: INFO: Found daemon set daemon-set in namespace daemonsets-3950 with labels: map[daemonset-name:daemon-set] annotations: map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Jan 29 03:37:29.259: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 01/29/23 03:37:29.267
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3950, will wait for the garbage collector to delete the pods 01/29/23 03:37:29.267
    Jan 29 03:37:29.338: INFO: Deleting DaemonSet.extensions daemon-set took: 13.884277ms
    Jan 29 03:37:29.439: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.788706ms
    Jan 29 03:37:32.146: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 29 03:37:32.146: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 29 03:37:32.152: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"5962628"},"items":null}

    Jan 29 03:37:32.158: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"5962628"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan 29 03:37:32.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-3950" for this suite. 01/29/23 03:37:32.212
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:37:32.232
Jan 29 03:37:32.232: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename emptydir 01/29/23 03:37:32.233
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:37:32.262
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:37:32.268
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
STEP: Creating a pod to test emptydir 0777 on node default medium 01/29/23 03:37:32.273
Jan 29 03:37:32.289: INFO: Waiting up to 5m0s for pod "pod-ff16ccf2-a251-495a-8f53-6e4dca4e9a7c" in namespace "emptydir-6060" to be "Succeeded or Failed"
Jan 29 03:37:32.295: INFO: Pod "pod-ff16ccf2-a251-495a-8f53-6e4dca4e9a7c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.127923ms
Jan 29 03:37:34.305: INFO: Pod "pod-ff16ccf2-a251-495a-8f53-6e4dca4e9a7c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016366912s
Jan 29 03:37:36.302: INFO: Pod "pod-ff16ccf2-a251-495a-8f53-6e4dca4e9a7c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012920125s
Jan 29 03:37:38.302: INFO: Pod "pod-ff16ccf2-a251-495a-8f53-6e4dca4e9a7c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013231024s
STEP: Saw pod success 01/29/23 03:37:38.302
Jan 29 03:37:38.302: INFO: Pod "pod-ff16ccf2-a251-495a-8f53-6e4dca4e9a7c" satisfied condition "Succeeded or Failed"
Jan 29 03:37:38.308: INFO: Trying to get logs from node slave2 pod pod-ff16ccf2-a251-495a-8f53-6e4dca4e9a7c container test-container: <nil>
STEP: delete the pod 01/29/23 03:37:38.327
Jan 29 03:37:38.433: INFO: Waiting for pod pod-ff16ccf2-a251-495a-8f53-6e4dca4e9a7c to disappear
Jan 29 03:37:38.438: INFO: Pod pod-ff16ccf2-a251-495a-8f53-6e4dca4e9a7c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 29 03:37:38.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6060" for this suite. 01/29/23 03:37:38.463
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":163,"skipped":3075,"failed":0}
------------------------------
• [SLOW TEST] [6.243 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:37:32.232
    Jan 29 03:37:32.232: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename emptydir 01/29/23 03:37:32.233
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:37:32.262
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:37:32.268
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:216
    STEP: Creating a pod to test emptydir 0777 on node default medium 01/29/23 03:37:32.273
    Jan 29 03:37:32.289: INFO: Waiting up to 5m0s for pod "pod-ff16ccf2-a251-495a-8f53-6e4dca4e9a7c" in namespace "emptydir-6060" to be "Succeeded or Failed"
    Jan 29 03:37:32.295: INFO: Pod "pod-ff16ccf2-a251-495a-8f53-6e4dca4e9a7c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.127923ms
    Jan 29 03:37:34.305: INFO: Pod "pod-ff16ccf2-a251-495a-8f53-6e4dca4e9a7c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016366912s
    Jan 29 03:37:36.302: INFO: Pod "pod-ff16ccf2-a251-495a-8f53-6e4dca4e9a7c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012920125s
    Jan 29 03:37:38.302: INFO: Pod "pod-ff16ccf2-a251-495a-8f53-6e4dca4e9a7c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013231024s
    STEP: Saw pod success 01/29/23 03:37:38.302
    Jan 29 03:37:38.302: INFO: Pod "pod-ff16ccf2-a251-495a-8f53-6e4dca4e9a7c" satisfied condition "Succeeded or Failed"
    Jan 29 03:37:38.308: INFO: Trying to get logs from node slave2 pod pod-ff16ccf2-a251-495a-8f53-6e4dca4e9a7c container test-container: <nil>
    STEP: delete the pod 01/29/23 03:37:38.327
    Jan 29 03:37:38.433: INFO: Waiting for pod pod-ff16ccf2-a251-495a-8f53-6e4dca4e9a7c to disappear
    Jan 29 03:37:38.438: INFO: Pod pod-ff16ccf2-a251-495a-8f53-6e4dca4e9a7c no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 29 03:37:38.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-6060" for this suite. 01/29/23 03:37:38.463
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:37:38.476
Jan 29 03:37:38.476: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename downward-api 01/29/23 03:37:38.478
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:37:38.508
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:37:38.514
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
STEP: Creating a pod to test downward API volume plugin 01/29/23 03:37:38.52
Jan 29 03:37:38.541: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fe3ecd1c-b0f4-456a-b5c5-9060886f267b" in namespace "downward-api-5550" to be "Succeeded or Failed"
Jan 29 03:37:38.547: INFO: Pod "downwardapi-volume-fe3ecd1c-b0f4-456a-b5c5-9060886f267b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.984001ms
Jan 29 03:37:40.555: INFO: Pod "downwardapi-volume-fe3ecd1c-b0f4-456a-b5c5-9060886f267b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013927074s
Jan 29 03:37:42.555: INFO: Pod "downwardapi-volume-fe3ecd1c-b0f4-456a-b5c5-9060886f267b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014456715s
Jan 29 03:37:44.555: INFO: Pod "downwardapi-volume-fe3ecd1c-b0f4-456a-b5c5-9060886f267b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014668054s
STEP: Saw pod success 01/29/23 03:37:44.555
Jan 29 03:37:44.556: INFO: Pod "downwardapi-volume-fe3ecd1c-b0f4-456a-b5c5-9060886f267b" satisfied condition "Succeeded or Failed"
Jan 29 03:37:44.561: INFO: Trying to get logs from node slave2 pod downwardapi-volume-fe3ecd1c-b0f4-456a-b5c5-9060886f267b container client-container: <nil>
STEP: delete the pod 01/29/23 03:37:44.579
Jan 29 03:37:44.673: INFO: Waiting for pod downwardapi-volume-fe3ecd1c-b0f4-456a-b5c5-9060886f267b to disappear
Jan 29 03:37:44.678: INFO: Pod downwardapi-volume-fe3ecd1c-b0f4-456a-b5c5-9060886f267b no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 29 03:37:44.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5550" for this suite. 01/29/23 03:37:44.687
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","completed":164,"skipped":3085,"failed":0}
------------------------------
• [SLOW TEST] [6.222 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:37:38.476
    Jan 29 03:37:38.476: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename downward-api 01/29/23 03:37:38.478
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:37:38.508
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:37:38.514
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:206
    STEP: Creating a pod to test downward API volume plugin 01/29/23 03:37:38.52
    Jan 29 03:37:38.541: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fe3ecd1c-b0f4-456a-b5c5-9060886f267b" in namespace "downward-api-5550" to be "Succeeded or Failed"
    Jan 29 03:37:38.547: INFO: Pod "downwardapi-volume-fe3ecd1c-b0f4-456a-b5c5-9060886f267b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.984001ms
    Jan 29 03:37:40.555: INFO: Pod "downwardapi-volume-fe3ecd1c-b0f4-456a-b5c5-9060886f267b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013927074s
    Jan 29 03:37:42.555: INFO: Pod "downwardapi-volume-fe3ecd1c-b0f4-456a-b5c5-9060886f267b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014456715s
    Jan 29 03:37:44.555: INFO: Pod "downwardapi-volume-fe3ecd1c-b0f4-456a-b5c5-9060886f267b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014668054s
    STEP: Saw pod success 01/29/23 03:37:44.555
    Jan 29 03:37:44.556: INFO: Pod "downwardapi-volume-fe3ecd1c-b0f4-456a-b5c5-9060886f267b" satisfied condition "Succeeded or Failed"
    Jan 29 03:37:44.561: INFO: Trying to get logs from node slave2 pod downwardapi-volume-fe3ecd1c-b0f4-456a-b5c5-9060886f267b container client-container: <nil>
    STEP: delete the pod 01/29/23 03:37:44.579
    Jan 29 03:37:44.673: INFO: Waiting for pod downwardapi-volume-fe3ecd1c-b0f4-456a-b5c5-9060886f267b to disappear
    Jan 29 03:37:44.678: INFO: Pod downwardapi-volume-fe3ecd1c-b0f4-456a-b5c5-9060886f267b no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 29 03:37:44.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-5550" for this suite. 01/29/23 03:37:44.687
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:37:44.7
Jan 29 03:37:44.700: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename services 01/29/23 03:37:44.702
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:37:44.729
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:37:44.735
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
STEP: creating service multi-endpoint-test in namespace services-7379 01/29/23 03:37:44.741
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7379 to expose endpoints map[] 01/29/23 03:37:44.758
Jan 29 03:37:44.769: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Jan 29 03:37:45.788: INFO: successfully validated that service multi-endpoint-test in namespace services-7379 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-7379 01/29/23 03:37:45.788
Jan 29 03:37:45.806: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-7379" to be "running and ready"
Jan 29 03:37:45.813: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.784627ms
Jan 29 03:37:45.813: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 29 03:37:47.822: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015335984s
Jan 29 03:37:47.822: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 29 03:37:49.820: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.01375513s
Jan 29 03:37:49.820: INFO: The phase of Pod pod1 is Running (Ready = true)
Jan 29 03:37:49.820: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7379 to expose endpoints map[pod1:[100]] 01/29/23 03:37:49.826
Jan 29 03:37:49.860: INFO: successfully validated that service multi-endpoint-test in namespace services-7379 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-7379 01/29/23 03:37:49.86
Jan 29 03:37:49.872: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-7379" to be "running and ready"
Jan 29 03:37:49.879: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.09825ms
Jan 29 03:37:49.880: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 29 03:37:51.887: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.01467356s
Jan 29 03:37:51.887: INFO: The phase of Pod pod2 is Running (Ready = true)
Jan 29 03:37:51.887: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7379 to expose endpoints map[pod1:[100] pod2:[101]] 01/29/23 03:37:51.894
Jan 29 03:37:51.926: INFO: successfully validated that service multi-endpoint-test in namespace services-7379 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 01/29/23 03:37:51.926
Jan 29 03:37:51.926: INFO: Creating new exec pod
Jan 29 03:37:51.942: INFO: Waiting up to 5m0s for pod "execpod7lvsm" in namespace "services-7379" to be "running"
Jan 29 03:37:51.949: INFO: Pod "execpod7lvsm": Phase="Pending", Reason="", readiness=false. Elapsed: 6.981569ms
Jan 29 03:37:53.959: INFO: Pod "execpod7lvsm": Phase="Running", Reason="", readiness=true. Elapsed: 2.016732994s
Jan 29 03:37:53.959: INFO: Pod "execpod7lvsm" satisfied condition "running"
Jan 29 03:37:54.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-7379 exec execpod7lvsm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
Jan 29 03:37:55.202: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Jan 29 03:37:55.202: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 29 03:37:55.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-7379 exec execpod7lvsm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.105.57.238 80'
Jan 29 03:37:55.439: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.105.57.238 80\nConnection to 100.105.57.238 80 port [tcp/http] succeeded!\n"
Jan 29 03:37:55.440: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 29 03:37:55.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-7379 exec execpod7lvsm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
Jan 29 03:37:55.682: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Jan 29 03:37:55.682: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 29 03:37:55.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-7379 exec execpod7lvsm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.105.57.238 81'
Jan 29 03:37:55.935: INFO: stderr: "+ nc -v -t -w 2 100.105.57.238 81+ \necho hostName\nConnection to 100.105.57.238 81 port [tcp/*] succeeded!\n"
Jan 29 03:37:55.935: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-7379 01/29/23 03:37:55.935
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7379 to expose endpoints map[pod2:[101]] 01/29/23 03:37:56.016
Jan 29 03:37:56.046: INFO: successfully validated that service multi-endpoint-test in namespace services-7379 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-7379 01/29/23 03:37:56.046
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7379 to expose endpoints map[] 01/29/23 03:37:56.142
Jan 29 03:37:56.165: INFO: successfully validated that service multi-endpoint-test in namespace services-7379 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 29 03:37:56.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7379" for this suite. 01/29/23 03:37:56.236
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","completed":165,"skipped":3109,"failed":0}
------------------------------
• [SLOW TEST] [11.547 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:37:44.7
    Jan 29 03:37:44.700: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename services 01/29/23 03:37:44.702
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:37:44.729
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:37:44.735
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:852
    STEP: creating service multi-endpoint-test in namespace services-7379 01/29/23 03:37:44.741
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7379 to expose endpoints map[] 01/29/23 03:37:44.758
    Jan 29 03:37:44.769: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
    Jan 29 03:37:45.788: INFO: successfully validated that service multi-endpoint-test in namespace services-7379 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-7379 01/29/23 03:37:45.788
    Jan 29 03:37:45.806: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-7379" to be "running and ready"
    Jan 29 03:37:45.813: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.784627ms
    Jan 29 03:37:45.813: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan 29 03:37:47.822: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015335984s
    Jan 29 03:37:47.822: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan 29 03:37:49.820: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.01375513s
    Jan 29 03:37:49.820: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jan 29 03:37:49.820: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7379 to expose endpoints map[pod1:[100]] 01/29/23 03:37:49.826
    Jan 29 03:37:49.860: INFO: successfully validated that service multi-endpoint-test in namespace services-7379 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-7379 01/29/23 03:37:49.86
    Jan 29 03:37:49.872: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-7379" to be "running and ready"
    Jan 29 03:37:49.879: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.09825ms
    Jan 29 03:37:49.880: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 29 03:37:51.887: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.01467356s
    Jan 29 03:37:51.887: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jan 29 03:37:51.887: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7379 to expose endpoints map[pod1:[100] pod2:[101]] 01/29/23 03:37:51.894
    Jan 29 03:37:51.926: INFO: successfully validated that service multi-endpoint-test in namespace services-7379 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 01/29/23 03:37:51.926
    Jan 29 03:37:51.926: INFO: Creating new exec pod
    Jan 29 03:37:51.942: INFO: Waiting up to 5m0s for pod "execpod7lvsm" in namespace "services-7379" to be "running"
    Jan 29 03:37:51.949: INFO: Pod "execpod7lvsm": Phase="Pending", Reason="", readiness=false. Elapsed: 6.981569ms
    Jan 29 03:37:53.959: INFO: Pod "execpod7lvsm": Phase="Running", Reason="", readiness=true. Elapsed: 2.016732994s
    Jan 29 03:37:53.959: INFO: Pod "execpod7lvsm" satisfied condition "running"
    Jan 29 03:37:54.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-7379 exec execpod7lvsm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
    Jan 29 03:37:55.202: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Jan 29 03:37:55.202: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 29 03:37:55.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-7379 exec execpod7lvsm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.105.57.238 80'
    Jan 29 03:37:55.439: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.105.57.238 80\nConnection to 100.105.57.238 80 port [tcp/http] succeeded!\n"
    Jan 29 03:37:55.440: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 29 03:37:55.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-7379 exec execpod7lvsm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
    Jan 29 03:37:55.682: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Jan 29 03:37:55.682: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 29 03:37:55.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-7379 exec execpod7lvsm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.105.57.238 81'
    Jan 29 03:37:55.935: INFO: stderr: "+ nc -v -t -w 2 100.105.57.238 81+ \necho hostName\nConnection to 100.105.57.238 81 port [tcp/*] succeeded!\n"
    Jan 29 03:37:55.935: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-7379 01/29/23 03:37:55.935
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7379 to expose endpoints map[pod2:[101]] 01/29/23 03:37:56.016
    Jan 29 03:37:56.046: INFO: successfully validated that service multi-endpoint-test in namespace services-7379 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-7379 01/29/23 03:37:56.046
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7379 to expose endpoints map[] 01/29/23 03:37:56.142
    Jan 29 03:37:56.165: INFO: successfully validated that service multi-endpoint-test in namespace services-7379 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 29 03:37:56.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-7379" for this suite. 01/29/23 03:37:56.236
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:37:56.25
Jan 29 03:37:56.250: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename dns 01/29/23 03:37:56.252
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:37:56.288
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:37:56.293
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 01/29/23 03:37:56.305
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4665.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4665.svc.cluster.local; sleep 1; done
 01/29/23 03:37:56.312
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4665.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4665.svc.cluster.local; sleep 1; done
 01/29/23 03:37:56.313
STEP: creating a pod to probe DNS 01/29/23 03:37:56.313
STEP: submitting the pod to kubernetes 01/29/23 03:37:56.313
Jan 29 03:37:56.331: INFO: Waiting up to 15m0s for pod "dns-test-e7b5ad42-f109-45e6-b6a7-91299142731a" in namespace "dns-4665" to be "running"
Jan 29 03:37:56.337: INFO: Pod "dns-test-e7b5ad42-f109-45e6-b6a7-91299142731a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.76122ms
Jan 29 03:37:58.344: INFO: Pod "dns-test-e7b5ad42-f109-45e6-b6a7-91299142731a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012505764s
Jan 29 03:38:00.346: INFO: Pod "dns-test-e7b5ad42-f109-45e6-b6a7-91299142731a": Phase="Running", Reason="", readiness=true. Elapsed: 4.014711957s
Jan 29 03:38:00.346: INFO: Pod "dns-test-e7b5ad42-f109-45e6-b6a7-91299142731a" satisfied condition "running"
STEP: retrieving the pod 01/29/23 03:38:00.346
STEP: looking for the results for each expected name from probers 01/29/23 03:38:00.353
Jan 29 03:38:00.367: INFO: DNS probes using dns-test-e7b5ad42-f109-45e6-b6a7-91299142731a succeeded

STEP: deleting the pod 01/29/23 03:38:00.367
STEP: changing the externalName to bar.example.com 01/29/23 03:38:00.461
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4665.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4665.svc.cluster.local; sleep 1; done
 01/29/23 03:38:00.474
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4665.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4665.svc.cluster.local; sleep 1; done
 01/29/23 03:38:00.474
STEP: creating a second pod to probe DNS 01/29/23 03:38:00.474
STEP: submitting the pod to kubernetes 01/29/23 03:38:00.474
Jan 29 03:38:00.487: INFO: Waiting up to 15m0s for pod "dns-test-4c41a3a5-1366-432c-a103-dae9a6e39ee8" in namespace "dns-4665" to be "running"
Jan 29 03:38:00.494: INFO: Pod "dns-test-4c41a3a5-1366-432c-a103-dae9a6e39ee8": Phase="Pending", Reason="", readiness=false. Elapsed: 7.046009ms
Jan 29 03:38:02.503: INFO: Pod "dns-test-4c41a3a5-1366-432c-a103-dae9a6e39ee8": Phase="Running", Reason="", readiness=true. Elapsed: 2.016609313s
Jan 29 03:38:02.504: INFO: Pod "dns-test-4c41a3a5-1366-432c-a103-dae9a6e39ee8" satisfied condition "running"
STEP: retrieving the pod 01/29/23 03:38:02.504
STEP: looking for the results for each expected name from probers 01/29/23 03:38:02.51
Jan 29 03:38:02.520: INFO: File wheezy_udp@dns-test-service-3.dns-4665.svc.cluster.local from pod  dns-4665/dns-test-4c41a3a5-1366-432c-a103-dae9a6e39ee8 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 29 03:38:02.528: INFO: File jessie_udp@dns-test-service-3.dns-4665.svc.cluster.local from pod  dns-4665/dns-test-4c41a3a5-1366-432c-a103-dae9a6e39ee8 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 29 03:38:02.528: INFO: Lookups using dns-4665/dns-test-4c41a3a5-1366-432c-a103-dae9a6e39ee8 failed for: [wheezy_udp@dns-test-service-3.dns-4665.svc.cluster.local jessie_udp@dns-test-service-3.dns-4665.svc.cluster.local]

Jan 29 03:38:07.543: INFO: DNS probes using dns-test-4c41a3a5-1366-432c-a103-dae9a6e39ee8 succeeded

STEP: deleting the pod 01/29/23 03:38:07.543
STEP: changing the service to type=ClusterIP 01/29/23 03:38:07.637
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4665.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-4665.svc.cluster.local; sleep 1; done
 01/29/23 03:38:07.659
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4665.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-4665.svc.cluster.local; sleep 1; done
 01/29/23 03:38:07.66
STEP: creating a third pod to probe DNS 01/29/23 03:38:07.66
STEP: submitting the pod to kubernetes 01/29/23 03:38:07.668
Jan 29 03:38:07.694: INFO: Waiting up to 15m0s for pod "dns-test-8d9f0574-f6f4-490e-9a24-da91fc8e166b" in namespace "dns-4665" to be "running"
Jan 29 03:38:07.703: INFO: Pod "dns-test-8d9f0574-f6f4-490e-9a24-da91fc8e166b": Phase="Pending", Reason="", readiness=false. Elapsed: 9.104144ms
Jan 29 03:38:09.710: INFO: Pod "dns-test-8d9f0574-f6f4-490e-9a24-da91fc8e166b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016574074s
Jan 29 03:38:11.710: INFO: Pod "dns-test-8d9f0574-f6f4-490e-9a24-da91fc8e166b": Phase="Running", Reason="", readiness=true. Elapsed: 4.01639681s
Jan 29 03:38:11.710: INFO: Pod "dns-test-8d9f0574-f6f4-490e-9a24-da91fc8e166b" satisfied condition "running"
STEP: retrieving the pod 01/29/23 03:38:11.71
STEP: looking for the results for each expected name from probers 01/29/23 03:38:11.716
Jan 29 03:38:11.731: INFO: DNS probes using dns-test-8d9f0574-f6f4-490e-9a24-da91fc8e166b succeeded

STEP: deleting the pod 01/29/23 03:38:11.732
STEP: deleting the test externalName service 01/29/23 03:38:11.83
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan 29 03:38:11.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4665" for this suite. 01/29/23 03:38:11.877
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","completed":166,"skipped":3125,"failed":0}
------------------------------
• [SLOW TEST] [15.642 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:37:56.25
    Jan 29 03:37:56.250: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename dns 01/29/23 03:37:56.252
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:37:56.288
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:37:56.293
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 01/29/23 03:37:56.305
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4665.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4665.svc.cluster.local; sleep 1; done
     01/29/23 03:37:56.312
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4665.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4665.svc.cluster.local; sleep 1; done
     01/29/23 03:37:56.313
    STEP: creating a pod to probe DNS 01/29/23 03:37:56.313
    STEP: submitting the pod to kubernetes 01/29/23 03:37:56.313
    Jan 29 03:37:56.331: INFO: Waiting up to 15m0s for pod "dns-test-e7b5ad42-f109-45e6-b6a7-91299142731a" in namespace "dns-4665" to be "running"
    Jan 29 03:37:56.337: INFO: Pod "dns-test-e7b5ad42-f109-45e6-b6a7-91299142731a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.76122ms
    Jan 29 03:37:58.344: INFO: Pod "dns-test-e7b5ad42-f109-45e6-b6a7-91299142731a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012505764s
    Jan 29 03:38:00.346: INFO: Pod "dns-test-e7b5ad42-f109-45e6-b6a7-91299142731a": Phase="Running", Reason="", readiness=true. Elapsed: 4.014711957s
    Jan 29 03:38:00.346: INFO: Pod "dns-test-e7b5ad42-f109-45e6-b6a7-91299142731a" satisfied condition "running"
    STEP: retrieving the pod 01/29/23 03:38:00.346
    STEP: looking for the results for each expected name from probers 01/29/23 03:38:00.353
    Jan 29 03:38:00.367: INFO: DNS probes using dns-test-e7b5ad42-f109-45e6-b6a7-91299142731a succeeded

    STEP: deleting the pod 01/29/23 03:38:00.367
    STEP: changing the externalName to bar.example.com 01/29/23 03:38:00.461
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4665.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4665.svc.cluster.local; sleep 1; done
     01/29/23 03:38:00.474
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4665.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4665.svc.cluster.local; sleep 1; done
     01/29/23 03:38:00.474
    STEP: creating a second pod to probe DNS 01/29/23 03:38:00.474
    STEP: submitting the pod to kubernetes 01/29/23 03:38:00.474
    Jan 29 03:38:00.487: INFO: Waiting up to 15m0s for pod "dns-test-4c41a3a5-1366-432c-a103-dae9a6e39ee8" in namespace "dns-4665" to be "running"
    Jan 29 03:38:00.494: INFO: Pod "dns-test-4c41a3a5-1366-432c-a103-dae9a6e39ee8": Phase="Pending", Reason="", readiness=false. Elapsed: 7.046009ms
    Jan 29 03:38:02.503: INFO: Pod "dns-test-4c41a3a5-1366-432c-a103-dae9a6e39ee8": Phase="Running", Reason="", readiness=true. Elapsed: 2.016609313s
    Jan 29 03:38:02.504: INFO: Pod "dns-test-4c41a3a5-1366-432c-a103-dae9a6e39ee8" satisfied condition "running"
    STEP: retrieving the pod 01/29/23 03:38:02.504
    STEP: looking for the results for each expected name from probers 01/29/23 03:38:02.51
    Jan 29 03:38:02.520: INFO: File wheezy_udp@dns-test-service-3.dns-4665.svc.cluster.local from pod  dns-4665/dns-test-4c41a3a5-1366-432c-a103-dae9a6e39ee8 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan 29 03:38:02.528: INFO: File jessie_udp@dns-test-service-3.dns-4665.svc.cluster.local from pod  dns-4665/dns-test-4c41a3a5-1366-432c-a103-dae9a6e39ee8 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan 29 03:38:02.528: INFO: Lookups using dns-4665/dns-test-4c41a3a5-1366-432c-a103-dae9a6e39ee8 failed for: [wheezy_udp@dns-test-service-3.dns-4665.svc.cluster.local jessie_udp@dns-test-service-3.dns-4665.svc.cluster.local]

    Jan 29 03:38:07.543: INFO: DNS probes using dns-test-4c41a3a5-1366-432c-a103-dae9a6e39ee8 succeeded

    STEP: deleting the pod 01/29/23 03:38:07.543
    STEP: changing the service to type=ClusterIP 01/29/23 03:38:07.637
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4665.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-4665.svc.cluster.local; sleep 1; done
     01/29/23 03:38:07.659
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4665.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-4665.svc.cluster.local; sleep 1; done
     01/29/23 03:38:07.66
    STEP: creating a third pod to probe DNS 01/29/23 03:38:07.66
    STEP: submitting the pod to kubernetes 01/29/23 03:38:07.668
    Jan 29 03:38:07.694: INFO: Waiting up to 15m0s for pod "dns-test-8d9f0574-f6f4-490e-9a24-da91fc8e166b" in namespace "dns-4665" to be "running"
    Jan 29 03:38:07.703: INFO: Pod "dns-test-8d9f0574-f6f4-490e-9a24-da91fc8e166b": Phase="Pending", Reason="", readiness=false. Elapsed: 9.104144ms
    Jan 29 03:38:09.710: INFO: Pod "dns-test-8d9f0574-f6f4-490e-9a24-da91fc8e166b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016574074s
    Jan 29 03:38:11.710: INFO: Pod "dns-test-8d9f0574-f6f4-490e-9a24-da91fc8e166b": Phase="Running", Reason="", readiness=true. Elapsed: 4.01639681s
    Jan 29 03:38:11.710: INFO: Pod "dns-test-8d9f0574-f6f4-490e-9a24-da91fc8e166b" satisfied condition "running"
    STEP: retrieving the pod 01/29/23 03:38:11.71
    STEP: looking for the results for each expected name from probers 01/29/23 03:38:11.716
    Jan 29 03:38:11.731: INFO: DNS probes using dns-test-8d9f0574-f6f4-490e-9a24-da91fc8e166b succeeded

    STEP: deleting the pod 01/29/23 03:38:11.732
    STEP: deleting the test externalName service 01/29/23 03:38:11.83
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan 29 03:38:11.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-4665" for this suite. 01/29/23 03:38:11.877
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:38:11.896
Jan 29 03:38:11.896: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename services 01/29/23 03:38:11.897
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:38:11.945
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:38:11.951
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
STEP: creating a service externalname-service with the type=ExternalName in namespace services-1004 01/29/23 03:38:11.958
STEP: changing the ExternalName service to type=ClusterIP 01/29/23 03:38:11.965
STEP: creating replication controller externalname-service in namespace services-1004 01/29/23 03:38:11.994
I0129 03:38:12.004890      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-1004, replica count: 2
I0129 03:38:15.055458      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 29 03:38:15.055: INFO: Creating new exec pod
Jan 29 03:38:15.077: INFO: Waiting up to 5m0s for pod "execpodt8z7h" in namespace "services-1004" to be "running"
Jan 29 03:38:15.092: INFO: Pod "execpodt8z7h": Phase="Pending", Reason="", readiness=false. Elapsed: 14.663702ms
Jan 29 03:38:17.105: INFO: Pod "execpodt8z7h": Phase="Running", Reason="", readiness=true. Elapsed: 2.027391209s
Jan 29 03:38:17.105: INFO: Pod "execpodt8z7h" satisfied condition "running"
Jan 29 03:38:18.105: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-1004 exec execpodt8z7h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jan 29 03:38:18.359: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 29 03:38:18.359: INFO: stdout: ""
Jan 29 03:38:19.360: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-1004 exec execpodt8z7h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jan 29 03:38:19.611: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 29 03:38:19.611: INFO: stdout: "externalname-service-t5wxd"
Jan 29 03:38:19.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-1004 exec execpodt8z7h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.105.249.13 80'
Jan 29 03:38:19.862: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.105.249.13 80\nConnection to 100.105.249.13 80 port [tcp/http] succeeded!\n"
Jan 29 03:38:19.862: INFO: stdout: "externalname-service-zzhqp"
Jan 29 03:38:19.862: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 29 03:38:19.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1004" for this suite. 01/29/23 03:38:19.921
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","completed":167,"skipped":3174,"failed":0}
------------------------------
• [SLOW TEST] [8.043 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:38:11.896
    Jan 29 03:38:11.896: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename services 01/29/23 03:38:11.897
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:38:11.945
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:38:11.951
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1404
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-1004 01/29/23 03:38:11.958
    STEP: changing the ExternalName service to type=ClusterIP 01/29/23 03:38:11.965
    STEP: creating replication controller externalname-service in namespace services-1004 01/29/23 03:38:11.994
    I0129 03:38:12.004890      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-1004, replica count: 2
    I0129 03:38:15.055458      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 29 03:38:15.055: INFO: Creating new exec pod
    Jan 29 03:38:15.077: INFO: Waiting up to 5m0s for pod "execpodt8z7h" in namespace "services-1004" to be "running"
    Jan 29 03:38:15.092: INFO: Pod "execpodt8z7h": Phase="Pending", Reason="", readiness=false. Elapsed: 14.663702ms
    Jan 29 03:38:17.105: INFO: Pod "execpodt8z7h": Phase="Running", Reason="", readiness=true. Elapsed: 2.027391209s
    Jan 29 03:38:17.105: INFO: Pod "execpodt8z7h" satisfied condition "running"
    Jan 29 03:38:18.105: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-1004 exec execpodt8z7h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Jan 29 03:38:18.359: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jan 29 03:38:18.359: INFO: stdout: ""
    Jan 29 03:38:19.360: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-1004 exec execpodt8z7h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Jan 29 03:38:19.611: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jan 29 03:38:19.611: INFO: stdout: "externalname-service-t5wxd"
    Jan 29 03:38:19.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-1004 exec execpodt8z7h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.105.249.13 80'
    Jan 29 03:38:19.862: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.105.249.13 80\nConnection to 100.105.249.13 80 port [tcp/http] succeeded!\n"
    Jan 29 03:38:19.862: INFO: stdout: "externalname-service-zzhqp"
    Jan 29 03:38:19.862: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 29 03:38:19.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-1004" for this suite. 01/29/23 03:38:19.921
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:38:19.94
Jan 29 03:38:19.940: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename disruption 01/29/23 03:38:19.942
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:38:19.982
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:38:19.99
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
STEP: Waiting for the pdb to be processed 01/29/23 03:38:20.018
STEP: Waiting for all pods to be running 01/29/23 03:38:20.091
Jan 29 03:38:20.099: INFO: running pods: 0 < 3
Jan 29 03:38:22.106: INFO: running pods: 2 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jan 29 03:38:24.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-5703" for this suite. 01/29/23 03:38:24.122
{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","completed":168,"skipped":3183,"failed":0}
------------------------------
• [4.200 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:38:19.94
    Jan 29 03:38:19.940: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename disruption 01/29/23 03:38:19.942
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:38:19.982
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:38:19.99
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:140
    STEP: Waiting for the pdb to be processed 01/29/23 03:38:20.018
    STEP: Waiting for all pods to be running 01/29/23 03:38:20.091
    Jan 29 03:38:20.099: INFO: running pods: 0 < 3
    Jan 29 03:38:22.106: INFO: running pods: 2 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jan 29 03:38:24.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-5703" for this suite. 01/29/23 03:38:24.122
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:38:24.143
Jan 29 03:38:24.143: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename services 01/29/23 03:38:24.144
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:38:24.175
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:38:24.181
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206
STEP: fetching services 01/29/23 03:38:24.186
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 29 03:38:24.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2176" for this suite. 01/29/23 03:38:24.202
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","completed":169,"skipped":3208,"failed":0}
------------------------------
• [0.071 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:38:24.143
    Jan 29 03:38:24.143: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename services 01/29/23 03:38:24.144
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:38:24.175
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:38:24.181
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3206
    STEP: fetching services 01/29/23 03:38:24.186
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 29 03:38:24.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-2176" for this suite. 01/29/23 03:38:24.202
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:38:24.214
Jan 29 03:38:24.214: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename projected 01/29/23 03:38:24.216
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:38:24.248
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:38:24.253
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
STEP: Creating a pod to test downward API volume plugin 01/29/23 03:38:24.26
Jan 29 03:38:24.280: INFO: Waiting up to 5m0s for pod "downwardapi-volume-12b485ec-0fb9-409a-b498-d34c0395814e" in namespace "projected-8221" to be "Succeeded or Failed"
Jan 29 03:38:24.287: INFO: Pod "downwardapi-volume-12b485ec-0fb9-409a-b498-d34c0395814e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.12829ms
Jan 29 03:38:26.295: INFO: Pod "downwardapi-volume-12b485ec-0fb9-409a-b498-d34c0395814e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014536999s
Jan 29 03:38:28.294: INFO: Pod "downwardapi-volume-12b485ec-0fb9-409a-b498-d34c0395814e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013723891s
STEP: Saw pod success 01/29/23 03:38:28.294
Jan 29 03:38:28.294: INFO: Pod "downwardapi-volume-12b485ec-0fb9-409a-b498-d34c0395814e" satisfied condition "Succeeded or Failed"
Jan 29 03:38:28.299: INFO: Trying to get logs from node slave1 pod downwardapi-volume-12b485ec-0fb9-409a-b498-d34c0395814e container client-container: <nil>
STEP: delete the pod 01/29/23 03:38:28.331
Jan 29 03:38:28.396: INFO: Waiting for pod downwardapi-volume-12b485ec-0fb9-409a-b498-d34c0395814e to disappear
Jan 29 03:38:28.402: INFO: Pod downwardapi-volume-12b485ec-0fb9-409a-b498-d34c0395814e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 29 03:38:28.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8221" for this suite. 01/29/23 03:38:28.412
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","completed":170,"skipped":3209,"failed":0}
------------------------------
• [4.211 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:38:24.214
    Jan 29 03:38:24.214: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename projected 01/29/23 03:38:24.216
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:38:24.248
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:38:24.253
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:220
    STEP: Creating a pod to test downward API volume plugin 01/29/23 03:38:24.26
    Jan 29 03:38:24.280: INFO: Waiting up to 5m0s for pod "downwardapi-volume-12b485ec-0fb9-409a-b498-d34c0395814e" in namespace "projected-8221" to be "Succeeded or Failed"
    Jan 29 03:38:24.287: INFO: Pod "downwardapi-volume-12b485ec-0fb9-409a-b498-d34c0395814e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.12829ms
    Jan 29 03:38:26.295: INFO: Pod "downwardapi-volume-12b485ec-0fb9-409a-b498-d34c0395814e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014536999s
    Jan 29 03:38:28.294: INFO: Pod "downwardapi-volume-12b485ec-0fb9-409a-b498-d34c0395814e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013723891s
    STEP: Saw pod success 01/29/23 03:38:28.294
    Jan 29 03:38:28.294: INFO: Pod "downwardapi-volume-12b485ec-0fb9-409a-b498-d34c0395814e" satisfied condition "Succeeded or Failed"
    Jan 29 03:38:28.299: INFO: Trying to get logs from node slave1 pod downwardapi-volume-12b485ec-0fb9-409a-b498-d34c0395814e container client-container: <nil>
    STEP: delete the pod 01/29/23 03:38:28.331
    Jan 29 03:38:28.396: INFO: Waiting for pod downwardapi-volume-12b485ec-0fb9-409a-b498-d34c0395814e to disappear
    Jan 29 03:38:28.402: INFO: Pod downwardapi-volume-12b485ec-0fb9-409a-b498-d34c0395814e no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 29 03:38:28.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8221" for this suite. 01/29/23 03:38:28.412
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:38:28.426
Jan 29 03:38:28.426: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename svcaccounts 01/29/23 03:38:28.427
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:38:28.461
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:38:28.468
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
Jan 29 03:38:28.506: INFO: created pod
Jan 29 03:38:28.506: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-1662" to be "Succeeded or Failed"
Jan 29 03:38:28.512: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 5.991602ms
Jan 29 03:38:30.527: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020647042s
Jan 29 03:38:32.521: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014533377s
Jan 29 03:38:34.525: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01819812s
STEP: Saw pod success 01/29/23 03:38:34.525
Jan 29 03:38:34.525: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Jan 29 03:39:04.526: INFO: polling logs
Jan 29 03:39:04.551: INFO: Pod logs: 
I0129 03:38:29.656903       1 log.go:195] OK: Got token
I0129 03:38:29.657028       1 log.go:195] validating with in-cluster discovery
I0129 03:38:29.657491       1 log.go:195] OK: got issuer https://kubernetes.default.svc.cluster.local
I0129 03:38:29.657550       1 log.go:195] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-1662:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1674964108, NotBefore:1674963508, IssuedAt:1674963508, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1662", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"c814c838-fa39-4751-8199-ca00b81a1561"}}}
I0129 03:38:29.674970       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
I0129 03:38:29.688455       1 log.go:195] OK: Validated signature on JWT
I0129 03:38:29.688607       1 log.go:195] OK: Got valid claims from token!
I0129 03:38:29.688663       1 log.go:195] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-1662:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1674964108, NotBefore:1674963508, IssuedAt:1674963508, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1662", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"c814c838-fa39-4751-8199-ca00b81a1561"}}}

Jan 29 03:39:04.551: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jan 29 03:39:04.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1662" for this suite. 01/29/23 03:39:04.589
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","completed":171,"skipped":3215,"failed":0}
------------------------------
• [SLOW TEST] [36.179 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:38:28.426
    Jan 29 03:38:28.426: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename svcaccounts 01/29/23 03:38:28.427
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:38:28.461
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:38:28.468
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:528
    Jan 29 03:38:28.506: INFO: created pod
    Jan 29 03:38:28.506: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-1662" to be "Succeeded or Failed"
    Jan 29 03:38:28.512: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 5.991602ms
    Jan 29 03:38:30.527: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020647042s
    Jan 29 03:38:32.521: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014533377s
    Jan 29 03:38:34.525: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01819812s
    STEP: Saw pod success 01/29/23 03:38:34.525
    Jan 29 03:38:34.525: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Jan 29 03:39:04.526: INFO: polling logs
    Jan 29 03:39:04.551: INFO: Pod logs: 
    I0129 03:38:29.656903       1 log.go:195] OK: Got token
    I0129 03:38:29.657028       1 log.go:195] validating with in-cluster discovery
    I0129 03:38:29.657491       1 log.go:195] OK: got issuer https://kubernetes.default.svc.cluster.local
    I0129 03:38:29.657550       1 log.go:195] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-1662:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1674964108, NotBefore:1674963508, IssuedAt:1674963508, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1662", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"c814c838-fa39-4751-8199-ca00b81a1561"}}}
    I0129 03:38:29.674970       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
    I0129 03:38:29.688455       1 log.go:195] OK: Validated signature on JWT
    I0129 03:38:29.688607       1 log.go:195] OK: Got valid claims from token!
    I0129 03:38:29.688663       1 log.go:195] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-1662:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1674964108, NotBefore:1674963508, IssuedAt:1674963508, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1662", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"c814c838-fa39-4751-8199-ca00b81a1561"}}}

    Jan 29 03:39:04.551: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jan 29 03:39:04.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-1662" for this suite. 01/29/23 03:39:04.589
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:39:04.605
Jan 29 03:39:04.605: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename ephemeral-containers-test 01/29/23 03:39:04.607
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:39:04.659
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:39:04.665
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 01/29/23 03:39:04.671
Jan 29 03:39:04.689: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-3644" to be "running and ready"
Jan 29 03:39:04.695: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.566959ms
Jan 29 03:39:04.695: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 29 03:39:06.710: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.0203788s
Jan 29 03:39:06.710: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Jan 29 03:39:06.710: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 01/29/23 03:39:06.722
Jan 29 03:39:06.735: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-3644" to be "container debugger running"
Jan 29 03:39:06.742: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 7.497713ms
Jan 29 03:39:08.751: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.016193511s
Jan 29 03:39:10.751: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.016289769s
Jan 29 03:39:10.751: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 01/29/23 03:39:10.751
Jan 29 03:39:10.751: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-3644 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 29 03:39:10.751: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
Jan 29 03:39:10.752: INFO: ExecWithOptions: Clientset creation
Jan 29 03:39:10.752: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/ephemeral-containers-test-3644/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Jan 29 03:39:10.879: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:187
Jan 29 03:39:10.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ephemeral-containers-test-3644" for this suite. 01/29/23 03:39:10.903
{"msg":"PASSED [sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]","completed":172,"skipped":3215,"failed":0}
------------------------------
• [SLOW TEST] [6.309 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:39:04.605
    Jan 29 03:39:04.605: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename ephemeral-containers-test 01/29/23 03:39:04.607
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:39:04.659
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:39:04.665
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 01/29/23 03:39:04.671
    Jan 29 03:39:04.689: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-3644" to be "running and ready"
    Jan 29 03:39:04.695: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.566959ms
    Jan 29 03:39:04.695: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan 29 03:39:06.710: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.0203788s
    Jan 29 03:39:06.710: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Jan 29 03:39:06.710: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 01/29/23 03:39:06.722
    Jan 29 03:39:06.735: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-3644" to be "container debugger running"
    Jan 29 03:39:06.742: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 7.497713ms
    Jan 29 03:39:08.751: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.016193511s
    Jan 29 03:39:10.751: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.016289769s
    Jan 29 03:39:10.751: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 01/29/23 03:39:10.751
    Jan 29 03:39:10.751: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-3644 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 29 03:39:10.751: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    Jan 29 03:39:10.752: INFO: ExecWithOptions: Clientset creation
    Jan 29 03:39:10.752: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/ephemeral-containers-test-3644/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Jan 29 03:39:10.879: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan 29 03:39:10.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ephemeral-containers-test-3644" for this suite. 01/29/23 03:39:10.903
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:39:10.916
Jan 29 03:39:10.916: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename pods 01/29/23 03:39:10.918
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:39:10.954
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:39:10.96
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 01/29/23 03:39:10.966
STEP: submitting the pod to kubernetes 01/29/23 03:39:10.966
STEP: verifying QOS class is set on the pod 01/29/23 03:39:10.986
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:187
Jan 29 03:39:10.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7121" for this suite. 01/29/23 03:39:11.006
{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","completed":173,"skipped":3239,"failed":0}
------------------------------
• [0.108 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:39:10.916
    Jan 29 03:39:10.916: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename pods 01/29/23 03:39:10.918
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:39:10.954
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:39:10.96
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 01/29/23 03:39:10.966
    STEP: submitting the pod to kubernetes 01/29/23 03:39:10.966
    STEP: verifying QOS class is set on the pod 01/29/23 03:39:10.986
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:187
    Jan 29 03:39:10.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-7121" for this suite. 01/29/23 03:39:11.006
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:39:11.026
Jan 29 03:39:11.026: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename kubectl 01/29/23 03:39:11.027
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:39:11.068
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:39:11.074
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
STEP: create deployment with httpd image 01/29/23 03:39:11.081
Jan 29 03:39:11.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8557 create -f -'
Jan 29 03:39:11.456: INFO: stderr: ""
Jan 29 03:39:11.456: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 01/29/23 03:39:11.456
Jan 29 03:39:11.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8557 diff -f -'
Jan 29 03:39:11.898: INFO: rc: 1
Jan 29 03:39:11.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8557 delete -f -'
Jan 29 03:39:12.015: INFO: stderr: ""
Jan 29 03:39:12.015: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 29 03:39:12.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8557" for this suite. 01/29/23 03:39:12.025
{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","completed":174,"skipped":3258,"failed":0}
------------------------------
• [1.011 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:923
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:929

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:39:11.026
    Jan 29 03:39:11.026: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename kubectl 01/29/23 03:39:11.027
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:39:11.068
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:39:11.074
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:929
    STEP: create deployment with httpd image 01/29/23 03:39:11.081
    Jan 29 03:39:11.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8557 create -f -'
    Jan 29 03:39:11.456: INFO: stderr: ""
    Jan 29 03:39:11.456: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 01/29/23 03:39:11.456
    Jan 29 03:39:11.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8557 diff -f -'
    Jan 29 03:39:11.898: INFO: rc: 1
    Jan 29 03:39:11.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8557 delete -f -'
    Jan 29 03:39:12.015: INFO: stderr: ""
    Jan 29 03:39:12.015: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 29 03:39:12.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-8557" for this suite. 01/29/23 03:39:12.025
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1810
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:39:12.038
Jan 29 03:39:12.038: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename kubectl 01/29/23 03:39:12.039
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:39:12.074
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:39:12.08
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1810
STEP: Starting the proxy 01/29/23 03:39:12.086
Jan 29 03:39:12.086: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-5968 proxy --unix-socket=/tmp/kubectl-proxy-unix2503134112/test'
STEP: retrieving proxy /api/ output 01/29/23 03:39:12.165
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 29 03:39:12.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5968" for this suite. 01/29/23 03:39:12.176
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","completed":175,"skipped":3267,"failed":0}
------------------------------
• [0.152 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:39:12.038
    Jan 29 03:39:12.038: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename kubectl 01/29/23 03:39:12.039
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:39:12.074
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:39:12.08
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1810
    STEP: Starting the proxy 01/29/23 03:39:12.086
    Jan 29 03:39:12.086: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-5968 proxy --unix-socket=/tmp/kubectl-proxy-unix2503134112/test'
    STEP: retrieving proxy /api/ output 01/29/23 03:39:12.165
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 29 03:39:12.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-5968" for this suite. 01/29/23 03:39:12.176
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:39:12.191
Jan 29 03:39:12.191: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename projected 01/29/23 03:39:12.192
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:39:12.222
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:39:12.228
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
STEP: Creating a pod to test downward API volume plugin 01/29/23 03:39:12.233
Jan 29 03:39:12.253: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3cbd0b95-52cc-44ce-8154-bacc1dfa978c" in namespace "projected-275" to be "Succeeded or Failed"
Jan 29 03:39:12.259: INFO: Pod "downwardapi-volume-3cbd0b95-52cc-44ce-8154-bacc1dfa978c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.041022ms
Jan 29 03:39:14.267: INFO: Pod "downwardapi-volume-3cbd0b95-52cc-44ce-8154-bacc1dfa978c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014150817s
Jan 29 03:39:16.268: INFO: Pod "downwardapi-volume-3cbd0b95-52cc-44ce-8154-bacc1dfa978c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015122681s
STEP: Saw pod success 01/29/23 03:39:16.268
Jan 29 03:39:16.268: INFO: Pod "downwardapi-volume-3cbd0b95-52cc-44ce-8154-bacc1dfa978c" satisfied condition "Succeeded or Failed"
Jan 29 03:39:16.274: INFO: Trying to get logs from node slave1 pod downwardapi-volume-3cbd0b95-52cc-44ce-8154-bacc1dfa978c container client-container: <nil>
STEP: delete the pod 01/29/23 03:39:16.291
Jan 29 03:39:16.397: INFO: Waiting for pod downwardapi-volume-3cbd0b95-52cc-44ce-8154-bacc1dfa978c to disappear
Jan 29 03:39:16.404: INFO: Pod downwardapi-volume-3cbd0b95-52cc-44ce-8154-bacc1dfa978c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 29 03:39:16.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-275" for this suite. 01/29/23 03:39:16.414
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":176,"skipped":3286,"failed":0}
------------------------------
• [4.252 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:39:12.191
    Jan 29 03:39:12.191: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename projected 01/29/23 03:39:12.192
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:39:12.222
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:39:12.228
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:248
    STEP: Creating a pod to test downward API volume plugin 01/29/23 03:39:12.233
    Jan 29 03:39:12.253: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3cbd0b95-52cc-44ce-8154-bacc1dfa978c" in namespace "projected-275" to be "Succeeded or Failed"
    Jan 29 03:39:12.259: INFO: Pod "downwardapi-volume-3cbd0b95-52cc-44ce-8154-bacc1dfa978c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.041022ms
    Jan 29 03:39:14.267: INFO: Pod "downwardapi-volume-3cbd0b95-52cc-44ce-8154-bacc1dfa978c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014150817s
    Jan 29 03:39:16.268: INFO: Pod "downwardapi-volume-3cbd0b95-52cc-44ce-8154-bacc1dfa978c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015122681s
    STEP: Saw pod success 01/29/23 03:39:16.268
    Jan 29 03:39:16.268: INFO: Pod "downwardapi-volume-3cbd0b95-52cc-44ce-8154-bacc1dfa978c" satisfied condition "Succeeded or Failed"
    Jan 29 03:39:16.274: INFO: Trying to get logs from node slave1 pod downwardapi-volume-3cbd0b95-52cc-44ce-8154-bacc1dfa978c container client-container: <nil>
    STEP: delete the pod 01/29/23 03:39:16.291
    Jan 29 03:39:16.397: INFO: Waiting for pod downwardapi-volume-3cbd0b95-52cc-44ce-8154-bacc1dfa978c to disappear
    Jan 29 03:39:16.404: INFO: Pod downwardapi-volume-3cbd0b95-52cc-44ce-8154-bacc1dfa978c no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 29 03:39:16.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-275" for this suite. 01/29/23 03:39:16.414
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:39:16.444
Jan 29 03:39:16.444: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename services 01/29/23 03:39:16.445
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:39:16.537
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:39:16.543
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
STEP: creating service endpoint-test2 in namespace services-2511 01/29/23 03:39:16.549
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2511 to expose endpoints map[] 01/29/23 03:39:16.573
Jan 29 03:39:16.587: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Jan 29 03:39:17.604: INFO: successfully validated that service endpoint-test2 in namespace services-2511 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-2511 01/29/23 03:39:17.604
Jan 29 03:39:17.625: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-2511" to be "running and ready"
Jan 29 03:39:17.632: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.608666ms
Jan 29 03:39:17.632: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 29 03:39:19.640: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.015019623s
Jan 29 03:39:19.640: INFO: The phase of Pod pod1 is Running (Ready = true)
Jan 29 03:39:19.640: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2511 to expose endpoints map[pod1:[80]] 01/29/23 03:39:19.646
Jan 29 03:39:19.668: INFO: successfully validated that service endpoint-test2 in namespace services-2511 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 01/29/23 03:39:19.668
Jan 29 03:39:19.668: INFO: Creating new exec pod
Jan 29 03:39:19.680: INFO: Waiting up to 5m0s for pod "execpods7djd" in namespace "services-2511" to be "running"
Jan 29 03:39:19.686: INFO: Pod "execpods7djd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.296244ms
Jan 29 03:39:21.694: INFO: Pod "execpods7djd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013977276s
Jan 29 03:39:23.694: INFO: Pod "execpods7djd": Phase="Running", Reason="", readiness=true. Elapsed: 4.014115554s
Jan 29 03:39:23.694: INFO: Pod "execpods7djd" satisfied condition "running"
Jan 29 03:39:24.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-2511 exec execpods7djd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jan 29 03:39:24.945: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan 29 03:39:24.945: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 29 03:39:24.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-2511 exec execpods7djd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.105.181.191 80'
Jan 29 03:39:25.191: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.105.181.191 80\nConnection to 100.105.181.191 80 port [tcp/http] succeeded!\n"
Jan 29 03:39:25.191: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-2511 01/29/23 03:39:25.191
Jan 29 03:39:25.205: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-2511" to be "running and ready"
Jan 29 03:39:25.212: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.026609ms
Jan 29 03:39:25.212: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 29 03:39:27.220: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.015432926s
Jan 29 03:39:27.220: INFO: The phase of Pod pod2 is Running (Ready = true)
Jan 29 03:39:27.220: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2511 to expose endpoints map[pod1:[80] pod2:[80]] 01/29/23 03:39:27.227
Jan 29 03:39:27.263: INFO: successfully validated that service endpoint-test2 in namespace services-2511 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 01/29/23 03:39:27.263
Jan 29 03:39:28.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-2511 exec execpods7djd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jan 29 03:39:28.520: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan 29 03:39:28.520: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 29 03:39:28.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-2511 exec execpods7djd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.105.181.191 80'
Jan 29 03:39:28.761: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.105.181.191 80\nConnection to 100.105.181.191 80 port [tcp/http] succeeded!\n"
Jan 29 03:39:28.761: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-2511 01/29/23 03:39:28.762
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2511 to expose endpoints map[pod2:[80]] 01/29/23 03:39:28.892
Jan 29 03:39:28.925: INFO: successfully validated that service endpoint-test2 in namespace services-2511 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 01/29/23 03:39:28.926
Jan 29 03:39:29.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-2511 exec execpods7djd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jan 29 03:39:30.163: INFO: stderr: "+ + echonc hostName -v\n -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan 29 03:39:30.163: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 29 03:39:30.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-2511 exec execpods7djd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.105.181.191 80'
Jan 29 03:39:30.415: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.105.181.191 80\nConnection to 100.105.181.191 80 port [tcp/http] succeeded!\n"
Jan 29 03:39:30.415: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-2511 01/29/23 03:39:30.415
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2511 to expose endpoints map[] 01/29/23 03:39:30.538
Jan 29 03:39:30.558: INFO: successfully validated that service endpoint-test2 in namespace services-2511 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 29 03:39:30.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2511" for this suite. 01/29/23 03:39:30.609
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","completed":177,"skipped":3287,"failed":0}
------------------------------
• [SLOW TEST] [14.178 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:39:16.444
    Jan 29 03:39:16.444: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename services 01/29/23 03:39:16.445
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:39:16.537
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:39:16.543
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:791
    STEP: creating service endpoint-test2 in namespace services-2511 01/29/23 03:39:16.549
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2511 to expose endpoints map[] 01/29/23 03:39:16.573
    Jan 29 03:39:16.587: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
    Jan 29 03:39:17.604: INFO: successfully validated that service endpoint-test2 in namespace services-2511 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-2511 01/29/23 03:39:17.604
    Jan 29 03:39:17.625: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-2511" to be "running and ready"
    Jan 29 03:39:17.632: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.608666ms
    Jan 29 03:39:17.632: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan 29 03:39:19.640: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.015019623s
    Jan 29 03:39:19.640: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jan 29 03:39:19.640: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2511 to expose endpoints map[pod1:[80]] 01/29/23 03:39:19.646
    Jan 29 03:39:19.668: INFO: successfully validated that service endpoint-test2 in namespace services-2511 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 01/29/23 03:39:19.668
    Jan 29 03:39:19.668: INFO: Creating new exec pod
    Jan 29 03:39:19.680: INFO: Waiting up to 5m0s for pod "execpods7djd" in namespace "services-2511" to be "running"
    Jan 29 03:39:19.686: INFO: Pod "execpods7djd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.296244ms
    Jan 29 03:39:21.694: INFO: Pod "execpods7djd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013977276s
    Jan 29 03:39:23.694: INFO: Pod "execpods7djd": Phase="Running", Reason="", readiness=true. Elapsed: 4.014115554s
    Jan 29 03:39:23.694: INFO: Pod "execpods7djd" satisfied condition "running"
    Jan 29 03:39:24.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-2511 exec execpods7djd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Jan 29 03:39:24.945: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jan 29 03:39:24.945: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 29 03:39:24.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-2511 exec execpods7djd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.105.181.191 80'
    Jan 29 03:39:25.191: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.105.181.191 80\nConnection to 100.105.181.191 80 port [tcp/http] succeeded!\n"
    Jan 29 03:39:25.191: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Creating pod pod2 in namespace services-2511 01/29/23 03:39:25.191
    Jan 29 03:39:25.205: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-2511" to be "running and ready"
    Jan 29 03:39:25.212: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.026609ms
    Jan 29 03:39:25.212: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 29 03:39:27.220: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.015432926s
    Jan 29 03:39:27.220: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jan 29 03:39:27.220: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2511 to expose endpoints map[pod1:[80] pod2:[80]] 01/29/23 03:39:27.227
    Jan 29 03:39:27.263: INFO: successfully validated that service endpoint-test2 in namespace services-2511 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 01/29/23 03:39:27.263
    Jan 29 03:39:28.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-2511 exec execpods7djd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Jan 29 03:39:28.520: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jan 29 03:39:28.520: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 29 03:39:28.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-2511 exec execpods7djd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.105.181.191 80'
    Jan 29 03:39:28.761: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.105.181.191 80\nConnection to 100.105.181.191 80 port [tcp/http] succeeded!\n"
    Jan 29 03:39:28.761: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-2511 01/29/23 03:39:28.762
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2511 to expose endpoints map[pod2:[80]] 01/29/23 03:39:28.892
    Jan 29 03:39:28.925: INFO: successfully validated that service endpoint-test2 in namespace services-2511 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 01/29/23 03:39:28.926
    Jan 29 03:39:29.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-2511 exec execpods7djd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Jan 29 03:39:30.163: INFO: stderr: "+ + echonc hostName -v\n -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jan 29 03:39:30.163: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 29 03:39:30.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-2511 exec execpods7djd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.105.181.191 80'
    Jan 29 03:39:30.415: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.105.181.191 80\nConnection to 100.105.181.191 80 port [tcp/http] succeeded!\n"
    Jan 29 03:39:30.415: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod2 in namespace services-2511 01/29/23 03:39:30.415
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2511 to expose endpoints map[] 01/29/23 03:39:30.538
    Jan 29 03:39:30.558: INFO: successfully validated that service endpoint-test2 in namespace services-2511 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 29 03:39:30.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-2511" for this suite. 01/29/23 03:39:30.609
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:39:30.628
Jan 29 03:39:30.628: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename deployment 01/29/23 03:39:30.629
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:39:30.666
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:39:30.674
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
Jan 29 03:39:30.713: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jan 29 03:39:35.723: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/29/23 03:39:35.723
Jan 29 03:39:35.723: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jan 29 03:39:37.734: INFO: Creating deployment "test-rollover-deployment"
Jan 29 03:39:37.760: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jan 29 03:39:39.780: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jan 29 03:39:39.806: INFO: Ensure that both replica sets have 1 created replica
Jan 29 03:39:39.832: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jan 29 03:39:39.852: INFO: Updating deployment test-rollover-deployment
Jan 29 03:39:39.852: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jan 29 03:39:41.866: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jan 29 03:39:41.882: INFO: Make sure deployment "test-rollover-deployment" is complete
Jan 29 03:39:41.897: INFO: all replica sets need to contain the pod-template-hash label
Jan 29 03:39:41.897: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 29, 3, 39, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 3, 39, 37, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 29, 3, 39, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 3, 39, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 29 03:39:43.914: INFO: all replica sets need to contain the pod-template-hash label
Jan 29 03:39:43.914: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 29, 3, 39, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 3, 39, 37, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 29, 3, 39, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 3, 39, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 29 03:39:45.912: INFO: all replica sets need to contain the pod-template-hash label
Jan 29 03:39:45.912: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 29, 3, 39, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 3, 39, 37, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 29, 3, 39, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 3, 39, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 29 03:39:47.913: INFO: all replica sets need to contain the pod-template-hash label
Jan 29 03:39:47.913: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 29, 3, 39, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 3, 39, 37, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 29, 3, 39, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 3, 39, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 29 03:39:49.911: INFO: all replica sets need to contain the pod-template-hash label
Jan 29 03:39:49.911: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 29, 3, 39, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 3, 39, 37, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 29, 3, 39, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 3, 39, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 29 03:39:51.912: INFO: all replica sets need to contain the pod-template-hash label
Jan 29 03:39:51.913: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 29, 3, 39, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 3, 39, 37, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 29, 3, 39, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 3, 39, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 29 03:39:53.912: INFO: 
Jan 29 03:39:53.912: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 29 03:39:53.930: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-6422  d5424f73-faa0-4fde-adc7-3637cf37b42f 5964125 2 2023-01-29 03:39:37 +0000 UTC <nil> <nil> map[name:rollover-pod] map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/revision:2] [] [] []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0x400662c0f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-29 03:39:37 +0000 UTC,LastTransitionTime:2023-01-29 03:39:37 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6d45fd857b" has successfully progressed.,LastUpdateTime:2023-01-29 03:39:52 +0000 UTC,LastTransitionTime:2023-01-29 03:39:37 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 29 03:39:53.938: INFO: New ReplicaSet "test-rollover-deployment-6d45fd857b" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6d45fd857b  deployment-6422  87a9274f-a594-426d-ba16-5daece68f2cc 5964114 2 2023-01-29 03:39:39 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment d5424f73-faa0-4fde-adc7-3637cf37b42f 0x400662c65e 0x400662c65f}] [] []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6d45fd857b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0x400662c6f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 29 03:39:53.938: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jan 29 03:39:53.938: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-6422  64f38949-b585-477b-b09c-f68b15942167 5964124 2 2023-01-29 03:39:30 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment d5424f73-faa0-4fde-adc7-3637cf37b42f 0x400662c49e 0x400662c49f}] [] []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0x400662c508 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 29 03:39:53.938: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-6422  166f59f4-a082-4b57-b932-33380b496037 5964048 2 2023-01-29 03:39:37 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment d5424f73-faa0-4fde-adc7-3637cf37b42f 0x400662c56e 0x400662c56f}] [] []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0x400662c5f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 29 03:39:53.945: INFO: Pod "test-rollover-deployment-6d45fd857b-cskkp" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6d45fd857b-cskkp test-rollover-deployment-6d45fd857b- deployment-6422  f4a6e69c-538a-41bb-af5c-d1c7a679af0b 5964068 0 2023-01-29 03:39:39 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "100.101.49.45"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "100.101.49.45"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet test-rollover-deployment-6d45fd857b 87a9274f-a594-426d-ba16-5daece68f2cc 0x400662cc67 0x400662cc68}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w6wfv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w6wfv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:39:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:39:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:39:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:39:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.245,PodIP:100.101.49.45,StartTime:2023-01-29 03:39:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-29 03:39:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry-jinan-lab.inspurcloud.cn/library/cke/agnhost-arm64:2.40,ImageID:docker-pullable://registry-jinan-lab.inspurcloud.cn/library/cke/agnhost-arm64@sha256:3df3d52919cdbe24d42667d1be02605b720b80c8133f2203c4a78f77e3f2429e,ContainerID:docker://998fc2f393ea9e6493b3d6cac980e0955711ba4642634ffacfb5cb66f42edc47,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.101.49.45,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan 29 03:39:53.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6422" for this suite. 01/29/23 03:39:53.955
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","completed":178,"skipped":3361,"failed":0}
------------------------------
• [SLOW TEST] [23.339 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:39:30.628
    Jan 29 03:39:30.628: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename deployment 01/29/23 03:39:30.629
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:39:30.666
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:39:30.674
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    Jan 29 03:39:30.713: INFO: Pod name rollover-pod: Found 0 pods out of 1
    Jan 29 03:39:35.723: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/29/23 03:39:35.723
    Jan 29 03:39:35.723: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Jan 29 03:39:37.734: INFO: Creating deployment "test-rollover-deployment"
    Jan 29 03:39:37.760: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Jan 29 03:39:39.780: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Jan 29 03:39:39.806: INFO: Ensure that both replica sets have 1 created replica
    Jan 29 03:39:39.832: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Jan 29 03:39:39.852: INFO: Updating deployment test-rollover-deployment
    Jan 29 03:39:39.852: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Jan 29 03:39:41.866: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Jan 29 03:39:41.882: INFO: Make sure deployment "test-rollover-deployment" is complete
    Jan 29 03:39:41.897: INFO: all replica sets need to contain the pod-template-hash label
    Jan 29 03:39:41.897: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 29, 3, 39, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 3, 39, 37, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 29, 3, 39, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 3, 39, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 29 03:39:43.914: INFO: all replica sets need to contain the pod-template-hash label
    Jan 29 03:39:43.914: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 29, 3, 39, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 3, 39, 37, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 29, 3, 39, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 3, 39, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 29 03:39:45.912: INFO: all replica sets need to contain the pod-template-hash label
    Jan 29 03:39:45.912: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 29, 3, 39, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 3, 39, 37, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 29, 3, 39, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 3, 39, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 29 03:39:47.913: INFO: all replica sets need to contain the pod-template-hash label
    Jan 29 03:39:47.913: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 29, 3, 39, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 3, 39, 37, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 29, 3, 39, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 3, 39, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 29 03:39:49.911: INFO: all replica sets need to contain the pod-template-hash label
    Jan 29 03:39:49.911: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 29, 3, 39, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 3, 39, 37, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 29, 3, 39, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 3, 39, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 29 03:39:51.912: INFO: all replica sets need to contain the pod-template-hash label
    Jan 29 03:39:51.913: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 29, 3, 39, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 3, 39, 37, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 29, 3, 39, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 3, 39, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 29 03:39:53.912: INFO: 
    Jan 29 03:39:53.912: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 29 03:39:53.930: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-6422  d5424f73-faa0-4fde-adc7-3637cf37b42f 5964125 2 2023-01-29 03:39:37 +0000 UTC <nil> <nil> map[name:rollover-pod] map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/revision:2] [] [] []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0x400662c0f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-29 03:39:37 +0000 UTC,LastTransitionTime:2023-01-29 03:39:37 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6d45fd857b" has successfully progressed.,LastUpdateTime:2023-01-29 03:39:52 +0000 UTC,LastTransitionTime:2023-01-29 03:39:37 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan 29 03:39:53.938: INFO: New ReplicaSet "test-rollover-deployment-6d45fd857b" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6d45fd857b  deployment-6422  87a9274f-a594-426d-ba16-5daece68f2cc 5964114 2 2023-01-29 03:39:39 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment d5424f73-faa0-4fde-adc7-3637cf37b42f 0x400662c65e 0x400662c65f}] [] []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6d45fd857b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0x400662c6f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 29 03:39:53.938: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Jan 29 03:39:53.938: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-6422  64f38949-b585-477b-b09c-f68b15942167 5964124 2 2023-01-29 03:39:30 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment d5424f73-faa0-4fde-adc7-3637cf37b42f 0x400662c49e 0x400662c49f}] [] []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0x400662c508 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 29 03:39:53.938: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-6422  166f59f4-a082-4b57-b932-33380b496037 5964048 2 2023-01-29 03:39:37 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment d5424f73-faa0-4fde-adc7-3637cf37b42f 0x400662c56e 0x400662c56f}] [] []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0x400662c5f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 29 03:39:53.945: INFO: Pod "test-rollover-deployment-6d45fd857b-cskkp" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6d45fd857b-cskkp test-rollover-deployment-6d45fd857b- deployment-6422  f4a6e69c-538a-41bb-af5c-d1c7a679af0b 5964068 0 2023-01-29 03:39:39 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "",
        "ips": [
            "100.101.49.45"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "",
        "ips": [
            "100.101.49.45"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet test-rollover-deployment-6d45fd857b 87a9274f-a594-426d-ba16-5daece68f2cc 0x400662cc67 0x400662cc68}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w6wfv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w6wfv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:39:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:39:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:39:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:39:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.245,PodIP:100.101.49.45,StartTime:2023-01-29 03:39:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-29 03:39:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry-jinan-lab.inspurcloud.cn/library/cke/agnhost-arm64:2.40,ImageID:docker-pullable://registry-jinan-lab.inspurcloud.cn/library/cke/agnhost-arm64@sha256:3df3d52919cdbe24d42667d1be02605b720b80c8133f2203c4a78f77e3f2429e,ContainerID:docker://998fc2f393ea9e6493b3d6cac980e0955711ba4642634ffacfb5cb66f42edc47,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.101.49.45,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan 29 03:39:53.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-6422" for this suite. 01/29/23 03:39:53.955
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:39:53.968
Jan 29 03:39:53.968: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 01/29/23 03:39:53.969
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:39:54.003
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:39:54.008
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 01/29/23 03:39:54.014
STEP: Creating hostNetwork=false pod 01/29/23 03:39:54.014
Jan 29 03:39:54.035: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-1143" to be "running and ready"
Jan 29 03:39:54.045: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 10.125631ms
Jan 29 03:39:54.045: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 29 03:39:56.052: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.017077297s
Jan 29 03:39:56.052: INFO: The phase of Pod test-pod is Running (Ready = true)
Jan 29 03:39:56.052: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 01/29/23 03:39:56.058
Jan 29 03:39:56.070: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-1143" to be "running and ready"
Jan 29 03:39:56.076: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.75892ms
Jan 29 03:39:56.076: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 29 03:39:58.086: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.015694807s
Jan 29 03:39:58.086: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Jan 29 03:39:58.086: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 01/29/23 03:39:58.093
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 01/29/23 03:39:58.093
Jan 29 03:39:58.093: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1143 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 29 03:39:58.093: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
Jan 29 03:39:58.095: INFO: ExecWithOptions: Clientset creation
Jan 29 03:39:58.095: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1143/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 29 03:39:58.239: INFO: Exec stderr: ""
Jan 29 03:39:58.239: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1143 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 29 03:39:58.239: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
Jan 29 03:39:58.240: INFO: ExecWithOptions: Clientset creation
Jan 29 03:39:58.240: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1143/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 29 03:39:58.363: INFO: Exec stderr: ""
Jan 29 03:39:58.363: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1143 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 29 03:39:58.363: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
Jan 29 03:39:58.364: INFO: ExecWithOptions: Clientset creation
Jan 29 03:39:58.364: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1143/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 29 03:39:58.483: INFO: Exec stderr: ""
Jan 29 03:39:58.483: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1143 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 29 03:39:58.484: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
Jan 29 03:39:58.485: INFO: ExecWithOptions: Clientset creation
Jan 29 03:39:58.485: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1143/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 29 03:39:58.613: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 01/29/23 03:39:58.613
Jan 29 03:39:58.613: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1143 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 29 03:39:58.613: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
Jan 29 03:39:58.614: INFO: ExecWithOptions: Clientset creation
Jan 29 03:39:58.614: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1143/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Jan 29 03:39:58.749: INFO: Exec stderr: ""
Jan 29 03:39:58.749: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1143 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 29 03:39:58.749: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
Jan 29 03:39:58.750: INFO: ExecWithOptions: Clientset creation
Jan 29 03:39:58.750: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1143/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Jan 29 03:39:58.875: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 01/29/23 03:39:58.875
Jan 29 03:39:58.875: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1143 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 29 03:39:58.875: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
Jan 29 03:39:58.876: INFO: ExecWithOptions: Clientset creation
Jan 29 03:39:58.876: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1143/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 29 03:39:59.021: INFO: Exec stderr: ""
Jan 29 03:39:59.021: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1143 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 29 03:39:59.021: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
Jan 29 03:39:59.022: INFO: ExecWithOptions: Clientset creation
Jan 29 03:39:59.022: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1143/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 29 03:39:59.181: INFO: Exec stderr: ""
Jan 29 03:39:59.181: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1143 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 29 03:39:59.181: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
Jan 29 03:39:59.182: INFO: ExecWithOptions: Clientset creation
Jan 29 03:39:59.182: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1143/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 29 03:39:59.319: INFO: Exec stderr: ""
Jan 29 03:39:59.319: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1143 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 29 03:39:59.319: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
Jan 29 03:39:59.320: INFO: ExecWithOptions: Clientset creation
Jan 29 03:39:59.320: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1143/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 29 03:39:59.458: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:187
Jan 29 03:39:59.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-1143" for this suite. 01/29/23 03:39:59.469
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","completed":179,"skipped":3363,"failed":0}
------------------------------
• [SLOW TEST] [5.514 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:39:53.968
    Jan 29 03:39:53.968: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 01/29/23 03:39:53.969
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:39:54.003
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:39:54.008
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 01/29/23 03:39:54.014
    STEP: Creating hostNetwork=false pod 01/29/23 03:39:54.014
    Jan 29 03:39:54.035: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-1143" to be "running and ready"
    Jan 29 03:39:54.045: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 10.125631ms
    Jan 29 03:39:54.045: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan 29 03:39:56.052: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.017077297s
    Jan 29 03:39:56.052: INFO: The phase of Pod test-pod is Running (Ready = true)
    Jan 29 03:39:56.052: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 01/29/23 03:39:56.058
    Jan 29 03:39:56.070: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-1143" to be "running and ready"
    Jan 29 03:39:56.076: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.75892ms
    Jan 29 03:39:56.076: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan 29 03:39:58.086: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.015694807s
    Jan 29 03:39:58.086: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Jan 29 03:39:58.086: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 01/29/23 03:39:58.093
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 01/29/23 03:39:58.093
    Jan 29 03:39:58.093: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1143 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 29 03:39:58.093: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    Jan 29 03:39:58.095: INFO: ExecWithOptions: Clientset creation
    Jan 29 03:39:58.095: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1143/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan 29 03:39:58.239: INFO: Exec stderr: ""
    Jan 29 03:39:58.239: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1143 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 29 03:39:58.239: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    Jan 29 03:39:58.240: INFO: ExecWithOptions: Clientset creation
    Jan 29 03:39:58.240: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1143/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan 29 03:39:58.363: INFO: Exec stderr: ""
    Jan 29 03:39:58.363: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1143 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 29 03:39:58.363: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    Jan 29 03:39:58.364: INFO: ExecWithOptions: Clientset creation
    Jan 29 03:39:58.364: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1143/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan 29 03:39:58.483: INFO: Exec stderr: ""
    Jan 29 03:39:58.483: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1143 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 29 03:39:58.484: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    Jan 29 03:39:58.485: INFO: ExecWithOptions: Clientset creation
    Jan 29 03:39:58.485: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1143/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan 29 03:39:58.613: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 01/29/23 03:39:58.613
    Jan 29 03:39:58.613: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1143 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 29 03:39:58.613: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    Jan 29 03:39:58.614: INFO: ExecWithOptions: Clientset creation
    Jan 29 03:39:58.614: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1143/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Jan 29 03:39:58.749: INFO: Exec stderr: ""
    Jan 29 03:39:58.749: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1143 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 29 03:39:58.749: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    Jan 29 03:39:58.750: INFO: ExecWithOptions: Clientset creation
    Jan 29 03:39:58.750: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1143/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Jan 29 03:39:58.875: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 01/29/23 03:39:58.875
    Jan 29 03:39:58.875: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1143 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 29 03:39:58.875: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    Jan 29 03:39:58.876: INFO: ExecWithOptions: Clientset creation
    Jan 29 03:39:58.876: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1143/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan 29 03:39:59.021: INFO: Exec stderr: ""
    Jan 29 03:39:59.021: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1143 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 29 03:39:59.021: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    Jan 29 03:39:59.022: INFO: ExecWithOptions: Clientset creation
    Jan 29 03:39:59.022: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1143/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan 29 03:39:59.181: INFO: Exec stderr: ""
    Jan 29 03:39:59.181: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1143 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 29 03:39:59.181: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    Jan 29 03:39:59.182: INFO: ExecWithOptions: Clientset creation
    Jan 29 03:39:59.182: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1143/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan 29 03:39:59.319: INFO: Exec stderr: ""
    Jan 29 03:39:59.319: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1143 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 29 03:39:59.319: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    Jan 29 03:39:59.320: INFO: ExecWithOptions: Clientset creation
    Jan 29 03:39:59.320: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1143/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan 29 03:39:59.458: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:187
    Jan 29 03:39:59.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-1143" for this suite. 01/29/23 03:39:59.469
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:39:59.485
Jan 29 03:39:59.485: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename projected 01/29/23 03:39:59.486
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:39:59.518
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:39:59.524
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
STEP: Creating projection with secret that has name projected-secret-test-5268f026-8be1-492f-b492-4f8a5f64f0f6 01/29/23 03:39:59.529
STEP: Creating a pod to test consume secrets 01/29/23 03:39:59.536
Jan 29 03:39:59.554: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5b4b15ba-dcff-4e16-960c-93548ea60b02" in namespace "projected-125" to be "Succeeded or Failed"
Jan 29 03:39:59.560: INFO: Pod "pod-projected-secrets-5b4b15ba-dcff-4e16-960c-93548ea60b02": Phase="Pending", Reason="", readiness=false. Elapsed: 5.7982ms
Jan 29 03:40:01.570: INFO: Pod "pod-projected-secrets-5b4b15ba-dcff-4e16-960c-93548ea60b02": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015367265s
Jan 29 03:40:03.569: INFO: Pod "pod-projected-secrets-5b4b15ba-dcff-4e16-960c-93548ea60b02": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014409296s
STEP: Saw pod success 01/29/23 03:40:03.569
Jan 29 03:40:03.569: INFO: Pod "pod-projected-secrets-5b4b15ba-dcff-4e16-960c-93548ea60b02" satisfied condition "Succeeded or Failed"
Jan 29 03:40:03.578: INFO: Trying to get logs from node slave2 pod pod-projected-secrets-5b4b15ba-dcff-4e16-960c-93548ea60b02 container projected-secret-volume-test: <nil>
STEP: delete the pod 01/29/23 03:40:03.595
Jan 29 03:40:03.743: INFO: Waiting for pod pod-projected-secrets-5b4b15ba-dcff-4e16-960c-93548ea60b02 to disappear
Jan 29 03:40:03.751: INFO: Pod pod-projected-secrets-5b4b15ba-dcff-4e16-960c-93548ea60b02 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan 29 03:40:03.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-125" for this suite. 01/29/23 03:40:03.764
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":180,"skipped":3393,"failed":0}
------------------------------
• [4.309 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:39:59.485
    Jan 29 03:39:59.485: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename projected 01/29/23 03:39:59.486
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:39:59.518
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:39:59.524
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:66
    STEP: Creating projection with secret that has name projected-secret-test-5268f026-8be1-492f-b492-4f8a5f64f0f6 01/29/23 03:39:59.529
    STEP: Creating a pod to test consume secrets 01/29/23 03:39:59.536
    Jan 29 03:39:59.554: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5b4b15ba-dcff-4e16-960c-93548ea60b02" in namespace "projected-125" to be "Succeeded or Failed"
    Jan 29 03:39:59.560: INFO: Pod "pod-projected-secrets-5b4b15ba-dcff-4e16-960c-93548ea60b02": Phase="Pending", Reason="", readiness=false. Elapsed: 5.7982ms
    Jan 29 03:40:01.570: INFO: Pod "pod-projected-secrets-5b4b15ba-dcff-4e16-960c-93548ea60b02": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015367265s
    Jan 29 03:40:03.569: INFO: Pod "pod-projected-secrets-5b4b15ba-dcff-4e16-960c-93548ea60b02": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014409296s
    STEP: Saw pod success 01/29/23 03:40:03.569
    Jan 29 03:40:03.569: INFO: Pod "pod-projected-secrets-5b4b15ba-dcff-4e16-960c-93548ea60b02" satisfied condition "Succeeded or Failed"
    Jan 29 03:40:03.578: INFO: Trying to get logs from node slave2 pod pod-projected-secrets-5b4b15ba-dcff-4e16-960c-93548ea60b02 container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/29/23 03:40:03.595
    Jan 29 03:40:03.743: INFO: Waiting for pod pod-projected-secrets-5b4b15ba-dcff-4e16-960c-93548ea60b02 to disappear
    Jan 29 03:40:03.751: INFO: Pod pod-projected-secrets-5b4b15ba-dcff-4e16-960c-93548ea60b02 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan 29 03:40:03.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-125" for this suite. 01/29/23 03:40:03.764
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:40:03.794
Jan 29 03:40:03.795: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename crd-publish-openapi 01/29/23 03:40:03.796
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:40:03.836
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:40:03.847
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
Jan 29 03:40:03.857: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/29/23 03:40:08.828
Jan 29 03:40:08.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=crd-publish-openapi-9958 --namespace=crd-publish-openapi-9958 create -f -'
Jan 29 03:40:10.269: INFO: stderr: ""
Jan 29 03:40:10.269: INFO: stdout: "e2e-test-crd-publish-openapi-3705-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jan 29 03:40:10.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=crd-publish-openapi-9958 --namespace=crd-publish-openapi-9958 delete e2e-test-crd-publish-openapi-3705-crds test-cr'
Jan 29 03:40:10.407: INFO: stderr: ""
Jan 29 03:40:10.407: INFO: stdout: "e2e-test-crd-publish-openapi-3705-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Jan 29 03:40:10.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=crd-publish-openapi-9958 --namespace=crd-publish-openapi-9958 apply -f -'
Jan 29 03:40:11.934: INFO: stderr: ""
Jan 29 03:40:11.934: INFO: stdout: "e2e-test-crd-publish-openapi-3705-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jan 29 03:40:11.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=crd-publish-openapi-9958 --namespace=crd-publish-openapi-9958 delete e2e-test-crd-publish-openapi-3705-crds test-cr'
Jan 29 03:40:12.104: INFO: stderr: ""
Jan 29 03:40:12.104: INFO: stdout: "e2e-test-crd-publish-openapi-3705-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 01/29/23 03:40:12.104
Jan 29 03:40:12.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=crd-publish-openapi-9958 explain e2e-test-crd-publish-openapi-3705-crds'
Jan 29 03:40:12.508: INFO: stderr: ""
Jan 29 03:40:12.508: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3705-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 29 03:40:17.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9958" for this suite. 01/29/23 03:40:17.405
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","completed":181,"skipped":3396,"failed":0}
------------------------------
• [SLOW TEST] [13.622 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:40:03.794
    Jan 29 03:40:03.795: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename crd-publish-openapi 01/29/23 03:40:03.796
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:40:03.836
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:40:03.847
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:193
    Jan 29 03:40:03.857: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/29/23 03:40:08.828
    Jan 29 03:40:08.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=crd-publish-openapi-9958 --namespace=crd-publish-openapi-9958 create -f -'
    Jan 29 03:40:10.269: INFO: stderr: ""
    Jan 29 03:40:10.269: INFO: stdout: "e2e-test-crd-publish-openapi-3705-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Jan 29 03:40:10.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=crd-publish-openapi-9958 --namespace=crd-publish-openapi-9958 delete e2e-test-crd-publish-openapi-3705-crds test-cr'
    Jan 29 03:40:10.407: INFO: stderr: ""
    Jan 29 03:40:10.407: INFO: stdout: "e2e-test-crd-publish-openapi-3705-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Jan 29 03:40:10.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=crd-publish-openapi-9958 --namespace=crd-publish-openapi-9958 apply -f -'
    Jan 29 03:40:11.934: INFO: stderr: ""
    Jan 29 03:40:11.934: INFO: stdout: "e2e-test-crd-publish-openapi-3705-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Jan 29 03:40:11.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=crd-publish-openapi-9958 --namespace=crd-publish-openapi-9958 delete e2e-test-crd-publish-openapi-3705-crds test-cr'
    Jan 29 03:40:12.104: INFO: stderr: ""
    Jan 29 03:40:12.104: INFO: stdout: "e2e-test-crd-publish-openapi-3705-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 01/29/23 03:40:12.104
    Jan 29 03:40:12.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=crd-publish-openapi-9958 explain e2e-test-crd-publish-openapi-3705-crds'
    Jan 29 03:40:12.508: INFO: stderr: ""
    Jan 29 03:40:12.508: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3705-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 29 03:40:17.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-9958" for this suite. 01/29/23 03:40:17.405
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:40:17.417
Jan 29 03:40:17.418: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename pods 01/29/23 03:40:17.419
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:40:17.457
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:40:17.463
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
Jan 29 03:40:17.473: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: creating the pod 01/29/23 03:40:17.474
STEP: submitting the pod to kubernetes 01/29/23 03:40:17.474
Jan 29 03:40:17.501: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-4c797824-47ff-4611-9b88-16e8bf1a96b1" in namespace "pods-2311" to be "running and ready"
Jan 29 03:40:17.508: INFO: Pod "pod-logs-websocket-4c797824-47ff-4611-9b88-16e8bf1a96b1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.328504ms
Jan 29 03:40:17.508: INFO: The phase of Pod pod-logs-websocket-4c797824-47ff-4611-9b88-16e8bf1a96b1 is Pending, waiting for it to be Running (with Ready = true)
Jan 29 03:40:19.516: INFO: Pod "pod-logs-websocket-4c797824-47ff-4611-9b88-16e8bf1a96b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.014504959s
Jan 29 03:40:19.516: INFO: The phase of Pod pod-logs-websocket-4c797824-47ff-4611-9b88-16e8bf1a96b1 is Running (Ready = true)
Jan 29 03:40:19.516: INFO: Pod "pod-logs-websocket-4c797824-47ff-4611-9b88-16e8bf1a96b1" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 29 03:40:19.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2311" for this suite. 01/29/23 03:40:19.571
{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","completed":182,"skipped":3405,"failed":0}
------------------------------
• [2.163 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:40:17.417
    Jan 29 03:40:17.418: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename pods 01/29/23 03:40:17.419
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:40:17.457
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:40:17.463
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:617
    Jan 29 03:40:17.473: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: creating the pod 01/29/23 03:40:17.474
    STEP: submitting the pod to kubernetes 01/29/23 03:40:17.474
    Jan 29 03:40:17.501: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-4c797824-47ff-4611-9b88-16e8bf1a96b1" in namespace "pods-2311" to be "running and ready"
    Jan 29 03:40:17.508: INFO: Pod "pod-logs-websocket-4c797824-47ff-4611-9b88-16e8bf1a96b1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.328504ms
    Jan 29 03:40:17.508: INFO: The phase of Pod pod-logs-websocket-4c797824-47ff-4611-9b88-16e8bf1a96b1 is Pending, waiting for it to be Running (with Ready = true)
    Jan 29 03:40:19.516: INFO: Pod "pod-logs-websocket-4c797824-47ff-4611-9b88-16e8bf1a96b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.014504959s
    Jan 29 03:40:19.516: INFO: The phase of Pod pod-logs-websocket-4c797824-47ff-4611-9b88-16e8bf1a96b1 is Running (Ready = true)
    Jan 29 03:40:19.516: INFO: Pod "pod-logs-websocket-4c797824-47ff-4611-9b88-16e8bf1a96b1" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 29 03:40:19.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-2311" for this suite. 01/29/23 03:40:19.571
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:40:19.582
Jan 29 03:40:19.582: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename webhook 01/29/23 03:40:19.583
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:40:19.609
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:40:19.615
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/29/23 03:40:19.642
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/29/23 03:40:20.708
STEP: Deploying the webhook pod 01/29/23 03:40:20.722
STEP: Wait for the deployment to be ready 01/29/23 03:40:20.745
Jan 29 03:40:20.759: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/29/23 03:40:22.779
STEP: Verifying the service has paired with the endpoint 01/29/23 03:40:22.801
Jan 29 03:40:23.801: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 01/29/23 03:40:23.807
STEP: create a pod that should be updated by the webhook 01/29/23 03:40:23.832
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 29 03:40:23.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1996" for this suite. 01/29/23 03:40:23.886
STEP: Destroying namespace "webhook-1996-markers" for this suite. 01/29/23 03:40:23.896
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","completed":183,"skipped":3414,"failed":0}
------------------------------
• [4.410 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:40:19.582
    Jan 29 03:40:19.582: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename webhook 01/29/23 03:40:19.583
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:40:19.609
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:40:19.615
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/29/23 03:40:19.642
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/29/23 03:40:20.708
    STEP: Deploying the webhook pod 01/29/23 03:40:20.722
    STEP: Wait for the deployment to be ready 01/29/23 03:40:20.745
    Jan 29 03:40:20.759: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/29/23 03:40:22.779
    STEP: Verifying the service has paired with the endpoint 01/29/23 03:40:22.801
    Jan 29 03:40:23.801: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:263
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 01/29/23 03:40:23.807
    STEP: create a pod that should be updated by the webhook 01/29/23 03:40:23.832
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 29 03:40:23.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-1996" for this suite. 01/29/23 03:40:23.886
    STEP: Destroying namespace "webhook-1996-markers" for this suite. 01/29/23 03:40:23.896
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:40:23.995
Jan 29 03:40:23.996: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename init-container 01/29/23 03:40:24
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:40:24.041
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:40:24.048
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
STEP: creating the pod 01/29/23 03:40:24.054
Jan 29 03:40:24.054: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Jan 29 03:40:29.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2134" for this suite. 01/29/23 03:40:29.761
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","completed":184,"skipped":3451,"failed":0}
------------------------------
• [SLOW TEST] [5.775 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:40:23.995
    Jan 29 03:40:23.996: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename init-container 01/29/23 03:40:24
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:40:24.041
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:40:24.048
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:176
    STEP: creating the pod 01/29/23 03:40:24.054
    Jan 29 03:40:24.054: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan 29 03:40:29.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-2134" for this suite. 01/29/23 03:40:29.761
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:40:29.775
Jan 29 03:40:29.775: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename sched-pred 01/29/23 03:40:29.776
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:40:29.807
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:40:29.812
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jan 29 03:40:29.818: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 29 03:40:29.836: INFO: Waiting for terminating namespaces to be deleted...
Jan 29 03:40:29.842: INFO: 
Logging pods the apiserver thinks is on node master1 before test
Jan 29 03:40:29.857: INFO: calico-kube-controllers-5c6d4b68d6-6p9cm from kube-system started at 2023-01-11 07:48:50 +0000 UTC (1 container statuses recorded)
Jan 29 03:40:29.857: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jan 29 03:40:29.857: INFO: calico-node-j4qnr from kube-system started at 2023-01-11 07:49:07 +0000 UTC (1 container statuses recorded)
Jan 29 03:40:29.857: INFO: 	Container calico-node ready: true, restart count 2
Jan 29 03:40:29.857: INFO: cke-admission-daemonset-g5mvj from kube-system started at 2023-01-11 07:51:31 +0000 UTC (1 container statuses recorded)
Jan 29 03:40:29.857: INFO: 	Container cke-admission ready: true, restart count 0
Jan 29 03:40:29.857: INFO: cke-controller-manager-master1 from kube-system started at 2023-01-11 07:49:37 +0000 UTC (1 container statuses recorded)
Jan 29 03:40:29.857: INFO: 	Container cke-controller-manager ready: true, restart count 0
Jan 29 03:40:29.857: INFO: component-controller-manager-master1 from kube-system started at 2023-01-11 07:49:18 +0000 UTC (1 container statuses recorded)
Jan 29 03:40:29.857: INFO: 	Container component-controller-manager ready: true, restart count 0
Jan 29 03:40:29.857: INFO: coredns-tntlt from kube-system started at 2023-01-11 07:49:05 +0000 UTC (1 container statuses recorded)
Jan 29 03:40:29.857: INFO: 	Container coredns ready: true, restart count 0
Jan 29 03:40:29.857: INFO: keepalived-master1 from kube-system started at 2023-01-11 07:48:19 +0000 UTC (1 container statuses recorded)
Jan 29 03:40:29.857: INFO: 	Container keepalived ready: true, restart count 0
Jan 29 03:40:29.857: INFO: kube-apiserver-master1 from kube-system started at 2023-01-11 07:48:12 +0000 UTC (1 container statuses recorded)
Jan 29 03:40:29.857: INFO: 	Container kube-apiserver ready: true, restart count 0
Jan 29 03:40:29.857: INFO: kube-controller-manager-master1 from kube-system started at 2023-01-11 07:48:20 +0000 UTC (1 container statuses recorded)
Jan 29 03:40:29.857: INFO: 	Container kube-controller-manager ready: true, restart count 5
Jan 29 03:40:29.857: INFO: kube-multus-ds-8b89r from kube-system started at 2023-01-11 07:51:31 +0000 UTC (1 container statuses recorded)
Jan 29 03:40:29.857: INFO: 	Container kube-multus ready: true, restart count 0
Jan 29 03:40:29.857: INFO: kube-proxy-master1 from kube-system started at 2023-01-11 07:48:15 +0000 UTC (1 container statuses recorded)
Jan 29 03:40:29.857: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 29 03:40:29.857: INFO: kube-scheduler-master1 from kube-system started at 2023-01-11 07:48:21 +0000 UTC (1 container statuses recorded)
Jan 29 03:40:29.857: INFO: 	Container kube-scheduler ready: true, restart count 9
Jan 29 03:40:29.857: INFO: nginx-proxy-master1 from kube-system started at 2023-01-11 07:48:26 +0000 UTC (2 container statuses recorded)
Jan 29 03:40:29.857: INFO: 	Container kube-apiserver-healthcheck ready: true, restart count 0
Jan 29 03:40:29.857: INFO: 	Container nginx-proxy ready: true, restart count 0
Jan 29 03:40:29.857: INFO: node-problem-detector-dgrmp from kube-system started at 2023-01-11 07:51:49 +0000 UTC (1 container statuses recorded)
Jan 29 03:40:29.857: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan 29 03:40:29.857: INFO: 
Logging pods the apiserver thinks is on node master2 before test
Jan 29 03:40:29.872: INFO: calico-node-sx6hm from kube-system started at 2023-01-11 07:49:07 +0000 UTC (1 container statuses recorded)
Jan 29 03:40:29.872: INFO: 	Container calico-node ready: true, restart count 2
Jan 29 03:40:29.872: INFO: cke-admission-daemonset-6v8vg from kube-system started at 2023-01-11 07:51:31 +0000 UTC (1 container statuses recorded)
Jan 29 03:40:29.872: INFO: 	Container cke-admission ready: true, restart count 0
Jan 29 03:40:29.872: INFO: cke-controller-manager-master2 from kube-system started at 2023-01-11 07:49:04 +0000 UTC (1 container statuses recorded)
Jan 29 03:40:29.872: INFO: 	Container cke-controller-manager ready: true, restart count 1
Jan 29 03:40:29.872: INFO: component-controller-manager-master2 from kube-system started at 2023-01-11 07:49:02 +0000 UTC (1 container statuses recorded)
Jan 29 03:40:29.872: INFO: 	Container component-controller-manager ready: true, restart count 1
Jan 29 03:40:29.872: INFO: coredns-m9lv7 from kube-system started at 2023-01-11 07:50:16 +0000 UTC (1 container statuses recorded)
Jan 29 03:40:29.872: INFO: 	Container coredns ready: true, restart count 0
Jan 29 03:40:29.872: INFO: keepalived-master2 from kube-system started at 2023-01-11 07:47:57 +0000 UTC (1 container statuses recorded)
Jan 29 03:40:29.872: INFO: 	Container keepalived ready: true, restart count 0
Jan 29 03:40:29.872: INFO: kube-apiserver-master2 from kube-system started at 2023-01-11 07:47:49 +0000 UTC (1 container statuses recorded)
Jan 29 03:40:29.872: INFO: 	Container kube-apiserver ready: true, restart count 0
Jan 29 03:40:29.872: INFO: kube-controller-manager-master2 from kube-system started at 2023-01-11 07:47:51 +0000 UTC (1 container statuses recorded)
Jan 29 03:40:29.872: INFO: 	Container kube-controller-manager ready: true, restart count 4
Jan 29 03:40:29.872: INFO: kube-multus-ds-qq7kz from kube-system started at 2023-01-11 07:51:31 +0000 UTC (1 container statuses recorded)
Jan 29 03:40:29.872: INFO: 	Container kube-multus ready: true, restart count 0
Jan 29 03:40:29.872: INFO: kube-proxy-master2 from kube-system started at 2023-01-11 07:47:55 +0000 UTC (1 container statuses recorded)
Jan 29 03:40:29.872: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 29 03:40:29.872: INFO: kube-scheduler-master2 from kube-system started at 2023-01-11 07:48:09 +0000 UTC (1 container statuses recorded)
Jan 29 03:40:29.872: INFO: 	Container kube-scheduler ready: true, restart count 9
Jan 29 03:40:29.872: INFO: nginx-proxy-master2 from kube-system started at 2023-01-11 07:48:01 +0000 UTC (2 container statuses recorded)
Jan 29 03:40:29.872: INFO: 	Container kube-apiserver-healthcheck ready: true, restart count 0
Jan 29 03:40:29.872: INFO: 	Container nginx-proxy ready: true, restart count 0
Jan 29 03:40:29.872: INFO: node-problem-detector-w69xv from kube-system started at 2023-01-11 07:51:49 +0000 UTC (1 container statuses recorded)
Jan 29 03:40:29.872: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan 29 03:40:29.872: INFO: 
Logging pods the apiserver thinks is on node master3 before test
Jan 29 03:40:29.887: INFO: calico-node-9b26s from kube-system started at 2023-01-11 07:49:07 +0000 UTC (1 container statuses recorded)
Jan 29 03:40:29.887: INFO: 	Container calico-node ready: true, restart count 0
Jan 29 03:40:29.887: INFO: cke-admission-daemonset-mqt4k from kube-system started at 2023-01-11 07:51:31 +0000 UTC (1 container statuses recorded)
Jan 29 03:40:29.887: INFO: 	Container cke-admission ready: true, restart count 0
Jan 29 03:40:29.887: INFO: cke-controller-manager-master3 from kube-system started at 2023-01-11 07:49:08 +0000 UTC (1 container statuses recorded)
Jan 29 03:40:29.887: INFO: 	Container cke-controller-manager ready: true, restart count 0
Jan 29 03:40:29.887: INFO: component-controller-manager-master3 from kube-system started at 2023-01-11 07:49:17 +0000 UTC (1 container statuses recorded)
Jan 29 03:40:29.887: INFO: 	Container component-controller-manager ready: true, restart count 0
Jan 29 03:40:29.887: INFO: coredns-hwvn8 from kube-system started at 2023-01-11 07:49:26 +0000 UTC (1 container statuses recorded)
Jan 29 03:40:29.887: INFO: 	Container coredns ready: true, restart count 0
Jan 29 03:40:29.887: INFO: keepalived-master3 from kube-system started at 2023-01-11 07:48:26 +0000 UTC (1 container statuses recorded)
Jan 29 03:40:29.887: INFO: 	Container keepalived ready: true, restart count 0
Jan 29 03:40:29.887: INFO: kube-apiserver-master3 from kube-system started at 2023-01-11 07:48:44 +0000 UTC (1 container statuses recorded)
Jan 29 03:40:29.887: INFO: 	Container kube-apiserver ready: true, restart count 0
Jan 29 03:40:29.887: INFO: kube-controller-manager-master3 from kube-system started at 2023-01-11 07:48:27 +0000 UTC (1 container statuses recorded)
Jan 29 03:40:29.887: INFO: 	Container kube-controller-manager ready: true, restart count 4
Jan 29 03:40:29.887: INFO: kube-multus-ds-wrhjf from kube-system started at 2023-01-11 07:51:31 +0000 UTC (1 container statuses recorded)
Jan 29 03:40:29.887: INFO: 	Container kube-multus ready: true, restart count 0
Jan 29 03:40:29.887: INFO: kube-proxy-master3 from kube-system started at 2023-01-11 07:48:37 +0000 UTC (1 container statuses recorded)
Jan 29 03:40:29.887: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 29 03:40:29.887: INFO: kube-scheduler-master3 from kube-system started at 2023-01-11 07:48:30 +0000 UTC (1 container statuses recorded)
Jan 29 03:40:29.887: INFO: 	Container kube-scheduler ready: true, restart count 5
Jan 29 03:40:29.887: INFO: metrics-server-5584f68fbf-l689z from kube-system started at 2023-01-11 07:50:55 +0000 UTC (1 container statuses recorded)
Jan 29 03:40:29.887: INFO: 	Container metrics-server ready: true, restart count 0
Jan 29 03:40:29.887: INFO: nginx-proxy-master3 from kube-system started at 2023-01-11 07:48:32 +0000 UTC (2 container statuses recorded)
Jan 29 03:40:29.887: INFO: 	Container kube-apiserver-healthcheck ready: true, restart count 0
Jan 29 03:40:29.887: INFO: 	Container nginx-proxy ready: true, restart count 0
Jan 29 03:40:29.887: INFO: node-problem-detector-9987m from kube-system started at 2023-01-11 07:51:49 +0000 UTC (1 container statuses recorded)
Jan 29 03:40:29.887: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan 29 03:40:29.887: INFO: 
Logging pods the apiserver thinks is on node slave1 before test
Jan 29 03:40:29.902: INFO: pod-init-354e76fd-44c8-4c84-9ba3-6c43d50b5331 from init-container-2134 started at 2023-01-29 03:40:24 +0000 UTC (1 container statuses recorded)
Jan 29 03:40:29.902: INFO: 	Container run1 ready: false, restart count 0
Jan 29 03:40:29.902: INFO: calico-node-scr7m from kube-system started at 2023-01-11 07:49:07 +0000 UTC (1 container statuses recorded)
Jan 29 03:40:29.902: INFO: 	Container calico-node ready: true, restart count 0
Jan 29 03:40:29.902: INFO: kube-multus-ds-ng7xc from kube-system started at 2023-01-29 02:51:08 +0000 UTC (1 container statuses recorded)
Jan 29 03:40:29.902: INFO: 	Container kube-multus ready: true, restart count 0
Jan 29 03:40:29.902: INFO: kube-proxy-slave1 from kube-system started at 2023-01-11 07:48:42 +0000 UTC (1 container statuses recorded)
Jan 29 03:40:29.902: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 29 03:40:29.902: INFO: node-problem-detector-8bg82 from kube-system started at 2023-01-11 07:51:49 +0000 UTC (1 container statuses recorded)
Jan 29 03:40:29.902: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan 29 03:40:29.902: INFO: sonobuoy from sonobuoy started at 2023-01-29 02:56:33 +0000 UTC (1 container statuses recorded)
Jan 29 03:40:29.902: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 29 03:40:29.902: INFO: sonobuoy-e2e-job-3f32079945944ddf from sonobuoy started at 2023-01-29 02:56:35 +0000 UTC (2 container statuses recorded)
Jan 29 03:40:29.902: INFO: 	Container e2e ready: true, restart count 0
Jan 29 03:40:29.902: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 29 03:40:29.902: INFO: 
Logging pods the apiserver thinks is on node slave2 before test
Jan 29 03:40:29.915: INFO: calico-node-qhb5r from kube-system started at 2023-01-11 07:58:46 +0000 UTC (1 container statuses recorded)
Jan 29 03:40:29.915: INFO: 	Container calico-node ready: true, restart count 2
Jan 29 03:40:29.915: INFO: kube-multus-ds-8gtzz from kube-system started at 2023-01-29 02:43:13 +0000 UTC (1 container statuses recorded)
Jan 29 03:40:29.915: INFO: 	Container kube-multus ready: true, restart count 0
Jan 29 03:40:29.915: INFO: kube-proxy-slave2 from kube-system started at 2023-01-11 07:58:39 +0000 UTC (1 container statuses recorded)
Jan 29 03:40:29.915: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 29 03:40:29.915: INFO: node-problem-detector-m8cck from kube-system started at 2023-01-11 07:58:46 +0000 UTC (1 container statuses recorded)
Jan 29 03:40:29.915: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan 29 03:40:29.915: INFO: pod-logs-websocket-4c797824-47ff-4611-9b88-16e8bf1a96b1 from pods-2311 started at 2023-01-29 03:40:17 +0000 UTC (1 container statuses recorded)
Jan 29 03:40:29.915: INFO: 	Container main ready: true, restart count 0
Jan 29 03:40:29.915: INFO: pod-qos-class-fecd2a01-d2dc-439b-87eb-e4195a434040 from pods-7121 started at 2023-01-29 03:39:10 +0000 UTC (1 container statuses recorded)
Jan 29 03:40:29.915: INFO: 	Container agnhost ready: false, restart count 0
Jan 29 03:40:29.915: INFO: webhook-to-be-mutated from webhook-1996 started at 2023-01-29 03:40:23 +0000 UTC (1 container statuses recorded)
Jan 29 03:40:29.915: INFO: 	Container example ready: false, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
STEP: Trying to launch a pod without a label to get a node which can launch it. 01/29/23 03:40:29.915
Jan 29 03:40:29.933: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-7039" to be "running"
Jan 29 03:40:29.938: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 5.558959ms
Jan 29 03:40:31.945: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.011909981s
Jan 29 03:40:31.945: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 01/29/23 03:40:31.95
STEP: Trying to apply a random label on the found node. 01/29/23 03:40:32.019
STEP: verifying the node has the label kubernetes.io/e2e-3d9cc9e8-bb76-4ab3-aa24-d9a806a61506 95 01/29/23 03:40:32.03
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 01/29/23 03:40:32.036
Jan 29 03:40:32.050: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-7039" to be "not pending"
Jan 29 03:40:32.057: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.338784ms
Jan 29 03:40:34.065: INFO: Pod "pod4": Phase="Running", Reason="", readiness=false. Elapsed: 2.014138957s
Jan 29 03:40:34.065: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 192.168.122.244 on the node which pod4 resides and expect not scheduled 01/29/23 03:40:34.065
Jan 29 03:40:34.078: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-7039" to be "not pending"
Jan 29 03:40:34.084: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.799021ms
Jan 29 03:40:36.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012057662s
Jan 29 03:40:38.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012554063s
Jan 29 03:40:40.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013457347s
Jan 29 03:40:42.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.013410985s
Jan 29 03:40:44.092: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.013863886s
Jan 29 03:40:46.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.012968977s
Jan 29 03:40:48.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.012372591s
Jan 29 03:40:50.092: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.013803959s
Jan 29 03:40:52.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.013058971s
Jan 29 03:40:54.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.011996342s
Jan 29 03:40:56.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.012717865s
Jan 29 03:40:58.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.012584181s
Jan 29 03:41:00.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.011883754s
Jan 29 03:41:02.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.012219854s
Jan 29 03:41:04.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.012773276s
Jan 29 03:41:06.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.011826627s
Jan 29 03:41:08.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.01259699s
Jan 29 03:41:10.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.012589607s
Jan 29 03:41:12.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.012232723s
Jan 29 03:41:14.092: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.013697971s
Jan 29 03:41:16.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.013036804s
Jan 29 03:41:18.092: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.013787267s
Jan 29 03:41:20.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.012351295s
Jan 29 03:41:22.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.013213518s
Jan 29 03:41:24.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.013439178s
Jan 29 03:41:26.093: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.015113947s
Jan 29 03:41:28.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.011737822s
Jan 29 03:41:30.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.013010388s
Jan 29 03:41:32.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.013304268s
Jan 29 03:41:34.092: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.014445814s
Jan 29 03:41:36.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.013389604s
Jan 29 03:41:38.092: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.014221708s
Jan 29 03:41:40.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.013466921s
Jan 29 03:41:42.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.013378538s
Jan 29 03:41:44.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.012733451s
Jan 29 03:41:46.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.01284453s
Jan 29 03:41:48.092: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.014222897s
Jan 29 03:41:50.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.013491829s
Jan 29 03:41:52.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.012984164s
Jan 29 03:41:54.092: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.014337171s
Jan 29 03:41:56.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.013314281s
Jan 29 03:41:58.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.012514994s
Jan 29 03:42:00.092: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.01377916s
Jan 29 03:42:02.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.01269287s
Jan 29 03:42:04.092: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.0143347s
Jan 29 03:42:06.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.013496471s
Jan 29 03:42:08.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.01225838s
Jan 29 03:42:10.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.013074964s
Jan 29 03:42:12.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.011928733s
Jan 29 03:42:14.092: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.013747464s
Jan 29 03:42:16.092: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.013710461s
Jan 29 03:42:18.092: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.013624759s
Jan 29 03:42:20.092: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.013891398s
Jan 29 03:42:22.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.012433707s
Jan 29 03:42:24.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.013471394s
Jan 29 03:42:26.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.012570147s
Jan 29 03:42:28.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.013185211s
Jan 29 03:42:30.092: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.014050216s
Jan 29 03:42:32.092: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.013508551s
Jan 29 03:42:34.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.012239542s
Jan 29 03:42:36.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.012236681s
Jan 29 03:42:38.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.012744684s
Jan 29 03:42:40.092: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.013523249s
Jan 29 03:42:42.092: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.01375233s
Jan 29 03:42:44.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.012323819s
Jan 29 03:42:46.092: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.013731489s
Jan 29 03:42:48.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.013102443s
Jan 29 03:42:50.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.01270362s
Jan 29 03:42:52.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.012616839s
Jan 29 03:42:54.093: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.014712813s
Jan 29 03:42:56.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.012656298s
Jan 29 03:42:58.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.011858512s
Jan 29 03:43:00.092: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.013654923s
Jan 29 03:43:02.092: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.013621463s
Jan 29 03:43:04.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.012829276s
Jan 29 03:43:06.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.01344388s
Jan 29 03:43:08.102: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.023706431s
Jan 29 03:43:10.092: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.013889642s
Jan 29 03:43:12.092: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.013614299s
Jan 29 03:43:14.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.01244857s
Jan 29 03:43:16.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.012665851s
Jan 29 03:43:18.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.01689076s
Jan 29 03:43:20.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.012993732s
Jan 29 03:43:22.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.013003912s
Jan 29 03:43:24.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.012791449s
Jan 29 03:43:26.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.012437166s
Jan 29 03:43:28.092: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.013629934s
Jan 29 03:43:30.092: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.013913535s
Jan 29 03:43:32.093: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.014899482s
Jan 29 03:43:34.092: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.014290277s
Jan 29 03:43:36.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.012666345s
Jan 29 03:43:38.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.012017859s
Jan 29 03:43:40.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.01212302s
Jan 29 03:43:42.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.012385681s
Jan 29 03:43:44.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.013407127s
Jan 29 03:43:46.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.013370046s
Jan 29 03:43:48.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.013391906s
Jan 29 03:43:50.093: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.015364879s
Jan 29 03:43:52.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.016569767s
Jan 29 03:43:54.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.013491645s
Jan 29 03:43:56.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.012068414s
Jan 29 03:43:58.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.013461823s
Jan 29 03:44:00.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.012550976s
Jan 29 03:44:02.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.0131457s
Jan 29 03:44:04.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.013044318s
Jan 29 03:44:06.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.012432253s
Jan 29 03:44:08.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.013376159s
Jan 29 03:44:10.092: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.013893122s
Jan 29 03:44:12.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.012815914s
Jan 29 03:44:14.092: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.013593579s
Jan 29 03:44:16.092: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.014067542s
Jan 29 03:44:18.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.013246035s
Jan 29 03:44:20.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.012999913s
Jan 29 03:44:22.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.013219694s
Jan 29 03:44:24.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.012227746s
Jan 29 03:44:26.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.011557601s
Jan 29 03:44:28.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.01300225s
Jan 29 03:44:30.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.012404885s
Jan 29 03:44:32.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.013220254s
Jan 29 03:44:34.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.013118578s
Jan 29 03:44:36.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.013326803s
Jan 29 03:44:38.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.013354767s
Jan 29 03:44:40.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.013356231s
Jan 29 03:44:42.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.012922012s
Jan 29 03:44:44.092: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.013757742s
Jan 29 03:44:46.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.012641798s
Jan 29 03:44:48.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.012518981s
Jan 29 03:44:50.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.012294884s
Jan 29 03:44:52.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.012304308s
Jan 29 03:44:54.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.01202007s
Jan 29 03:44:56.093: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.014582931s
Jan 29 03:44:58.093: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.014973418s
Jan 29 03:45:00.092: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.013697773s
Jan 29 03:45:02.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.012364768s
Jan 29 03:45:04.092: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.0135368s
Jan 29 03:45:06.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.012396596s
Jan 29 03:45:08.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.012249359s
Jan 29 03:45:10.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.012193802s
Jan 29 03:45:12.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.012954952s
Jan 29 03:45:14.093: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.014598627s
Jan 29 03:45:16.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.012013333s
Jan 29 03:45:18.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.011681015s
Jan 29 03:45:20.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.013361611s
Jan 29 03:45:22.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.012564289s
Jan 29 03:45:24.092: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.014145424s
Jan 29 03:45:26.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.012115914s
Jan 29 03:45:28.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.011715815s
Jan 29 03:45:30.092: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.013903214s
Jan 29 03:45:32.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.012123886s
Jan 29 03:45:34.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.01219027s
Jan 29 03:45:34.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.018029131s
STEP: removing the label kubernetes.io/e2e-3d9cc9e8-bb76-4ab3-aa24-d9a806a61506 off the node slave1 01/29/23 03:45:34.096
STEP: verifying the node doesn't have the label kubernetes.io/e2e-3d9cc9e8-bb76-4ab3-aa24-d9a806a61506 01/29/23 03:45:34.112
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Jan 29 03:45:34.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7039" for this suite. 01/29/23 03:45:34.129
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","completed":185,"skipped":3508,"failed":0}
------------------------------
• [SLOW TEST] [304.372 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:40:29.775
    Jan 29 03:40:29.775: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename sched-pred 01/29/23 03:40:29.776
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:40:29.807
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:40:29.812
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Jan 29 03:40:29.818: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan 29 03:40:29.836: INFO: Waiting for terminating namespaces to be deleted...
    Jan 29 03:40:29.842: INFO: 
    Logging pods the apiserver thinks is on node master1 before test
    Jan 29 03:40:29.857: INFO: calico-kube-controllers-5c6d4b68d6-6p9cm from kube-system started at 2023-01-11 07:48:50 +0000 UTC (1 container statuses recorded)
    Jan 29 03:40:29.857: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Jan 29 03:40:29.857: INFO: calico-node-j4qnr from kube-system started at 2023-01-11 07:49:07 +0000 UTC (1 container statuses recorded)
    Jan 29 03:40:29.857: INFO: 	Container calico-node ready: true, restart count 2
    Jan 29 03:40:29.857: INFO: cke-admission-daemonset-g5mvj from kube-system started at 2023-01-11 07:51:31 +0000 UTC (1 container statuses recorded)
    Jan 29 03:40:29.857: INFO: 	Container cke-admission ready: true, restart count 0
    Jan 29 03:40:29.857: INFO: cke-controller-manager-master1 from kube-system started at 2023-01-11 07:49:37 +0000 UTC (1 container statuses recorded)
    Jan 29 03:40:29.857: INFO: 	Container cke-controller-manager ready: true, restart count 0
    Jan 29 03:40:29.857: INFO: component-controller-manager-master1 from kube-system started at 2023-01-11 07:49:18 +0000 UTC (1 container statuses recorded)
    Jan 29 03:40:29.857: INFO: 	Container component-controller-manager ready: true, restart count 0
    Jan 29 03:40:29.857: INFO: coredns-tntlt from kube-system started at 2023-01-11 07:49:05 +0000 UTC (1 container statuses recorded)
    Jan 29 03:40:29.857: INFO: 	Container coredns ready: true, restart count 0
    Jan 29 03:40:29.857: INFO: keepalived-master1 from kube-system started at 2023-01-11 07:48:19 +0000 UTC (1 container statuses recorded)
    Jan 29 03:40:29.857: INFO: 	Container keepalived ready: true, restart count 0
    Jan 29 03:40:29.857: INFO: kube-apiserver-master1 from kube-system started at 2023-01-11 07:48:12 +0000 UTC (1 container statuses recorded)
    Jan 29 03:40:29.857: INFO: 	Container kube-apiserver ready: true, restart count 0
    Jan 29 03:40:29.857: INFO: kube-controller-manager-master1 from kube-system started at 2023-01-11 07:48:20 +0000 UTC (1 container statuses recorded)
    Jan 29 03:40:29.857: INFO: 	Container kube-controller-manager ready: true, restart count 5
    Jan 29 03:40:29.857: INFO: kube-multus-ds-8b89r from kube-system started at 2023-01-11 07:51:31 +0000 UTC (1 container statuses recorded)
    Jan 29 03:40:29.857: INFO: 	Container kube-multus ready: true, restart count 0
    Jan 29 03:40:29.857: INFO: kube-proxy-master1 from kube-system started at 2023-01-11 07:48:15 +0000 UTC (1 container statuses recorded)
    Jan 29 03:40:29.857: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan 29 03:40:29.857: INFO: kube-scheduler-master1 from kube-system started at 2023-01-11 07:48:21 +0000 UTC (1 container statuses recorded)
    Jan 29 03:40:29.857: INFO: 	Container kube-scheduler ready: true, restart count 9
    Jan 29 03:40:29.857: INFO: nginx-proxy-master1 from kube-system started at 2023-01-11 07:48:26 +0000 UTC (2 container statuses recorded)
    Jan 29 03:40:29.857: INFO: 	Container kube-apiserver-healthcheck ready: true, restart count 0
    Jan 29 03:40:29.857: INFO: 	Container nginx-proxy ready: true, restart count 0
    Jan 29 03:40:29.857: INFO: node-problem-detector-dgrmp from kube-system started at 2023-01-11 07:51:49 +0000 UTC (1 container statuses recorded)
    Jan 29 03:40:29.857: INFO: 	Container node-problem-detector ready: true, restart count 0
    Jan 29 03:40:29.857: INFO: 
    Logging pods the apiserver thinks is on node master2 before test
    Jan 29 03:40:29.872: INFO: calico-node-sx6hm from kube-system started at 2023-01-11 07:49:07 +0000 UTC (1 container statuses recorded)
    Jan 29 03:40:29.872: INFO: 	Container calico-node ready: true, restart count 2
    Jan 29 03:40:29.872: INFO: cke-admission-daemonset-6v8vg from kube-system started at 2023-01-11 07:51:31 +0000 UTC (1 container statuses recorded)
    Jan 29 03:40:29.872: INFO: 	Container cke-admission ready: true, restart count 0
    Jan 29 03:40:29.872: INFO: cke-controller-manager-master2 from kube-system started at 2023-01-11 07:49:04 +0000 UTC (1 container statuses recorded)
    Jan 29 03:40:29.872: INFO: 	Container cke-controller-manager ready: true, restart count 1
    Jan 29 03:40:29.872: INFO: component-controller-manager-master2 from kube-system started at 2023-01-11 07:49:02 +0000 UTC (1 container statuses recorded)
    Jan 29 03:40:29.872: INFO: 	Container component-controller-manager ready: true, restart count 1
    Jan 29 03:40:29.872: INFO: coredns-m9lv7 from kube-system started at 2023-01-11 07:50:16 +0000 UTC (1 container statuses recorded)
    Jan 29 03:40:29.872: INFO: 	Container coredns ready: true, restart count 0
    Jan 29 03:40:29.872: INFO: keepalived-master2 from kube-system started at 2023-01-11 07:47:57 +0000 UTC (1 container statuses recorded)
    Jan 29 03:40:29.872: INFO: 	Container keepalived ready: true, restart count 0
    Jan 29 03:40:29.872: INFO: kube-apiserver-master2 from kube-system started at 2023-01-11 07:47:49 +0000 UTC (1 container statuses recorded)
    Jan 29 03:40:29.872: INFO: 	Container kube-apiserver ready: true, restart count 0
    Jan 29 03:40:29.872: INFO: kube-controller-manager-master2 from kube-system started at 2023-01-11 07:47:51 +0000 UTC (1 container statuses recorded)
    Jan 29 03:40:29.872: INFO: 	Container kube-controller-manager ready: true, restart count 4
    Jan 29 03:40:29.872: INFO: kube-multus-ds-qq7kz from kube-system started at 2023-01-11 07:51:31 +0000 UTC (1 container statuses recorded)
    Jan 29 03:40:29.872: INFO: 	Container kube-multus ready: true, restart count 0
    Jan 29 03:40:29.872: INFO: kube-proxy-master2 from kube-system started at 2023-01-11 07:47:55 +0000 UTC (1 container statuses recorded)
    Jan 29 03:40:29.872: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan 29 03:40:29.872: INFO: kube-scheduler-master2 from kube-system started at 2023-01-11 07:48:09 +0000 UTC (1 container statuses recorded)
    Jan 29 03:40:29.872: INFO: 	Container kube-scheduler ready: true, restart count 9
    Jan 29 03:40:29.872: INFO: nginx-proxy-master2 from kube-system started at 2023-01-11 07:48:01 +0000 UTC (2 container statuses recorded)
    Jan 29 03:40:29.872: INFO: 	Container kube-apiserver-healthcheck ready: true, restart count 0
    Jan 29 03:40:29.872: INFO: 	Container nginx-proxy ready: true, restart count 0
    Jan 29 03:40:29.872: INFO: node-problem-detector-w69xv from kube-system started at 2023-01-11 07:51:49 +0000 UTC (1 container statuses recorded)
    Jan 29 03:40:29.872: INFO: 	Container node-problem-detector ready: true, restart count 0
    Jan 29 03:40:29.872: INFO: 
    Logging pods the apiserver thinks is on node master3 before test
    Jan 29 03:40:29.887: INFO: calico-node-9b26s from kube-system started at 2023-01-11 07:49:07 +0000 UTC (1 container statuses recorded)
    Jan 29 03:40:29.887: INFO: 	Container calico-node ready: true, restart count 0
    Jan 29 03:40:29.887: INFO: cke-admission-daemonset-mqt4k from kube-system started at 2023-01-11 07:51:31 +0000 UTC (1 container statuses recorded)
    Jan 29 03:40:29.887: INFO: 	Container cke-admission ready: true, restart count 0
    Jan 29 03:40:29.887: INFO: cke-controller-manager-master3 from kube-system started at 2023-01-11 07:49:08 +0000 UTC (1 container statuses recorded)
    Jan 29 03:40:29.887: INFO: 	Container cke-controller-manager ready: true, restart count 0
    Jan 29 03:40:29.887: INFO: component-controller-manager-master3 from kube-system started at 2023-01-11 07:49:17 +0000 UTC (1 container statuses recorded)
    Jan 29 03:40:29.887: INFO: 	Container component-controller-manager ready: true, restart count 0
    Jan 29 03:40:29.887: INFO: coredns-hwvn8 from kube-system started at 2023-01-11 07:49:26 +0000 UTC (1 container statuses recorded)
    Jan 29 03:40:29.887: INFO: 	Container coredns ready: true, restart count 0
    Jan 29 03:40:29.887: INFO: keepalived-master3 from kube-system started at 2023-01-11 07:48:26 +0000 UTC (1 container statuses recorded)
    Jan 29 03:40:29.887: INFO: 	Container keepalived ready: true, restart count 0
    Jan 29 03:40:29.887: INFO: kube-apiserver-master3 from kube-system started at 2023-01-11 07:48:44 +0000 UTC (1 container statuses recorded)
    Jan 29 03:40:29.887: INFO: 	Container kube-apiserver ready: true, restart count 0
    Jan 29 03:40:29.887: INFO: kube-controller-manager-master3 from kube-system started at 2023-01-11 07:48:27 +0000 UTC (1 container statuses recorded)
    Jan 29 03:40:29.887: INFO: 	Container kube-controller-manager ready: true, restart count 4
    Jan 29 03:40:29.887: INFO: kube-multus-ds-wrhjf from kube-system started at 2023-01-11 07:51:31 +0000 UTC (1 container statuses recorded)
    Jan 29 03:40:29.887: INFO: 	Container kube-multus ready: true, restart count 0
    Jan 29 03:40:29.887: INFO: kube-proxy-master3 from kube-system started at 2023-01-11 07:48:37 +0000 UTC (1 container statuses recorded)
    Jan 29 03:40:29.887: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan 29 03:40:29.887: INFO: kube-scheduler-master3 from kube-system started at 2023-01-11 07:48:30 +0000 UTC (1 container statuses recorded)
    Jan 29 03:40:29.887: INFO: 	Container kube-scheduler ready: true, restart count 5
    Jan 29 03:40:29.887: INFO: metrics-server-5584f68fbf-l689z from kube-system started at 2023-01-11 07:50:55 +0000 UTC (1 container statuses recorded)
    Jan 29 03:40:29.887: INFO: 	Container metrics-server ready: true, restart count 0
    Jan 29 03:40:29.887: INFO: nginx-proxy-master3 from kube-system started at 2023-01-11 07:48:32 +0000 UTC (2 container statuses recorded)
    Jan 29 03:40:29.887: INFO: 	Container kube-apiserver-healthcheck ready: true, restart count 0
    Jan 29 03:40:29.887: INFO: 	Container nginx-proxy ready: true, restart count 0
    Jan 29 03:40:29.887: INFO: node-problem-detector-9987m from kube-system started at 2023-01-11 07:51:49 +0000 UTC (1 container statuses recorded)
    Jan 29 03:40:29.887: INFO: 	Container node-problem-detector ready: true, restart count 0
    Jan 29 03:40:29.887: INFO: 
    Logging pods the apiserver thinks is on node slave1 before test
    Jan 29 03:40:29.902: INFO: pod-init-354e76fd-44c8-4c84-9ba3-6c43d50b5331 from init-container-2134 started at 2023-01-29 03:40:24 +0000 UTC (1 container statuses recorded)
    Jan 29 03:40:29.902: INFO: 	Container run1 ready: false, restart count 0
    Jan 29 03:40:29.902: INFO: calico-node-scr7m from kube-system started at 2023-01-11 07:49:07 +0000 UTC (1 container statuses recorded)
    Jan 29 03:40:29.902: INFO: 	Container calico-node ready: true, restart count 0
    Jan 29 03:40:29.902: INFO: kube-multus-ds-ng7xc from kube-system started at 2023-01-29 02:51:08 +0000 UTC (1 container statuses recorded)
    Jan 29 03:40:29.902: INFO: 	Container kube-multus ready: true, restart count 0
    Jan 29 03:40:29.902: INFO: kube-proxy-slave1 from kube-system started at 2023-01-11 07:48:42 +0000 UTC (1 container statuses recorded)
    Jan 29 03:40:29.902: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan 29 03:40:29.902: INFO: node-problem-detector-8bg82 from kube-system started at 2023-01-11 07:51:49 +0000 UTC (1 container statuses recorded)
    Jan 29 03:40:29.902: INFO: 	Container node-problem-detector ready: true, restart count 0
    Jan 29 03:40:29.902: INFO: sonobuoy from sonobuoy started at 2023-01-29 02:56:33 +0000 UTC (1 container statuses recorded)
    Jan 29 03:40:29.902: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan 29 03:40:29.902: INFO: sonobuoy-e2e-job-3f32079945944ddf from sonobuoy started at 2023-01-29 02:56:35 +0000 UTC (2 container statuses recorded)
    Jan 29 03:40:29.902: INFO: 	Container e2e ready: true, restart count 0
    Jan 29 03:40:29.902: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 29 03:40:29.902: INFO: 
    Logging pods the apiserver thinks is on node slave2 before test
    Jan 29 03:40:29.915: INFO: calico-node-qhb5r from kube-system started at 2023-01-11 07:58:46 +0000 UTC (1 container statuses recorded)
    Jan 29 03:40:29.915: INFO: 	Container calico-node ready: true, restart count 2
    Jan 29 03:40:29.915: INFO: kube-multus-ds-8gtzz from kube-system started at 2023-01-29 02:43:13 +0000 UTC (1 container statuses recorded)
    Jan 29 03:40:29.915: INFO: 	Container kube-multus ready: true, restart count 0
    Jan 29 03:40:29.915: INFO: kube-proxy-slave2 from kube-system started at 2023-01-11 07:58:39 +0000 UTC (1 container statuses recorded)
    Jan 29 03:40:29.915: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan 29 03:40:29.915: INFO: node-problem-detector-m8cck from kube-system started at 2023-01-11 07:58:46 +0000 UTC (1 container statuses recorded)
    Jan 29 03:40:29.915: INFO: 	Container node-problem-detector ready: true, restart count 0
    Jan 29 03:40:29.915: INFO: pod-logs-websocket-4c797824-47ff-4611-9b88-16e8bf1a96b1 from pods-2311 started at 2023-01-29 03:40:17 +0000 UTC (1 container statuses recorded)
    Jan 29 03:40:29.915: INFO: 	Container main ready: true, restart count 0
    Jan 29 03:40:29.915: INFO: pod-qos-class-fecd2a01-d2dc-439b-87eb-e4195a434040 from pods-7121 started at 2023-01-29 03:39:10 +0000 UTC (1 container statuses recorded)
    Jan 29 03:40:29.915: INFO: 	Container agnhost ready: false, restart count 0
    Jan 29 03:40:29.915: INFO: webhook-to-be-mutated from webhook-1996 started at 2023-01-29 03:40:23 +0000 UTC (1 container statuses recorded)
    Jan 29 03:40:29.915: INFO: 	Container example ready: false, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:699
    STEP: Trying to launch a pod without a label to get a node which can launch it. 01/29/23 03:40:29.915
    Jan 29 03:40:29.933: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-7039" to be "running"
    Jan 29 03:40:29.938: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 5.558959ms
    Jan 29 03:40:31.945: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.011909981s
    Jan 29 03:40:31.945: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 01/29/23 03:40:31.95
    STEP: Trying to apply a random label on the found node. 01/29/23 03:40:32.019
    STEP: verifying the node has the label kubernetes.io/e2e-3d9cc9e8-bb76-4ab3-aa24-d9a806a61506 95 01/29/23 03:40:32.03
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 01/29/23 03:40:32.036
    Jan 29 03:40:32.050: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-7039" to be "not pending"
    Jan 29 03:40:32.057: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.338784ms
    Jan 29 03:40:34.065: INFO: Pod "pod4": Phase="Running", Reason="", readiness=false. Elapsed: 2.014138957s
    Jan 29 03:40:34.065: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 192.168.122.244 on the node which pod4 resides and expect not scheduled 01/29/23 03:40:34.065
    Jan 29 03:40:34.078: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-7039" to be "not pending"
    Jan 29 03:40:34.084: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.799021ms
    Jan 29 03:40:36.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012057662s
    Jan 29 03:40:38.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012554063s
    Jan 29 03:40:40.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013457347s
    Jan 29 03:40:42.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.013410985s
    Jan 29 03:40:44.092: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.013863886s
    Jan 29 03:40:46.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.012968977s
    Jan 29 03:40:48.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.012372591s
    Jan 29 03:40:50.092: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.013803959s
    Jan 29 03:40:52.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.013058971s
    Jan 29 03:40:54.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.011996342s
    Jan 29 03:40:56.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.012717865s
    Jan 29 03:40:58.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.012584181s
    Jan 29 03:41:00.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.011883754s
    Jan 29 03:41:02.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.012219854s
    Jan 29 03:41:04.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.012773276s
    Jan 29 03:41:06.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.011826627s
    Jan 29 03:41:08.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.01259699s
    Jan 29 03:41:10.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.012589607s
    Jan 29 03:41:12.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.012232723s
    Jan 29 03:41:14.092: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.013697971s
    Jan 29 03:41:16.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.013036804s
    Jan 29 03:41:18.092: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.013787267s
    Jan 29 03:41:20.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.012351295s
    Jan 29 03:41:22.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.013213518s
    Jan 29 03:41:24.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.013439178s
    Jan 29 03:41:26.093: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.015113947s
    Jan 29 03:41:28.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.011737822s
    Jan 29 03:41:30.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.013010388s
    Jan 29 03:41:32.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.013304268s
    Jan 29 03:41:34.092: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.014445814s
    Jan 29 03:41:36.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.013389604s
    Jan 29 03:41:38.092: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.014221708s
    Jan 29 03:41:40.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.013466921s
    Jan 29 03:41:42.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.013378538s
    Jan 29 03:41:44.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.012733451s
    Jan 29 03:41:46.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.01284453s
    Jan 29 03:41:48.092: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.014222897s
    Jan 29 03:41:50.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.013491829s
    Jan 29 03:41:52.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.012984164s
    Jan 29 03:41:54.092: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.014337171s
    Jan 29 03:41:56.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.013314281s
    Jan 29 03:41:58.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.012514994s
    Jan 29 03:42:00.092: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.01377916s
    Jan 29 03:42:02.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.01269287s
    Jan 29 03:42:04.092: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.0143347s
    Jan 29 03:42:06.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.013496471s
    Jan 29 03:42:08.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.01225838s
    Jan 29 03:42:10.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.013074964s
    Jan 29 03:42:12.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.011928733s
    Jan 29 03:42:14.092: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.013747464s
    Jan 29 03:42:16.092: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.013710461s
    Jan 29 03:42:18.092: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.013624759s
    Jan 29 03:42:20.092: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.013891398s
    Jan 29 03:42:22.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.012433707s
    Jan 29 03:42:24.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.013471394s
    Jan 29 03:42:26.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.012570147s
    Jan 29 03:42:28.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.013185211s
    Jan 29 03:42:30.092: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.014050216s
    Jan 29 03:42:32.092: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.013508551s
    Jan 29 03:42:34.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.012239542s
    Jan 29 03:42:36.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.012236681s
    Jan 29 03:42:38.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.012744684s
    Jan 29 03:42:40.092: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.013523249s
    Jan 29 03:42:42.092: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.01375233s
    Jan 29 03:42:44.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.012323819s
    Jan 29 03:42:46.092: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.013731489s
    Jan 29 03:42:48.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.013102443s
    Jan 29 03:42:50.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.01270362s
    Jan 29 03:42:52.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.012616839s
    Jan 29 03:42:54.093: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.014712813s
    Jan 29 03:42:56.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.012656298s
    Jan 29 03:42:58.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.011858512s
    Jan 29 03:43:00.092: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.013654923s
    Jan 29 03:43:02.092: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.013621463s
    Jan 29 03:43:04.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.012829276s
    Jan 29 03:43:06.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.01344388s
    Jan 29 03:43:08.102: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.023706431s
    Jan 29 03:43:10.092: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.013889642s
    Jan 29 03:43:12.092: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.013614299s
    Jan 29 03:43:14.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.01244857s
    Jan 29 03:43:16.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.012665851s
    Jan 29 03:43:18.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.01689076s
    Jan 29 03:43:20.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.012993732s
    Jan 29 03:43:22.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.013003912s
    Jan 29 03:43:24.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.012791449s
    Jan 29 03:43:26.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.012437166s
    Jan 29 03:43:28.092: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.013629934s
    Jan 29 03:43:30.092: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.013913535s
    Jan 29 03:43:32.093: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.014899482s
    Jan 29 03:43:34.092: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.014290277s
    Jan 29 03:43:36.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.012666345s
    Jan 29 03:43:38.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.012017859s
    Jan 29 03:43:40.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.01212302s
    Jan 29 03:43:42.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.012385681s
    Jan 29 03:43:44.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.013407127s
    Jan 29 03:43:46.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.013370046s
    Jan 29 03:43:48.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.013391906s
    Jan 29 03:43:50.093: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.015364879s
    Jan 29 03:43:52.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.016569767s
    Jan 29 03:43:54.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.013491645s
    Jan 29 03:43:56.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.012068414s
    Jan 29 03:43:58.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.013461823s
    Jan 29 03:44:00.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.012550976s
    Jan 29 03:44:02.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.0131457s
    Jan 29 03:44:04.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.013044318s
    Jan 29 03:44:06.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.012432253s
    Jan 29 03:44:08.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.013376159s
    Jan 29 03:44:10.092: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.013893122s
    Jan 29 03:44:12.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.012815914s
    Jan 29 03:44:14.092: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.013593579s
    Jan 29 03:44:16.092: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.014067542s
    Jan 29 03:44:18.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.013246035s
    Jan 29 03:44:20.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.012999913s
    Jan 29 03:44:22.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.013219694s
    Jan 29 03:44:24.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.012227746s
    Jan 29 03:44:26.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.011557601s
    Jan 29 03:44:28.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.01300225s
    Jan 29 03:44:30.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.012404885s
    Jan 29 03:44:32.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.013220254s
    Jan 29 03:44:34.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.013118578s
    Jan 29 03:44:36.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.013326803s
    Jan 29 03:44:38.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.013354767s
    Jan 29 03:44:40.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.013356231s
    Jan 29 03:44:42.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.012922012s
    Jan 29 03:44:44.092: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.013757742s
    Jan 29 03:44:46.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.012641798s
    Jan 29 03:44:48.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.012518981s
    Jan 29 03:44:50.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.012294884s
    Jan 29 03:44:52.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.012304308s
    Jan 29 03:44:54.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.01202007s
    Jan 29 03:44:56.093: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.014582931s
    Jan 29 03:44:58.093: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.014973418s
    Jan 29 03:45:00.092: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.013697773s
    Jan 29 03:45:02.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.012364768s
    Jan 29 03:45:04.092: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.0135368s
    Jan 29 03:45:06.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.012396596s
    Jan 29 03:45:08.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.012249359s
    Jan 29 03:45:10.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.012193802s
    Jan 29 03:45:12.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.012954952s
    Jan 29 03:45:14.093: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.014598627s
    Jan 29 03:45:16.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.012013333s
    Jan 29 03:45:18.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.011681015s
    Jan 29 03:45:20.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.013361611s
    Jan 29 03:45:22.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.012564289s
    Jan 29 03:45:24.092: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.014145424s
    Jan 29 03:45:26.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.012115914s
    Jan 29 03:45:28.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.011715815s
    Jan 29 03:45:30.092: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.013903214s
    Jan 29 03:45:32.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.012123886s
    Jan 29 03:45:34.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.01219027s
    Jan 29 03:45:34.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.018029131s
    STEP: removing the label kubernetes.io/e2e-3d9cc9e8-bb76-4ab3-aa24-d9a806a61506 off the node slave1 01/29/23 03:45:34.096
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-3d9cc9e8-bb76-4ab3-aa24-d9a806a61506 01/29/23 03:45:34.112
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Jan 29 03:45:34.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-7039" for this suite. 01/29/23 03:45:34.129
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:45:34.147
Jan 29 03:45:34.147: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename var-expansion 01/29/23 03:45:34.151
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:45:34.195
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:45:34.203
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
STEP: Creating a pod to test substitution in volume subpath 01/29/23 03:45:34.209
Jan 29 03:45:34.258: INFO: Waiting up to 5m0s for pod "var-expansion-a9fcae49-af2a-4c55-b6e6-2c2a9dfc551a" in namespace "var-expansion-1783" to be "Succeeded or Failed"
Jan 29 03:45:34.266: INFO: Pod "var-expansion-a9fcae49-af2a-4c55-b6e6-2c2a9dfc551a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.562913ms
Jan 29 03:45:36.272: INFO: Pod "var-expansion-a9fcae49-af2a-4c55-b6e6-2c2a9dfc551a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013737661s
Jan 29 03:45:38.272: INFO: Pod "var-expansion-a9fcae49-af2a-4c55-b6e6-2c2a9dfc551a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013410762s
STEP: Saw pod success 01/29/23 03:45:38.272
Jan 29 03:45:38.272: INFO: Pod "var-expansion-a9fcae49-af2a-4c55-b6e6-2c2a9dfc551a" satisfied condition "Succeeded or Failed"
Jan 29 03:45:38.278: INFO: Trying to get logs from node slave2 pod var-expansion-a9fcae49-af2a-4c55-b6e6-2c2a9dfc551a container dapi-container: <nil>
STEP: delete the pod 01/29/23 03:45:38.306
Jan 29 03:45:38.398: INFO: Waiting for pod var-expansion-a9fcae49-af2a-4c55-b6e6-2c2a9dfc551a to disappear
Jan 29 03:45:38.404: INFO: Pod var-expansion-a9fcae49-af2a-4c55-b6e6-2c2a9dfc551a no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan 29 03:45:38.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1783" for this suite. 01/29/23 03:45:38.412
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","completed":186,"skipped":3510,"failed":0}
------------------------------
• [4.274 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:45:34.147
    Jan 29 03:45:34.147: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename var-expansion 01/29/23 03:45:34.151
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:45:34.195
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:45:34.203
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:111
    STEP: Creating a pod to test substitution in volume subpath 01/29/23 03:45:34.209
    Jan 29 03:45:34.258: INFO: Waiting up to 5m0s for pod "var-expansion-a9fcae49-af2a-4c55-b6e6-2c2a9dfc551a" in namespace "var-expansion-1783" to be "Succeeded or Failed"
    Jan 29 03:45:34.266: INFO: Pod "var-expansion-a9fcae49-af2a-4c55-b6e6-2c2a9dfc551a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.562913ms
    Jan 29 03:45:36.272: INFO: Pod "var-expansion-a9fcae49-af2a-4c55-b6e6-2c2a9dfc551a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013737661s
    Jan 29 03:45:38.272: INFO: Pod "var-expansion-a9fcae49-af2a-4c55-b6e6-2c2a9dfc551a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013410762s
    STEP: Saw pod success 01/29/23 03:45:38.272
    Jan 29 03:45:38.272: INFO: Pod "var-expansion-a9fcae49-af2a-4c55-b6e6-2c2a9dfc551a" satisfied condition "Succeeded or Failed"
    Jan 29 03:45:38.278: INFO: Trying to get logs from node slave2 pod var-expansion-a9fcae49-af2a-4c55-b6e6-2c2a9dfc551a container dapi-container: <nil>
    STEP: delete the pod 01/29/23 03:45:38.306
    Jan 29 03:45:38.398: INFO: Waiting for pod var-expansion-a9fcae49-af2a-4c55-b6e6-2c2a9dfc551a to disappear
    Jan 29 03:45:38.404: INFO: Pod var-expansion-a9fcae49-af2a-4c55-b6e6-2c2a9dfc551a no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan 29 03:45:38.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-1783" for this suite. 01/29/23 03:45:38.412
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:45:38.421
Jan 29 03:45:38.422: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename discovery 01/29/23 03:45:38.423
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:45:38.448
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:45:38.454
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 01/29/23 03:45:38.461
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Jan 29 03:45:38.966: INFO: Checking APIGroup: apiregistration.k8s.io
Jan 29 03:45:38.969: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Jan 29 03:45:38.969: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Jan 29 03:45:38.969: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Jan 29 03:45:38.969: INFO: Checking APIGroup: apps
Jan 29 03:45:38.971: INFO: PreferredVersion.GroupVersion: apps/v1
Jan 29 03:45:38.971: INFO: Versions found [{apps/v1 v1}]
Jan 29 03:45:38.971: INFO: apps/v1 matches apps/v1
Jan 29 03:45:38.971: INFO: Checking APIGroup: events.k8s.io
Jan 29 03:45:38.973: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Jan 29 03:45:38.973: INFO: Versions found [{events.k8s.io/v1 v1}]
Jan 29 03:45:38.973: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Jan 29 03:45:38.973: INFO: Checking APIGroup: authentication.k8s.io
Jan 29 03:45:38.976: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Jan 29 03:45:38.976: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Jan 29 03:45:38.976: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Jan 29 03:45:38.976: INFO: Checking APIGroup: authorization.k8s.io
Jan 29 03:45:38.978: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Jan 29 03:45:38.978: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Jan 29 03:45:38.978: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Jan 29 03:45:38.978: INFO: Checking APIGroup: autoscaling
Jan 29 03:45:38.980: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Jan 29 03:45:38.980: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
Jan 29 03:45:38.980: INFO: autoscaling/v2 matches autoscaling/v2
Jan 29 03:45:38.980: INFO: Checking APIGroup: batch
Jan 29 03:45:38.981: INFO: PreferredVersion.GroupVersion: batch/v1
Jan 29 03:45:38.981: INFO: Versions found [{batch/v1 v1}]
Jan 29 03:45:38.981: INFO: batch/v1 matches batch/v1
Jan 29 03:45:38.981: INFO: Checking APIGroup: certificates.k8s.io
Jan 29 03:45:38.983: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Jan 29 03:45:38.983: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Jan 29 03:45:38.983: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Jan 29 03:45:38.983: INFO: Checking APIGroup: networking.k8s.io
Jan 29 03:45:38.985: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Jan 29 03:45:38.985: INFO: Versions found [{networking.k8s.io/v1 v1}]
Jan 29 03:45:38.985: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Jan 29 03:45:38.985: INFO: Checking APIGroup: policy
Jan 29 03:45:38.987: INFO: PreferredVersion.GroupVersion: policy/v1
Jan 29 03:45:38.987: INFO: Versions found [{policy/v1 v1}]
Jan 29 03:45:38.987: INFO: policy/v1 matches policy/v1
Jan 29 03:45:38.987: INFO: Checking APIGroup: rbac.authorization.k8s.io
Jan 29 03:45:38.989: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Jan 29 03:45:38.989: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Jan 29 03:45:38.989: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Jan 29 03:45:38.989: INFO: Checking APIGroup: storage.k8s.io
Jan 29 03:45:38.990: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Jan 29 03:45:38.990: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Jan 29 03:45:38.990: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Jan 29 03:45:38.990: INFO: Checking APIGroup: admissionregistration.k8s.io
Jan 29 03:45:38.992: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Jan 29 03:45:38.992: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Jan 29 03:45:38.992: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Jan 29 03:45:38.992: INFO: Checking APIGroup: apiextensions.k8s.io
Jan 29 03:45:38.994: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Jan 29 03:45:38.994: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Jan 29 03:45:38.994: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Jan 29 03:45:38.994: INFO: Checking APIGroup: scheduling.k8s.io
Jan 29 03:45:38.996: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Jan 29 03:45:38.996: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Jan 29 03:45:38.996: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Jan 29 03:45:38.996: INFO: Checking APIGroup: coordination.k8s.io
Jan 29 03:45:38.998: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Jan 29 03:45:38.998: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Jan 29 03:45:38.998: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Jan 29 03:45:38.998: INFO: Checking APIGroup: node.k8s.io
Jan 29 03:45:39.000: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Jan 29 03:45:39.000: INFO: Versions found [{node.k8s.io/v1 v1}]
Jan 29 03:45:39.000: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Jan 29 03:45:39.000: INFO: Checking APIGroup: discovery.k8s.io
Jan 29 03:45:39.002: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Jan 29 03:45:39.002: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Jan 29 03:45:39.002: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Jan 29 03:45:39.002: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Jan 29 03:45:39.004: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
Jan 29 03:45:39.004: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Jan 29 03:45:39.004: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
Jan 29 03:45:39.004: INFO: Checking APIGroup: cie.inspur.com
Jan 29 03:45:39.006: INFO: PreferredVersion.GroupVersion: cie.inspur.com/v1
Jan 29 03:45:39.006: INFO: Versions found [{cie.inspur.com/v1 v1} {cie.inspur.com/v1alpha1 v1alpha1}]
Jan 29 03:45:39.006: INFO: cie.inspur.com/v1 matches cie.inspur.com/v1
Jan 29 03:45:39.006: INFO: Checking APIGroup: k8s.cni.cncf.io
Jan 29 03:45:39.008: INFO: PreferredVersion.GroupVersion: k8s.cni.cncf.io/v1
Jan 29 03:45:39.008: INFO: Versions found [{k8s.cni.cncf.io/v1 v1}]
Jan 29 03:45:39.008: INFO: k8s.cni.cncf.io/v1 matches k8s.cni.cncf.io/v1
Jan 29 03:45:39.008: INFO: Checking APIGroup: cke.inspur.com
Jan 29 03:45:39.010: INFO: PreferredVersion.GroupVersion: cke.inspur.com/v1alpha1
Jan 29 03:45:39.010: INFO: Versions found [{cke.inspur.com/v1alpha1 v1alpha1}]
Jan 29 03:45:39.010: INFO: cke.inspur.com/v1alpha1 matches cke.inspur.com/v1alpha1
Jan 29 03:45:39.010: INFO: Checking APIGroup: metrics.k8s.io
Jan 29 03:45:39.011: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Jan 29 03:45:39.011: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Jan 29 03:45:39.012: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:187
Jan 29 03:45:39.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-8834" for this suite. 01/29/23 03:45:39.021
{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","completed":187,"skipped":3513,"failed":0}
------------------------------
• [0.608 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:45:38.421
    Jan 29 03:45:38.422: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename discovery 01/29/23 03:45:38.423
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:45:38.448
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:45:38.454
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 01/29/23 03:45:38.461
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Jan 29 03:45:38.966: INFO: Checking APIGroup: apiregistration.k8s.io
    Jan 29 03:45:38.969: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Jan 29 03:45:38.969: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Jan 29 03:45:38.969: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Jan 29 03:45:38.969: INFO: Checking APIGroup: apps
    Jan 29 03:45:38.971: INFO: PreferredVersion.GroupVersion: apps/v1
    Jan 29 03:45:38.971: INFO: Versions found [{apps/v1 v1}]
    Jan 29 03:45:38.971: INFO: apps/v1 matches apps/v1
    Jan 29 03:45:38.971: INFO: Checking APIGroup: events.k8s.io
    Jan 29 03:45:38.973: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Jan 29 03:45:38.973: INFO: Versions found [{events.k8s.io/v1 v1}]
    Jan 29 03:45:38.973: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Jan 29 03:45:38.973: INFO: Checking APIGroup: authentication.k8s.io
    Jan 29 03:45:38.976: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Jan 29 03:45:38.976: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Jan 29 03:45:38.976: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Jan 29 03:45:38.976: INFO: Checking APIGroup: authorization.k8s.io
    Jan 29 03:45:38.978: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Jan 29 03:45:38.978: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Jan 29 03:45:38.978: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Jan 29 03:45:38.978: INFO: Checking APIGroup: autoscaling
    Jan 29 03:45:38.980: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Jan 29 03:45:38.980: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
    Jan 29 03:45:38.980: INFO: autoscaling/v2 matches autoscaling/v2
    Jan 29 03:45:38.980: INFO: Checking APIGroup: batch
    Jan 29 03:45:38.981: INFO: PreferredVersion.GroupVersion: batch/v1
    Jan 29 03:45:38.981: INFO: Versions found [{batch/v1 v1}]
    Jan 29 03:45:38.981: INFO: batch/v1 matches batch/v1
    Jan 29 03:45:38.981: INFO: Checking APIGroup: certificates.k8s.io
    Jan 29 03:45:38.983: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Jan 29 03:45:38.983: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Jan 29 03:45:38.983: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Jan 29 03:45:38.983: INFO: Checking APIGroup: networking.k8s.io
    Jan 29 03:45:38.985: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Jan 29 03:45:38.985: INFO: Versions found [{networking.k8s.io/v1 v1}]
    Jan 29 03:45:38.985: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Jan 29 03:45:38.985: INFO: Checking APIGroup: policy
    Jan 29 03:45:38.987: INFO: PreferredVersion.GroupVersion: policy/v1
    Jan 29 03:45:38.987: INFO: Versions found [{policy/v1 v1}]
    Jan 29 03:45:38.987: INFO: policy/v1 matches policy/v1
    Jan 29 03:45:38.987: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Jan 29 03:45:38.989: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Jan 29 03:45:38.989: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Jan 29 03:45:38.989: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Jan 29 03:45:38.989: INFO: Checking APIGroup: storage.k8s.io
    Jan 29 03:45:38.990: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Jan 29 03:45:38.990: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Jan 29 03:45:38.990: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Jan 29 03:45:38.990: INFO: Checking APIGroup: admissionregistration.k8s.io
    Jan 29 03:45:38.992: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Jan 29 03:45:38.992: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Jan 29 03:45:38.992: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Jan 29 03:45:38.992: INFO: Checking APIGroup: apiextensions.k8s.io
    Jan 29 03:45:38.994: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Jan 29 03:45:38.994: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Jan 29 03:45:38.994: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Jan 29 03:45:38.994: INFO: Checking APIGroup: scheduling.k8s.io
    Jan 29 03:45:38.996: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Jan 29 03:45:38.996: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Jan 29 03:45:38.996: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Jan 29 03:45:38.996: INFO: Checking APIGroup: coordination.k8s.io
    Jan 29 03:45:38.998: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Jan 29 03:45:38.998: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Jan 29 03:45:38.998: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Jan 29 03:45:38.998: INFO: Checking APIGroup: node.k8s.io
    Jan 29 03:45:39.000: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Jan 29 03:45:39.000: INFO: Versions found [{node.k8s.io/v1 v1}]
    Jan 29 03:45:39.000: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Jan 29 03:45:39.000: INFO: Checking APIGroup: discovery.k8s.io
    Jan 29 03:45:39.002: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Jan 29 03:45:39.002: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Jan 29 03:45:39.002: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Jan 29 03:45:39.002: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Jan 29 03:45:39.004: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
    Jan 29 03:45:39.004: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
    Jan 29 03:45:39.004: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
    Jan 29 03:45:39.004: INFO: Checking APIGroup: cie.inspur.com
    Jan 29 03:45:39.006: INFO: PreferredVersion.GroupVersion: cie.inspur.com/v1
    Jan 29 03:45:39.006: INFO: Versions found [{cie.inspur.com/v1 v1} {cie.inspur.com/v1alpha1 v1alpha1}]
    Jan 29 03:45:39.006: INFO: cie.inspur.com/v1 matches cie.inspur.com/v1
    Jan 29 03:45:39.006: INFO: Checking APIGroup: k8s.cni.cncf.io
    Jan 29 03:45:39.008: INFO: PreferredVersion.GroupVersion: k8s.cni.cncf.io/v1
    Jan 29 03:45:39.008: INFO: Versions found [{k8s.cni.cncf.io/v1 v1}]
    Jan 29 03:45:39.008: INFO: k8s.cni.cncf.io/v1 matches k8s.cni.cncf.io/v1
    Jan 29 03:45:39.008: INFO: Checking APIGroup: cke.inspur.com
    Jan 29 03:45:39.010: INFO: PreferredVersion.GroupVersion: cke.inspur.com/v1alpha1
    Jan 29 03:45:39.010: INFO: Versions found [{cke.inspur.com/v1alpha1 v1alpha1}]
    Jan 29 03:45:39.010: INFO: cke.inspur.com/v1alpha1 matches cke.inspur.com/v1alpha1
    Jan 29 03:45:39.010: INFO: Checking APIGroup: metrics.k8s.io
    Jan 29 03:45:39.011: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
    Jan 29 03:45:39.011: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
    Jan 29 03:45:39.012: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:187
    Jan 29 03:45:39.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "discovery-8834" for this suite. 01/29/23 03:45:39.021
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:45:39.03
Jan 29 03:45:39.031: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename watch 01/29/23 03:45:39.032
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:45:39.058
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:45:39.063
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 01/29/23 03:45:39.069
STEP: creating a new configmap 01/29/23 03:45:39.071
STEP: modifying the configmap once 01/29/23 03:45:39.077
STEP: changing the label value of the configmap 01/29/23 03:45:39.088
STEP: Expecting to observe a delete notification for the watched object 01/29/23 03:45:39.098
Jan 29 03:45:39.099: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2387  2fed367b-da0e-4d87-9222-2ca5ffa568d4 5965880 0 2023-01-29 03:45:39 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] []},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 29 03:45:39.099: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2387  2fed367b-da0e-4d87-9222-2ca5ffa568d4 5965881 0 2023-01-29 03:45:39 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 29 03:45:39.099: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2387  2fed367b-da0e-4d87-9222-2ca5ffa568d4 5965882 0 2023-01-29 03:45:39 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 01/29/23 03:45:39.099
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 01/29/23 03:45:39.112
STEP: changing the label value of the configmap back 01/29/23 03:45:49.113
STEP: modifying the configmap a third time 01/29/23 03:45:49.127
STEP: deleting the configmap 01/29/23 03:45:49.14
STEP: Expecting to observe an add notification for the watched object when the label value was restored 01/29/23 03:45:49.152
Jan 29 03:45:49.152: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2387  2fed367b-da0e-4d87-9222-2ca5ffa568d4 5965966 0 2023-01-29 03:45:39 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 29 03:45:49.152: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2387  2fed367b-da0e-4d87-9222-2ca5ffa568d4 5965967 0 2023-01-29 03:45:39 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 29 03:45:49.152: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2387  2fed367b-da0e-4d87-9222-2ca5ffa568d4 5965968 0 2023-01-29 03:45:39 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jan 29 03:45:49.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2387" for this suite. 01/29/23 03:45:49.162
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","completed":188,"skipped":3517,"failed":0}
------------------------------
• [SLOW TEST] [10.139 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:45:39.03
    Jan 29 03:45:39.031: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename watch 01/29/23 03:45:39.032
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:45:39.058
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:45:39.063
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 01/29/23 03:45:39.069
    STEP: creating a new configmap 01/29/23 03:45:39.071
    STEP: modifying the configmap once 01/29/23 03:45:39.077
    STEP: changing the label value of the configmap 01/29/23 03:45:39.088
    STEP: Expecting to observe a delete notification for the watched object 01/29/23 03:45:39.098
    Jan 29 03:45:39.099: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2387  2fed367b-da0e-4d87-9222-2ca5ffa568d4 5965880 0 2023-01-29 03:45:39 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] []},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 29 03:45:39.099: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2387  2fed367b-da0e-4d87-9222-2ca5ffa568d4 5965881 0 2023-01-29 03:45:39 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 29 03:45:39.099: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2387  2fed367b-da0e-4d87-9222-2ca5ffa568d4 5965882 0 2023-01-29 03:45:39 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 01/29/23 03:45:39.099
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 01/29/23 03:45:39.112
    STEP: changing the label value of the configmap back 01/29/23 03:45:49.113
    STEP: modifying the configmap a third time 01/29/23 03:45:49.127
    STEP: deleting the configmap 01/29/23 03:45:49.14
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 01/29/23 03:45:49.152
    Jan 29 03:45:49.152: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2387  2fed367b-da0e-4d87-9222-2ca5ffa568d4 5965966 0 2023-01-29 03:45:39 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 29 03:45:49.152: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2387  2fed367b-da0e-4d87-9222-2ca5ffa568d4 5965967 0 2023-01-29 03:45:39 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 29 03:45:49.152: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2387  2fed367b-da0e-4d87-9222-2ca5ffa568d4 5965968 0 2023-01-29 03:45:39 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jan 29 03:45:49.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-2387" for this suite. 01/29/23 03:45:49.162
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:45:49.171
Jan 29 03:45:49.171: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename projected 01/29/23 03:45:49.172
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:45:49.2
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:45:49.206
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
STEP: Creating configMap with name projected-configmap-test-volume-d82f2d98-0ab7-4ec6-824c-7a80ef120eb4 01/29/23 03:45:49.212
STEP: Creating a pod to test consume configMaps 01/29/23 03:45:49.219
Jan 29 03:45:49.240: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5c720e4f-535e-47cb-b5a1-a8bfab2bd8eb" in namespace "projected-9401" to be "Succeeded or Failed"
Jan 29 03:45:49.246: INFO: Pod "pod-projected-configmaps-5c720e4f-535e-47cb-b5a1-a8bfab2bd8eb": Phase="Pending", Reason="", readiness=false. Elapsed: 5.84208ms
Jan 29 03:45:51.252: INFO: Pod "pod-projected-configmaps-5c720e4f-535e-47cb-b5a1-a8bfab2bd8eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012508911s
Jan 29 03:45:53.254: INFO: Pod "pod-projected-configmaps-5c720e4f-535e-47cb-b5a1-a8bfab2bd8eb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014077206s
Jan 29 03:45:55.251: INFO: Pod "pod-projected-configmaps-5c720e4f-535e-47cb-b5a1-a8bfab2bd8eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011633733s
STEP: Saw pod success 01/29/23 03:45:55.251
Jan 29 03:45:55.251: INFO: Pod "pod-projected-configmaps-5c720e4f-535e-47cb-b5a1-a8bfab2bd8eb" satisfied condition "Succeeded or Failed"
Jan 29 03:45:55.256: INFO: Trying to get logs from node slave2 pod pod-projected-configmaps-5c720e4f-535e-47cb-b5a1-a8bfab2bd8eb container agnhost-container: <nil>
STEP: delete the pod 01/29/23 03:45:55.27
Jan 29 03:45:55.355: INFO: Waiting for pod pod-projected-configmaps-5c720e4f-535e-47cb-b5a1-a8bfab2bd8eb to disappear
Jan 29 03:45:55.359: INFO: Pod pod-projected-configmaps-5c720e4f-535e-47cb-b5a1-a8bfab2bd8eb no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 29 03:45:55.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9401" for this suite. 01/29/23 03:45:55.367
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":189,"skipped":3526,"failed":0}
------------------------------
• [SLOW TEST] [6.203 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:45:49.171
    Jan 29 03:45:49.171: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename projected 01/29/23 03:45:49.172
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:45:49.2
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:45:49.206
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:46
    STEP: Creating configMap with name projected-configmap-test-volume-d82f2d98-0ab7-4ec6-824c-7a80ef120eb4 01/29/23 03:45:49.212
    STEP: Creating a pod to test consume configMaps 01/29/23 03:45:49.219
    Jan 29 03:45:49.240: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5c720e4f-535e-47cb-b5a1-a8bfab2bd8eb" in namespace "projected-9401" to be "Succeeded or Failed"
    Jan 29 03:45:49.246: INFO: Pod "pod-projected-configmaps-5c720e4f-535e-47cb-b5a1-a8bfab2bd8eb": Phase="Pending", Reason="", readiness=false. Elapsed: 5.84208ms
    Jan 29 03:45:51.252: INFO: Pod "pod-projected-configmaps-5c720e4f-535e-47cb-b5a1-a8bfab2bd8eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012508911s
    Jan 29 03:45:53.254: INFO: Pod "pod-projected-configmaps-5c720e4f-535e-47cb-b5a1-a8bfab2bd8eb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014077206s
    Jan 29 03:45:55.251: INFO: Pod "pod-projected-configmaps-5c720e4f-535e-47cb-b5a1-a8bfab2bd8eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011633733s
    STEP: Saw pod success 01/29/23 03:45:55.251
    Jan 29 03:45:55.251: INFO: Pod "pod-projected-configmaps-5c720e4f-535e-47cb-b5a1-a8bfab2bd8eb" satisfied condition "Succeeded or Failed"
    Jan 29 03:45:55.256: INFO: Trying to get logs from node slave2 pod pod-projected-configmaps-5c720e4f-535e-47cb-b5a1-a8bfab2bd8eb container agnhost-container: <nil>
    STEP: delete the pod 01/29/23 03:45:55.27
    Jan 29 03:45:55.355: INFO: Waiting for pod pod-projected-configmaps-5c720e4f-535e-47cb-b5a1-a8bfab2bd8eb to disappear
    Jan 29 03:45:55.359: INFO: Pod pod-projected-configmaps-5c720e4f-535e-47cb-b5a1-a8bfab2bd8eb no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 29 03:45:55.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9401" for this suite. 01/29/23 03:45:55.367
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:45:55.375
Jan 29 03:45:55.375: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename aggregator 01/29/23 03:45:55.377
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:45:55.402
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:45:55.412
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Jan 29 03:45:55.417: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 01/29/23 03:45:55.419
Jan 29 03:45:56.882: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
Jan 29 03:45:58.964: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 29 03:46:00.971: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 29 03:46:02.970: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 29 03:46:04.972: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 29 03:46:06.970: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 29 03:46:08.972: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 29 03:46:10.971: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 29 03:46:12.971: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 29 03:46:14.971: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 29 03:46:16.969: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 29 03:46:19.117: INFO: Waited 136.283274ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 01/29/23 03:46:19.188
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 01/29/23 03:46:19.194
STEP: List APIServices 01/29/23 03:46:19.202
Jan 29 03:46:19.213: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:187
Jan 29 03:46:19.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-1430" for this suite. 01/29/23 03:46:19.546
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","completed":190,"skipped":3530,"failed":0}
------------------------------
• [SLOW TEST] [24.221 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:45:55.375
    Jan 29 03:45:55.375: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename aggregator 01/29/23 03:45:55.377
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:45:55.402
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:45:55.412
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Jan 29 03:45:55.417: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 01/29/23 03:45:55.419
    Jan 29 03:45:56.882: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
    Jan 29 03:45:58.964: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 29 03:46:00.971: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 29 03:46:02.970: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 29 03:46:04.972: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 29 03:46:06.970: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 29 03:46:08.972: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 29 03:46:10.971: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 29 03:46:12.971: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 29 03:46:14.971: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 29 03:46:16.969: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 3, 45, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 29 03:46:19.117: INFO: Waited 136.283274ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 01/29/23 03:46:19.188
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 01/29/23 03:46:19.194
    STEP: List APIServices 01/29/23 03:46:19.202
    Jan 29 03:46:19.213: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:187
    Jan 29 03:46:19.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "aggregator-1430" for this suite. 01/29/23 03:46:19.546
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:46:19.598
Jan 29 03:46:19.598: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename pods 01/29/23 03:46:19.599
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:46:19.628
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:46:19.633
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
STEP: Create set of pods 01/29/23 03:46:19.639
Jan 29 03:46:19.655: INFO: created test-pod-1
Jan 29 03:46:19.671: INFO: created test-pod-2
Jan 29 03:46:19.688: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 01/29/23 03:46:19.688
Jan 29 03:46:19.688: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-1883' to be running and ready
Jan 29 03:46:19.708: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 29 03:46:19.708: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 29 03:46:19.708: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 29 03:46:19.708: INFO: 0 / 3 pods in namespace 'pods-1883' are running and ready (0 seconds elapsed)
Jan 29 03:46:19.708: INFO: expected 0 pod replicas in namespace 'pods-1883', 0 are Running and Ready.
Jan 29 03:46:19.708: INFO: POD         NODE    PHASE    GRACE  CONDITIONS
Jan 29 03:46:19.708: INFO: test-pod-1  slave2  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-29 03:46:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-29 03:46:19 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-29 03:46:19 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-29 03:46:19 +0000 UTC  }]
Jan 29 03:46:19.708: INFO: test-pod-2  slave2  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-29 03:46:19 +0000 UTC  }]
Jan 29 03:46:19.708: INFO: test-pod-3  slave2  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-29 03:46:19 +0000 UTC  }]
Jan 29 03:46:19.708: INFO: 
Jan 29 03:46:21.731: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 29 03:46:21.731: INFO: 2 / 3 pods in namespace 'pods-1883' are running and ready (2 seconds elapsed)
Jan 29 03:46:21.731: INFO: expected 0 pod replicas in namespace 'pods-1883', 0 are Running and Ready.
Jan 29 03:46:21.731: INFO: POD         NODE    PHASE    GRACE  CONDITIONS
Jan 29 03:46:21.731: INFO: test-pod-3  slave2  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-29 03:46:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-29 03:46:19 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-29 03:46:19 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-29 03:46:19 +0000 UTC  }]
Jan 29 03:46:21.731: INFO: 
Jan 29 03:46:23.728: INFO: 3 / 3 pods in namespace 'pods-1883' are running and ready (4 seconds elapsed)
Jan 29 03:46:23.728: INFO: expected 0 pod replicas in namespace 'pods-1883', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 01/29/23 03:46:23.85
Jan 29 03:46:23.856: INFO: Pod quantity 3 is different from expected quantity 0
Jan 29 03:46:24.865: INFO: Pod quantity 3 is different from expected quantity 0
Jan 29 03:46:25.864: INFO: Pod quantity 1 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 29 03:46:26.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1883" for this suite. 01/29/23 03:46:26.872
{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","completed":191,"skipped":3540,"failed":0}
------------------------------
• [SLOW TEST] [7.284 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:46:19.598
    Jan 29 03:46:19.598: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename pods 01/29/23 03:46:19.599
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:46:19.628
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:46:19.633
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:844
    STEP: Create set of pods 01/29/23 03:46:19.639
    Jan 29 03:46:19.655: INFO: created test-pod-1
    Jan 29 03:46:19.671: INFO: created test-pod-2
    Jan 29 03:46:19.688: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 01/29/23 03:46:19.688
    Jan 29 03:46:19.688: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-1883' to be running and ready
    Jan 29 03:46:19.708: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 29 03:46:19.708: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 29 03:46:19.708: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 29 03:46:19.708: INFO: 0 / 3 pods in namespace 'pods-1883' are running and ready (0 seconds elapsed)
    Jan 29 03:46:19.708: INFO: expected 0 pod replicas in namespace 'pods-1883', 0 are Running and Ready.
    Jan 29 03:46:19.708: INFO: POD         NODE    PHASE    GRACE  CONDITIONS
    Jan 29 03:46:19.708: INFO: test-pod-1  slave2  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-29 03:46:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-29 03:46:19 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-29 03:46:19 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-29 03:46:19 +0000 UTC  }]
    Jan 29 03:46:19.708: INFO: test-pod-2  slave2  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-29 03:46:19 +0000 UTC  }]
    Jan 29 03:46:19.708: INFO: test-pod-3  slave2  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-29 03:46:19 +0000 UTC  }]
    Jan 29 03:46:19.708: INFO: 
    Jan 29 03:46:21.731: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 29 03:46:21.731: INFO: 2 / 3 pods in namespace 'pods-1883' are running and ready (2 seconds elapsed)
    Jan 29 03:46:21.731: INFO: expected 0 pod replicas in namespace 'pods-1883', 0 are Running and Ready.
    Jan 29 03:46:21.731: INFO: POD         NODE    PHASE    GRACE  CONDITIONS
    Jan 29 03:46:21.731: INFO: test-pod-3  slave2  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-29 03:46:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-29 03:46:19 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-29 03:46:19 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-29 03:46:19 +0000 UTC  }]
    Jan 29 03:46:21.731: INFO: 
    Jan 29 03:46:23.728: INFO: 3 / 3 pods in namespace 'pods-1883' are running and ready (4 seconds elapsed)
    Jan 29 03:46:23.728: INFO: expected 0 pod replicas in namespace 'pods-1883', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 01/29/23 03:46:23.85
    Jan 29 03:46:23.856: INFO: Pod quantity 3 is different from expected quantity 0
    Jan 29 03:46:24.865: INFO: Pod quantity 3 is different from expected quantity 0
    Jan 29 03:46:25.864: INFO: Pod quantity 1 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 29 03:46:26.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-1883" for this suite. 01/29/23 03:46:26.872
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:46:26.883
Jan 29 03:46:26.883: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename endpointslice 01/29/23 03:46:26.884
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:46:26.913
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:46:26.919
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Jan 29 03:46:29.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-55" for this suite. 01/29/23 03:46:29.047
{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","completed":192,"skipped":3557,"failed":0}
------------------------------
• [2.178 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:46:26.883
    Jan 29 03:46:26.883: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename endpointslice 01/29/23 03:46:26.884
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:46:26.913
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:46:26.919
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:101
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Jan 29 03:46:29.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-55" for this suite. 01/29/23 03:46:29.047
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:46:29.062
Jan 29 03:46:29.062: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename projected 01/29/23 03:46:29.064
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:46:29.103
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:46:29.109
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
STEP: Creating configMap with name projected-configmap-test-volume-map-6a05496a-6f0a-4485-ad2a-ce22f8da9dc3 01/29/23 03:46:29.116
STEP: Creating a pod to test consume configMaps 01/29/23 03:46:29.126
Jan 29 03:46:29.151: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f5d1a6c8-f569-47eb-b19e-982ee22aad9d" in namespace "projected-7769" to be "Succeeded or Failed"
Jan 29 03:46:29.157: INFO: Pod "pod-projected-configmaps-f5d1a6c8-f569-47eb-b19e-982ee22aad9d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.606747ms
Jan 29 03:46:31.165: INFO: Pod "pod-projected-configmaps-f5d1a6c8-f569-47eb-b19e-982ee22aad9d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014320545s
Jan 29 03:46:33.164: INFO: Pod "pod-projected-configmaps-f5d1a6c8-f569-47eb-b19e-982ee22aad9d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013541523s
Jan 29 03:46:35.166: INFO: Pod "pod-projected-configmaps-f5d1a6c8-f569-47eb-b19e-982ee22aad9d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015638362s
STEP: Saw pod success 01/29/23 03:46:35.166
Jan 29 03:46:35.167: INFO: Pod "pod-projected-configmaps-f5d1a6c8-f569-47eb-b19e-982ee22aad9d" satisfied condition "Succeeded or Failed"
Jan 29 03:46:35.172: INFO: Trying to get logs from node slave2 pod pod-projected-configmaps-f5d1a6c8-f569-47eb-b19e-982ee22aad9d container agnhost-container: <nil>
STEP: delete the pod 01/29/23 03:46:35.186
Jan 29 03:46:35.281: INFO: Waiting for pod pod-projected-configmaps-f5d1a6c8-f569-47eb-b19e-982ee22aad9d to disappear
Jan 29 03:46:35.286: INFO: Pod pod-projected-configmaps-f5d1a6c8-f569-47eb-b19e-982ee22aad9d no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 29 03:46:35.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7769" for this suite. 01/29/23 03:46:35.295
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":193,"skipped":3560,"failed":0}
------------------------------
• [SLOW TEST] [6.241 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:46:29.062
    Jan 29 03:46:29.062: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename projected 01/29/23 03:46:29.064
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:46:29.103
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:46:29.109
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:108
    STEP: Creating configMap with name projected-configmap-test-volume-map-6a05496a-6f0a-4485-ad2a-ce22f8da9dc3 01/29/23 03:46:29.116
    STEP: Creating a pod to test consume configMaps 01/29/23 03:46:29.126
    Jan 29 03:46:29.151: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f5d1a6c8-f569-47eb-b19e-982ee22aad9d" in namespace "projected-7769" to be "Succeeded or Failed"
    Jan 29 03:46:29.157: INFO: Pod "pod-projected-configmaps-f5d1a6c8-f569-47eb-b19e-982ee22aad9d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.606747ms
    Jan 29 03:46:31.165: INFO: Pod "pod-projected-configmaps-f5d1a6c8-f569-47eb-b19e-982ee22aad9d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014320545s
    Jan 29 03:46:33.164: INFO: Pod "pod-projected-configmaps-f5d1a6c8-f569-47eb-b19e-982ee22aad9d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013541523s
    Jan 29 03:46:35.166: INFO: Pod "pod-projected-configmaps-f5d1a6c8-f569-47eb-b19e-982ee22aad9d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015638362s
    STEP: Saw pod success 01/29/23 03:46:35.166
    Jan 29 03:46:35.167: INFO: Pod "pod-projected-configmaps-f5d1a6c8-f569-47eb-b19e-982ee22aad9d" satisfied condition "Succeeded or Failed"
    Jan 29 03:46:35.172: INFO: Trying to get logs from node slave2 pod pod-projected-configmaps-f5d1a6c8-f569-47eb-b19e-982ee22aad9d container agnhost-container: <nil>
    STEP: delete the pod 01/29/23 03:46:35.186
    Jan 29 03:46:35.281: INFO: Waiting for pod pod-projected-configmaps-f5d1a6c8-f569-47eb-b19e-982ee22aad9d to disappear
    Jan 29 03:46:35.286: INFO: Pod pod-projected-configmaps-f5d1a6c8-f569-47eb-b19e-982ee22aad9d no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 29 03:46:35.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7769" for this suite. 01/29/23 03:46:35.295
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:46:35.305
Jan 29 03:46:35.306: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename container-probe 01/29/23 03:46:35.307
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:46:35.334
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:46:35.339
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
Jan 29 03:46:35.360: INFO: Waiting up to 5m0s for pod "test-webserver-c0cac157-9203-4f8f-9993-ce7b857105fd" in namespace "container-probe-5794" to be "running and ready"
Jan 29 03:46:35.367: INFO: Pod "test-webserver-c0cac157-9203-4f8f-9993-ce7b857105fd": Phase="Pending", Reason="", readiness=false. Elapsed: 7.07487ms
Jan 29 03:46:35.367: INFO: The phase of Pod test-webserver-c0cac157-9203-4f8f-9993-ce7b857105fd is Pending, waiting for it to be Running (with Ready = true)
Jan 29 03:46:37.376: INFO: Pod "test-webserver-c0cac157-9203-4f8f-9993-ce7b857105fd": Phase="Running", Reason="", readiness=false. Elapsed: 2.016201057s
Jan 29 03:46:37.377: INFO: The phase of Pod test-webserver-c0cac157-9203-4f8f-9993-ce7b857105fd is Running (Ready = false)
Jan 29 03:46:39.374: INFO: Pod "test-webserver-c0cac157-9203-4f8f-9993-ce7b857105fd": Phase="Running", Reason="", readiness=false. Elapsed: 4.013511741s
Jan 29 03:46:39.374: INFO: The phase of Pod test-webserver-c0cac157-9203-4f8f-9993-ce7b857105fd is Running (Ready = false)
Jan 29 03:46:41.375: INFO: Pod "test-webserver-c0cac157-9203-4f8f-9993-ce7b857105fd": Phase="Running", Reason="", readiness=false. Elapsed: 6.015024831s
Jan 29 03:46:41.375: INFO: The phase of Pod test-webserver-c0cac157-9203-4f8f-9993-ce7b857105fd is Running (Ready = false)
Jan 29 03:46:43.375: INFO: Pod "test-webserver-c0cac157-9203-4f8f-9993-ce7b857105fd": Phase="Running", Reason="", readiness=false. Elapsed: 8.014450166s
Jan 29 03:46:43.375: INFO: The phase of Pod test-webserver-c0cac157-9203-4f8f-9993-ce7b857105fd is Running (Ready = false)
Jan 29 03:46:45.375: INFO: Pod "test-webserver-c0cac157-9203-4f8f-9993-ce7b857105fd": Phase="Running", Reason="", readiness=false. Elapsed: 10.014829968s
Jan 29 03:46:45.375: INFO: The phase of Pod test-webserver-c0cac157-9203-4f8f-9993-ce7b857105fd is Running (Ready = false)
Jan 29 03:46:47.373: INFO: Pod "test-webserver-c0cac157-9203-4f8f-9993-ce7b857105fd": Phase="Running", Reason="", readiness=false. Elapsed: 12.013086055s
Jan 29 03:46:47.373: INFO: The phase of Pod test-webserver-c0cac157-9203-4f8f-9993-ce7b857105fd is Running (Ready = false)
Jan 29 03:46:49.376: INFO: Pod "test-webserver-c0cac157-9203-4f8f-9993-ce7b857105fd": Phase="Running", Reason="", readiness=false. Elapsed: 14.016079375s
Jan 29 03:46:49.376: INFO: The phase of Pod test-webserver-c0cac157-9203-4f8f-9993-ce7b857105fd is Running (Ready = false)
Jan 29 03:46:51.374: INFO: Pod "test-webserver-c0cac157-9203-4f8f-9993-ce7b857105fd": Phase="Running", Reason="", readiness=false. Elapsed: 16.013929519s
Jan 29 03:46:51.374: INFO: The phase of Pod test-webserver-c0cac157-9203-4f8f-9993-ce7b857105fd is Running (Ready = false)
Jan 29 03:46:53.375: INFO: Pod "test-webserver-c0cac157-9203-4f8f-9993-ce7b857105fd": Phase="Running", Reason="", readiness=false. Elapsed: 18.014383721s
Jan 29 03:46:53.375: INFO: The phase of Pod test-webserver-c0cac157-9203-4f8f-9993-ce7b857105fd is Running (Ready = false)
Jan 29 03:46:55.376: INFO: Pod "test-webserver-c0cac157-9203-4f8f-9993-ce7b857105fd": Phase="Running", Reason="", readiness=false. Elapsed: 20.016128193s
Jan 29 03:46:55.376: INFO: The phase of Pod test-webserver-c0cac157-9203-4f8f-9993-ce7b857105fd is Running (Ready = false)
Jan 29 03:46:57.375: INFO: Pod "test-webserver-c0cac157-9203-4f8f-9993-ce7b857105fd": Phase="Running", Reason="", readiness=true. Elapsed: 22.014667762s
Jan 29 03:46:57.375: INFO: The phase of Pod test-webserver-c0cac157-9203-4f8f-9993-ce7b857105fd is Running (Ready = true)
Jan 29 03:46:57.375: INFO: Pod "test-webserver-c0cac157-9203-4f8f-9993-ce7b857105fd" satisfied condition "running and ready"
Jan 29 03:46:57.381: INFO: Container started at 2023-01-29 03:46:36 +0000 UTC, pod became ready at 2023-01-29 03:46:55 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan 29 03:46:57.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5794" for this suite. 01/29/23 03:46:57.393
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","completed":194,"skipped":3582,"failed":0}
------------------------------
• [SLOW TEST] [22.098 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:46:35.305
    Jan 29 03:46:35.306: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename container-probe 01/29/23 03:46:35.307
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:46:35.334
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:46:35.339
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:68
    Jan 29 03:46:35.360: INFO: Waiting up to 5m0s for pod "test-webserver-c0cac157-9203-4f8f-9993-ce7b857105fd" in namespace "container-probe-5794" to be "running and ready"
    Jan 29 03:46:35.367: INFO: Pod "test-webserver-c0cac157-9203-4f8f-9993-ce7b857105fd": Phase="Pending", Reason="", readiness=false. Elapsed: 7.07487ms
    Jan 29 03:46:35.367: INFO: The phase of Pod test-webserver-c0cac157-9203-4f8f-9993-ce7b857105fd is Pending, waiting for it to be Running (with Ready = true)
    Jan 29 03:46:37.376: INFO: Pod "test-webserver-c0cac157-9203-4f8f-9993-ce7b857105fd": Phase="Running", Reason="", readiness=false. Elapsed: 2.016201057s
    Jan 29 03:46:37.377: INFO: The phase of Pod test-webserver-c0cac157-9203-4f8f-9993-ce7b857105fd is Running (Ready = false)
    Jan 29 03:46:39.374: INFO: Pod "test-webserver-c0cac157-9203-4f8f-9993-ce7b857105fd": Phase="Running", Reason="", readiness=false. Elapsed: 4.013511741s
    Jan 29 03:46:39.374: INFO: The phase of Pod test-webserver-c0cac157-9203-4f8f-9993-ce7b857105fd is Running (Ready = false)
    Jan 29 03:46:41.375: INFO: Pod "test-webserver-c0cac157-9203-4f8f-9993-ce7b857105fd": Phase="Running", Reason="", readiness=false. Elapsed: 6.015024831s
    Jan 29 03:46:41.375: INFO: The phase of Pod test-webserver-c0cac157-9203-4f8f-9993-ce7b857105fd is Running (Ready = false)
    Jan 29 03:46:43.375: INFO: Pod "test-webserver-c0cac157-9203-4f8f-9993-ce7b857105fd": Phase="Running", Reason="", readiness=false. Elapsed: 8.014450166s
    Jan 29 03:46:43.375: INFO: The phase of Pod test-webserver-c0cac157-9203-4f8f-9993-ce7b857105fd is Running (Ready = false)
    Jan 29 03:46:45.375: INFO: Pod "test-webserver-c0cac157-9203-4f8f-9993-ce7b857105fd": Phase="Running", Reason="", readiness=false. Elapsed: 10.014829968s
    Jan 29 03:46:45.375: INFO: The phase of Pod test-webserver-c0cac157-9203-4f8f-9993-ce7b857105fd is Running (Ready = false)
    Jan 29 03:46:47.373: INFO: Pod "test-webserver-c0cac157-9203-4f8f-9993-ce7b857105fd": Phase="Running", Reason="", readiness=false. Elapsed: 12.013086055s
    Jan 29 03:46:47.373: INFO: The phase of Pod test-webserver-c0cac157-9203-4f8f-9993-ce7b857105fd is Running (Ready = false)
    Jan 29 03:46:49.376: INFO: Pod "test-webserver-c0cac157-9203-4f8f-9993-ce7b857105fd": Phase="Running", Reason="", readiness=false. Elapsed: 14.016079375s
    Jan 29 03:46:49.376: INFO: The phase of Pod test-webserver-c0cac157-9203-4f8f-9993-ce7b857105fd is Running (Ready = false)
    Jan 29 03:46:51.374: INFO: Pod "test-webserver-c0cac157-9203-4f8f-9993-ce7b857105fd": Phase="Running", Reason="", readiness=false. Elapsed: 16.013929519s
    Jan 29 03:46:51.374: INFO: The phase of Pod test-webserver-c0cac157-9203-4f8f-9993-ce7b857105fd is Running (Ready = false)
    Jan 29 03:46:53.375: INFO: Pod "test-webserver-c0cac157-9203-4f8f-9993-ce7b857105fd": Phase="Running", Reason="", readiness=false. Elapsed: 18.014383721s
    Jan 29 03:46:53.375: INFO: The phase of Pod test-webserver-c0cac157-9203-4f8f-9993-ce7b857105fd is Running (Ready = false)
    Jan 29 03:46:55.376: INFO: Pod "test-webserver-c0cac157-9203-4f8f-9993-ce7b857105fd": Phase="Running", Reason="", readiness=false. Elapsed: 20.016128193s
    Jan 29 03:46:55.376: INFO: The phase of Pod test-webserver-c0cac157-9203-4f8f-9993-ce7b857105fd is Running (Ready = false)
    Jan 29 03:46:57.375: INFO: Pod "test-webserver-c0cac157-9203-4f8f-9993-ce7b857105fd": Phase="Running", Reason="", readiness=true. Elapsed: 22.014667762s
    Jan 29 03:46:57.375: INFO: The phase of Pod test-webserver-c0cac157-9203-4f8f-9993-ce7b857105fd is Running (Ready = true)
    Jan 29 03:46:57.375: INFO: Pod "test-webserver-c0cac157-9203-4f8f-9993-ce7b857105fd" satisfied condition "running and ready"
    Jan 29 03:46:57.381: INFO: Container started at 2023-01-29 03:46:36 +0000 UTC, pod became ready at 2023-01-29 03:46:55 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan 29 03:46:57.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-5794" for this suite. 01/29/23 03:46:57.393
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:46:57.404
Jan 29 03:46:57.404: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename secrets 01/29/23 03:46:57.405
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:46:57.432
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:46:57.438
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
STEP: Creating secret with name secret-test-fbfb9d05-3a2a-4d0d-baab-9a224940cc5f 01/29/23 03:46:57.444
STEP: Creating a pod to test consume secrets 01/29/23 03:46:57.451
Jan 29 03:46:57.467: INFO: Waiting up to 5m0s for pod "pod-secrets-7a53f666-9aad-42b9-958a-81884134991c" in namespace "secrets-302" to be "Succeeded or Failed"
Jan 29 03:46:57.473: INFO: Pod "pod-secrets-7a53f666-9aad-42b9-958a-81884134991c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.909742ms
Jan 29 03:46:59.480: INFO: Pod "pod-secrets-7a53f666-9aad-42b9-958a-81884134991c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013527174s
Jan 29 03:47:01.483: INFO: Pod "pod-secrets-7a53f666-9aad-42b9-958a-81884134991c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015718789s
Jan 29 03:47:03.480: INFO: Pod "pod-secrets-7a53f666-9aad-42b9-958a-81884134991c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013012749s
STEP: Saw pod success 01/29/23 03:47:03.48
Jan 29 03:47:03.480: INFO: Pod "pod-secrets-7a53f666-9aad-42b9-958a-81884134991c" satisfied condition "Succeeded or Failed"
Jan 29 03:47:03.486: INFO: Trying to get logs from node slave2 pod pod-secrets-7a53f666-9aad-42b9-958a-81884134991c container secret-volume-test: <nil>
STEP: delete the pod 01/29/23 03:47:03.501
Jan 29 03:47:03.570: INFO: Waiting for pod pod-secrets-7a53f666-9aad-42b9-958a-81884134991c to disappear
Jan 29 03:47:03.576: INFO: Pod pod-secrets-7a53f666-9aad-42b9-958a-81884134991c no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 29 03:47:03.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-302" for this suite. 01/29/23 03:47:03.585
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":195,"skipped":3584,"failed":0}
------------------------------
• [SLOW TEST] [6.190 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:46:57.404
    Jan 29 03:46:57.404: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename secrets 01/29/23 03:46:57.405
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:46:57.432
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:46:57.438
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:67
    STEP: Creating secret with name secret-test-fbfb9d05-3a2a-4d0d-baab-9a224940cc5f 01/29/23 03:46:57.444
    STEP: Creating a pod to test consume secrets 01/29/23 03:46:57.451
    Jan 29 03:46:57.467: INFO: Waiting up to 5m0s for pod "pod-secrets-7a53f666-9aad-42b9-958a-81884134991c" in namespace "secrets-302" to be "Succeeded or Failed"
    Jan 29 03:46:57.473: INFO: Pod "pod-secrets-7a53f666-9aad-42b9-958a-81884134991c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.909742ms
    Jan 29 03:46:59.480: INFO: Pod "pod-secrets-7a53f666-9aad-42b9-958a-81884134991c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013527174s
    Jan 29 03:47:01.483: INFO: Pod "pod-secrets-7a53f666-9aad-42b9-958a-81884134991c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015718789s
    Jan 29 03:47:03.480: INFO: Pod "pod-secrets-7a53f666-9aad-42b9-958a-81884134991c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013012749s
    STEP: Saw pod success 01/29/23 03:47:03.48
    Jan 29 03:47:03.480: INFO: Pod "pod-secrets-7a53f666-9aad-42b9-958a-81884134991c" satisfied condition "Succeeded or Failed"
    Jan 29 03:47:03.486: INFO: Trying to get logs from node slave2 pod pod-secrets-7a53f666-9aad-42b9-958a-81884134991c container secret-volume-test: <nil>
    STEP: delete the pod 01/29/23 03:47:03.501
    Jan 29 03:47:03.570: INFO: Waiting for pod pod-secrets-7a53f666-9aad-42b9-958a-81884134991c to disappear
    Jan 29 03:47:03.576: INFO: Pod pod-secrets-7a53f666-9aad-42b9-958a-81884134991c no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 29 03:47:03.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-302" for this suite. 01/29/23 03:47:03.585
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:47:03.595
Jan 29 03:47:03.595: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename disruption 01/29/23 03:47:03.596
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:47:03.622
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:47:03.628
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
STEP: creating the pdb 01/29/23 03:47:03.633
STEP: Waiting for the pdb to be processed 01/29/23 03:47:03.64
STEP: updating the pdb 01/29/23 03:47:05.654
STEP: Waiting for the pdb to be processed 01/29/23 03:47:05.667
STEP: patching the pdb 01/29/23 03:47:07.681
STEP: Waiting for the pdb to be processed 01/29/23 03:47:07.696
STEP: Waiting for the pdb to be deleted 01/29/23 03:47:09.721
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jan 29 03:47:09.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-5241" for this suite. 01/29/23 03:47:09.737
{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","completed":196,"skipped":3589,"failed":0}
------------------------------
• [SLOW TEST] [6.152 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:47:03.595
    Jan 29 03:47:03.595: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename disruption 01/29/23 03:47:03.596
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:47:03.622
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:47:03.628
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:107
    STEP: creating the pdb 01/29/23 03:47:03.633
    STEP: Waiting for the pdb to be processed 01/29/23 03:47:03.64
    STEP: updating the pdb 01/29/23 03:47:05.654
    STEP: Waiting for the pdb to be processed 01/29/23 03:47:05.667
    STEP: patching the pdb 01/29/23 03:47:07.681
    STEP: Waiting for the pdb to be processed 01/29/23 03:47:07.696
    STEP: Waiting for the pdb to be deleted 01/29/23 03:47:09.721
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jan 29 03:47:09.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-5241" for this suite. 01/29/23 03:47:09.737
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:47:09.748
Jan 29 03:47:09.748: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename container-probe 01/29/23 03:47:09.75
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:47:09.785
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:47:09.79
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
STEP: Creating pod liveness-a9481af9-6534-4a2d-b05f-4ab93332e6e7 in namespace container-probe-5893 01/29/23 03:47:09.795
Jan 29 03:47:09.813: INFO: Waiting up to 5m0s for pod "liveness-a9481af9-6534-4a2d-b05f-4ab93332e6e7" in namespace "container-probe-5893" to be "not pending"
Jan 29 03:47:09.818: INFO: Pod "liveness-a9481af9-6534-4a2d-b05f-4ab93332e6e7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.462018ms
Jan 29 03:47:11.826: INFO: Pod "liveness-a9481af9-6534-4a2d-b05f-4ab93332e6e7": Phase="Running", Reason="", readiness=true. Elapsed: 2.013062711s
Jan 29 03:47:11.826: INFO: Pod "liveness-a9481af9-6534-4a2d-b05f-4ab93332e6e7" satisfied condition "not pending"
Jan 29 03:47:11.826: INFO: Started pod liveness-a9481af9-6534-4a2d-b05f-4ab93332e6e7 in namespace container-probe-5893
STEP: checking the pod's current state and verifying that restartCount is present 01/29/23 03:47:11.826
Jan 29 03:47:11.834: INFO: Initial restart count of pod liveness-a9481af9-6534-4a2d-b05f-4ab93332e6e7 is 0
Jan 29 03:47:31.911: INFO: Restart count of pod container-probe-5893/liveness-a9481af9-6534-4a2d-b05f-4ab93332e6e7 is now 1 (20.077233332s elapsed)
Jan 29 03:47:51.998: INFO: Restart count of pod container-probe-5893/liveness-a9481af9-6534-4a2d-b05f-4ab93332e6e7 is now 2 (40.164178652s elapsed)
Jan 29 03:48:12.071: INFO: Restart count of pod container-probe-5893/liveness-a9481af9-6534-4a2d-b05f-4ab93332e6e7 is now 3 (1m0.237190475s elapsed)
Jan 29 03:48:32.141: INFO: Restart count of pod container-probe-5893/liveness-a9481af9-6534-4a2d-b05f-4ab93332e6e7 is now 4 (1m20.307477378s elapsed)
Jan 29 03:49:32.366: INFO: Restart count of pod container-probe-5893/liveness-a9481af9-6534-4a2d-b05f-4ab93332e6e7 is now 5 (2m20.53266779s elapsed)
STEP: deleting the pod 01/29/23 03:49:32.366
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan 29 03:49:32.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5893" for this suite. 01/29/23 03:49:32.476
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","completed":197,"skipped":3590,"failed":0}
------------------------------
• [SLOW TEST] [142.738 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:47:09.748
    Jan 29 03:47:09.748: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename container-probe 01/29/23 03:47:09.75
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:47:09.785
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:47:09.79
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:195
    STEP: Creating pod liveness-a9481af9-6534-4a2d-b05f-4ab93332e6e7 in namespace container-probe-5893 01/29/23 03:47:09.795
    Jan 29 03:47:09.813: INFO: Waiting up to 5m0s for pod "liveness-a9481af9-6534-4a2d-b05f-4ab93332e6e7" in namespace "container-probe-5893" to be "not pending"
    Jan 29 03:47:09.818: INFO: Pod "liveness-a9481af9-6534-4a2d-b05f-4ab93332e6e7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.462018ms
    Jan 29 03:47:11.826: INFO: Pod "liveness-a9481af9-6534-4a2d-b05f-4ab93332e6e7": Phase="Running", Reason="", readiness=true. Elapsed: 2.013062711s
    Jan 29 03:47:11.826: INFO: Pod "liveness-a9481af9-6534-4a2d-b05f-4ab93332e6e7" satisfied condition "not pending"
    Jan 29 03:47:11.826: INFO: Started pod liveness-a9481af9-6534-4a2d-b05f-4ab93332e6e7 in namespace container-probe-5893
    STEP: checking the pod's current state and verifying that restartCount is present 01/29/23 03:47:11.826
    Jan 29 03:47:11.834: INFO: Initial restart count of pod liveness-a9481af9-6534-4a2d-b05f-4ab93332e6e7 is 0
    Jan 29 03:47:31.911: INFO: Restart count of pod container-probe-5893/liveness-a9481af9-6534-4a2d-b05f-4ab93332e6e7 is now 1 (20.077233332s elapsed)
    Jan 29 03:47:51.998: INFO: Restart count of pod container-probe-5893/liveness-a9481af9-6534-4a2d-b05f-4ab93332e6e7 is now 2 (40.164178652s elapsed)
    Jan 29 03:48:12.071: INFO: Restart count of pod container-probe-5893/liveness-a9481af9-6534-4a2d-b05f-4ab93332e6e7 is now 3 (1m0.237190475s elapsed)
    Jan 29 03:48:32.141: INFO: Restart count of pod container-probe-5893/liveness-a9481af9-6534-4a2d-b05f-4ab93332e6e7 is now 4 (1m20.307477378s elapsed)
    Jan 29 03:49:32.366: INFO: Restart count of pod container-probe-5893/liveness-a9481af9-6534-4a2d-b05f-4ab93332e6e7 is now 5 (2m20.53266779s elapsed)
    STEP: deleting the pod 01/29/23 03:49:32.366
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan 29 03:49:32.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-5893" for this suite. 01/29/23 03:49:32.476
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:49:32.489
Jan 29 03:49:32.489: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename dns 01/29/23 03:49:32.49
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:49:32.531
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:49:32.537
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 01/29/23 03:49:32.543
Jan 29 03:49:32.560: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-6175  e52ccfc4-de1c-4c60-b01e-2367bc16fc96 5967248 0 2023-01-29 03:49:32 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dsxrs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dsxrs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 03:49:32.560: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-6175" to be "running and ready"
Jan 29 03:49:32.565: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 5.366738ms
Jan 29 03:49:32.566: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Jan 29 03:49:34.579: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.019163711s
Jan 29 03:49:34.579: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Jan 29 03:49:34.579: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 01/29/23 03:49:34.579
Jan 29 03:49:34.580: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-6175 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 29 03:49:34.580: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
Jan 29 03:49:34.581: INFO: ExecWithOptions: Clientset creation
Jan 29 03:49:34.581: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/dns-6175/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 01/29/23 03:49:34.747
Jan 29 03:49:34.747: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-6175 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 29 03:49:34.747: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
Jan 29 03:49:34.748: INFO: ExecWithOptions: Clientset creation
Jan 29 03:49:34.748: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/dns-6175/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 29 03:49:34.885: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan 29 03:49:34.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6175" for this suite. 01/29/23 03:49:34.966
{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","completed":198,"skipped":3617,"failed":0}
------------------------------
• [2.489 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:49:32.489
    Jan 29 03:49:32.489: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename dns 01/29/23 03:49:32.49
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:49:32.531
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:49:32.537
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 01/29/23 03:49:32.543
    Jan 29 03:49:32.560: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-6175  e52ccfc4-de1c-4c60-b01e-2367bc16fc96 5967248 0 2023-01-29 03:49:32 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dsxrs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dsxrs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 29 03:49:32.560: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-6175" to be "running and ready"
    Jan 29 03:49:32.565: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 5.366738ms
    Jan 29 03:49:32.566: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Jan 29 03:49:34.579: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.019163711s
    Jan 29 03:49:34.579: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Jan 29 03:49:34.579: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 01/29/23 03:49:34.579
    Jan 29 03:49:34.580: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-6175 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 29 03:49:34.580: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    Jan 29 03:49:34.581: INFO: ExecWithOptions: Clientset creation
    Jan 29 03:49:34.581: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/dns-6175/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 01/29/23 03:49:34.747
    Jan 29 03:49:34.747: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-6175 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 29 03:49:34.747: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    Jan 29 03:49:34.748: INFO: ExecWithOptions: Clientset creation
    Jan 29 03:49:34.748: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/dns-6175/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 29 03:49:34.885: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan 29 03:49:34.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-6175" for this suite. 01/29/23 03:49:34.966
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:49:34.979
Jan 29 03:49:34.979: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename cronjob 01/29/23 03:49:34.981
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:49:35.018
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:49:35.024
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 01/29/23 03:49:35.031
STEP: Ensuring a job is scheduled 01/29/23 03:49:35.046
STEP: Ensuring exactly one is scheduled 01/29/23 03:50:01.054
STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/29/23 03:50:01.061
STEP: Ensuring no more jobs are scheduled 01/29/23 03:50:01.068
STEP: Removing cronjob 01/29/23 03:55:01.083
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jan 29 03:55:01.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-3116" for this suite. 01/29/23 03:55:01.103
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","completed":199,"skipped":3623,"failed":0}
------------------------------
• [SLOW TEST] [326.134 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:49:34.979
    Jan 29 03:49:34.979: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename cronjob 01/29/23 03:49:34.981
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:49:35.018
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:49:35.024
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 01/29/23 03:49:35.031
    STEP: Ensuring a job is scheduled 01/29/23 03:49:35.046
    STEP: Ensuring exactly one is scheduled 01/29/23 03:50:01.054
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/29/23 03:50:01.061
    STEP: Ensuring no more jobs are scheduled 01/29/23 03:50:01.068
    STEP: Removing cronjob 01/29/23 03:55:01.083
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jan 29 03:55:01.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-3116" for this suite. 01/29/23 03:55:01.103
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:55:01.114
Jan 29 03:55:01.115: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename podtemplate 01/29/23 03:55:01.116
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:55:01.166
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:55:01.171
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Jan 29 03:55:01.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-429" for this suite. 01/29/23 03:55:01.237
{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","completed":200,"skipped":3630,"failed":0}
------------------------------
• [0.132 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:55:01.114
    Jan 29 03:55:01.115: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename podtemplate 01/29/23 03:55:01.116
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:55:01.166
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:55:01.171
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Jan 29 03:55:01.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-429" for this suite. 01/29/23 03:55:01.237
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:55:01.249
Jan 29 03:55:01.249: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename containers 01/29/23 03:55:01.25
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:55:01.285
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:55:01.291
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
STEP: Creating a pod to test override arguments 01/29/23 03:55:01.296
Jan 29 03:55:01.324: INFO: Waiting up to 5m0s for pod "client-containers-d76c91f9-33e8-4c3b-9d05-7718fc6cd1df" in namespace "containers-1251" to be "Succeeded or Failed"
Jan 29 03:55:01.331: INFO: Pod "client-containers-d76c91f9-33e8-4c3b-9d05-7718fc6cd1df": Phase="Pending", Reason="", readiness=false. Elapsed: 6.274804ms
Jan 29 03:55:03.339: INFO: Pod "client-containers-d76c91f9-33e8-4c3b-9d05-7718fc6cd1df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014868242s
Jan 29 03:55:05.338: INFO: Pod "client-containers-d76c91f9-33e8-4c3b-9d05-7718fc6cd1df": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013777112s
Jan 29 03:55:07.338: INFO: Pod "client-containers-d76c91f9-33e8-4c3b-9d05-7718fc6cd1df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013266766s
STEP: Saw pod success 01/29/23 03:55:07.338
Jan 29 03:55:07.338: INFO: Pod "client-containers-d76c91f9-33e8-4c3b-9d05-7718fc6cd1df" satisfied condition "Succeeded or Failed"
Jan 29 03:55:07.344: INFO: Trying to get logs from node slave2 pod client-containers-d76c91f9-33e8-4c3b-9d05-7718fc6cd1df container agnhost-container: <nil>
STEP: delete the pod 01/29/23 03:55:07.375
Jan 29 03:55:07.469: INFO: Waiting for pod client-containers-d76c91f9-33e8-4c3b-9d05-7718fc6cd1df to disappear
Jan 29 03:55:07.475: INFO: Pod client-containers-d76c91f9-33e8-4c3b-9d05-7718fc6cd1df no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Jan 29 03:55:07.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1251" for this suite. 01/29/23 03:55:07.483
{"msg":"PASSED [sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]","completed":201,"skipped":3656,"failed":0}
------------------------------
• [SLOW TEST] [6.245 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:55:01.249
    Jan 29 03:55:01.249: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename containers 01/29/23 03:55:01.25
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:55:01.285
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:55:01.291
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:58
    STEP: Creating a pod to test override arguments 01/29/23 03:55:01.296
    Jan 29 03:55:01.324: INFO: Waiting up to 5m0s for pod "client-containers-d76c91f9-33e8-4c3b-9d05-7718fc6cd1df" in namespace "containers-1251" to be "Succeeded or Failed"
    Jan 29 03:55:01.331: INFO: Pod "client-containers-d76c91f9-33e8-4c3b-9d05-7718fc6cd1df": Phase="Pending", Reason="", readiness=false. Elapsed: 6.274804ms
    Jan 29 03:55:03.339: INFO: Pod "client-containers-d76c91f9-33e8-4c3b-9d05-7718fc6cd1df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014868242s
    Jan 29 03:55:05.338: INFO: Pod "client-containers-d76c91f9-33e8-4c3b-9d05-7718fc6cd1df": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013777112s
    Jan 29 03:55:07.338: INFO: Pod "client-containers-d76c91f9-33e8-4c3b-9d05-7718fc6cd1df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013266766s
    STEP: Saw pod success 01/29/23 03:55:07.338
    Jan 29 03:55:07.338: INFO: Pod "client-containers-d76c91f9-33e8-4c3b-9d05-7718fc6cd1df" satisfied condition "Succeeded or Failed"
    Jan 29 03:55:07.344: INFO: Trying to get logs from node slave2 pod client-containers-d76c91f9-33e8-4c3b-9d05-7718fc6cd1df container agnhost-container: <nil>
    STEP: delete the pod 01/29/23 03:55:07.375
    Jan 29 03:55:07.469: INFO: Waiting for pod client-containers-d76c91f9-33e8-4c3b-9d05-7718fc6cd1df to disappear
    Jan 29 03:55:07.475: INFO: Pod client-containers-d76c91f9-33e8-4c3b-9d05-7718fc6cd1df no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Jan 29 03:55:07.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-1251" for this suite. 01/29/23 03:55:07.483
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:55:07.496
Jan 29 03:55:07.496: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename downward-api 01/29/23 03:55:07.498
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:55:07.528
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:55:07.533
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
STEP: Creating a pod to test downward API volume plugin 01/29/23 03:55:07.538
Jan 29 03:55:07.556: INFO: Waiting up to 5m0s for pod "downwardapi-volume-87bc8000-5d09-4b12-8505-1c2598e15559" in namespace "downward-api-5235" to be "Succeeded or Failed"
Jan 29 03:55:07.562: INFO: Pod "downwardapi-volume-87bc8000-5d09-4b12-8505-1c2598e15559": Phase="Pending", Reason="", readiness=false. Elapsed: 6.617447ms
Jan 29 03:55:09.570: INFO: Pod "downwardapi-volume-87bc8000-5d09-4b12-8505-1c2598e15559": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014288658s
Jan 29 03:55:11.571: INFO: Pod "downwardapi-volume-87bc8000-5d09-4b12-8505-1c2598e15559": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015264343s
Jan 29 03:55:13.571: INFO: Pod "downwardapi-volume-87bc8000-5d09-4b12-8505-1c2598e15559": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014688816s
STEP: Saw pod success 01/29/23 03:55:13.571
Jan 29 03:55:13.571: INFO: Pod "downwardapi-volume-87bc8000-5d09-4b12-8505-1c2598e15559" satisfied condition "Succeeded or Failed"
Jan 29 03:55:13.582: INFO: Trying to get logs from node slave2 pod downwardapi-volume-87bc8000-5d09-4b12-8505-1c2598e15559 container client-container: <nil>
STEP: delete the pod 01/29/23 03:55:13.601
Jan 29 03:55:13.684: INFO: Waiting for pod downwardapi-volume-87bc8000-5d09-4b12-8505-1c2598e15559 to disappear
Jan 29 03:55:13.690: INFO: Pod downwardapi-volume-87bc8000-5d09-4b12-8505-1c2598e15559 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 29 03:55:13.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5235" for this suite. 01/29/23 03:55:13.708
{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":202,"skipped":3683,"failed":0}
------------------------------
• [SLOW TEST] [6.221 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:55:07.496
    Jan 29 03:55:07.496: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename downward-api 01/29/23 03:55:07.498
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:55:07.528
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:55:07.533
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:83
    STEP: Creating a pod to test downward API volume plugin 01/29/23 03:55:07.538
    Jan 29 03:55:07.556: INFO: Waiting up to 5m0s for pod "downwardapi-volume-87bc8000-5d09-4b12-8505-1c2598e15559" in namespace "downward-api-5235" to be "Succeeded or Failed"
    Jan 29 03:55:07.562: INFO: Pod "downwardapi-volume-87bc8000-5d09-4b12-8505-1c2598e15559": Phase="Pending", Reason="", readiness=false. Elapsed: 6.617447ms
    Jan 29 03:55:09.570: INFO: Pod "downwardapi-volume-87bc8000-5d09-4b12-8505-1c2598e15559": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014288658s
    Jan 29 03:55:11.571: INFO: Pod "downwardapi-volume-87bc8000-5d09-4b12-8505-1c2598e15559": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015264343s
    Jan 29 03:55:13.571: INFO: Pod "downwardapi-volume-87bc8000-5d09-4b12-8505-1c2598e15559": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014688816s
    STEP: Saw pod success 01/29/23 03:55:13.571
    Jan 29 03:55:13.571: INFO: Pod "downwardapi-volume-87bc8000-5d09-4b12-8505-1c2598e15559" satisfied condition "Succeeded or Failed"
    Jan 29 03:55:13.582: INFO: Trying to get logs from node slave2 pod downwardapi-volume-87bc8000-5d09-4b12-8505-1c2598e15559 container client-container: <nil>
    STEP: delete the pod 01/29/23 03:55:13.601
    Jan 29 03:55:13.684: INFO: Waiting for pod downwardapi-volume-87bc8000-5d09-4b12-8505-1c2598e15559 to disappear
    Jan 29 03:55:13.690: INFO: Pod downwardapi-volume-87bc8000-5d09-4b12-8505-1c2598e15559 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 29 03:55:13.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-5235" for this suite. 01/29/23 03:55:13.708
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:55:13.718
Jan 29 03:55:13.719: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename secrets 01/29/23 03:55:13.72
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:55:13.751
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:55:13.757
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
STEP: Creating projection with secret that has name secret-emptykey-test-c7e569d5-69f0-4201-bb8c-727fcc495d03 01/29/23 03:55:13.767
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Jan 29 03:55:13.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5448" for this suite. 01/29/23 03:55:13.779
{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","completed":203,"skipped":3685,"failed":0}
------------------------------
• [0.075 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:55:13.718
    Jan 29 03:55:13.719: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename secrets 01/29/23 03:55:13.72
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:55:13.751
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:55:13.757
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:139
    STEP: Creating projection with secret that has name secret-emptykey-test-c7e569d5-69f0-4201-bb8c-727fcc495d03 01/29/23 03:55:13.767
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Jan 29 03:55:13.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-5448" for this suite. 01/29/23 03:55:13.779
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:55:13.795
Jan 29 03:55:13.795: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename webhook 01/29/23 03:55:13.796
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:55:13.826
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:55:13.833
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/29/23 03:55:13.86
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/29/23 03:55:15.234
STEP: Deploying the webhook pod 01/29/23 03:55:15.248
STEP: Wait for the deployment to be ready 01/29/23 03:55:15.268
Jan 29 03:55:15.279: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/29/23 03:55:17.297
STEP: Verifying the service has paired with the endpoint 01/29/23 03:55:17.313
Jan 29 03:55:18.314: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
Jan 29 03:55:18.320: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9644-crds.webhook.example.com via the AdmissionRegistration API 01/29/23 03:55:18.837
STEP: Creating a custom resource while v1 is storage version 01/29/23 03:55:18.861
STEP: Patching Custom Resource Definition to set v2 as storage 01/29/23 03:55:20.947
STEP: Patching the custom resource while v2 is storage version 01/29/23 03:55:20.976
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 29 03:55:21.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4699" for this suite. 01/29/23 03:55:21.585
STEP: Destroying namespace "webhook-4699-markers" for this suite. 01/29/23 03:55:21.595
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","completed":204,"skipped":3690,"failed":0}
------------------------------
• [SLOW TEST] [7.938 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:55:13.795
    Jan 29 03:55:13.795: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename webhook 01/29/23 03:55:13.796
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:55:13.826
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:55:13.833
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/29/23 03:55:13.86
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/29/23 03:55:15.234
    STEP: Deploying the webhook pod 01/29/23 03:55:15.248
    STEP: Wait for the deployment to be ready 01/29/23 03:55:15.268
    Jan 29 03:55:15.279: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/29/23 03:55:17.297
    STEP: Verifying the service has paired with the endpoint 01/29/23 03:55:17.313
    Jan 29 03:55:18.314: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:322
    Jan 29 03:55:18.320: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9644-crds.webhook.example.com via the AdmissionRegistration API 01/29/23 03:55:18.837
    STEP: Creating a custom resource while v1 is storage version 01/29/23 03:55:18.861
    STEP: Patching Custom Resource Definition to set v2 as storage 01/29/23 03:55:20.947
    STEP: Patching the custom resource while v2 is storage version 01/29/23 03:55:20.976
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 29 03:55:21.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-4699" for this suite. 01/29/23 03:55:21.585
    STEP: Destroying namespace "webhook-4699-markers" for this suite. 01/29/23 03:55:21.595
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:55:21.735
Jan 29 03:55:21.736: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename limitrange 01/29/23 03:55:21.737
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:55:21.799
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:55:21.807
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
STEP: Creating a LimitRange 01/29/23 03:55:21.813
STEP: Setting up watch 01/29/23 03:55:21.813
STEP: Submitting a LimitRange 01/29/23 03:55:21.922
STEP: Verifying LimitRange creation was observed 01/29/23 03:55:21.929
STEP: Fetching the LimitRange to ensure it has proper values 01/29/23 03:55:21.93
Jan 29 03:55:21.950: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jan 29 03:55:21.950: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 01/29/23 03:55:21.95
STEP: Ensuring Pod has resource requirements applied from LimitRange 01/29/23 03:55:21.969
Jan 29 03:55:21.990: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jan 29 03:55:21.990: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 01/29/23 03:55:21.99
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 01/29/23 03:55:22.023
Jan 29 03:55:22.045: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Jan 29 03:55:22.045: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 01/29/23 03:55:22.045
STEP: Failing to create a Pod with more than max resources 01/29/23 03:55:22.051
STEP: Updating a LimitRange 01/29/23 03:55:22.073
STEP: Verifying LimitRange updating is effective 01/29/23 03:55:22.086
STEP: Creating a Pod with less than former min resources 01/29/23 03:55:24.093
STEP: Failing to create a Pod with more than max resources 01/29/23 03:55:24.107
STEP: Deleting a LimitRange 01/29/23 03:55:24.116
STEP: Verifying the LimitRange was deleted 01/29/23 03:55:24.125
Jan 29 03:55:29.133: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 01/29/23 03:55:29.133
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:187
Jan 29 03:55:29.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-3615" for this suite. 01/29/23 03:55:29.16
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","completed":205,"skipped":3721,"failed":0}
------------------------------
• [SLOW TEST] [7.434 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:55:21.735
    Jan 29 03:55:21.736: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename limitrange 01/29/23 03:55:21.737
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:55:21.799
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:55:21.807
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:57
    STEP: Creating a LimitRange 01/29/23 03:55:21.813
    STEP: Setting up watch 01/29/23 03:55:21.813
    STEP: Submitting a LimitRange 01/29/23 03:55:21.922
    STEP: Verifying LimitRange creation was observed 01/29/23 03:55:21.929
    STEP: Fetching the LimitRange to ensure it has proper values 01/29/23 03:55:21.93
    Jan 29 03:55:21.950: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Jan 29 03:55:21.950: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 01/29/23 03:55:21.95
    STEP: Ensuring Pod has resource requirements applied from LimitRange 01/29/23 03:55:21.969
    Jan 29 03:55:21.990: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Jan 29 03:55:21.990: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 01/29/23 03:55:21.99
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 01/29/23 03:55:22.023
    Jan 29 03:55:22.045: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Jan 29 03:55:22.045: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 01/29/23 03:55:22.045
    STEP: Failing to create a Pod with more than max resources 01/29/23 03:55:22.051
    STEP: Updating a LimitRange 01/29/23 03:55:22.073
    STEP: Verifying LimitRange updating is effective 01/29/23 03:55:22.086
    STEP: Creating a Pod with less than former min resources 01/29/23 03:55:24.093
    STEP: Failing to create a Pod with more than max resources 01/29/23 03:55:24.107
    STEP: Deleting a LimitRange 01/29/23 03:55:24.116
    STEP: Verifying the LimitRange was deleted 01/29/23 03:55:24.125
    Jan 29 03:55:29.133: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 01/29/23 03:55:29.133
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:187
    Jan 29 03:55:29.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "limitrange-3615" for this suite. 01/29/23 03:55:29.16
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:55:29.17
Jan 29 03:55:29.170: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename dns 01/29/23 03:55:29.171
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:55:29.197
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:55:29.206
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 01/29/23 03:55:29.211
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7852.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-7852.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 01/29/23 03:55:29.218
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7852.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-7852.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 01/29/23 03:55:29.219
STEP: creating a pod to probe DNS 01/29/23 03:55:29.219
STEP: submitting the pod to kubernetes 01/29/23 03:55:29.219
Jan 29 03:55:29.239: INFO: Waiting up to 15m0s for pod "dns-test-1d9cc681-6021-4bec-8fd4-515d901f8d5f" in namespace "dns-7852" to be "running"
Jan 29 03:55:29.245: INFO: Pod "dns-test-1d9cc681-6021-4bec-8fd4-515d901f8d5f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.978562ms
Jan 29 03:55:31.251: INFO: Pod "dns-test-1d9cc681-6021-4bec-8fd4-515d901f8d5f": Phase="Running", Reason="", readiness=true. Elapsed: 2.012413085s
Jan 29 03:55:31.251: INFO: Pod "dns-test-1d9cc681-6021-4bec-8fd4-515d901f8d5f" satisfied condition "running"
STEP: retrieving the pod 01/29/23 03:55:31.251
STEP: looking for the results for each expected name from probers 01/29/23 03:55:31.257
Jan 29 03:55:31.283: INFO: DNS probes using dns-7852/dns-test-1d9cc681-6021-4bec-8fd4-515d901f8d5f succeeded

STEP: deleting the pod 01/29/23 03:55:31.284
STEP: deleting the test headless service 01/29/23 03:55:31.351
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan 29 03:55:31.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7852" for this suite. 01/29/23 03:55:31.382
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [Conformance]","completed":206,"skipped":3721,"failed":0}
------------------------------
• [2.225 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:55:29.17
    Jan 29 03:55:29.170: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename dns 01/29/23 03:55:29.171
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:55:29.197
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:55:29.206
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 01/29/23 03:55:29.211
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7852.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-7852.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     01/29/23 03:55:29.218
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7852.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-7852.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     01/29/23 03:55:29.219
    STEP: creating a pod to probe DNS 01/29/23 03:55:29.219
    STEP: submitting the pod to kubernetes 01/29/23 03:55:29.219
    Jan 29 03:55:29.239: INFO: Waiting up to 15m0s for pod "dns-test-1d9cc681-6021-4bec-8fd4-515d901f8d5f" in namespace "dns-7852" to be "running"
    Jan 29 03:55:29.245: INFO: Pod "dns-test-1d9cc681-6021-4bec-8fd4-515d901f8d5f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.978562ms
    Jan 29 03:55:31.251: INFO: Pod "dns-test-1d9cc681-6021-4bec-8fd4-515d901f8d5f": Phase="Running", Reason="", readiness=true. Elapsed: 2.012413085s
    Jan 29 03:55:31.251: INFO: Pod "dns-test-1d9cc681-6021-4bec-8fd4-515d901f8d5f" satisfied condition "running"
    STEP: retrieving the pod 01/29/23 03:55:31.251
    STEP: looking for the results for each expected name from probers 01/29/23 03:55:31.257
    Jan 29 03:55:31.283: INFO: DNS probes using dns-7852/dns-test-1d9cc681-6021-4bec-8fd4-515d901f8d5f succeeded

    STEP: deleting the pod 01/29/23 03:55:31.284
    STEP: deleting the test headless service 01/29/23 03:55:31.351
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan 29 03:55:31.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-7852" for this suite. 01/29/23 03:55:31.382
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:55:31.397
Jan 29 03:55:31.397: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename endpointslice 01/29/23 03:55:31.398
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:55:31.425
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:55:31.43
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
Jan 29 03:55:31.451: INFO: Endpoints addresses: [192.168.122.241 192.168.122.242 192.168.122.243] , ports: [6443]
Jan 29 03:55:31.452: INFO: EndpointSlices addresses: [192.168.122.241 192.168.122.242 192.168.122.243] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Jan 29 03:55:31.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-2640" for this suite. 01/29/23 03:55:31.46
{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","completed":207,"skipped":3748,"failed":0}
------------------------------
• [0.072 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:55:31.397
    Jan 29 03:55:31.397: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename endpointslice 01/29/23 03:55:31.398
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:55:31.425
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:55:31.43
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:65
    Jan 29 03:55:31.451: INFO: Endpoints addresses: [192.168.122.241 192.168.122.242 192.168.122.243] , ports: [6443]
    Jan 29 03:55:31.452: INFO: EndpointSlices addresses: [192.168.122.241 192.168.122.242 192.168.122.243] , ports: [6443]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Jan 29 03:55:31.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-2640" for this suite. 01/29/23 03:55:31.46
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:55:31.47
Jan 29 03:55:31.470: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename daemonsets 01/29/23 03:55:31.471
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:55:31.498
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:55:31.503
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
Jan 29 03:55:31.556: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 01/29/23 03:55:31.569
Jan 29 03:55:31.576: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 29 03:55:31.576: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 01/29/23 03:55:31.576
Jan 29 03:55:31.606: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 29 03:55:31.606: INFO: Node slave2 is running 0 daemon pod, expected 1
Jan 29 03:55:32.613: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 29 03:55:32.613: INFO: Node slave2 is running 0 daemon pod, expected 1
Jan 29 03:55:33.613: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 29 03:55:33.613: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 01/29/23 03:55:33.62
Jan 29 03:55:33.644: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 29 03:55:33.644: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Jan 29 03:55:34.652: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 29 03:55:34.652: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 01/29/23 03:55:34.652
Jan 29 03:55:34.680: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 29 03:55:34.680: INFO: Node slave2 is running 0 daemon pod, expected 1
Jan 29 03:55:35.687: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 29 03:55:35.687: INFO: Node slave2 is running 0 daemon pod, expected 1
Jan 29 03:55:36.704: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 29 03:55:36.704: INFO: Node slave2 is running 0 daemon pod, expected 1
Jan 29 03:55:37.687: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 29 03:55:37.687: INFO: Node slave2 is running 0 daemon pod, expected 1
Jan 29 03:55:38.687: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 29 03:55:38.687: INFO: Node slave2 is running 0 daemon pod, expected 1
Jan 29 03:55:39.687: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 29 03:55:39.687: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 01/29/23 03:55:39.699
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1113, will wait for the garbage collector to delete the pods 01/29/23 03:55:39.699
Jan 29 03:55:39.769: INFO: Deleting DaemonSet.extensions daemon-set took: 13.114672ms
Jan 29 03:55:39.869: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.511123ms
Jan 29 03:55:42.077: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 29 03:55:42.077: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 29 03:55:42.084: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"5969127"},"items":null}

Jan 29 03:55:42.089: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"5969127"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan 29 03:55:42.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1113" for this suite. 01/29/23 03:55:42.152
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","completed":208,"skipped":3750,"failed":0}
------------------------------
• [SLOW TEST] [10.691 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:55:31.47
    Jan 29 03:55:31.470: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename daemonsets 01/29/23 03:55:31.471
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:55:31.498
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:55:31.503
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:193
    Jan 29 03:55:31.556: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 01/29/23 03:55:31.569
    Jan 29 03:55:31.576: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 29 03:55:31.576: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 01/29/23 03:55:31.576
    Jan 29 03:55:31.606: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 29 03:55:31.606: INFO: Node slave2 is running 0 daemon pod, expected 1
    Jan 29 03:55:32.613: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 29 03:55:32.613: INFO: Node slave2 is running 0 daemon pod, expected 1
    Jan 29 03:55:33.613: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 29 03:55:33.613: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 01/29/23 03:55:33.62
    Jan 29 03:55:33.644: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 29 03:55:33.644: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
    Jan 29 03:55:34.652: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 29 03:55:34.652: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 01/29/23 03:55:34.652
    Jan 29 03:55:34.680: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 29 03:55:34.680: INFO: Node slave2 is running 0 daemon pod, expected 1
    Jan 29 03:55:35.687: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 29 03:55:35.687: INFO: Node slave2 is running 0 daemon pod, expected 1
    Jan 29 03:55:36.704: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 29 03:55:36.704: INFO: Node slave2 is running 0 daemon pod, expected 1
    Jan 29 03:55:37.687: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 29 03:55:37.687: INFO: Node slave2 is running 0 daemon pod, expected 1
    Jan 29 03:55:38.687: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 29 03:55:38.687: INFO: Node slave2 is running 0 daemon pod, expected 1
    Jan 29 03:55:39.687: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 29 03:55:39.687: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 01/29/23 03:55:39.699
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1113, will wait for the garbage collector to delete the pods 01/29/23 03:55:39.699
    Jan 29 03:55:39.769: INFO: Deleting DaemonSet.extensions daemon-set took: 13.114672ms
    Jan 29 03:55:39.869: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.511123ms
    Jan 29 03:55:42.077: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 29 03:55:42.077: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 29 03:55:42.084: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"5969127"},"items":null}

    Jan 29 03:55:42.089: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"5969127"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan 29 03:55:42.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-1113" for this suite. 01/29/23 03:55:42.152
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:55:42.163
Jan 29 03:55:42.164: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename deployment 01/29/23 03:55:42.165
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:55:42.193
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:55:42.199
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Jan 29 03:55:42.222: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jan 29 03:55:47.230: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/29/23 03:55:47.23
Jan 29 03:55:47.230: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 01/29/23 03:55:47.256
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 29 03:55:47.277: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-9632  5fb1465f-cfaf-4f90-9bdc-04c2c11f53ac 5969174 1 2023-01-29 03:55:47 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated] [] [] []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0x4005fcc098 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Jan 29 03:55:47.287: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Jan 29 03:55:47.287: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Jan 29 03:55:47.288: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-9632  43793281-3379-411e-be5a-a32492f363e8 5969175 1 2023-01-29 03:55:42 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated] [{apps/v1 Deployment test-cleanup-deployment 5fb1465f-cfaf-4f90-9bdc-04c2c11f53ac 0x4003f6cb5e 0x4003f6cb5f}] [] []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0x4003f6cbc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 29 03:55:47.297: INFO: Pod "test-cleanup-controller-2rb65" is available:
&Pod{ObjectMeta:{test-cleanup-controller-2rb65 test-cleanup-controller- deployment-9632  016eafde-3f28-4045-adc2-fc4211c5ee47 5969156 0 2023-01-29 03:55:42 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "100.101.49.72"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "100.101.49.72"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet test-cleanup-controller 43793281-3379-411e-be5a-a32492f363e8 0x4005c70f17 0x4005c70f18}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8p8ng,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8p8ng,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:55:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:55:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:55:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:55:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.245,PodIP:100.101.49.72,StartTime:2023-01-29 03:55:42 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-29 03:55:43 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry-jinan-lab.inspurcloud.cn/library/cke/httpd-arm64:2.4.38-2,ImageID:docker-pullable://registry-jinan-lab.inspurcloud.cn/library/cke/httpd-arm64@sha256:a4f0279e65f8d239ea154d08f0572c5419f43e4ed748bee8a1a1b762d0d36893,ContainerID:docker://5c81d88b2dd64b19aa9da495684c4227248b1b0c14837c52783681fab4e297b7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.101.49.72,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan 29 03:55:47.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9632" for this suite. 01/29/23 03:55:47.309
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","completed":209,"skipped":3783,"failed":0}
------------------------------
• [SLOW TEST] [5.168 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:55:42.163
    Jan 29 03:55:42.164: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename deployment 01/29/23 03:55:42.165
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:55:42.193
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:55:42.199
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Jan 29 03:55:42.222: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Jan 29 03:55:47.230: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/29/23 03:55:47.23
    Jan 29 03:55:47.230: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 01/29/23 03:55:47.256
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 29 03:55:47.277: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-9632  5fb1465f-cfaf-4f90-9bdc-04c2c11f53ac 5969174 1 2023-01-29 03:55:47 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated] [] [] []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0x4005fcc098 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

    Jan 29 03:55:47.287: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
    Jan 29 03:55:47.287: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
    Jan 29 03:55:47.288: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-9632  43793281-3379-411e-be5a-a32492f363e8 5969175 1 2023-01-29 03:55:42 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated] [{apps/v1 Deployment test-cleanup-deployment 5fb1465f-cfaf-4f90-9bdc-04c2c11f53ac 0x4003f6cb5e 0x4003f6cb5f}] [] []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0x4003f6cbc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 29 03:55:47.297: INFO: Pod "test-cleanup-controller-2rb65" is available:
    &Pod{ObjectMeta:{test-cleanup-controller-2rb65 test-cleanup-controller- deployment-9632  016eafde-3f28-4045-adc2-fc4211c5ee47 5969156 0 2023-01-29 03:55:42 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "",
        "ips": [
            "100.101.49.72"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "",
        "ips": [
            "100.101.49.72"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet test-cleanup-controller 43793281-3379-411e-be5a-a32492f363e8 0x4005c70f17 0x4005c70f18}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8p8ng,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8p8ng,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:55:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:55:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:55:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 03:55:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.245,PodIP:100.101.49.72,StartTime:2023-01-29 03:55:42 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-29 03:55:43 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry-jinan-lab.inspurcloud.cn/library/cke/httpd-arm64:2.4.38-2,ImageID:docker-pullable://registry-jinan-lab.inspurcloud.cn/library/cke/httpd-arm64@sha256:a4f0279e65f8d239ea154d08f0572c5419f43e4ed748bee8a1a1b762d0d36893,ContainerID:docker://5c81d88b2dd64b19aa9da495684c4227248b1b0c14837c52783681fab4e297b7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.101.49.72,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan 29 03:55:47.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-9632" for this suite. 01/29/23 03:55:47.309
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:55:47.332
Jan 29 03:55:47.333: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename crd-publish-openapi 01/29/23 03:55:47.334
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:55:47.382
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:55:47.388
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 01/29/23 03:55:47.397
Jan 29 03:55:47.398: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 01/29/23 03:56:10.246
Jan 29 03:56:10.247: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
Jan 29 03:56:16.945: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 29 03:56:38.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9481" for this suite. 01/29/23 03:56:38.415
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","completed":210,"skipped":3785,"failed":0}
------------------------------
• [SLOW TEST] [51.092 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:55:47.332
    Jan 29 03:55:47.333: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename crd-publish-openapi 01/29/23 03:55:47.334
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:55:47.382
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:55:47.388
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:308
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 01/29/23 03:55:47.397
    Jan 29 03:55:47.398: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 01/29/23 03:56:10.246
    Jan 29 03:56:10.247: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    Jan 29 03:56:16.945: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 29 03:56:38.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-9481" for this suite. 01/29/23 03:56:38.415
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:56:38.426
Jan 29 03:56:38.426: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename var-expansion 01/29/23 03:56:38.427
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:56:38.455
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:56:38.463
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
STEP: Creating a pod to test substitution in container's command 01/29/23 03:56:38.468
Jan 29 03:56:38.488: INFO: Waiting up to 5m0s for pod "var-expansion-702ea4c7-9646-45e3-8d52-89f69e23c816" in namespace "var-expansion-3168" to be "Succeeded or Failed"
Jan 29 03:56:38.494: INFO: Pod "var-expansion-702ea4c7-9646-45e3-8d52-89f69e23c816": Phase="Pending", Reason="", readiness=false. Elapsed: 6.128543ms
Jan 29 03:56:40.502: INFO: Pod "var-expansion-702ea4c7-9646-45e3-8d52-89f69e23c816": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014034596s
Jan 29 03:56:42.503: INFO: Pod "var-expansion-702ea4c7-9646-45e3-8d52-89f69e23c816": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014716658s
STEP: Saw pod success 01/29/23 03:56:42.503
Jan 29 03:56:42.503: INFO: Pod "var-expansion-702ea4c7-9646-45e3-8d52-89f69e23c816" satisfied condition "Succeeded or Failed"
Jan 29 03:56:42.509: INFO: Trying to get logs from node slave2 pod var-expansion-702ea4c7-9646-45e3-8d52-89f69e23c816 container dapi-container: <nil>
STEP: delete the pod 01/29/23 03:56:42.525
Jan 29 03:56:42.588: INFO: Waiting for pod var-expansion-702ea4c7-9646-45e3-8d52-89f69e23c816 to disappear
Jan 29 03:56:42.593: INFO: Pod var-expansion-702ea4c7-9646-45e3-8d52-89f69e23c816 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan 29 03:56:42.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3168" for this suite. 01/29/23 03:56:42.601
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","completed":211,"skipped":3814,"failed":0}
------------------------------
• [4.184 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:56:38.426
    Jan 29 03:56:38.426: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename var-expansion 01/29/23 03:56:38.427
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:56:38.455
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:56:38.463
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:72
    STEP: Creating a pod to test substitution in container's command 01/29/23 03:56:38.468
    Jan 29 03:56:38.488: INFO: Waiting up to 5m0s for pod "var-expansion-702ea4c7-9646-45e3-8d52-89f69e23c816" in namespace "var-expansion-3168" to be "Succeeded or Failed"
    Jan 29 03:56:38.494: INFO: Pod "var-expansion-702ea4c7-9646-45e3-8d52-89f69e23c816": Phase="Pending", Reason="", readiness=false. Elapsed: 6.128543ms
    Jan 29 03:56:40.502: INFO: Pod "var-expansion-702ea4c7-9646-45e3-8d52-89f69e23c816": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014034596s
    Jan 29 03:56:42.503: INFO: Pod "var-expansion-702ea4c7-9646-45e3-8d52-89f69e23c816": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014716658s
    STEP: Saw pod success 01/29/23 03:56:42.503
    Jan 29 03:56:42.503: INFO: Pod "var-expansion-702ea4c7-9646-45e3-8d52-89f69e23c816" satisfied condition "Succeeded or Failed"
    Jan 29 03:56:42.509: INFO: Trying to get logs from node slave2 pod var-expansion-702ea4c7-9646-45e3-8d52-89f69e23c816 container dapi-container: <nil>
    STEP: delete the pod 01/29/23 03:56:42.525
    Jan 29 03:56:42.588: INFO: Waiting for pod var-expansion-702ea4c7-9646-45e3-8d52-89f69e23c816 to disappear
    Jan 29 03:56:42.593: INFO: Pod var-expansion-702ea4c7-9646-45e3-8d52-89f69e23c816 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan 29 03:56:42.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-3168" for this suite. 01/29/23 03:56:42.601
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:56:42.611
Jan 29 03:56:42.611: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename kubelet-test 01/29/23 03:56:42.612
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:56:42.64
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:56:42.646
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jan 29 03:56:46.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4909" for this suite. 01/29/23 03:56:46.694
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","completed":212,"skipped":3814,"failed":0}
------------------------------
• [4.094 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:56:42.611
    Jan 29 03:56:42.611: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename kubelet-test 01/29/23 03:56:42.612
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:56:42.64
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:56:42.646
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jan 29 03:56:46.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-4909" for this suite. 01/29/23 03:56:46.694
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:56:46.705
Jan 29 03:56:46.705: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename replicaset 01/29/23 03:56:46.707
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:56:46.737
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:56:46.744
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 01/29/23 03:56:46.751
Jan 29 03:56:46.773: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 29 03:56:51.781: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/29/23 03:56:51.781
STEP: getting scale subresource 01/29/23 03:56:51.781
STEP: updating a scale subresource 01/29/23 03:56:51.787
STEP: verifying the replicaset Spec.Replicas was modified 01/29/23 03:56:51.794
STEP: Patch a scale subresource 01/29/23 03:56:51.8
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jan 29 03:56:51.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-7663" for this suite. 01/29/23 03:56:51.829
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","completed":213,"skipped":3817,"failed":0}
------------------------------
• [SLOW TEST] [5.138 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:56:46.705
    Jan 29 03:56:46.705: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename replicaset 01/29/23 03:56:46.707
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:56:46.737
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:56:46.744
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 01/29/23 03:56:46.751
    Jan 29 03:56:46.773: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan 29 03:56:51.781: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/29/23 03:56:51.781
    STEP: getting scale subresource 01/29/23 03:56:51.781
    STEP: updating a scale subresource 01/29/23 03:56:51.787
    STEP: verifying the replicaset Spec.Replicas was modified 01/29/23 03:56:51.794
    STEP: Patch a scale subresource 01/29/23 03:56:51.8
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jan 29 03:56:51.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-7663" for this suite. 01/29/23 03:56:51.829
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:56:51.844
Jan 29 03:56:51.844: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename ingressclass 01/29/23 03:56:51.845
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:56:51.876
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:56:51.885
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 01/29/23 03:56:51.891
STEP: getting /apis/networking.k8s.io 01/29/23 03:56:51.895
STEP: getting /apis/networking.k8s.iov1 01/29/23 03:56:51.897
STEP: creating 01/29/23 03:56:51.899
STEP: getting 01/29/23 03:56:51.923
STEP: listing 01/29/23 03:56:51.929
STEP: watching 01/29/23 03:56:51.935
Jan 29 03:56:51.935: INFO: starting watch
STEP: patching 01/29/23 03:56:51.937
STEP: updating 01/29/23 03:56:51.944
Jan 29 03:56:51.952: INFO: waiting for watch events with expected annotations
Jan 29 03:56:51.952: INFO: saw patched and updated annotations
STEP: deleting 01/29/23 03:56:51.953
STEP: deleting a collection 01/29/23 03:56:51.974
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:187
Jan 29 03:56:52.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-3713" for this suite. 01/29/23 03:56:52.01
{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","completed":214,"skipped":3819,"failed":0}
------------------------------
• [0.177 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:56:51.844
    Jan 29 03:56:51.844: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename ingressclass 01/29/23 03:56:51.845
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:56:51.876
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:56:51.885
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 01/29/23 03:56:51.891
    STEP: getting /apis/networking.k8s.io 01/29/23 03:56:51.895
    STEP: getting /apis/networking.k8s.iov1 01/29/23 03:56:51.897
    STEP: creating 01/29/23 03:56:51.899
    STEP: getting 01/29/23 03:56:51.923
    STEP: listing 01/29/23 03:56:51.929
    STEP: watching 01/29/23 03:56:51.935
    Jan 29 03:56:51.935: INFO: starting watch
    STEP: patching 01/29/23 03:56:51.937
    STEP: updating 01/29/23 03:56:51.944
    Jan 29 03:56:51.952: INFO: waiting for watch events with expected annotations
    Jan 29 03:56:51.952: INFO: saw patched and updated annotations
    STEP: deleting 01/29/23 03:56:51.953
    STEP: deleting a collection 01/29/23 03:56:51.974
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:187
    Jan 29 03:56:52.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingressclass-3713" for this suite. 01/29/23 03:56:52.01
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:56:52.021
Jan 29 03:56:52.022: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename projected 01/29/23 03:56:52.023
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:56:52.051
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:56:52.057
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
STEP: Creating configMap with name projected-configmap-test-volume-map-a14c4f76-092a-495c-b885-7c4d7202121f 01/29/23 03:56:52.063
STEP: Creating a pod to test consume configMaps 01/29/23 03:56:52.071
Jan 29 03:56:52.090: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1b68b798-8fde-41f4-a78e-1c2f7b8d40d0" in namespace "projected-4885" to be "Succeeded or Failed"
Jan 29 03:56:52.096: INFO: Pod "pod-projected-configmaps-1b68b798-8fde-41f4-a78e-1c2f7b8d40d0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.593839ms
Jan 29 03:56:54.103: INFO: Pod "pod-projected-configmaps-1b68b798-8fde-41f4-a78e-1c2f7b8d40d0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011983801s
Jan 29 03:56:56.104: INFO: Pod "pod-projected-configmaps-1b68b798-8fde-41f4-a78e-1c2f7b8d40d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013938212s
STEP: Saw pod success 01/29/23 03:56:56.105
Jan 29 03:56:56.105: INFO: Pod "pod-projected-configmaps-1b68b798-8fde-41f4-a78e-1c2f7b8d40d0" satisfied condition "Succeeded or Failed"
Jan 29 03:56:56.110: INFO: Trying to get logs from node slave2 pod pod-projected-configmaps-1b68b798-8fde-41f4-a78e-1c2f7b8d40d0 container agnhost-container: <nil>
STEP: delete the pod 01/29/23 03:56:56.124
Jan 29 03:56:56.220: INFO: Waiting for pod pod-projected-configmaps-1b68b798-8fde-41f4-a78e-1c2f7b8d40d0 to disappear
Jan 29 03:56:56.226: INFO: Pod pod-projected-configmaps-1b68b798-8fde-41f4-a78e-1c2f7b8d40d0 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 29 03:56:56.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4885" for this suite. 01/29/23 03:56:56.24
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":215,"skipped":3824,"failed":0}
------------------------------
• [4.230 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:56:52.021
    Jan 29 03:56:52.022: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename projected 01/29/23 03:56:52.023
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:56:52.051
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:56:52.057
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:98
    STEP: Creating configMap with name projected-configmap-test-volume-map-a14c4f76-092a-495c-b885-7c4d7202121f 01/29/23 03:56:52.063
    STEP: Creating a pod to test consume configMaps 01/29/23 03:56:52.071
    Jan 29 03:56:52.090: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1b68b798-8fde-41f4-a78e-1c2f7b8d40d0" in namespace "projected-4885" to be "Succeeded or Failed"
    Jan 29 03:56:52.096: INFO: Pod "pod-projected-configmaps-1b68b798-8fde-41f4-a78e-1c2f7b8d40d0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.593839ms
    Jan 29 03:56:54.103: INFO: Pod "pod-projected-configmaps-1b68b798-8fde-41f4-a78e-1c2f7b8d40d0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011983801s
    Jan 29 03:56:56.104: INFO: Pod "pod-projected-configmaps-1b68b798-8fde-41f4-a78e-1c2f7b8d40d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013938212s
    STEP: Saw pod success 01/29/23 03:56:56.105
    Jan 29 03:56:56.105: INFO: Pod "pod-projected-configmaps-1b68b798-8fde-41f4-a78e-1c2f7b8d40d0" satisfied condition "Succeeded or Failed"
    Jan 29 03:56:56.110: INFO: Trying to get logs from node slave2 pod pod-projected-configmaps-1b68b798-8fde-41f4-a78e-1c2f7b8d40d0 container agnhost-container: <nil>
    STEP: delete the pod 01/29/23 03:56:56.124
    Jan 29 03:56:56.220: INFO: Waiting for pod pod-projected-configmaps-1b68b798-8fde-41f4-a78e-1c2f7b8d40d0 to disappear
    Jan 29 03:56:56.226: INFO: Pod pod-projected-configmaps-1b68b798-8fde-41f4-a78e-1c2f7b8d40d0 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 29 03:56:56.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4885" for this suite. 01/29/23 03:56:56.24
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:56:56.253
Jan 29 03:56:56.254: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename replicaset 01/29/23 03:56:56.255
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:56:56.282
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:56:56.288
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 01/29/23 03:56:56.293
STEP: Verify that the required pods have come up 01/29/23 03:56:56.304
Jan 29 03:56:56.310: INFO: Pod name sample-pod: Found 0 pods out of 3
Jan 29 03:57:01.322: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 01/29/23 03:57:01.322
Jan 29 03:57:01.328: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 01/29/23 03:57:01.328
STEP: DeleteCollection of the ReplicaSets 01/29/23 03:57:01.335
STEP: After DeleteCollection verify that ReplicaSets have been deleted 01/29/23 03:57:01.348
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jan 29 03:57:01.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6789" for this suite. 01/29/23 03:57:01.366
{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","completed":216,"skipped":3851,"failed":0}
------------------------------
• [SLOW TEST] [5.129 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:56:56.253
    Jan 29 03:56:56.254: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename replicaset 01/29/23 03:56:56.255
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:56:56.282
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:56:56.288
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 01/29/23 03:56:56.293
    STEP: Verify that the required pods have come up 01/29/23 03:56:56.304
    Jan 29 03:56:56.310: INFO: Pod name sample-pod: Found 0 pods out of 3
    Jan 29 03:57:01.322: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 01/29/23 03:57:01.322
    Jan 29 03:57:01.328: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 01/29/23 03:57:01.328
    STEP: DeleteCollection of the ReplicaSets 01/29/23 03:57:01.335
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 01/29/23 03:57:01.348
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jan 29 03:57:01.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-6789" for this suite. 01/29/23 03:57:01.366
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:57:01.383
Jan 29 03:57:01.383: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename resourcequota 01/29/23 03:57:01.385
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:57:01.427
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:57:01.435
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
STEP: Counting existing ResourceQuota 01/29/23 03:57:01.441
STEP: Creating a ResourceQuota 01/29/23 03:57:06.449
STEP: Ensuring resource quota status is calculated 01/29/23 03:57:06.457
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 29 03:57:08.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8258" for this suite. 01/29/23 03:57:08.477
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","completed":217,"skipped":3852,"failed":0}
------------------------------
• [SLOW TEST] [7.102 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:57:01.383
    Jan 29 03:57:01.383: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename resourcequota 01/29/23 03:57:01.385
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:57:01.427
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:57:01.435
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:65
    STEP: Counting existing ResourceQuota 01/29/23 03:57:01.441
    STEP: Creating a ResourceQuota 01/29/23 03:57:06.449
    STEP: Ensuring resource quota status is calculated 01/29/23 03:57:06.457
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 29 03:57:08.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-8258" for this suite. 01/29/23 03:57:08.477
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:57:08.486
Jan 29 03:57:08.486: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename proxy 01/29/23 03:57:08.488
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:57:08.518
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:57:08.523
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Jan 29 03:57:08.528: INFO: Creating pod...
Jan 29 03:57:08.543: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-6672" to be "running"
Jan 29 03:57:08.549: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 5.974482ms
Jan 29 03:57:10.558: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.014143356s
Jan 29 03:57:10.558: INFO: Pod "agnhost" satisfied condition "running"
Jan 29 03:57:10.558: INFO: Creating service...
Jan 29 03:57:10.577: INFO: Starting http.Client for https://100.105.0.1:443/api/v1/namespaces/proxy-6672/pods/agnhost/proxy?method=DELETE
Jan 29 03:57:10.589: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 29 03:57:10.589: INFO: Starting http.Client for https://100.105.0.1:443/api/v1/namespaces/proxy-6672/pods/agnhost/proxy?method=OPTIONS
Jan 29 03:57:10.597: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 29 03:57:10.597: INFO: Starting http.Client for https://100.105.0.1:443/api/v1/namespaces/proxy-6672/pods/agnhost/proxy?method=PATCH
Jan 29 03:57:10.606: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 29 03:57:10.606: INFO: Starting http.Client for https://100.105.0.1:443/api/v1/namespaces/proxy-6672/pods/agnhost/proxy?method=POST
Jan 29 03:57:10.616: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 29 03:57:10.616: INFO: Starting http.Client for https://100.105.0.1:443/api/v1/namespaces/proxy-6672/pods/agnhost/proxy?method=PUT
Jan 29 03:57:10.624: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan 29 03:57:10.624: INFO: Starting http.Client for https://100.105.0.1:443/api/v1/namespaces/proxy-6672/services/e2e-proxy-test-service/proxy?method=DELETE
Jan 29 03:57:10.638: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 29 03:57:10.638: INFO: Starting http.Client for https://100.105.0.1:443/api/v1/namespaces/proxy-6672/services/e2e-proxy-test-service/proxy?method=OPTIONS
Jan 29 03:57:10.649: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 29 03:57:10.649: INFO: Starting http.Client for https://100.105.0.1:443/api/v1/namespaces/proxy-6672/services/e2e-proxy-test-service/proxy?method=PATCH
Jan 29 03:57:10.661: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 29 03:57:10.661: INFO: Starting http.Client for https://100.105.0.1:443/api/v1/namespaces/proxy-6672/services/e2e-proxy-test-service/proxy?method=POST
Jan 29 03:57:10.677: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 29 03:57:10.677: INFO: Starting http.Client for https://100.105.0.1:443/api/v1/namespaces/proxy-6672/services/e2e-proxy-test-service/proxy?method=PUT
Jan 29 03:57:10.688: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan 29 03:57:10.688: INFO: Starting http.Client for https://100.105.0.1:443/api/v1/namespaces/proxy-6672/pods/agnhost/proxy?method=GET
Jan 29 03:57:10.695: INFO: http.Client request:GET StatusCode:301
Jan 29 03:57:10.695: INFO: Starting http.Client for https://100.105.0.1:443/api/v1/namespaces/proxy-6672/services/e2e-proxy-test-service/proxy?method=GET
Jan 29 03:57:10.704: INFO: http.Client request:GET StatusCode:301
Jan 29 03:57:10.704: INFO: Starting http.Client for https://100.105.0.1:443/api/v1/namespaces/proxy-6672/pods/agnhost/proxy?method=HEAD
Jan 29 03:57:10.710: INFO: http.Client request:HEAD StatusCode:301
Jan 29 03:57:10.710: INFO: Starting http.Client for https://100.105.0.1:443/api/v1/namespaces/proxy-6672/services/e2e-proxy-test-service/proxy?method=HEAD
Jan 29 03:57:10.720: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Jan 29 03:57:10.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-6672" for this suite. 01/29/23 03:57:10.731
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]","completed":218,"skipped":3852,"failed":0}
------------------------------
• [2.257 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:57:08.486
    Jan 29 03:57:08.486: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename proxy 01/29/23 03:57:08.488
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:57:08.518
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:57:08.523
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Jan 29 03:57:08.528: INFO: Creating pod...
    Jan 29 03:57:08.543: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-6672" to be "running"
    Jan 29 03:57:08.549: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 5.974482ms
    Jan 29 03:57:10.558: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.014143356s
    Jan 29 03:57:10.558: INFO: Pod "agnhost" satisfied condition "running"
    Jan 29 03:57:10.558: INFO: Creating service...
    Jan 29 03:57:10.577: INFO: Starting http.Client for https://100.105.0.1:443/api/v1/namespaces/proxy-6672/pods/agnhost/proxy?method=DELETE
    Jan 29 03:57:10.589: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan 29 03:57:10.589: INFO: Starting http.Client for https://100.105.0.1:443/api/v1/namespaces/proxy-6672/pods/agnhost/proxy?method=OPTIONS
    Jan 29 03:57:10.597: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan 29 03:57:10.597: INFO: Starting http.Client for https://100.105.0.1:443/api/v1/namespaces/proxy-6672/pods/agnhost/proxy?method=PATCH
    Jan 29 03:57:10.606: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan 29 03:57:10.606: INFO: Starting http.Client for https://100.105.0.1:443/api/v1/namespaces/proxy-6672/pods/agnhost/proxy?method=POST
    Jan 29 03:57:10.616: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan 29 03:57:10.616: INFO: Starting http.Client for https://100.105.0.1:443/api/v1/namespaces/proxy-6672/pods/agnhost/proxy?method=PUT
    Jan 29 03:57:10.624: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jan 29 03:57:10.624: INFO: Starting http.Client for https://100.105.0.1:443/api/v1/namespaces/proxy-6672/services/e2e-proxy-test-service/proxy?method=DELETE
    Jan 29 03:57:10.638: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan 29 03:57:10.638: INFO: Starting http.Client for https://100.105.0.1:443/api/v1/namespaces/proxy-6672/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Jan 29 03:57:10.649: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan 29 03:57:10.649: INFO: Starting http.Client for https://100.105.0.1:443/api/v1/namespaces/proxy-6672/services/e2e-proxy-test-service/proxy?method=PATCH
    Jan 29 03:57:10.661: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan 29 03:57:10.661: INFO: Starting http.Client for https://100.105.0.1:443/api/v1/namespaces/proxy-6672/services/e2e-proxy-test-service/proxy?method=POST
    Jan 29 03:57:10.677: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan 29 03:57:10.677: INFO: Starting http.Client for https://100.105.0.1:443/api/v1/namespaces/proxy-6672/services/e2e-proxy-test-service/proxy?method=PUT
    Jan 29 03:57:10.688: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jan 29 03:57:10.688: INFO: Starting http.Client for https://100.105.0.1:443/api/v1/namespaces/proxy-6672/pods/agnhost/proxy?method=GET
    Jan 29 03:57:10.695: INFO: http.Client request:GET StatusCode:301
    Jan 29 03:57:10.695: INFO: Starting http.Client for https://100.105.0.1:443/api/v1/namespaces/proxy-6672/services/e2e-proxy-test-service/proxy?method=GET
    Jan 29 03:57:10.704: INFO: http.Client request:GET StatusCode:301
    Jan 29 03:57:10.704: INFO: Starting http.Client for https://100.105.0.1:443/api/v1/namespaces/proxy-6672/pods/agnhost/proxy?method=HEAD
    Jan 29 03:57:10.710: INFO: http.Client request:HEAD StatusCode:301
    Jan 29 03:57:10.710: INFO: Starting http.Client for https://100.105.0.1:443/api/v1/namespaces/proxy-6672/services/e2e-proxy-test-service/proxy?method=HEAD
    Jan 29 03:57:10.720: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Jan 29 03:57:10.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-6672" for this suite. 01/29/23 03:57:10.731
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:57:10.745
Jan 29 03:57:10.745: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename podtemplate 01/29/23 03:57:10.746
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:57:10.777
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:57:10.783
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 01/29/23 03:57:10.788
STEP: Replace a pod template 01/29/23 03:57:10.796
Jan 29 03:57:10.809: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Jan 29 03:57:10.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-9062" for this suite. 01/29/23 03:57:10.817
{"msg":"PASSED [sig-node] PodTemplates should replace a pod template [Conformance]","completed":219,"skipped":3863,"failed":0}
------------------------------
• [0.082 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:57:10.745
    Jan 29 03:57:10.745: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename podtemplate 01/29/23 03:57:10.746
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:57:10.777
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:57:10.783
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 01/29/23 03:57:10.788
    STEP: Replace a pod template 01/29/23 03:57:10.796
    Jan 29 03:57:10.809: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Jan 29 03:57:10.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-9062" for this suite. 01/29/23 03:57:10.817
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:57:10.828
Jan 29 03:57:10.828: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename endpointslice 01/29/23 03:57:10.829
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:57:10.861
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:57:10.87
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
STEP: getting /apis 01/29/23 03:57:10.876
STEP: getting /apis/discovery.k8s.io 01/29/23 03:57:10.881
STEP: getting /apis/discovery.k8s.iov1 01/29/23 03:57:10.884
STEP: creating 01/29/23 03:57:10.886
STEP: getting 01/29/23 03:57:10.917
STEP: listing 01/29/23 03:57:10.923
STEP: watching 01/29/23 03:57:10.929
Jan 29 03:57:10.929: INFO: starting watch
STEP: cluster-wide listing 01/29/23 03:57:10.931
STEP: cluster-wide watching 01/29/23 03:57:10.939
Jan 29 03:57:10.939: INFO: starting watch
STEP: patching 01/29/23 03:57:10.941
STEP: updating 01/29/23 03:57:10.949
Jan 29 03:57:10.962: INFO: waiting for watch events with expected annotations
Jan 29 03:57:10.962: INFO: saw patched and updated annotations
STEP: deleting 01/29/23 03:57:10.962
STEP: deleting a collection 01/29/23 03:57:10.986
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Jan 29 03:57:11.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-8170" for this suite. 01/29/23 03:57:11.021
{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","completed":220,"skipped":3874,"failed":0}
------------------------------
• [0.203 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:57:10.828
    Jan 29 03:57:10.828: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename endpointslice 01/29/23 03:57:10.829
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:57:10.861
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:57:10.87
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:352
    STEP: getting /apis 01/29/23 03:57:10.876
    STEP: getting /apis/discovery.k8s.io 01/29/23 03:57:10.881
    STEP: getting /apis/discovery.k8s.iov1 01/29/23 03:57:10.884
    STEP: creating 01/29/23 03:57:10.886
    STEP: getting 01/29/23 03:57:10.917
    STEP: listing 01/29/23 03:57:10.923
    STEP: watching 01/29/23 03:57:10.929
    Jan 29 03:57:10.929: INFO: starting watch
    STEP: cluster-wide listing 01/29/23 03:57:10.931
    STEP: cluster-wide watching 01/29/23 03:57:10.939
    Jan 29 03:57:10.939: INFO: starting watch
    STEP: patching 01/29/23 03:57:10.941
    STEP: updating 01/29/23 03:57:10.949
    Jan 29 03:57:10.962: INFO: waiting for watch events with expected annotations
    Jan 29 03:57:10.962: INFO: saw patched and updated annotations
    STEP: deleting 01/29/23 03:57:10.962
    STEP: deleting a collection 01/29/23 03:57:10.986
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Jan 29 03:57:11.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-8170" for this suite. 01/29/23 03:57:11.021
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:57:11.032
Jan 29 03:57:11.032: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename events 01/29/23 03:57:11.033
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:57:11.112
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:57:11.117
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 01/29/23 03:57:11.122
STEP: listing events in all namespaces 01/29/23 03:57:11.129
STEP: listing events in test namespace 01/29/23 03:57:11.138
STEP: listing events with field selection filtering on source 01/29/23 03:57:11.144
STEP: listing events with field selection filtering on reportingController 01/29/23 03:57:11.149
STEP: getting the test event 01/29/23 03:57:11.156
STEP: patching the test event 01/29/23 03:57:11.161
STEP: getting the test event 01/29/23 03:57:11.172
STEP: updating the test event 01/29/23 03:57:11.178
STEP: getting the test event 01/29/23 03:57:11.187
STEP: deleting the test event 01/29/23 03:57:11.193
STEP: listing events in all namespaces 01/29/23 03:57:11.205
STEP: listing events in test namespace 01/29/23 03:57:11.215
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
Jan 29 03:57:11.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-259" for this suite. 01/29/23 03:57:11.23
{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","completed":221,"skipped":3880,"failed":0}
------------------------------
• [0.208 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:57:11.032
    Jan 29 03:57:11.032: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename events 01/29/23 03:57:11.033
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:57:11.112
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:57:11.117
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 01/29/23 03:57:11.122
    STEP: listing events in all namespaces 01/29/23 03:57:11.129
    STEP: listing events in test namespace 01/29/23 03:57:11.138
    STEP: listing events with field selection filtering on source 01/29/23 03:57:11.144
    STEP: listing events with field selection filtering on reportingController 01/29/23 03:57:11.149
    STEP: getting the test event 01/29/23 03:57:11.156
    STEP: patching the test event 01/29/23 03:57:11.161
    STEP: getting the test event 01/29/23 03:57:11.172
    STEP: updating the test event 01/29/23 03:57:11.178
    STEP: getting the test event 01/29/23 03:57:11.187
    STEP: deleting the test event 01/29/23 03:57:11.193
    STEP: listing events in all namespaces 01/29/23 03:57:11.205
    STEP: listing events in test namespace 01/29/23 03:57:11.215
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    Jan 29 03:57:11.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-259" for this suite. 01/29/23 03:57:11.23
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:57:11.24
Jan 29 03:57:11.240: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename kubelet-test 01/29/23 03:57:11.242
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:57:11.271
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:57:11.277
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Jan 29 03:57:11.300: INFO: Waiting up to 5m0s for pod "busybox-scheduling-a158f40e-0540-4183-bd35-744bd0511dab" in namespace "kubelet-test-6371" to be "running and ready"
Jan 29 03:57:11.306: INFO: Pod "busybox-scheduling-a158f40e-0540-4183-bd35-744bd0511dab": Phase="Pending", Reason="", readiness=false. Elapsed: 6.143003ms
Jan 29 03:57:11.306: INFO: The phase of Pod busybox-scheduling-a158f40e-0540-4183-bd35-744bd0511dab is Pending, waiting for it to be Running (with Ready = true)
Jan 29 03:57:13.314: INFO: Pod "busybox-scheduling-a158f40e-0540-4183-bd35-744bd0511dab": Phase="Running", Reason="", readiness=true. Elapsed: 2.013548952s
Jan 29 03:57:13.314: INFO: The phase of Pod busybox-scheduling-a158f40e-0540-4183-bd35-744bd0511dab is Running (Ready = true)
Jan 29 03:57:13.314: INFO: Pod "busybox-scheduling-a158f40e-0540-4183-bd35-744bd0511dab" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jan 29 03:57:13.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6371" for this suite. 01/29/23 03:57:13.344
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","completed":222,"skipped":3881,"failed":0}
------------------------------
• [2.113 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:57:11.24
    Jan 29 03:57:11.240: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename kubelet-test 01/29/23 03:57:11.242
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:57:11.271
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:57:11.277
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Jan 29 03:57:11.300: INFO: Waiting up to 5m0s for pod "busybox-scheduling-a158f40e-0540-4183-bd35-744bd0511dab" in namespace "kubelet-test-6371" to be "running and ready"
    Jan 29 03:57:11.306: INFO: Pod "busybox-scheduling-a158f40e-0540-4183-bd35-744bd0511dab": Phase="Pending", Reason="", readiness=false. Elapsed: 6.143003ms
    Jan 29 03:57:11.306: INFO: The phase of Pod busybox-scheduling-a158f40e-0540-4183-bd35-744bd0511dab is Pending, waiting for it to be Running (with Ready = true)
    Jan 29 03:57:13.314: INFO: Pod "busybox-scheduling-a158f40e-0540-4183-bd35-744bd0511dab": Phase="Running", Reason="", readiness=true. Elapsed: 2.013548952s
    Jan 29 03:57:13.314: INFO: The phase of Pod busybox-scheduling-a158f40e-0540-4183-bd35-744bd0511dab is Running (Ready = true)
    Jan 29 03:57:13.314: INFO: Pod "busybox-scheduling-a158f40e-0540-4183-bd35-744bd0511dab" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jan 29 03:57:13.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-6371" for this suite. 01/29/23 03:57:13.344
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:57:13.357
Jan 29 03:57:13.357: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename webhook 01/29/23 03:57:13.358
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:57:13.385
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:57:13.39
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/29/23 03:57:13.416
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/29/23 03:57:14.85
STEP: Deploying the webhook pod 01/29/23 03:57:14.881
STEP: Wait for the deployment to be ready 01/29/23 03:57:14.908
Jan 29 03:57:14.922: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jan 29 03:57:16.943: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 29, 3, 57, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 3, 57, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 29, 3, 57, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 3, 57, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/29/23 03:57:18.951
STEP: Verifying the service has paired with the endpoint 01/29/23 03:57:18.968
Jan 29 03:57:19.969: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
STEP: Registering the webhook via the AdmissionRegistration API 01/29/23 03:57:19.975
STEP: create a pod 01/29/23 03:57:20
Jan 29 03:57:20.018: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-9088" to be "running"
Jan 29 03:57:20.024: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.073383ms
Jan 29 03:57:22.030: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011858921s
Jan 29 03:57:22.030: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 01/29/23 03:57:22.03
Jan 29 03:57:22.030: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=webhook-9088 attach --namespace=webhook-9088 to-be-attached-pod -i -c=container1'
Jan 29 03:57:22.167: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 29 03:57:22.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9088" for this suite. 01/29/23 03:57:22.188
STEP: Destroying namespace "webhook-9088-markers" for this suite. 01/29/23 03:57:22.198
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","completed":223,"skipped":3928,"failed":0}
------------------------------
• [SLOW TEST] [8.950 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:57:13.357
    Jan 29 03:57:13.357: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename webhook 01/29/23 03:57:13.358
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:57:13.385
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:57:13.39
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/29/23 03:57:13.416
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/29/23 03:57:14.85
    STEP: Deploying the webhook pod 01/29/23 03:57:14.881
    STEP: Wait for the deployment to be ready 01/29/23 03:57:14.908
    Jan 29 03:57:14.922: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    Jan 29 03:57:16.943: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 29, 3, 57, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 3, 57, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 29, 3, 57, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 3, 57, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/29/23 03:57:18.951
    STEP: Verifying the service has paired with the endpoint 01/29/23 03:57:18.968
    Jan 29 03:57:19.969: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:208
    STEP: Registering the webhook via the AdmissionRegistration API 01/29/23 03:57:19.975
    STEP: create a pod 01/29/23 03:57:20
    Jan 29 03:57:20.018: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-9088" to be "running"
    Jan 29 03:57:20.024: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.073383ms
    Jan 29 03:57:22.030: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011858921s
    Jan 29 03:57:22.030: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 01/29/23 03:57:22.03
    Jan 29 03:57:22.030: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=webhook-9088 attach --namespace=webhook-9088 to-be-attached-pod -i -c=container1'
    Jan 29 03:57:22.167: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 29 03:57:22.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-9088" for this suite. 01/29/23 03:57:22.188
    STEP: Destroying namespace "webhook-9088-markers" for this suite. 01/29/23 03:57:22.198
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:57:22.309
Jan 29 03:57:22.309: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename taint-multiple-pods 01/29/23 03:57:22.311
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:57:22.364
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:57:22.371
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:348
Jan 29 03:57:22.379: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 29 03:58:22.431: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
Jan 29 03:58:22.438: INFO: Starting informer...
STEP: Starting pods... 01/29/23 03:58:22.438
Jan 29 03:58:22.674: INFO: Pod1 is running on slave2. Tainting Node
Jan 29 03:58:22.890: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-959" to be "running"
Jan 29 03:58:22.896: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.823441ms
Jan 29 03:58:24.904: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.013949215s
Jan 29 03:58:24.905: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Jan 29 03:58:24.905: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-959" to be "running"
Jan 29 03:58:24.910: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 5.454098ms
Jan 29 03:58:24.910: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Jan 29 03:58:24.910: INFO: Pod2 is running on slave2. Tainting Node
STEP: Trying to apply a taint on the Node 01/29/23 03:58:24.91
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/29/23 03:58:24.925
STEP: Waiting for Pod1 and Pod2 to be deleted 01/29/23 03:58:24.936
Jan 29 03:58:31.652: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Jan 29 03:58:51.626: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/29/23 03:58:51.642
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:187
Jan 29 03:58:51.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-959" for this suite. 01/29/23 03:58:51.662
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","completed":224,"skipped":3946,"failed":0}
------------------------------
• [SLOW TEST] [89.363 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:57:22.309
    Jan 29 03:57:22.309: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename taint-multiple-pods 01/29/23 03:57:22.311
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:57:22.364
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:57:22.371
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:348
    Jan 29 03:57:22.379: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 29 03:58:22.431: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:420
    Jan 29 03:58:22.438: INFO: Starting informer...
    STEP: Starting pods... 01/29/23 03:58:22.438
    Jan 29 03:58:22.674: INFO: Pod1 is running on slave2. Tainting Node
    Jan 29 03:58:22.890: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-959" to be "running"
    Jan 29 03:58:22.896: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.823441ms
    Jan 29 03:58:24.904: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.013949215s
    Jan 29 03:58:24.905: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Jan 29 03:58:24.905: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-959" to be "running"
    Jan 29 03:58:24.910: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 5.454098ms
    Jan 29 03:58:24.910: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Jan 29 03:58:24.910: INFO: Pod2 is running on slave2. Tainting Node
    STEP: Trying to apply a taint on the Node 01/29/23 03:58:24.91
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/29/23 03:58:24.925
    STEP: Waiting for Pod1 and Pod2 to be deleted 01/29/23 03:58:24.936
    Jan 29 03:58:31.652: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Jan 29 03:58:51.626: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/29/23 03:58:51.642
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:187
    Jan 29 03:58:51.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "taint-multiple-pods-959" for this suite. 01/29/23 03:58:51.662
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:58:51.673
Jan 29 03:58:51.674: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename watch 01/29/23 03:58:51.675
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:58:51.707
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:58:51.712
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 01/29/23 03:58:51.717
STEP: creating a watch on configmaps with label B 01/29/23 03:58:51.72
STEP: creating a watch on configmaps with label A or B 01/29/23 03:58:51.722
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 01/29/23 03:58:51.724
Jan 29 03:58:51.730: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8650  e108015f-f808-4fc1-856d-d2df4e0a4472 5970556 0 2023-01-29 03:58:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] []},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 29 03:58:51.731: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8650  e108015f-f808-4fc1-856d-d2df4e0a4472 5970556 0 2023-01-29 03:58:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] []},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 01/29/23 03:58:51.731
Jan 29 03:58:51.742: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8650  e108015f-f808-4fc1-856d-d2df4e0a4472 5970557 0 2023-01-29 03:58:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 29 03:58:51.742: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8650  e108015f-f808-4fc1-856d-d2df4e0a4472 5970557 0 2023-01-29 03:58:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 01/29/23 03:58:51.742
Jan 29 03:58:51.754: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8650  e108015f-f808-4fc1-856d-d2df4e0a4472 5970559 0 2023-01-29 03:58:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 29 03:58:51.754: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8650  e108015f-f808-4fc1-856d-d2df4e0a4472 5970559 0 2023-01-29 03:58:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 01/29/23 03:58:51.754
Jan 29 03:58:51.765: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8650  e108015f-f808-4fc1-856d-d2df4e0a4472 5970560 0 2023-01-29 03:58:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 29 03:58:51.765: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8650  e108015f-f808-4fc1-856d-d2df4e0a4472 5970560 0 2023-01-29 03:58:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 01/29/23 03:58:51.766
Jan 29 03:58:51.772: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8650  f4d0ff89-85c8-433e-9952-66b19f8eb45c 5970561 0 2023-01-29 03:58:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] []},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 29 03:58:51.772: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8650  f4d0ff89-85c8-433e-9952-66b19f8eb45c 5970561 0 2023-01-29 03:58:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] []},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 01/29/23 03:59:01.772
Jan 29 03:59:01.783: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8650  f4d0ff89-85c8-433e-9952-66b19f8eb45c 5970632 0 2023-01-29 03:58:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] []},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 29 03:59:01.783: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8650  f4d0ff89-85c8-433e-9952-66b19f8eb45c 5970632 0 2023-01-29 03:58:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] []},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jan 29 03:59:11.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8650" for this suite. 01/29/23 03:59:11.797
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","completed":225,"skipped":3957,"failed":0}
------------------------------
• [SLOW TEST] [20.134 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:58:51.673
    Jan 29 03:58:51.674: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename watch 01/29/23 03:58:51.675
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:58:51.707
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:58:51.712
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 01/29/23 03:58:51.717
    STEP: creating a watch on configmaps with label B 01/29/23 03:58:51.72
    STEP: creating a watch on configmaps with label A or B 01/29/23 03:58:51.722
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 01/29/23 03:58:51.724
    Jan 29 03:58:51.730: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8650  e108015f-f808-4fc1-856d-d2df4e0a4472 5970556 0 2023-01-29 03:58:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] []},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 29 03:58:51.731: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8650  e108015f-f808-4fc1-856d-d2df4e0a4472 5970556 0 2023-01-29 03:58:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] []},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 01/29/23 03:58:51.731
    Jan 29 03:58:51.742: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8650  e108015f-f808-4fc1-856d-d2df4e0a4472 5970557 0 2023-01-29 03:58:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 29 03:58:51.742: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8650  e108015f-f808-4fc1-856d-d2df4e0a4472 5970557 0 2023-01-29 03:58:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 01/29/23 03:58:51.742
    Jan 29 03:58:51.754: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8650  e108015f-f808-4fc1-856d-d2df4e0a4472 5970559 0 2023-01-29 03:58:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 29 03:58:51.754: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8650  e108015f-f808-4fc1-856d-d2df4e0a4472 5970559 0 2023-01-29 03:58:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 01/29/23 03:58:51.754
    Jan 29 03:58:51.765: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8650  e108015f-f808-4fc1-856d-d2df4e0a4472 5970560 0 2023-01-29 03:58:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 29 03:58:51.765: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8650  e108015f-f808-4fc1-856d-d2df4e0a4472 5970560 0 2023-01-29 03:58:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 01/29/23 03:58:51.766
    Jan 29 03:58:51.772: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8650  f4d0ff89-85c8-433e-9952-66b19f8eb45c 5970561 0 2023-01-29 03:58:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] []},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 29 03:58:51.772: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8650  f4d0ff89-85c8-433e-9952-66b19f8eb45c 5970561 0 2023-01-29 03:58:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] []},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 01/29/23 03:59:01.772
    Jan 29 03:59:01.783: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8650  f4d0ff89-85c8-433e-9952-66b19f8eb45c 5970632 0 2023-01-29 03:58:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] []},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 29 03:59:01.783: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8650  f4d0ff89-85c8-433e-9952-66b19f8eb45c 5970632 0 2023-01-29 03:58:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] []},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jan 29 03:59:11.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-8650" for this suite. 01/29/23 03:59:11.797
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:59:11.809
Jan 29 03:59:11.809: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename secrets 01/29/23 03:59:11.81
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:59:11.874
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:59:11.881
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
STEP: Creating secret with name secret-test-5ebd12b2-1372-442d-a51d-e339b87cad1c 01/29/23 03:59:11.887
STEP: Creating a pod to test consume secrets 01/29/23 03:59:11.895
Jan 29 03:59:11.912: INFO: Waiting up to 5m0s for pod "pod-secrets-10c5a244-8a10-4f05-a633-6af12d15f387" in namespace "secrets-9591" to be "Succeeded or Failed"
Jan 29 03:59:11.922: INFO: Pod "pod-secrets-10c5a244-8a10-4f05-a633-6af12d15f387": Phase="Pending", Reason="", readiness=false. Elapsed: 10.148791ms
Jan 29 03:59:13.931: INFO: Pod "pod-secrets-10c5a244-8a10-4f05-a633-6af12d15f387": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019308533s
Jan 29 03:59:15.930: INFO: Pod "pod-secrets-10c5a244-8a10-4f05-a633-6af12d15f387": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018610645s
STEP: Saw pod success 01/29/23 03:59:15.93
Jan 29 03:59:15.931: INFO: Pod "pod-secrets-10c5a244-8a10-4f05-a633-6af12d15f387" satisfied condition "Succeeded or Failed"
Jan 29 03:59:15.937: INFO: Trying to get logs from node slave2 pod pod-secrets-10c5a244-8a10-4f05-a633-6af12d15f387 container secret-env-test: <nil>
STEP: delete the pod 01/29/23 03:59:15.972
Jan 29 03:59:16.042: INFO: Waiting for pod pod-secrets-10c5a244-8a10-4f05-a633-6af12d15f387 to disappear
Jan 29 03:59:16.048: INFO: Pod pod-secrets-10c5a244-8a10-4f05-a633-6af12d15f387 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Jan 29 03:59:16.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9591" for this suite. 01/29/23 03:59:16.057
{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","completed":226,"skipped":3968,"failed":0}
------------------------------
• [4.256 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:59:11.809
    Jan 29 03:59:11.809: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename secrets 01/29/23 03:59:11.81
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:59:11.874
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:59:11.881
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:45
    STEP: Creating secret with name secret-test-5ebd12b2-1372-442d-a51d-e339b87cad1c 01/29/23 03:59:11.887
    STEP: Creating a pod to test consume secrets 01/29/23 03:59:11.895
    Jan 29 03:59:11.912: INFO: Waiting up to 5m0s for pod "pod-secrets-10c5a244-8a10-4f05-a633-6af12d15f387" in namespace "secrets-9591" to be "Succeeded or Failed"
    Jan 29 03:59:11.922: INFO: Pod "pod-secrets-10c5a244-8a10-4f05-a633-6af12d15f387": Phase="Pending", Reason="", readiness=false. Elapsed: 10.148791ms
    Jan 29 03:59:13.931: INFO: Pod "pod-secrets-10c5a244-8a10-4f05-a633-6af12d15f387": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019308533s
    Jan 29 03:59:15.930: INFO: Pod "pod-secrets-10c5a244-8a10-4f05-a633-6af12d15f387": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018610645s
    STEP: Saw pod success 01/29/23 03:59:15.93
    Jan 29 03:59:15.931: INFO: Pod "pod-secrets-10c5a244-8a10-4f05-a633-6af12d15f387" satisfied condition "Succeeded or Failed"
    Jan 29 03:59:15.937: INFO: Trying to get logs from node slave2 pod pod-secrets-10c5a244-8a10-4f05-a633-6af12d15f387 container secret-env-test: <nil>
    STEP: delete the pod 01/29/23 03:59:15.972
    Jan 29 03:59:16.042: INFO: Waiting for pod pod-secrets-10c5a244-8a10-4f05-a633-6af12d15f387 to disappear
    Jan 29 03:59:16.048: INFO: Pod pod-secrets-10c5a244-8a10-4f05-a633-6af12d15f387 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Jan 29 03:59:16.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-9591" for this suite. 01/29/23 03:59:16.057
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:59:16.068
Jan 29 03:59:16.068: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename downward-api 01/29/23 03:59:16.069
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:59:16.113
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:59:16.119
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
STEP: Creating a pod to test downward API volume plugin 01/29/23 03:59:16.124
Jan 29 03:59:16.141: INFO: Waiting up to 5m0s for pod "downwardapi-volume-67205061-e4a7-4b54-a28f-41c72172a93d" in namespace "downward-api-1819" to be "Succeeded or Failed"
Jan 29 03:59:16.148: INFO: Pod "downwardapi-volume-67205061-e4a7-4b54-a28f-41c72172a93d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.633247ms
Jan 29 03:59:18.155: INFO: Pod "downwardapi-volume-67205061-e4a7-4b54-a28f-41c72172a93d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013795174s
Jan 29 03:59:20.156: INFO: Pod "downwardapi-volume-67205061-e4a7-4b54-a28f-41c72172a93d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01500984s
STEP: Saw pod success 01/29/23 03:59:20.156
Jan 29 03:59:20.156: INFO: Pod "downwardapi-volume-67205061-e4a7-4b54-a28f-41c72172a93d" satisfied condition "Succeeded or Failed"
Jan 29 03:59:20.164: INFO: Trying to get logs from node slave2 pod downwardapi-volume-67205061-e4a7-4b54-a28f-41c72172a93d container client-container: <nil>
STEP: delete the pod 01/29/23 03:59:20.178
Jan 29 03:59:20.266: INFO: Waiting for pod downwardapi-volume-67205061-e4a7-4b54-a28f-41c72172a93d to disappear
Jan 29 03:59:20.271: INFO: Pod downwardapi-volume-67205061-e4a7-4b54-a28f-41c72172a93d no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 29 03:59:20.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1819" for this suite. 01/29/23 03:59:20.282
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","completed":227,"skipped":4005,"failed":0}
------------------------------
• [4.223 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:59:16.068
    Jan 29 03:59:16.068: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename downward-api 01/29/23 03:59:16.069
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:59:16.113
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:59:16.119
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:220
    STEP: Creating a pod to test downward API volume plugin 01/29/23 03:59:16.124
    Jan 29 03:59:16.141: INFO: Waiting up to 5m0s for pod "downwardapi-volume-67205061-e4a7-4b54-a28f-41c72172a93d" in namespace "downward-api-1819" to be "Succeeded or Failed"
    Jan 29 03:59:16.148: INFO: Pod "downwardapi-volume-67205061-e4a7-4b54-a28f-41c72172a93d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.633247ms
    Jan 29 03:59:18.155: INFO: Pod "downwardapi-volume-67205061-e4a7-4b54-a28f-41c72172a93d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013795174s
    Jan 29 03:59:20.156: INFO: Pod "downwardapi-volume-67205061-e4a7-4b54-a28f-41c72172a93d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01500984s
    STEP: Saw pod success 01/29/23 03:59:20.156
    Jan 29 03:59:20.156: INFO: Pod "downwardapi-volume-67205061-e4a7-4b54-a28f-41c72172a93d" satisfied condition "Succeeded or Failed"
    Jan 29 03:59:20.164: INFO: Trying to get logs from node slave2 pod downwardapi-volume-67205061-e4a7-4b54-a28f-41c72172a93d container client-container: <nil>
    STEP: delete the pod 01/29/23 03:59:20.178
    Jan 29 03:59:20.266: INFO: Waiting for pod downwardapi-volume-67205061-e4a7-4b54-a28f-41c72172a93d to disappear
    Jan 29 03:59:20.271: INFO: Pod downwardapi-volume-67205061-e4a7-4b54-a28f-41c72172a93d no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 29 03:59:20.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-1819" for this suite. 01/29/23 03:59:20.282
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:59:20.292
Jan 29 03:59:20.292: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename projected 01/29/23 03:59:20.294
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:59:20.324
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:59:20.33
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
STEP: Creating a pod to test downward API volume plugin 01/29/23 03:59:20.336
Jan 29 03:59:20.353: INFO: Waiting up to 5m0s for pod "downwardapi-volume-92310d1f-0f9b-4bbe-bc1d-dcc6b01cd8d7" in namespace "projected-6678" to be "Succeeded or Failed"
Jan 29 03:59:20.358: INFO: Pod "downwardapi-volume-92310d1f-0f9b-4bbe-bc1d-dcc6b01cd8d7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.402698ms
Jan 29 03:59:22.366: INFO: Pod "downwardapi-volume-92310d1f-0f9b-4bbe-bc1d-dcc6b01cd8d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01318489s
Jan 29 03:59:24.365: INFO: Pod "downwardapi-volume-92310d1f-0f9b-4bbe-bc1d-dcc6b01cd8d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012460042s
STEP: Saw pod success 01/29/23 03:59:24.365
Jan 29 03:59:24.365: INFO: Pod "downwardapi-volume-92310d1f-0f9b-4bbe-bc1d-dcc6b01cd8d7" satisfied condition "Succeeded or Failed"
Jan 29 03:59:24.371: INFO: Trying to get logs from node slave2 pod downwardapi-volume-92310d1f-0f9b-4bbe-bc1d-dcc6b01cd8d7 container client-container: <nil>
STEP: delete the pod 01/29/23 03:59:24.385
Jan 29 03:59:24.481: INFO: Waiting for pod downwardapi-volume-92310d1f-0f9b-4bbe-bc1d-dcc6b01cd8d7 to disappear
Jan 29 03:59:24.486: INFO: Pod downwardapi-volume-92310d1f-0f9b-4bbe-bc1d-dcc6b01cd8d7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 29 03:59:24.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6678" for this suite. 01/29/23 03:59:24.495
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":228,"skipped":4006,"failed":0}
------------------------------
• [4.212 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:59:20.292
    Jan 29 03:59:20.292: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename projected 01/29/23 03:59:20.294
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:59:20.324
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:59:20.33
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:260
    STEP: Creating a pod to test downward API volume plugin 01/29/23 03:59:20.336
    Jan 29 03:59:20.353: INFO: Waiting up to 5m0s for pod "downwardapi-volume-92310d1f-0f9b-4bbe-bc1d-dcc6b01cd8d7" in namespace "projected-6678" to be "Succeeded or Failed"
    Jan 29 03:59:20.358: INFO: Pod "downwardapi-volume-92310d1f-0f9b-4bbe-bc1d-dcc6b01cd8d7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.402698ms
    Jan 29 03:59:22.366: INFO: Pod "downwardapi-volume-92310d1f-0f9b-4bbe-bc1d-dcc6b01cd8d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01318489s
    Jan 29 03:59:24.365: INFO: Pod "downwardapi-volume-92310d1f-0f9b-4bbe-bc1d-dcc6b01cd8d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012460042s
    STEP: Saw pod success 01/29/23 03:59:24.365
    Jan 29 03:59:24.365: INFO: Pod "downwardapi-volume-92310d1f-0f9b-4bbe-bc1d-dcc6b01cd8d7" satisfied condition "Succeeded or Failed"
    Jan 29 03:59:24.371: INFO: Trying to get logs from node slave2 pod downwardapi-volume-92310d1f-0f9b-4bbe-bc1d-dcc6b01cd8d7 container client-container: <nil>
    STEP: delete the pod 01/29/23 03:59:24.385
    Jan 29 03:59:24.481: INFO: Waiting for pod downwardapi-volume-92310d1f-0f9b-4bbe-bc1d-dcc6b01cd8d7 to disappear
    Jan 29 03:59:24.486: INFO: Pod downwardapi-volume-92310d1f-0f9b-4bbe-bc1d-dcc6b01cd8d7 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 29 03:59:24.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6678" for this suite. 01/29/23 03:59:24.495
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:59:24.505
Jan 29 03:59:24.505: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename kubectl 01/29/23 03:59:24.507
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:59:24.535
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:59:24.543
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
Jan 29 03:59:24.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-2429 version'
Jan 29 03:59:24.654: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Jan 29 03:59:24.654: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.4\", GitCommit:\"872a965c6c6526caa949f0c6ac028ef7aff3fb78\", GitTreeState:\"clean\", BuildDate:\"2022-11-09T13:36:36Z\", GoVersion:\"go1.19.3\", Compiler:\"gc\", Platform:\"linux/arm64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"20\", GitVersion:\"v1.25.4-1\", GitCommit:\"3c441e391ee2e869feadddea121b3937bcb10bbb\", GitTreeState:\"archive\", BuildDate:\"2022-12-06T12:00:01Z\", GoVersion:\"go1.19.3\", Compiler:\"gc\", Platform:\"linux/arm64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 29 03:59:24.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2429" for this suite. 01/29/23 03:59:24.663
{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","completed":229,"skipped":4012,"failed":0}
------------------------------
• [0.167 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1677
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1683

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:59:24.505
    Jan 29 03:59:24.505: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename kubectl 01/29/23 03:59:24.507
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:59:24.535
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:59:24.543
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1683
    Jan 29 03:59:24.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-2429 version'
    Jan 29 03:59:24.654: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Jan 29 03:59:24.654: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.4\", GitCommit:\"872a965c6c6526caa949f0c6ac028ef7aff3fb78\", GitTreeState:\"clean\", BuildDate:\"2022-11-09T13:36:36Z\", GoVersion:\"go1.19.3\", Compiler:\"gc\", Platform:\"linux/arm64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"20\", GitVersion:\"v1.25.4-1\", GitCommit:\"3c441e391ee2e869feadddea121b3937bcb10bbb\", GitTreeState:\"archive\", BuildDate:\"2022-12-06T12:00:01Z\", GoVersion:\"go1.19.3\", Compiler:\"gc\", Platform:\"linux/arm64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 29 03:59:24.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-2429" for this suite. 01/29/23 03:59:24.663
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:59:24.672
Jan 29 03:59:24.673: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename statefulset 01/29/23 03:59:24.674
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:59:24.703
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:59:24.71
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-7846 01/29/23 03:59:24.715
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
STEP: Creating statefulset ss in namespace statefulset-7846 01/29/23 03:59:24.723
Jan 29 03:59:24.743: INFO: Found 0 stateful pods, waiting for 1
Jan 29 03:59:34.751: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 01/29/23 03:59:34.762
STEP: updating a scale subresource 01/29/23 03:59:34.768
STEP: verifying the statefulset Spec.Replicas was modified 01/29/23 03:59:34.776
STEP: Patch a scale subresource 01/29/23 03:59:34.782
STEP: verifying the statefulset Spec.Replicas was modified 01/29/23 03:59:34.792
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 29 03:59:34.800: INFO: Deleting all statefulset in ns statefulset-7846
Jan 29 03:59:34.807: INFO: Scaling statefulset ss to 0
Jan 29 03:59:44.872: INFO: Waiting for statefulset status.replicas updated to 0
Jan 29 03:59:44.878: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan 29 03:59:44.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7846" for this suite. 01/29/23 03:59:44.92
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","completed":230,"skipped":4014,"failed":0}
------------------------------
• [SLOW TEST] [20.257 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:846

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:59:24.672
    Jan 29 03:59:24.673: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename statefulset 01/29/23 03:59:24.674
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:59:24.703
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:59:24.71
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-7846 01/29/23 03:59:24.715
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:846
    STEP: Creating statefulset ss in namespace statefulset-7846 01/29/23 03:59:24.723
    Jan 29 03:59:24.743: INFO: Found 0 stateful pods, waiting for 1
    Jan 29 03:59:34.751: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 01/29/23 03:59:34.762
    STEP: updating a scale subresource 01/29/23 03:59:34.768
    STEP: verifying the statefulset Spec.Replicas was modified 01/29/23 03:59:34.776
    STEP: Patch a scale subresource 01/29/23 03:59:34.782
    STEP: verifying the statefulset Spec.Replicas was modified 01/29/23 03:59:34.792
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan 29 03:59:34.800: INFO: Deleting all statefulset in ns statefulset-7846
    Jan 29 03:59:34.807: INFO: Scaling statefulset ss to 0
    Jan 29 03:59:44.872: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 29 03:59:44.878: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan 29 03:59:44.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-7846" for this suite. 01/29/23 03:59:44.92
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:59:44.933
Jan 29 03:59:44.933: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename services 01/29/23 03:59:44.934
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:59:44.962
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:59:44.969
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
STEP: creating a service nodeport-service with the type=NodePort in namespace services-3275 01/29/23 03:59:44.975
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/29/23 03:59:45.001
STEP: creating service externalsvc in namespace services-3275 01/29/23 03:59:45.002
STEP: creating replication controller externalsvc in namespace services-3275 01/29/23 03:59:45.026
I0129 03:59:45.039950      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-3275, replica count: 2
I0129 03:59:48.093916      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 01/29/23 03:59:48.1
Jan 29 03:59:48.131: INFO: Creating new exec pod
Jan 29 03:59:48.147: INFO: Waiting up to 5m0s for pod "execpoddfz9v" in namespace "services-3275" to be "running"
Jan 29 03:59:48.154: INFO: Pod "execpoddfz9v": Phase="Pending", Reason="", readiness=false. Elapsed: 6.305964ms
Jan 29 03:59:50.161: INFO: Pod "execpoddfz9v": Phase="Running", Reason="", readiness=true. Elapsed: 2.013575649s
Jan 29 03:59:50.161: INFO: Pod "execpoddfz9v" satisfied condition "running"
Jan 29 03:59:50.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-3275 exec execpoddfz9v -- /bin/sh -x -c nslookup nodeport-service.services-3275.svc.cluster.local'
Jan 29 03:59:50.429: INFO: stderr: "+ nslookup nodeport-service.services-3275.svc.cluster.local\n"
Jan 29 03:59:50.429: INFO: stdout: "Server:\t\t100.105.0.3\nAddress:\t100.105.0.3#53\n\nnodeport-service.services-3275.svc.cluster.local\tcanonical name = externalsvc.services-3275.svc.cluster.local.\nName:\texternalsvc.services-3275.svc.cluster.local\nAddress: 100.105.215.137\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-3275, will wait for the garbage collector to delete the pods 01/29/23 03:59:50.429
Jan 29 03:59:50.500: INFO: Deleting ReplicationController externalsvc took: 12.968791ms
Jan 29 03:59:50.600: INFO: Terminating ReplicationController externalsvc pods took: 100.710264ms
Jan 29 03:59:52.942: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 29 03:59:52.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3275" for this suite. 01/29/23 03:59:52.973
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","completed":231,"skipped":4059,"failed":0}
------------------------------
• [SLOW TEST] [8.050 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:59:44.933
    Jan 29 03:59:44.933: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename services 01/29/23 03:59:44.934
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:59:44.962
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:59:44.969
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1523
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-3275 01/29/23 03:59:44.975
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/29/23 03:59:45.001
    STEP: creating service externalsvc in namespace services-3275 01/29/23 03:59:45.002
    STEP: creating replication controller externalsvc in namespace services-3275 01/29/23 03:59:45.026
    I0129 03:59:45.039950      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-3275, replica count: 2
    I0129 03:59:48.093916      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 01/29/23 03:59:48.1
    Jan 29 03:59:48.131: INFO: Creating new exec pod
    Jan 29 03:59:48.147: INFO: Waiting up to 5m0s for pod "execpoddfz9v" in namespace "services-3275" to be "running"
    Jan 29 03:59:48.154: INFO: Pod "execpoddfz9v": Phase="Pending", Reason="", readiness=false. Elapsed: 6.305964ms
    Jan 29 03:59:50.161: INFO: Pod "execpoddfz9v": Phase="Running", Reason="", readiness=true. Elapsed: 2.013575649s
    Jan 29 03:59:50.161: INFO: Pod "execpoddfz9v" satisfied condition "running"
    Jan 29 03:59:50.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-3275 exec execpoddfz9v -- /bin/sh -x -c nslookup nodeport-service.services-3275.svc.cluster.local'
    Jan 29 03:59:50.429: INFO: stderr: "+ nslookup nodeport-service.services-3275.svc.cluster.local\n"
    Jan 29 03:59:50.429: INFO: stdout: "Server:\t\t100.105.0.3\nAddress:\t100.105.0.3#53\n\nnodeport-service.services-3275.svc.cluster.local\tcanonical name = externalsvc.services-3275.svc.cluster.local.\nName:\texternalsvc.services-3275.svc.cluster.local\nAddress: 100.105.215.137\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-3275, will wait for the garbage collector to delete the pods 01/29/23 03:59:50.429
    Jan 29 03:59:50.500: INFO: Deleting ReplicationController externalsvc took: 12.968791ms
    Jan 29 03:59:50.600: INFO: Terminating ReplicationController externalsvc pods took: 100.710264ms
    Jan 29 03:59:52.942: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 29 03:59:52.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3275" for this suite. 01/29/23 03:59:52.973
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:59:52.984
Jan 29 03:59:52.984: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename events 01/29/23 03:59:52.985
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:59:53.018
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:59:53.024
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 01/29/23 03:59:53.03
STEP: listing all events in all namespaces 01/29/23 03:59:53.04
STEP: patching the test event 01/29/23 03:59:53.049
STEP: fetching the test event 01/29/23 03:59:53.058
STEP: updating the test event 01/29/23 03:59:53.063
STEP: getting the test event 01/29/23 03:59:53.077
STEP: deleting the test event 01/29/23 03:59:53.084
STEP: listing all events in all namespaces 01/29/23 03:59:53.095
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
Jan 29 03:59:53.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5828" for this suite. 01/29/23 03:59:53.112
{"msg":"PASSED [sig-instrumentation] Events should manage the lifecycle of an event [Conformance]","completed":232,"skipped":4072,"failed":0}
------------------------------
• [0.138 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:59:52.984
    Jan 29 03:59:52.984: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename events 01/29/23 03:59:52.985
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:59:53.018
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:59:53.024
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 01/29/23 03:59:53.03
    STEP: listing all events in all namespaces 01/29/23 03:59:53.04
    STEP: patching the test event 01/29/23 03:59:53.049
    STEP: fetching the test event 01/29/23 03:59:53.058
    STEP: updating the test event 01/29/23 03:59:53.063
    STEP: getting the test event 01/29/23 03:59:53.077
    STEP: deleting the test event 01/29/23 03:59:53.084
    STEP: listing all events in all namespaces 01/29/23 03:59:53.095
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    Jan 29 03:59:53.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-5828" for this suite. 01/29/23 03:59:53.112
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:59:53.123
Jan 29 03:59:53.123: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename secrets 01/29/23 03:59:53.124
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:59:53.149
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:59:53.155
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
STEP: Creating secret with name secret-test-11cdccef-735a-4b8b-add3-6ebff7d9f9e9 01/29/23 03:59:53.159
STEP: Creating a pod to test consume secrets 01/29/23 03:59:53.168
Jan 29 03:59:53.184: INFO: Waiting up to 5m0s for pod "pod-secrets-1aa72b70-6724-4912-ab18-e4056e42ed57" in namespace "secrets-3808" to be "Succeeded or Failed"
Jan 29 03:59:53.189: INFO: Pod "pod-secrets-1aa72b70-6724-4912-ab18-e4056e42ed57": Phase="Pending", Reason="", readiness=false. Elapsed: 5.178456ms
Jan 29 03:59:55.197: INFO: Pod "pod-secrets-1aa72b70-6724-4912-ab18-e4056e42ed57": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012767363s
Jan 29 03:59:57.196: INFO: Pod "pod-secrets-1aa72b70-6724-4912-ab18-e4056e42ed57": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011857511s
STEP: Saw pod success 01/29/23 03:59:57.196
Jan 29 03:59:57.196: INFO: Pod "pod-secrets-1aa72b70-6724-4912-ab18-e4056e42ed57" satisfied condition "Succeeded or Failed"
Jan 29 03:59:57.202: INFO: Trying to get logs from node slave2 pod pod-secrets-1aa72b70-6724-4912-ab18-e4056e42ed57 container secret-volume-test: <nil>
STEP: delete the pod 01/29/23 03:59:57.216
Jan 29 03:59:57.310: INFO: Waiting for pod pod-secrets-1aa72b70-6724-4912-ab18-e4056e42ed57 to disappear
Jan 29 03:59:57.316: INFO: Pod pod-secrets-1aa72b70-6724-4912-ab18-e4056e42ed57 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 29 03:59:57.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3808" for this suite. 01/29/23 03:59:57.325
{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":233,"skipped":4081,"failed":0}
------------------------------
• [4.213 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:59:53.123
    Jan 29 03:59:53.123: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename secrets 01/29/23 03:59:53.124
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:59:53.149
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:59:53.155
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:124
    STEP: Creating secret with name secret-test-11cdccef-735a-4b8b-add3-6ebff7d9f9e9 01/29/23 03:59:53.159
    STEP: Creating a pod to test consume secrets 01/29/23 03:59:53.168
    Jan 29 03:59:53.184: INFO: Waiting up to 5m0s for pod "pod-secrets-1aa72b70-6724-4912-ab18-e4056e42ed57" in namespace "secrets-3808" to be "Succeeded or Failed"
    Jan 29 03:59:53.189: INFO: Pod "pod-secrets-1aa72b70-6724-4912-ab18-e4056e42ed57": Phase="Pending", Reason="", readiness=false. Elapsed: 5.178456ms
    Jan 29 03:59:55.197: INFO: Pod "pod-secrets-1aa72b70-6724-4912-ab18-e4056e42ed57": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012767363s
    Jan 29 03:59:57.196: INFO: Pod "pod-secrets-1aa72b70-6724-4912-ab18-e4056e42ed57": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011857511s
    STEP: Saw pod success 01/29/23 03:59:57.196
    Jan 29 03:59:57.196: INFO: Pod "pod-secrets-1aa72b70-6724-4912-ab18-e4056e42ed57" satisfied condition "Succeeded or Failed"
    Jan 29 03:59:57.202: INFO: Trying to get logs from node slave2 pod pod-secrets-1aa72b70-6724-4912-ab18-e4056e42ed57 container secret-volume-test: <nil>
    STEP: delete the pod 01/29/23 03:59:57.216
    Jan 29 03:59:57.310: INFO: Waiting for pod pod-secrets-1aa72b70-6724-4912-ab18-e4056e42ed57 to disappear
    Jan 29 03:59:57.316: INFO: Pod pod-secrets-1aa72b70-6724-4912-ab18-e4056e42ed57 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 29 03:59:57.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-3808" for this suite. 01/29/23 03:59:57.325
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 03:59:57.338
Jan 29 03:59:57.338: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename daemonsets 01/29/23 03:59:57.339
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:59:57.369
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:59:57.375
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
STEP: Creating simple DaemonSet "daemon-set" 01/29/23 03:59:57.441
STEP: Check that daemon pods launch on every node of the cluster. 01/29/23 03:59:57.455
Jan 29 03:59:57.472: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 29 03:59:57.472: INFO: Node master1 is running 0 daemon pod, expected 1
Jan 29 03:59:58.489: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 29 03:59:58.489: INFO: Node master1 is running 0 daemon pod, expected 1
Jan 29 03:59:59.489: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan 29 03:59:59.489: INFO: Node master1 is running 0 daemon pod, expected 1
Jan 29 04:00:00.491: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Jan 29 04:00:00.491: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 01/29/23 04:00:00.498
Jan 29 04:00:00.557: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Jan 29 04:00:00.557: INFO: Node master2 is running 0 daemon pod, expected 1
Jan 29 04:00:01.575: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Jan 29 04:00:01.576: INFO: Node master2 is running 0 daemon pod, expected 1
Jan 29 04:00:02.587: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Jan 29 04:00:02.587: INFO: Node master2 is running 0 daemon pod, expected 1
Jan 29 04:00:03.582: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Jan 29 04:00:03.582: INFO: Node master2 is running 0 daemon pod, expected 1
Jan 29 04:00:04.583: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Jan 29 04:00:04.583: INFO: Node master2 is running 0 daemon pod, expected 1
Jan 29 04:00:05.575: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Jan 29 04:00:05.575: INFO: Node master2 is running 0 daemon pod, expected 1
Jan 29 04:00:06.574: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Jan 29 04:00:06.574: INFO: Node master2 is running 0 daemon pod, expected 1
Jan 29 04:00:07.575: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Jan 29 04:00:07.575: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 01/29/23 04:00:07.582
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5617, will wait for the garbage collector to delete the pods 01/29/23 04:00:07.582
Jan 29 04:00:07.650: INFO: Deleting DaemonSet.extensions daemon-set took: 11.981844ms
Jan 29 04:00:07.851: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.756165ms
Jan 29 04:00:10.558: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 29 04:00:10.558: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 29 04:00:10.564: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"5971456"},"items":null}

Jan 29 04:00:10.570: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"5971456"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan 29 04:00:10.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5617" for this suite. 01/29/23 04:00:10.625
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","completed":234,"skipped":4084,"failed":0}
------------------------------
• [SLOW TEST] [13.297 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 03:59:57.338
    Jan 29 03:59:57.338: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename daemonsets 01/29/23 03:59:57.339
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 03:59:57.369
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 03:59:57.375
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:165
    STEP: Creating simple DaemonSet "daemon-set" 01/29/23 03:59:57.441
    STEP: Check that daemon pods launch on every node of the cluster. 01/29/23 03:59:57.455
    Jan 29 03:59:57.472: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 29 03:59:57.472: INFO: Node master1 is running 0 daemon pod, expected 1
    Jan 29 03:59:58.489: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 29 03:59:58.489: INFO: Node master1 is running 0 daemon pod, expected 1
    Jan 29 03:59:59.489: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan 29 03:59:59.489: INFO: Node master1 is running 0 daemon pod, expected 1
    Jan 29 04:00:00.491: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
    Jan 29 04:00:00.491: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 01/29/23 04:00:00.498
    Jan 29 04:00:00.557: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Jan 29 04:00:00.557: INFO: Node master2 is running 0 daemon pod, expected 1
    Jan 29 04:00:01.575: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Jan 29 04:00:01.576: INFO: Node master2 is running 0 daemon pod, expected 1
    Jan 29 04:00:02.587: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Jan 29 04:00:02.587: INFO: Node master2 is running 0 daemon pod, expected 1
    Jan 29 04:00:03.582: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Jan 29 04:00:03.582: INFO: Node master2 is running 0 daemon pod, expected 1
    Jan 29 04:00:04.583: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Jan 29 04:00:04.583: INFO: Node master2 is running 0 daemon pod, expected 1
    Jan 29 04:00:05.575: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Jan 29 04:00:05.575: INFO: Node master2 is running 0 daemon pod, expected 1
    Jan 29 04:00:06.574: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Jan 29 04:00:06.574: INFO: Node master2 is running 0 daemon pod, expected 1
    Jan 29 04:00:07.575: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
    Jan 29 04:00:07.575: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 01/29/23 04:00:07.582
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5617, will wait for the garbage collector to delete the pods 01/29/23 04:00:07.582
    Jan 29 04:00:07.650: INFO: Deleting DaemonSet.extensions daemon-set took: 11.981844ms
    Jan 29 04:00:07.851: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.756165ms
    Jan 29 04:00:10.558: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 29 04:00:10.558: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 29 04:00:10.564: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"5971456"},"items":null}

    Jan 29 04:00:10.570: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"5971456"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan 29 04:00:10.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-5617" for this suite. 01/29/23 04:00:10.625
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:00:10.636
Jan 29 04:00:10.636: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename containers 01/29/23 04:00:10.638
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:00:10.67
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:00:10.676
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
STEP: Creating a pod to test override all 01/29/23 04:00:10.685
Jan 29 04:00:10.703: INFO: Waiting up to 5m0s for pod "client-containers-e9213a62-746b-4356-bce3-12c3337f8908" in namespace "containers-6580" to be "Succeeded or Failed"
Jan 29 04:00:10.710: INFO: Pod "client-containers-e9213a62-746b-4356-bce3-12c3337f8908": Phase="Pending", Reason="", readiness=false. Elapsed: 6.716667ms
Jan 29 04:00:12.722: INFO: Pod "client-containers-e9213a62-746b-4356-bce3-12c3337f8908": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019297108s
Jan 29 04:00:14.717: INFO: Pod "client-containers-e9213a62-746b-4356-bce3-12c3337f8908": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013784584s
Jan 29 04:00:16.717: INFO: Pod "client-containers-e9213a62-746b-4356-bce3-12c3337f8908": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014410242s
STEP: Saw pod success 01/29/23 04:00:16.717
Jan 29 04:00:16.718: INFO: Pod "client-containers-e9213a62-746b-4356-bce3-12c3337f8908" satisfied condition "Succeeded or Failed"
Jan 29 04:00:16.723: INFO: Trying to get logs from node slave2 pod client-containers-e9213a62-746b-4356-bce3-12c3337f8908 container agnhost-container: <nil>
STEP: delete the pod 01/29/23 04:00:16.739
Jan 29 04:00:16.805: INFO: Waiting for pod client-containers-e9213a62-746b-4356-bce3-12c3337f8908 to disappear
Jan 29 04:00:16.810: INFO: Pod client-containers-e9213a62-746b-4356-bce3-12c3337f8908 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Jan 29 04:00:16.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6580" for this suite. 01/29/23 04:00:16.818
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","completed":235,"skipped":4094,"failed":0}
------------------------------
• [SLOW TEST] [6.191 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:00:10.636
    Jan 29 04:00:10.636: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename containers 01/29/23 04:00:10.638
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:00:10.67
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:00:10.676
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:86
    STEP: Creating a pod to test override all 01/29/23 04:00:10.685
    Jan 29 04:00:10.703: INFO: Waiting up to 5m0s for pod "client-containers-e9213a62-746b-4356-bce3-12c3337f8908" in namespace "containers-6580" to be "Succeeded or Failed"
    Jan 29 04:00:10.710: INFO: Pod "client-containers-e9213a62-746b-4356-bce3-12c3337f8908": Phase="Pending", Reason="", readiness=false. Elapsed: 6.716667ms
    Jan 29 04:00:12.722: INFO: Pod "client-containers-e9213a62-746b-4356-bce3-12c3337f8908": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019297108s
    Jan 29 04:00:14.717: INFO: Pod "client-containers-e9213a62-746b-4356-bce3-12c3337f8908": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013784584s
    Jan 29 04:00:16.717: INFO: Pod "client-containers-e9213a62-746b-4356-bce3-12c3337f8908": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014410242s
    STEP: Saw pod success 01/29/23 04:00:16.717
    Jan 29 04:00:16.718: INFO: Pod "client-containers-e9213a62-746b-4356-bce3-12c3337f8908" satisfied condition "Succeeded or Failed"
    Jan 29 04:00:16.723: INFO: Trying to get logs from node slave2 pod client-containers-e9213a62-746b-4356-bce3-12c3337f8908 container agnhost-container: <nil>
    STEP: delete the pod 01/29/23 04:00:16.739
    Jan 29 04:00:16.805: INFO: Waiting for pod client-containers-e9213a62-746b-4356-bce3-12c3337f8908 to disappear
    Jan 29 04:00:16.810: INFO: Pod client-containers-e9213a62-746b-4356-bce3-12c3337f8908 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Jan 29 04:00:16.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-6580" for this suite. 01/29/23 04:00:16.818
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:00:16.828
Jan 29 04:00:16.828: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename projected 01/29/23 04:00:16.829
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:00:16.859
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:00:16.864
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
STEP: Creating configMap with name projected-configmap-test-volume-map-b0e7d9e8-041a-4028-a46d-62a35343b294 01/29/23 04:00:16.869
STEP: Creating a pod to test consume configMaps 01/29/23 04:00:16.876
Jan 29 04:00:16.892: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-476283a8-7f44-4756-a0fc-a2c1e8569c7b" in namespace "projected-54" to be "Succeeded or Failed"
Jan 29 04:00:16.898: INFO: Pod "pod-projected-configmaps-476283a8-7f44-4756-a0fc-a2c1e8569c7b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.989442ms
Jan 29 04:00:18.904: INFO: Pod "pod-projected-configmaps-476283a8-7f44-4756-a0fc-a2c1e8569c7b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012674722s
Jan 29 04:00:20.905: INFO: Pod "pod-projected-configmaps-476283a8-7f44-4756-a0fc-a2c1e8569c7b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0131759s
STEP: Saw pod success 01/29/23 04:00:20.905
Jan 29 04:00:20.905: INFO: Pod "pod-projected-configmaps-476283a8-7f44-4756-a0fc-a2c1e8569c7b" satisfied condition "Succeeded or Failed"
Jan 29 04:00:20.911: INFO: Trying to get logs from node slave2 pod pod-projected-configmaps-476283a8-7f44-4756-a0fc-a2c1e8569c7b container agnhost-container: <nil>
STEP: delete the pod 01/29/23 04:00:20.925
Jan 29 04:00:21.005: INFO: Waiting for pod pod-projected-configmaps-476283a8-7f44-4756-a0fc-a2c1e8569c7b to disappear
Jan 29 04:00:21.010: INFO: Pod pod-projected-configmaps-476283a8-7f44-4756-a0fc-a2c1e8569c7b no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 29 04:00:21.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-54" for this suite. 01/29/23 04:00:21.02
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":236,"skipped":4109,"failed":0}
------------------------------
• [4.200 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:00:16.828
    Jan 29 04:00:16.828: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename projected 01/29/23 04:00:16.829
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:00:16.859
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:00:16.864
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:88
    STEP: Creating configMap with name projected-configmap-test-volume-map-b0e7d9e8-041a-4028-a46d-62a35343b294 01/29/23 04:00:16.869
    STEP: Creating a pod to test consume configMaps 01/29/23 04:00:16.876
    Jan 29 04:00:16.892: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-476283a8-7f44-4756-a0fc-a2c1e8569c7b" in namespace "projected-54" to be "Succeeded or Failed"
    Jan 29 04:00:16.898: INFO: Pod "pod-projected-configmaps-476283a8-7f44-4756-a0fc-a2c1e8569c7b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.989442ms
    Jan 29 04:00:18.904: INFO: Pod "pod-projected-configmaps-476283a8-7f44-4756-a0fc-a2c1e8569c7b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012674722s
    Jan 29 04:00:20.905: INFO: Pod "pod-projected-configmaps-476283a8-7f44-4756-a0fc-a2c1e8569c7b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0131759s
    STEP: Saw pod success 01/29/23 04:00:20.905
    Jan 29 04:00:20.905: INFO: Pod "pod-projected-configmaps-476283a8-7f44-4756-a0fc-a2c1e8569c7b" satisfied condition "Succeeded or Failed"
    Jan 29 04:00:20.911: INFO: Trying to get logs from node slave2 pod pod-projected-configmaps-476283a8-7f44-4756-a0fc-a2c1e8569c7b container agnhost-container: <nil>
    STEP: delete the pod 01/29/23 04:00:20.925
    Jan 29 04:00:21.005: INFO: Waiting for pod pod-projected-configmaps-476283a8-7f44-4756-a0fc-a2c1e8569c7b to disappear
    Jan 29 04:00:21.010: INFO: Pod pod-projected-configmaps-476283a8-7f44-4756-a0fc-a2c1e8569c7b no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 29 04:00:21.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-54" for this suite. 01/29/23 04:00:21.02
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:00:21.03
Jan 29 04:00:21.030: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename statefulset 01/29/23 04:00:21.031
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:00:21.056
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:00:21.062
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-930 01/29/23 04:00:21.067
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
STEP: Initializing watcher for selector baz=blah,foo=bar 01/29/23 04:00:21.074
STEP: Creating stateful set ss in namespace statefulset-930 01/29/23 04:00:21.081
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-930 01/29/23 04:00:21.094
Jan 29 04:00:21.100: INFO: Found 0 stateful pods, waiting for 1
Jan 29 04:00:31.107: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 01/29/23 04:00:31.107
Jan 29 04:00:31.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=statefulset-930 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 29 04:00:31.353: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 29 04:00:31.353: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 29 04:00:31.353: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 29 04:00:31.360: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan 29 04:00:41.369: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 29 04:00:41.369: INFO: Waiting for statefulset status.replicas updated to 0
Jan 29 04:00:41.392: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.99999962s
Jan 29 04:00:42.399: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.994237543s
Jan 29 04:00:43.409: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.987015435s
Jan 29 04:00:44.415: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.977585492s
Jan 29 04:00:45.422: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.971264251s
Jan 29 04:00:46.429: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.964189465s
Jan 29 04:00:47.435: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.957541661s
Jan 29 04:00:48.442: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.951121659s
Jan 29 04:00:49.449: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.944180294s
Jan 29 04:00:50.455: INFO: Verifying statefulset ss doesn't scale past 1 for another 937.267329ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-930 01/29/23 04:00:51.456
Jan 29 04:00:51.465: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=statefulset-930 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 29 04:00:51.709: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 29 04:00:51.709: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 29 04:00:51.709: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 29 04:00:51.715: INFO: Found 1 stateful pods, waiting for 3
Jan 29 04:01:01.727: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 29 04:01:01.727: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 29 04:01:01.727: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 01/29/23 04:01:01.727
STEP: Scale down will halt with unhealthy stateful pod 01/29/23 04:01:01.727
Jan 29 04:01:01.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=statefulset-930 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 29 04:01:01.980: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 29 04:01:01.980: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 29 04:01:01.980: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 29 04:01:01.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=statefulset-930 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 29 04:01:02.232: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 29 04:01:02.232: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 29 04:01:02.232: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 29 04:01:02.232: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=statefulset-930 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 29 04:01:02.553: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 29 04:01:02.553: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 29 04:01:02.553: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 29 04:01:02.553: INFO: Waiting for statefulset status.replicas updated to 0
Jan 29 04:01:02.560: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jan 29 04:01:12.574: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 29 04:01:12.574: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan 29 04:01:12.574: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan 29 04:01:12.598: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999954s
Jan 29 04:01:13.605: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.990154114s
Jan 29 04:01:14.611: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.983618952s
Jan 29 04:01:15.618: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.97713311s
Jan 29 04:01:16.626: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.970200404s
Jan 29 04:01:17.633: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.962303172s
Jan 29 04:01:18.640: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.955080505s
Jan 29 04:01:19.648: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.948566602s
Jan 29 04:01:20.655: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.940798171s
Jan 29 04:01:21.662: INFO: Verifying statefulset ss doesn't scale past 3 for another 933.494723ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-930 01/29/23 04:01:22.663
Jan 29 04:01:22.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=statefulset-930 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 29 04:01:22.903: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 29 04:01:22.903: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 29 04:01:22.903: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 29 04:01:22.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=statefulset-930 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 29 04:01:23.153: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 29 04:01:23.153: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 29 04:01:23.153: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 29 04:01:23.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=statefulset-930 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 29 04:01:23.417: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 29 04:01:23.417: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 29 04:01:23.417: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 29 04:01:23.417: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 01/29/23 04:01:33.449
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 29 04:01:33.449: INFO: Deleting all statefulset in ns statefulset-930
Jan 29 04:01:33.455: INFO: Scaling statefulset ss to 0
Jan 29 04:01:33.472: INFO: Waiting for statefulset status.replicas updated to 0
Jan 29 04:01:33.478: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan 29 04:01:33.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-930" for this suite. 01/29/23 04:01:33.517
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","completed":237,"skipped":4131,"failed":0}
------------------------------
• [SLOW TEST] [72.496 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:585

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:00:21.03
    Jan 29 04:00:21.030: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename statefulset 01/29/23 04:00:21.031
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:00:21.056
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:00:21.062
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-930 01/29/23 04:00:21.067
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:585
    STEP: Initializing watcher for selector baz=blah,foo=bar 01/29/23 04:00:21.074
    STEP: Creating stateful set ss in namespace statefulset-930 01/29/23 04:00:21.081
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-930 01/29/23 04:00:21.094
    Jan 29 04:00:21.100: INFO: Found 0 stateful pods, waiting for 1
    Jan 29 04:00:31.107: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 01/29/23 04:00:31.107
    Jan 29 04:00:31.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=statefulset-930 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 29 04:00:31.353: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 29 04:00:31.353: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 29 04:00:31.353: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 29 04:00:31.360: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Jan 29 04:00:41.369: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan 29 04:00:41.369: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 29 04:00:41.392: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.99999962s
    Jan 29 04:00:42.399: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.994237543s
    Jan 29 04:00:43.409: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.987015435s
    Jan 29 04:00:44.415: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.977585492s
    Jan 29 04:00:45.422: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.971264251s
    Jan 29 04:00:46.429: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.964189465s
    Jan 29 04:00:47.435: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.957541661s
    Jan 29 04:00:48.442: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.951121659s
    Jan 29 04:00:49.449: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.944180294s
    Jan 29 04:00:50.455: INFO: Verifying statefulset ss doesn't scale past 1 for another 937.267329ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-930 01/29/23 04:00:51.456
    Jan 29 04:00:51.465: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=statefulset-930 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 29 04:00:51.709: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 29 04:00:51.709: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 29 04:00:51.709: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 29 04:00:51.715: INFO: Found 1 stateful pods, waiting for 3
    Jan 29 04:01:01.727: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 29 04:01:01.727: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 29 04:01:01.727: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 01/29/23 04:01:01.727
    STEP: Scale down will halt with unhealthy stateful pod 01/29/23 04:01:01.727
    Jan 29 04:01:01.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=statefulset-930 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 29 04:01:01.980: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 29 04:01:01.980: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 29 04:01:01.980: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 29 04:01:01.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=statefulset-930 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 29 04:01:02.232: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 29 04:01:02.232: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 29 04:01:02.232: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 29 04:01:02.232: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=statefulset-930 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 29 04:01:02.553: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 29 04:01:02.553: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 29 04:01:02.553: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 29 04:01:02.553: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 29 04:01:02.560: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
    Jan 29 04:01:12.574: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan 29 04:01:12.574: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Jan 29 04:01:12.574: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Jan 29 04:01:12.598: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999954s
    Jan 29 04:01:13.605: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.990154114s
    Jan 29 04:01:14.611: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.983618952s
    Jan 29 04:01:15.618: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.97713311s
    Jan 29 04:01:16.626: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.970200404s
    Jan 29 04:01:17.633: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.962303172s
    Jan 29 04:01:18.640: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.955080505s
    Jan 29 04:01:19.648: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.948566602s
    Jan 29 04:01:20.655: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.940798171s
    Jan 29 04:01:21.662: INFO: Verifying statefulset ss doesn't scale past 3 for another 933.494723ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-930 01/29/23 04:01:22.663
    Jan 29 04:01:22.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=statefulset-930 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 29 04:01:22.903: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 29 04:01:22.903: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 29 04:01:22.903: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 29 04:01:22.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=statefulset-930 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 29 04:01:23.153: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 29 04:01:23.153: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 29 04:01:23.153: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 29 04:01:23.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=statefulset-930 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 29 04:01:23.417: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 29 04:01:23.417: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 29 04:01:23.417: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 29 04:01:23.417: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 01/29/23 04:01:33.449
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan 29 04:01:33.449: INFO: Deleting all statefulset in ns statefulset-930
    Jan 29 04:01:33.455: INFO: Scaling statefulset ss to 0
    Jan 29 04:01:33.472: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 29 04:01:33.478: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan 29 04:01:33.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-930" for this suite. 01/29/23 04:01:33.517
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:01:33.527
Jan 29 04:01:33.527: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename configmap 01/29/23 04:01:33.528
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:01:33.557
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:01:33.562
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
STEP: Creating configMap with name configmap-test-volume-map-0c248292-ecda-436d-a723-45d744c6b6a3 01/29/23 04:01:33.567
STEP: Creating a pod to test consume configMaps 01/29/23 04:01:33.574
Jan 29 04:01:33.590: INFO: Waiting up to 5m0s for pod "pod-configmaps-195302eb-20c7-4dc0-bbea-55041fe1dd4d" in namespace "configmap-8448" to be "Succeeded or Failed"
Jan 29 04:01:33.595: INFO: Pod "pod-configmaps-195302eb-20c7-4dc0-bbea-55041fe1dd4d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.494418ms
Jan 29 04:01:35.602: INFO: Pod "pod-configmaps-195302eb-20c7-4dc0-bbea-55041fe1dd4d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011933057s
Jan 29 04:01:37.602: INFO: Pod "pod-configmaps-195302eb-20c7-4dc0-bbea-55041fe1dd4d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012362014s
Jan 29 04:01:39.603: INFO: Pod "pod-configmaps-195302eb-20c7-4dc0-bbea-55041fe1dd4d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013269794s
STEP: Saw pod success 01/29/23 04:01:39.603
Jan 29 04:01:39.603: INFO: Pod "pod-configmaps-195302eb-20c7-4dc0-bbea-55041fe1dd4d" satisfied condition "Succeeded or Failed"
Jan 29 04:01:39.609: INFO: Trying to get logs from node slave2 pod pod-configmaps-195302eb-20c7-4dc0-bbea-55041fe1dd4d container agnhost-container: <nil>
STEP: delete the pod 01/29/23 04:01:39.624
Jan 29 04:01:39.722: INFO: Waiting for pod pod-configmaps-195302eb-20c7-4dc0-bbea-55041fe1dd4d to disappear
Jan 29 04:01:39.727: INFO: Pod pod-configmaps-195302eb-20c7-4dc0-bbea-55041fe1dd4d no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 29 04:01:39.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8448" for this suite. 01/29/23 04:01:39.736
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":238,"skipped":4139,"failed":0}
------------------------------
• [SLOW TEST] [6.218 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:01:33.527
    Jan 29 04:01:33.527: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename configmap 01/29/23 04:01:33.528
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:01:33.557
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:01:33.562
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:108
    STEP: Creating configMap with name configmap-test-volume-map-0c248292-ecda-436d-a723-45d744c6b6a3 01/29/23 04:01:33.567
    STEP: Creating a pod to test consume configMaps 01/29/23 04:01:33.574
    Jan 29 04:01:33.590: INFO: Waiting up to 5m0s for pod "pod-configmaps-195302eb-20c7-4dc0-bbea-55041fe1dd4d" in namespace "configmap-8448" to be "Succeeded or Failed"
    Jan 29 04:01:33.595: INFO: Pod "pod-configmaps-195302eb-20c7-4dc0-bbea-55041fe1dd4d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.494418ms
    Jan 29 04:01:35.602: INFO: Pod "pod-configmaps-195302eb-20c7-4dc0-bbea-55041fe1dd4d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011933057s
    Jan 29 04:01:37.602: INFO: Pod "pod-configmaps-195302eb-20c7-4dc0-bbea-55041fe1dd4d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012362014s
    Jan 29 04:01:39.603: INFO: Pod "pod-configmaps-195302eb-20c7-4dc0-bbea-55041fe1dd4d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013269794s
    STEP: Saw pod success 01/29/23 04:01:39.603
    Jan 29 04:01:39.603: INFO: Pod "pod-configmaps-195302eb-20c7-4dc0-bbea-55041fe1dd4d" satisfied condition "Succeeded or Failed"
    Jan 29 04:01:39.609: INFO: Trying to get logs from node slave2 pod pod-configmaps-195302eb-20c7-4dc0-bbea-55041fe1dd4d container agnhost-container: <nil>
    STEP: delete the pod 01/29/23 04:01:39.624
    Jan 29 04:01:39.722: INFO: Waiting for pod pod-configmaps-195302eb-20c7-4dc0-bbea-55041fe1dd4d to disappear
    Jan 29 04:01:39.727: INFO: Pod pod-configmaps-195302eb-20c7-4dc0-bbea-55041fe1dd4d no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 29 04:01:39.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-8448" for this suite. 01/29/23 04:01:39.736
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:01:39.747
Jan 29 04:01:39.747: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename kubectl 01/29/23 04:01:39.749
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:01:39.785
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:01:39.792
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1570
STEP: creating an pod 01/29/23 04:01:39.798
Jan 29 04:01:39.798: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-5920 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Jan 29 04:01:39.944: INFO: stderr: ""
Jan 29 04:01:39.944: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
STEP: Waiting for log generator to start. 01/29/23 04:01:39.944
Jan 29 04:01:39.944: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Jan 29 04:01:39.944: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-5920" to be "running and ready, or succeeded"
Jan 29 04:01:39.951: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 6.342525ms
Jan 29 04:01:39.951: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'slave2' to be 'Running' but was 'Pending'
Jan 29 04:01:41.959: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014424995s
Jan 29 04:01:41.959: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'slave2' to be 'Running' but was 'Pending'
Jan 29 04:01:43.958: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.013813104s
Jan 29 04:01:43.958: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Jan 29 04:01:43.958: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 01/29/23 04:01:43.958
Jan 29 04:01:43.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-5920 logs logs-generator logs-generator'
Jan 29 04:01:44.086: INFO: stderr: ""
Jan 29 04:01:44.086: INFO: stdout: "I0129 04:01:41.097750       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/6xwx 521\nI0129 04:01:41.298281       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/2ds 273\nI0129 04:01:41.498695       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/xft 267\nI0129 04:01:41.698024       1 logs_generator.go:76] 3 GET /api/v1/namespaces/kube-system/pods/vtrg 468\nI0129 04:01:41.898570       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/kfkb 273\nI0129 04:01:42.097923       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/df5 407\nI0129 04:01:42.298395       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/dz2g 280\nI0129 04:01:42.498823       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/5mb 278\nI0129 04:01:42.698282       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/pvl 477\nI0129 04:01:42.898710       1 logs_generator.go:76] 9 GET /api/v1/namespaces/kube-system/pods/2m4 437\nI0129 04:01:43.098013       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/ns/pods/khv 585\nI0129 04:01:43.298461       1 logs_generator.go:76] 11 GET /api/v1/namespaces/kube-system/pods/5zq 595\nI0129 04:01:43.497816       1 logs_generator.go:76] 12 POST /api/v1/namespaces/ns/pods/fs5 406\nI0129 04:01:43.698244       1 logs_generator.go:76] 13 POST /api/v1/namespaces/default/pods/qmr 398\nI0129 04:01:43.898702       1 logs_generator.go:76] 14 GET /api/v1/namespaces/kube-system/pods/t5v 297\n"
STEP: limiting log lines 01/29/23 04:01:44.086
Jan 29 04:01:44.086: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-5920 logs logs-generator logs-generator --tail=1'
Jan 29 04:01:44.233: INFO: stderr: ""
Jan 29 04:01:44.233: INFO: stdout: "I0129 04:01:44.097915       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/8h8w 452\n"
Jan 29 04:01:44.233: INFO: got output "I0129 04:01:44.097915       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/8h8w 452\n"
STEP: limiting log bytes 01/29/23 04:01:44.233
Jan 29 04:01:44.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-5920 logs logs-generator logs-generator --limit-bytes=1'
Jan 29 04:01:44.358: INFO: stderr: ""
Jan 29 04:01:44.359: INFO: stdout: "I"
Jan 29 04:01:44.359: INFO: got output "I"
STEP: exposing timestamps 01/29/23 04:01:44.359
Jan 29 04:01:44.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-5920 logs logs-generator logs-generator --tail=1 --timestamps'
Jan 29 04:01:44.479: INFO: stderr: ""
Jan 29 04:01:44.479: INFO: stdout: "2023-01-29T04:01:44.298536846Z I0129 04:01:44.298343       1 logs_generator.go:76] 16 POST /api/v1/namespaces/kube-system/pods/zfh 474\n"
Jan 29 04:01:44.479: INFO: got output "2023-01-29T04:01:44.298536846Z I0129 04:01:44.298343       1 logs_generator.go:76] 16 POST /api/v1/namespaces/kube-system/pods/zfh 474\n"
STEP: restricting to a time range 01/29/23 04:01:44.479
Jan 29 04:01:46.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-5920 logs logs-generator logs-generator --since=1s'
Jan 29 04:01:47.121: INFO: stderr: ""
Jan 29 04:01:47.121: INFO: stdout: "I0129 04:01:46.298603       1 logs_generator.go:76] 26 PUT /api/v1/namespaces/ns/pods/flv 409\nI0129 04:01:46.497935       1 logs_generator.go:76] 27 POST /api/v1/namespaces/ns/pods/gbd5 453\nI0129 04:01:46.698366       1 logs_generator.go:76] 28 GET /api/v1/namespaces/default/pods/q6d2 275\nI0129 04:01:46.898795       1 logs_generator.go:76] 29 PUT /api/v1/namespaces/ns/pods/5dd 315\nI0129 04:01:47.098228       1 logs_generator.go:76] 30 GET /api/v1/namespaces/ns/pods/92z 364\n"
Jan 29 04:01:47.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-5920 logs logs-generator logs-generator --since=24h'
Jan 29 04:01:47.247: INFO: stderr: ""
Jan 29 04:01:47.247: INFO: stdout: "I0129 04:01:41.097750       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/6xwx 521\nI0129 04:01:41.298281       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/2ds 273\nI0129 04:01:41.498695       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/xft 267\nI0129 04:01:41.698024       1 logs_generator.go:76] 3 GET /api/v1/namespaces/kube-system/pods/vtrg 468\nI0129 04:01:41.898570       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/kfkb 273\nI0129 04:01:42.097923       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/df5 407\nI0129 04:01:42.298395       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/dz2g 280\nI0129 04:01:42.498823       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/5mb 278\nI0129 04:01:42.698282       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/pvl 477\nI0129 04:01:42.898710       1 logs_generator.go:76] 9 GET /api/v1/namespaces/kube-system/pods/2m4 437\nI0129 04:01:43.098013       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/ns/pods/khv 585\nI0129 04:01:43.298461       1 logs_generator.go:76] 11 GET /api/v1/namespaces/kube-system/pods/5zq 595\nI0129 04:01:43.497816       1 logs_generator.go:76] 12 POST /api/v1/namespaces/ns/pods/fs5 406\nI0129 04:01:43.698244       1 logs_generator.go:76] 13 POST /api/v1/namespaces/default/pods/qmr 398\nI0129 04:01:43.898702       1 logs_generator.go:76] 14 GET /api/v1/namespaces/kube-system/pods/t5v 297\nI0129 04:01:44.097915       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/8h8w 452\nI0129 04:01:44.298343       1 logs_generator.go:76] 16 POST /api/v1/namespaces/kube-system/pods/zfh 474\nI0129 04:01:44.498774       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/kube-system/pods/hm5 593\nI0129 04:01:44.698190       1 logs_generator.go:76] 18 POST /api/v1/namespaces/default/pods/c9b6 573\nI0129 04:01:44.898617       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/6t4j 323\nI0129 04:01:45.097940       1 logs_generator.go:76] 20 GET /api/v1/namespaces/kube-system/pods/cl2 538\nI0129 04:01:45.298358       1 logs_generator.go:76] 21 GET /api/v1/namespaces/default/pods/pw6 496\nI0129 04:01:45.498819       1 logs_generator.go:76] 22 POST /api/v1/namespaces/default/pods/z442 263\nI0129 04:01:45.698268       1 logs_generator.go:76] 23 GET /api/v1/namespaces/default/pods/phf9 270\nI0129 04:01:45.898681       1 logs_generator.go:76] 24 POST /api/v1/namespaces/ns/pods/c59 459\nI0129 04:01:46.098119       1 logs_generator.go:76] 25 PUT /api/v1/namespaces/kube-system/pods/zpdw 268\nI0129 04:01:46.298603       1 logs_generator.go:76] 26 PUT /api/v1/namespaces/ns/pods/flv 409\nI0129 04:01:46.497935       1 logs_generator.go:76] 27 POST /api/v1/namespaces/ns/pods/gbd5 453\nI0129 04:01:46.698366       1 logs_generator.go:76] 28 GET /api/v1/namespaces/default/pods/q6d2 275\nI0129 04:01:46.898795       1 logs_generator.go:76] 29 PUT /api/v1/namespaces/ns/pods/5dd 315\nI0129 04:01:47.098228       1 logs_generator.go:76] 30 GET /api/v1/namespaces/ns/pods/92z 364\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1575
Jan 29 04:01:47.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-5920 delete pod logs-generator'
Jan 29 04:01:48.528: INFO: stderr: ""
Jan 29 04:01:48.528: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 29 04:01:48.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5920" for this suite. 01/29/23 04:01:48.536
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","completed":239,"skipped":4170,"failed":0}
------------------------------
• [SLOW TEST] [8.797 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1567
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1590

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:01:39.747
    Jan 29 04:01:39.747: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename kubectl 01/29/23 04:01:39.749
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:01:39.785
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:01:39.792
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1570
    STEP: creating an pod 01/29/23 04:01:39.798
    Jan 29 04:01:39.798: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-5920 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Jan 29 04:01:39.944: INFO: stderr: ""
    Jan 29 04:01:39.944: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1590
    STEP: Waiting for log generator to start. 01/29/23 04:01:39.944
    Jan 29 04:01:39.944: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Jan 29 04:01:39.944: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-5920" to be "running and ready, or succeeded"
    Jan 29 04:01:39.951: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 6.342525ms
    Jan 29 04:01:39.951: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'slave2' to be 'Running' but was 'Pending'
    Jan 29 04:01:41.959: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014424995s
    Jan 29 04:01:41.959: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'slave2' to be 'Running' but was 'Pending'
    Jan 29 04:01:43.958: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.013813104s
    Jan 29 04:01:43.958: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Jan 29 04:01:43.958: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 01/29/23 04:01:43.958
    Jan 29 04:01:43.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-5920 logs logs-generator logs-generator'
    Jan 29 04:01:44.086: INFO: stderr: ""
    Jan 29 04:01:44.086: INFO: stdout: "I0129 04:01:41.097750       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/6xwx 521\nI0129 04:01:41.298281       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/2ds 273\nI0129 04:01:41.498695       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/xft 267\nI0129 04:01:41.698024       1 logs_generator.go:76] 3 GET /api/v1/namespaces/kube-system/pods/vtrg 468\nI0129 04:01:41.898570       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/kfkb 273\nI0129 04:01:42.097923       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/df5 407\nI0129 04:01:42.298395       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/dz2g 280\nI0129 04:01:42.498823       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/5mb 278\nI0129 04:01:42.698282       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/pvl 477\nI0129 04:01:42.898710       1 logs_generator.go:76] 9 GET /api/v1/namespaces/kube-system/pods/2m4 437\nI0129 04:01:43.098013       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/ns/pods/khv 585\nI0129 04:01:43.298461       1 logs_generator.go:76] 11 GET /api/v1/namespaces/kube-system/pods/5zq 595\nI0129 04:01:43.497816       1 logs_generator.go:76] 12 POST /api/v1/namespaces/ns/pods/fs5 406\nI0129 04:01:43.698244       1 logs_generator.go:76] 13 POST /api/v1/namespaces/default/pods/qmr 398\nI0129 04:01:43.898702       1 logs_generator.go:76] 14 GET /api/v1/namespaces/kube-system/pods/t5v 297\n"
    STEP: limiting log lines 01/29/23 04:01:44.086
    Jan 29 04:01:44.086: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-5920 logs logs-generator logs-generator --tail=1'
    Jan 29 04:01:44.233: INFO: stderr: ""
    Jan 29 04:01:44.233: INFO: stdout: "I0129 04:01:44.097915       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/8h8w 452\n"
    Jan 29 04:01:44.233: INFO: got output "I0129 04:01:44.097915       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/8h8w 452\n"
    STEP: limiting log bytes 01/29/23 04:01:44.233
    Jan 29 04:01:44.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-5920 logs logs-generator logs-generator --limit-bytes=1'
    Jan 29 04:01:44.358: INFO: stderr: ""
    Jan 29 04:01:44.359: INFO: stdout: "I"
    Jan 29 04:01:44.359: INFO: got output "I"
    STEP: exposing timestamps 01/29/23 04:01:44.359
    Jan 29 04:01:44.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-5920 logs logs-generator logs-generator --tail=1 --timestamps'
    Jan 29 04:01:44.479: INFO: stderr: ""
    Jan 29 04:01:44.479: INFO: stdout: "2023-01-29T04:01:44.298536846Z I0129 04:01:44.298343       1 logs_generator.go:76] 16 POST /api/v1/namespaces/kube-system/pods/zfh 474\n"
    Jan 29 04:01:44.479: INFO: got output "2023-01-29T04:01:44.298536846Z I0129 04:01:44.298343       1 logs_generator.go:76] 16 POST /api/v1/namespaces/kube-system/pods/zfh 474\n"
    STEP: restricting to a time range 01/29/23 04:01:44.479
    Jan 29 04:01:46.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-5920 logs logs-generator logs-generator --since=1s'
    Jan 29 04:01:47.121: INFO: stderr: ""
    Jan 29 04:01:47.121: INFO: stdout: "I0129 04:01:46.298603       1 logs_generator.go:76] 26 PUT /api/v1/namespaces/ns/pods/flv 409\nI0129 04:01:46.497935       1 logs_generator.go:76] 27 POST /api/v1/namespaces/ns/pods/gbd5 453\nI0129 04:01:46.698366       1 logs_generator.go:76] 28 GET /api/v1/namespaces/default/pods/q6d2 275\nI0129 04:01:46.898795       1 logs_generator.go:76] 29 PUT /api/v1/namespaces/ns/pods/5dd 315\nI0129 04:01:47.098228       1 logs_generator.go:76] 30 GET /api/v1/namespaces/ns/pods/92z 364\n"
    Jan 29 04:01:47.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-5920 logs logs-generator logs-generator --since=24h'
    Jan 29 04:01:47.247: INFO: stderr: ""
    Jan 29 04:01:47.247: INFO: stdout: "I0129 04:01:41.097750       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/6xwx 521\nI0129 04:01:41.298281       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/2ds 273\nI0129 04:01:41.498695       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/xft 267\nI0129 04:01:41.698024       1 logs_generator.go:76] 3 GET /api/v1/namespaces/kube-system/pods/vtrg 468\nI0129 04:01:41.898570       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/kfkb 273\nI0129 04:01:42.097923       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/df5 407\nI0129 04:01:42.298395       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/dz2g 280\nI0129 04:01:42.498823       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/5mb 278\nI0129 04:01:42.698282       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/pvl 477\nI0129 04:01:42.898710       1 logs_generator.go:76] 9 GET /api/v1/namespaces/kube-system/pods/2m4 437\nI0129 04:01:43.098013       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/ns/pods/khv 585\nI0129 04:01:43.298461       1 logs_generator.go:76] 11 GET /api/v1/namespaces/kube-system/pods/5zq 595\nI0129 04:01:43.497816       1 logs_generator.go:76] 12 POST /api/v1/namespaces/ns/pods/fs5 406\nI0129 04:01:43.698244       1 logs_generator.go:76] 13 POST /api/v1/namespaces/default/pods/qmr 398\nI0129 04:01:43.898702       1 logs_generator.go:76] 14 GET /api/v1/namespaces/kube-system/pods/t5v 297\nI0129 04:01:44.097915       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/8h8w 452\nI0129 04:01:44.298343       1 logs_generator.go:76] 16 POST /api/v1/namespaces/kube-system/pods/zfh 474\nI0129 04:01:44.498774       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/kube-system/pods/hm5 593\nI0129 04:01:44.698190       1 logs_generator.go:76] 18 POST /api/v1/namespaces/default/pods/c9b6 573\nI0129 04:01:44.898617       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/6t4j 323\nI0129 04:01:45.097940       1 logs_generator.go:76] 20 GET /api/v1/namespaces/kube-system/pods/cl2 538\nI0129 04:01:45.298358       1 logs_generator.go:76] 21 GET /api/v1/namespaces/default/pods/pw6 496\nI0129 04:01:45.498819       1 logs_generator.go:76] 22 POST /api/v1/namespaces/default/pods/z442 263\nI0129 04:01:45.698268       1 logs_generator.go:76] 23 GET /api/v1/namespaces/default/pods/phf9 270\nI0129 04:01:45.898681       1 logs_generator.go:76] 24 POST /api/v1/namespaces/ns/pods/c59 459\nI0129 04:01:46.098119       1 logs_generator.go:76] 25 PUT /api/v1/namespaces/kube-system/pods/zpdw 268\nI0129 04:01:46.298603       1 logs_generator.go:76] 26 PUT /api/v1/namespaces/ns/pods/flv 409\nI0129 04:01:46.497935       1 logs_generator.go:76] 27 POST /api/v1/namespaces/ns/pods/gbd5 453\nI0129 04:01:46.698366       1 logs_generator.go:76] 28 GET /api/v1/namespaces/default/pods/q6d2 275\nI0129 04:01:46.898795       1 logs_generator.go:76] 29 PUT /api/v1/namespaces/ns/pods/5dd 315\nI0129 04:01:47.098228       1 logs_generator.go:76] 30 GET /api/v1/namespaces/ns/pods/92z 364\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1575
    Jan 29 04:01:47.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-5920 delete pod logs-generator'
    Jan 29 04:01:48.528: INFO: stderr: ""
    Jan 29 04:01:48.528: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 29 04:01:48.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-5920" for this suite. 01/29/23 04:01:48.536
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:01:48.545
Jan 29 04:01:48.545: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename namespaces 01/29/23 04:01:48.547
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:01:48.594
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:01:48.599
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
STEP: Read namespace status 01/29/23 04:01:48.603
Jan 29 04:01:48.611: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 01/29/23 04:01:48.611
Jan 29 04:01:48.618: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 01/29/23 04:01:48.618
Jan 29 04:01:48.630: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Jan 29 04:01:48.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-8459" for this suite. 01/29/23 04:01:48.639
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]","completed":240,"skipped":4171,"failed":0}
------------------------------
• [0.105 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:01:48.545
    Jan 29 04:01:48.545: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename namespaces 01/29/23 04:01:48.547
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:01:48.594
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:01:48.599
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:298
    STEP: Read namespace status 01/29/23 04:01:48.603
    Jan 29 04:01:48.611: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 01/29/23 04:01:48.611
    Jan 29 04:01:48.618: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 01/29/23 04:01:48.618
    Jan 29 04:01:48.630: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Jan 29 04:01:48.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-8459" for this suite. 01/29/23 04:01:48.639
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:01:48.651
Jan 29 04:01:48.651: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename emptydir-wrapper 01/29/23 04:01:48.653
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:01:48.685
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:01:48.69
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Jan 29 04:01:48.723: INFO: Waiting up to 5m0s for pod "pod-secrets-f72fae89-ce4a-4f10-a82c-108a5b26cc20" in namespace "emptydir-wrapper-2860" to be "running and ready"
Jan 29 04:01:48.730: INFO: Pod "pod-secrets-f72fae89-ce4a-4f10-a82c-108a5b26cc20": Phase="Pending", Reason="", readiness=false. Elapsed: 6.369885ms
Jan 29 04:01:48.730: INFO: The phase of Pod pod-secrets-f72fae89-ce4a-4f10-a82c-108a5b26cc20 is Pending, waiting for it to be Running (with Ready = true)
Jan 29 04:01:50.737: INFO: Pod "pod-secrets-f72fae89-ce4a-4f10-a82c-108a5b26cc20": Phase="Running", Reason="", readiness=true. Elapsed: 2.013770323s
Jan 29 04:01:50.737: INFO: The phase of Pod pod-secrets-f72fae89-ce4a-4f10-a82c-108a5b26cc20 is Running (Ready = true)
Jan 29 04:01:50.737: INFO: Pod "pod-secrets-f72fae89-ce4a-4f10-a82c-108a5b26cc20" satisfied condition "running and ready"
STEP: Cleaning up the secret 01/29/23 04:01:50.743
STEP: Cleaning up the configmap 01/29/23 04:01:50.753
STEP: Cleaning up the pod 01/29/23 04:01:50.762
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
Jan 29 04:01:50.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-2860" for this suite. 01/29/23 04:01:50.843
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","completed":241,"skipped":4173,"failed":0}
------------------------------
• [2.201 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:01:48.651
    Jan 29 04:01:48.651: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename emptydir-wrapper 01/29/23 04:01:48.653
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:01:48.685
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:01:48.69
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Jan 29 04:01:48.723: INFO: Waiting up to 5m0s for pod "pod-secrets-f72fae89-ce4a-4f10-a82c-108a5b26cc20" in namespace "emptydir-wrapper-2860" to be "running and ready"
    Jan 29 04:01:48.730: INFO: Pod "pod-secrets-f72fae89-ce4a-4f10-a82c-108a5b26cc20": Phase="Pending", Reason="", readiness=false. Elapsed: 6.369885ms
    Jan 29 04:01:48.730: INFO: The phase of Pod pod-secrets-f72fae89-ce4a-4f10-a82c-108a5b26cc20 is Pending, waiting for it to be Running (with Ready = true)
    Jan 29 04:01:50.737: INFO: Pod "pod-secrets-f72fae89-ce4a-4f10-a82c-108a5b26cc20": Phase="Running", Reason="", readiness=true. Elapsed: 2.013770323s
    Jan 29 04:01:50.737: INFO: The phase of Pod pod-secrets-f72fae89-ce4a-4f10-a82c-108a5b26cc20 is Running (Ready = true)
    Jan 29 04:01:50.737: INFO: Pod "pod-secrets-f72fae89-ce4a-4f10-a82c-108a5b26cc20" satisfied condition "running and ready"
    STEP: Cleaning up the secret 01/29/23 04:01:50.743
    STEP: Cleaning up the configmap 01/29/23 04:01:50.753
    STEP: Cleaning up the pod 01/29/23 04:01:50.762
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    Jan 29 04:01:50.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-2860" for this suite. 01/29/23 04:01:50.843
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:01:50.852
Jan 29 04:01:50.852: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename custom-resource-definition 01/29/23 04:01:50.854
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:01:50.882
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:01:50.891
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Jan 29 04:01:50.897: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 29 04:01:57.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8340" for this suite. 01/29/23 04:01:57.281
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","completed":242,"skipped":4174,"failed":0}
------------------------------
• [SLOW TEST] [6.438 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:01:50.852
    Jan 29 04:01:50.852: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename custom-resource-definition 01/29/23 04:01:50.854
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:01:50.882
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:01:50.891
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Jan 29 04:01:50.897: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 29 04:01:57.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-8340" for this suite. 01/29/23 04:01:57.281
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:01:57.292
Jan 29 04:01:57.292: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename emptydir 01/29/23 04:01:57.293
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:01:57.323
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:01:57.329
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
STEP: Creating a pod to test emptydir 0666 on tmpfs 01/29/23 04:01:57.337
Jan 29 04:01:57.355: INFO: Waiting up to 5m0s for pod "pod-ee28c492-4db8-4709-8451-6ffa28ee5c6e" in namespace "emptydir-6071" to be "Succeeded or Failed"
Jan 29 04:01:57.361: INFO: Pod "pod-ee28c492-4db8-4709-8451-6ffa28ee5c6e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.978262ms
Jan 29 04:01:59.368: INFO: Pod "pod-ee28c492-4db8-4709-8451-6ffa28ee5c6e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012750036s
Jan 29 04:02:01.369: INFO: Pod "pod-ee28c492-4db8-4709-8451-6ffa28ee5c6e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01382001s
STEP: Saw pod success 01/29/23 04:02:01.369
Jan 29 04:02:01.369: INFO: Pod "pod-ee28c492-4db8-4709-8451-6ffa28ee5c6e" satisfied condition "Succeeded or Failed"
Jan 29 04:02:01.375: INFO: Trying to get logs from node slave2 pod pod-ee28c492-4db8-4709-8451-6ffa28ee5c6e container test-container: <nil>
STEP: delete the pod 01/29/23 04:02:01.389
Jan 29 04:02:01.487: INFO: Waiting for pod pod-ee28c492-4db8-4709-8451-6ffa28ee5c6e to disappear
Jan 29 04:02:01.493: INFO: Pod pod-ee28c492-4db8-4709-8451-6ffa28ee5c6e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 29 04:02:01.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6071" for this suite. 01/29/23 04:02:01.503
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":243,"skipped":4181,"failed":0}
------------------------------
• [4.220 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:01:57.292
    Jan 29 04:01:57.292: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename emptydir 01/29/23 04:01:57.293
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:01:57.323
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:01:57.329
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:106
    STEP: Creating a pod to test emptydir 0666 on tmpfs 01/29/23 04:01:57.337
    Jan 29 04:01:57.355: INFO: Waiting up to 5m0s for pod "pod-ee28c492-4db8-4709-8451-6ffa28ee5c6e" in namespace "emptydir-6071" to be "Succeeded or Failed"
    Jan 29 04:01:57.361: INFO: Pod "pod-ee28c492-4db8-4709-8451-6ffa28ee5c6e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.978262ms
    Jan 29 04:01:59.368: INFO: Pod "pod-ee28c492-4db8-4709-8451-6ffa28ee5c6e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012750036s
    Jan 29 04:02:01.369: INFO: Pod "pod-ee28c492-4db8-4709-8451-6ffa28ee5c6e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01382001s
    STEP: Saw pod success 01/29/23 04:02:01.369
    Jan 29 04:02:01.369: INFO: Pod "pod-ee28c492-4db8-4709-8451-6ffa28ee5c6e" satisfied condition "Succeeded or Failed"
    Jan 29 04:02:01.375: INFO: Trying to get logs from node slave2 pod pod-ee28c492-4db8-4709-8451-6ffa28ee5c6e container test-container: <nil>
    STEP: delete the pod 01/29/23 04:02:01.389
    Jan 29 04:02:01.487: INFO: Waiting for pod pod-ee28c492-4db8-4709-8451-6ffa28ee5c6e to disappear
    Jan 29 04:02:01.493: INFO: Pod pod-ee28c492-4db8-4709-8451-6ffa28ee5c6e no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 29 04:02:01.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-6071" for this suite. 01/29/23 04:02:01.503
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:02:01.512
Jan 29 04:02:01.513: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename secrets 01/29/23 04:02:01.514
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:02:01.555
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:02:01.563
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
STEP: Creating secret with name s-test-opt-del-8811f859-e356-480d-9971-b327bc048587 01/29/23 04:02:01.586
STEP: Creating secret with name s-test-opt-upd-f3807d33-d380-4e82-86c7-b8120334be9b 01/29/23 04:02:01.596
STEP: Creating the pod 01/29/23 04:02:01.61
Jan 29 04:02:01.641: INFO: Waiting up to 5m0s for pod "pod-secrets-13fc941e-26fe-49c2-a58f-4048132e1fd3" in namespace "secrets-5902" to be "running and ready"
Jan 29 04:02:01.650: INFO: Pod "pod-secrets-13fc941e-26fe-49c2-a58f-4048132e1fd3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.351398ms
Jan 29 04:02:01.650: INFO: The phase of Pod pod-secrets-13fc941e-26fe-49c2-a58f-4048132e1fd3 is Pending, waiting for it to be Running (with Ready = true)
Jan 29 04:02:03.659: INFO: Pod "pod-secrets-13fc941e-26fe-49c2-a58f-4048132e1fd3": Phase="Running", Reason="", readiness=true. Elapsed: 2.017529269s
Jan 29 04:02:03.659: INFO: The phase of Pod pod-secrets-13fc941e-26fe-49c2-a58f-4048132e1fd3 is Running (Ready = true)
Jan 29 04:02:03.659: INFO: Pod "pod-secrets-13fc941e-26fe-49c2-a58f-4048132e1fd3" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-8811f859-e356-480d-9971-b327bc048587 01/29/23 04:02:03.712
STEP: Updating secret s-test-opt-upd-f3807d33-d380-4e82-86c7-b8120334be9b 01/29/23 04:02:03.723
STEP: Creating secret with name s-test-opt-create-15a35f13-511a-4fbe-99e8-a0e3df0f5de5 01/29/23 04:02:03.73
STEP: waiting to observe update in volume 01/29/23 04:02:03.739
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 29 04:02:05.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5902" for this suite. 01/29/23 04:02:05.805
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":244,"skipped":4183,"failed":0}
------------------------------
• [4.301 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:02:01.512
    Jan 29 04:02:01.513: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename secrets 01/29/23 04:02:01.514
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:02:01.555
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:02:01.563
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:204
    STEP: Creating secret with name s-test-opt-del-8811f859-e356-480d-9971-b327bc048587 01/29/23 04:02:01.586
    STEP: Creating secret with name s-test-opt-upd-f3807d33-d380-4e82-86c7-b8120334be9b 01/29/23 04:02:01.596
    STEP: Creating the pod 01/29/23 04:02:01.61
    Jan 29 04:02:01.641: INFO: Waiting up to 5m0s for pod "pod-secrets-13fc941e-26fe-49c2-a58f-4048132e1fd3" in namespace "secrets-5902" to be "running and ready"
    Jan 29 04:02:01.650: INFO: Pod "pod-secrets-13fc941e-26fe-49c2-a58f-4048132e1fd3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.351398ms
    Jan 29 04:02:01.650: INFO: The phase of Pod pod-secrets-13fc941e-26fe-49c2-a58f-4048132e1fd3 is Pending, waiting for it to be Running (with Ready = true)
    Jan 29 04:02:03.659: INFO: Pod "pod-secrets-13fc941e-26fe-49c2-a58f-4048132e1fd3": Phase="Running", Reason="", readiness=true. Elapsed: 2.017529269s
    Jan 29 04:02:03.659: INFO: The phase of Pod pod-secrets-13fc941e-26fe-49c2-a58f-4048132e1fd3 is Running (Ready = true)
    Jan 29 04:02:03.659: INFO: Pod "pod-secrets-13fc941e-26fe-49c2-a58f-4048132e1fd3" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-8811f859-e356-480d-9971-b327bc048587 01/29/23 04:02:03.712
    STEP: Updating secret s-test-opt-upd-f3807d33-d380-4e82-86c7-b8120334be9b 01/29/23 04:02:03.723
    STEP: Creating secret with name s-test-opt-create-15a35f13-511a-4fbe-99e8-a0e3df0f5de5 01/29/23 04:02:03.73
    STEP: waiting to observe update in volume 01/29/23 04:02:03.739
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 29 04:02:05.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-5902" for this suite. 01/29/23 04:02:05.805
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:02:05.815
Jan 29 04:02:05.815: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename gc 01/29/23 04:02:05.816
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:02:05.841
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:02:05.847
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 01/29/23 04:02:05.862
STEP: create the rc2 01/29/23 04:02:05.87
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 01/29/23 04:02:11.087
STEP: delete the rc simpletest-rc-to-be-deleted 01/29/23 04:02:15.157
STEP: wait for the rc to be deleted 01/29/23 04:02:15.349
Jan 29 04:02:20.434: INFO: 66 pods remaining
Jan 29 04:02:20.435: INFO: 66 pods has nil DeletionTimestamp
Jan 29 04:02:20.435: INFO: 
STEP: Gathering metrics 01/29/23 04:02:25.39
Jan 29 04:02:25.449: INFO: Waiting up to 5m0s for pod "kube-controller-manager-master3" in namespace "kube-system" to be "running and ready"
Jan 29 04:02:25.458: INFO: Pod "kube-controller-manager-master3": Phase="Running", Reason="", readiness=true. Elapsed: 8.57658ms
Jan 29 04:02:25.458: INFO: The phase of Pod kube-controller-manager-master3 is Running (Ready = true)
Jan 29 04:02:25.458: INFO: Pod "kube-controller-manager-master3" satisfied condition "running and ready"
E0129 04:02:25.554702      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:02:25.554702      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:02:26.634373      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:02:26.634373      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:02:29.025027      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:02:29.025027      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:02:30.108620      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:02:30.108620      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:02:31.225035      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:02:31.225035      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:02:32.301324      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:02:32.301324      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:02:33.378913      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:02:33.378913      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:02:34.716632      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:02:34.716632      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:02:35.805658      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:02:35.805658      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:02:36.886215      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:02:36.886215      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:02:37.964559      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:02:37.964559      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:02:40.126897      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:02:40.126897      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:02:41.229435      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:02:41.229435      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:02:42.306538      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:02:42.306538      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:02:43.392804      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:02:43.392804      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:02:44.461653      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:02:44.461653      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:02:45.545427      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:02:45.545427      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:02:46.638247      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:02:46.638247      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:02:47.726359      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:02:47.726359      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:02:48.803630      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:02:48.803630      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:02:48.968575      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:02:48.968575      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:02:50.054180      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:02:50.054180      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:02:51.126513      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:02:51.126513      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:02:52.200610      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:02:52.200610      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:02:53.295239      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:02:53.295239      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:02:54.644498      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:02:54.644498      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:02:55.729960      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:02:55.729960      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:02:56.821322      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:02:56.821322      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:02:57.896264      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:02:57.896264      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:02:58.976182      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:02:58.976182      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:00.047361      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:00.047361      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:00.969334      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:00.969334      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:02.045453      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:02.045453      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:03.120328      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:03.120328      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:04.198787      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:04.198787      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:07.566301      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:07.566301      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:08.638817      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:08.638817      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:09.714479      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:09.714479      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:10.788628      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:10.788628      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:11.860407      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:11.860407      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:11.968664      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:11.968664      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:13.049879      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:13.049879      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:14.122238      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:14.122238      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:15.193981      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:15.193981      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:17.354596      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:17.354596      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:18.429109      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:18.429109      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:19.628688      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:19.628688      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:20.738370      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:20.738370      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:21.843772      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:21.843772      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:22.919445      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:22.919445      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:23.959959      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:23.959959      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:25.036901      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:25.036901      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:26.112265      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:26.112265      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:27.188829      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:27.188829      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:28.267965      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:28.267965      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:29.354104      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:29.354104      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:30.429922      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:30.429922      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:31.509627      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:31.509627      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:32.606070      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:32.606070      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:33.683956      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:33.683956      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:34.788345      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:34.788345      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:34.971664      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:34.971664      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:36.056051      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:36.056051      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:37.136623      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:37.136623      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:38.209979      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:38.209979      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:39.290825      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:39.290825      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:40.369042      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:40.369042      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:41.451217      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:41.451217      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:42.524927      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:42.524927      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:43.603048      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:43.603048      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:44.676429      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:44.676429      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:45.754664      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:45.754664      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:45.839516      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:45.839516      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:46.920145      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:46.920145      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:47.998307      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:47.998307      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:49.073894      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:49.073894      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:50.149834      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:50.149834      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:51.225321      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:51.225321      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:53.394925      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:53.394925      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:54.472574      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:54.472574      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:55.547023      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:55.547023      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:56.625494      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:03:56.625494      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
Jan 29 04:03:56.625: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
Jan 29 04:03:56.625: INFO: Deleting pod "simpletest-rc-to-be-deleted-2khsl" in namespace "gc-5074"
Jan 29 04:03:56.730: INFO: Deleting pod "simpletest-rc-to-be-deleted-2trvh" in namespace "gc-5074"
Jan 29 04:03:56.836: INFO: Deleting pod "simpletest-rc-to-be-deleted-2wbvb" in namespace "gc-5074"
Jan 29 04:03:56.959: INFO: Deleting pod "simpletest-rc-to-be-deleted-46jmg" in namespace "gc-5074"
Jan 29 04:03:57.070: INFO: Deleting pod "simpletest-rc-to-be-deleted-498bs" in namespace "gc-5074"
Jan 29 04:03:57.155: INFO: Deleting pod "simpletest-rc-to-be-deleted-4h429" in namespace "gc-5074"
Jan 29 04:03:57.295: INFO: Deleting pod "simpletest-rc-to-be-deleted-4jb4c" in namespace "gc-5074"
Jan 29 04:03:57.401: INFO: Deleting pod "simpletest-rc-to-be-deleted-4mfqj" in namespace "gc-5074"
Jan 29 04:03:57.488: INFO: Deleting pod "simpletest-rc-to-be-deleted-4qmcv" in namespace "gc-5074"
Jan 29 04:03:57.647: INFO: Deleting pod "simpletest-rc-to-be-deleted-58mqd" in namespace "gc-5074"
Jan 29 04:03:57.892: INFO: Deleting pod "simpletest-rc-to-be-deleted-58mtw" in namespace "gc-5074"
Jan 29 04:03:58.050: INFO: Deleting pod "simpletest-rc-to-be-deleted-5cdfj" in namespace "gc-5074"
Jan 29 04:03:58.355: INFO: Deleting pod "simpletest-rc-to-be-deleted-5vg97" in namespace "gc-5074"
Jan 29 04:03:58.494: INFO: Deleting pod "simpletest-rc-to-be-deleted-62p6r" in namespace "gc-5074"
Jan 29 04:03:58.610: INFO: Deleting pod "simpletest-rc-to-be-deleted-6jrd7" in namespace "gc-5074"
Jan 29 04:03:58.773: INFO: Deleting pod "simpletest-rc-to-be-deleted-6pzwl" in namespace "gc-5074"
Jan 29 04:03:58.956: INFO: Deleting pod "simpletest-rc-to-be-deleted-6rqpk" in namespace "gc-5074"
Jan 29 04:03:59.093: INFO: Deleting pod "simpletest-rc-to-be-deleted-72z6s" in namespace "gc-5074"
Jan 29 04:03:59.224: INFO: Deleting pod "simpletest-rc-to-be-deleted-74m6k" in namespace "gc-5074"
Jan 29 04:03:59.325: INFO: Deleting pod "simpletest-rc-to-be-deleted-755nv" in namespace "gc-5074"
Jan 29 04:03:59.425: INFO: Deleting pod "simpletest-rc-to-be-deleted-77z84" in namespace "gc-5074"
Jan 29 04:03:59.552: INFO: Deleting pod "simpletest-rc-to-be-deleted-7gskl" in namespace "gc-5074"
Jan 29 04:03:59.697: INFO: Deleting pod "simpletest-rc-to-be-deleted-7vwvh" in namespace "gc-5074"
Jan 29 04:03:59.815: INFO: Deleting pod "simpletest-rc-to-be-deleted-7z7sk" in namespace "gc-5074"
Jan 29 04:03:59.947: INFO: Deleting pod "simpletest-rc-to-be-deleted-84jpm" in namespace "gc-5074"
Jan 29 04:04:00.061: INFO: Deleting pod "simpletest-rc-to-be-deleted-87cbg" in namespace "gc-5074"
Jan 29 04:04:00.144: INFO: Deleting pod "simpletest-rc-to-be-deleted-8kzxc" in namespace "gc-5074"
Jan 29 04:04:00.225: INFO: Deleting pod "simpletest-rc-to-be-deleted-8zpfd" in namespace "gc-5074"
Jan 29 04:04:00.333: INFO: Deleting pod "simpletest-rc-to-be-deleted-9fkn5" in namespace "gc-5074"
Jan 29 04:04:00.486: INFO: Deleting pod "simpletest-rc-to-be-deleted-9vh79" in namespace "gc-5074"
Jan 29 04:04:00.637: INFO: Deleting pod "simpletest-rc-to-be-deleted-c29cg" in namespace "gc-5074"
Jan 29 04:04:00.791: INFO: Deleting pod "simpletest-rc-to-be-deleted-cjjbq" in namespace "gc-5074"
Jan 29 04:04:00.972: INFO: Deleting pod "simpletest-rc-to-be-deleted-cnbrx" in namespace "gc-5074"
Jan 29 04:04:01.100: INFO: Deleting pod "simpletest-rc-to-be-deleted-cwc2k" in namespace "gc-5074"
Jan 29 04:04:01.272: INFO: Deleting pod "simpletest-rc-to-be-deleted-dml4n" in namespace "gc-5074"
Jan 29 04:04:01.378: INFO: Deleting pod "simpletest-rc-to-be-deleted-fjpth" in namespace "gc-5074"
Jan 29 04:04:01.626: INFO: Deleting pod "simpletest-rc-to-be-deleted-g5gmp" in namespace "gc-5074"
Jan 29 04:04:01.748: INFO: Deleting pod "simpletest-rc-to-be-deleted-g6k99" in namespace "gc-5074"
Jan 29 04:04:01.864: INFO: Deleting pod "simpletest-rc-to-be-deleted-g7jcd" in namespace "gc-5074"
Jan 29 04:04:02.006: INFO: Deleting pod "simpletest-rc-to-be-deleted-g7mth" in namespace "gc-5074"
Jan 29 04:04:02.121: INFO: Deleting pod "simpletest-rc-to-be-deleted-gktdz" in namespace "gc-5074"
Jan 29 04:04:02.204: INFO: Deleting pod "simpletest-rc-to-be-deleted-gl2vj" in namespace "gc-5074"
Jan 29 04:04:02.301: INFO: Deleting pod "simpletest-rc-to-be-deleted-gp6sj" in namespace "gc-5074"
Jan 29 04:04:02.525: INFO: Deleting pod "simpletest-rc-to-be-deleted-hmwk2" in namespace "gc-5074"
Jan 29 04:04:02.643: INFO: Deleting pod "simpletest-rc-to-be-deleted-hnk4k" in namespace "gc-5074"
Jan 29 04:04:02.744: INFO: Deleting pod "simpletest-rc-to-be-deleted-hr9z4" in namespace "gc-5074"
Jan 29 04:04:02.903: INFO: Deleting pod "simpletest-rc-to-be-deleted-ht26p" in namespace "gc-5074"
Jan 29 04:04:03.120: INFO: Deleting pod "simpletest-rc-to-be-deleted-j7jvj" in namespace "gc-5074"
Jan 29 04:04:03.219: INFO: Deleting pod "simpletest-rc-to-be-deleted-jlv8f" in namespace "gc-5074"
Jan 29 04:04:03.296: INFO: Deleting pod "simpletest-rc-to-be-deleted-jqs69" in namespace "gc-5074"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan 29 04:04:03.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5074" for this suite. 01/29/23 04:04:03.382
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","completed":245,"skipped":4187,"failed":0}
------------------------------
• [SLOW TEST] [117.584 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:02:05.815
    Jan 29 04:02:05.815: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename gc 01/29/23 04:02:05.816
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:02:05.841
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:02:05.847
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 01/29/23 04:02:05.862
    STEP: create the rc2 01/29/23 04:02:05.87
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 01/29/23 04:02:11.087
    STEP: delete the rc simpletest-rc-to-be-deleted 01/29/23 04:02:15.157
    STEP: wait for the rc to be deleted 01/29/23 04:02:15.349
    Jan 29 04:02:20.434: INFO: 66 pods remaining
    Jan 29 04:02:20.435: INFO: 66 pods has nil DeletionTimestamp
    Jan 29 04:02:20.435: INFO: 
    STEP: Gathering metrics 01/29/23 04:02:25.39
    Jan 29 04:02:25.449: INFO: Waiting up to 5m0s for pod "kube-controller-manager-master3" in namespace "kube-system" to be "running and ready"
    Jan 29 04:02:25.458: INFO: Pod "kube-controller-manager-master3": Phase="Running", Reason="", readiness=true. Elapsed: 8.57658ms
    Jan 29 04:02:25.458: INFO: The phase of Pod kube-controller-manager-master3 is Running (Ready = true)
    Jan 29 04:02:25.458: INFO: Pod "kube-controller-manager-master3" satisfied condition "running and ready"
    E0129 04:02:25.554702      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:02:26.634373      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:02:29.025027      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:02:30.108620      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:02:31.225035      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:02:32.301324      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:02:33.378913      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:02:34.716632      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:02:35.805658      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:02:36.886215      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:02:37.964559      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:02:40.126897      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:02:41.229435      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:02:42.306538      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:02:43.392804      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:02:44.461653      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:02:45.545427      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:02:46.638247      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:02:47.726359      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:02:48.803630      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:02:48.968575      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:02:50.054180      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:02:51.126513      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:02:52.200610      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:02:53.295239      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:02:54.644498      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:02:55.729960      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:02:56.821322      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:02:57.896264      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:02:58.976182      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:03:00.047361      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:03:00.969334      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:03:02.045453      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:03:03.120328      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:03:04.198787      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:03:07.566301      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:03:08.638817      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:03:09.714479      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:03:10.788628      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:03:11.860407      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:03:11.968664      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:03:13.049879      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:03:14.122238      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:03:15.193981      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:03:17.354596      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:03:18.429109      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:03:19.628688      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:03:20.738370      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:03:21.843772      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:03:22.919445      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:03:23.959959      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:03:25.036901      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:03:26.112265      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:03:27.188829      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:03:28.267965      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:03:29.354104      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:03:30.429922      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:03:31.509627      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:03:32.606070      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:03:33.683956      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:03:34.788345      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:03:34.971664      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:03:36.056051      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:03:37.136623      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:03:38.209979      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:03:39.290825      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:03:40.369042      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:03:41.451217      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:03:42.524927      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:03:43.603048      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:03:44.676429      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:03:45.754664      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:03:45.839516      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:03:46.920145      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:03:47.998307      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:03:49.073894      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:03:50.149834      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:03:51.225321      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:03:53.394925      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:03:54.472574      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:03:55.547023      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:03:56.625494      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    Jan 29 04:03:56.625: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
    Jan 29 04:03:56.625: INFO: Deleting pod "simpletest-rc-to-be-deleted-2khsl" in namespace "gc-5074"
    Jan 29 04:03:56.730: INFO: Deleting pod "simpletest-rc-to-be-deleted-2trvh" in namespace "gc-5074"
    Jan 29 04:03:56.836: INFO: Deleting pod "simpletest-rc-to-be-deleted-2wbvb" in namespace "gc-5074"
    Jan 29 04:03:56.959: INFO: Deleting pod "simpletest-rc-to-be-deleted-46jmg" in namespace "gc-5074"
    Jan 29 04:03:57.070: INFO: Deleting pod "simpletest-rc-to-be-deleted-498bs" in namespace "gc-5074"
    Jan 29 04:03:57.155: INFO: Deleting pod "simpletest-rc-to-be-deleted-4h429" in namespace "gc-5074"
    Jan 29 04:03:57.295: INFO: Deleting pod "simpletest-rc-to-be-deleted-4jb4c" in namespace "gc-5074"
    Jan 29 04:03:57.401: INFO: Deleting pod "simpletest-rc-to-be-deleted-4mfqj" in namespace "gc-5074"
    Jan 29 04:03:57.488: INFO: Deleting pod "simpletest-rc-to-be-deleted-4qmcv" in namespace "gc-5074"
    Jan 29 04:03:57.647: INFO: Deleting pod "simpletest-rc-to-be-deleted-58mqd" in namespace "gc-5074"
    Jan 29 04:03:57.892: INFO: Deleting pod "simpletest-rc-to-be-deleted-58mtw" in namespace "gc-5074"
    Jan 29 04:03:58.050: INFO: Deleting pod "simpletest-rc-to-be-deleted-5cdfj" in namespace "gc-5074"
    Jan 29 04:03:58.355: INFO: Deleting pod "simpletest-rc-to-be-deleted-5vg97" in namespace "gc-5074"
    Jan 29 04:03:58.494: INFO: Deleting pod "simpletest-rc-to-be-deleted-62p6r" in namespace "gc-5074"
    Jan 29 04:03:58.610: INFO: Deleting pod "simpletest-rc-to-be-deleted-6jrd7" in namespace "gc-5074"
    Jan 29 04:03:58.773: INFO: Deleting pod "simpletest-rc-to-be-deleted-6pzwl" in namespace "gc-5074"
    Jan 29 04:03:58.956: INFO: Deleting pod "simpletest-rc-to-be-deleted-6rqpk" in namespace "gc-5074"
    Jan 29 04:03:59.093: INFO: Deleting pod "simpletest-rc-to-be-deleted-72z6s" in namespace "gc-5074"
    Jan 29 04:03:59.224: INFO: Deleting pod "simpletest-rc-to-be-deleted-74m6k" in namespace "gc-5074"
    Jan 29 04:03:59.325: INFO: Deleting pod "simpletest-rc-to-be-deleted-755nv" in namespace "gc-5074"
    Jan 29 04:03:59.425: INFO: Deleting pod "simpletest-rc-to-be-deleted-77z84" in namespace "gc-5074"
    Jan 29 04:03:59.552: INFO: Deleting pod "simpletest-rc-to-be-deleted-7gskl" in namespace "gc-5074"
    Jan 29 04:03:59.697: INFO: Deleting pod "simpletest-rc-to-be-deleted-7vwvh" in namespace "gc-5074"
    Jan 29 04:03:59.815: INFO: Deleting pod "simpletest-rc-to-be-deleted-7z7sk" in namespace "gc-5074"
    Jan 29 04:03:59.947: INFO: Deleting pod "simpletest-rc-to-be-deleted-84jpm" in namespace "gc-5074"
    Jan 29 04:04:00.061: INFO: Deleting pod "simpletest-rc-to-be-deleted-87cbg" in namespace "gc-5074"
    Jan 29 04:04:00.144: INFO: Deleting pod "simpletest-rc-to-be-deleted-8kzxc" in namespace "gc-5074"
    Jan 29 04:04:00.225: INFO: Deleting pod "simpletest-rc-to-be-deleted-8zpfd" in namespace "gc-5074"
    Jan 29 04:04:00.333: INFO: Deleting pod "simpletest-rc-to-be-deleted-9fkn5" in namespace "gc-5074"
    Jan 29 04:04:00.486: INFO: Deleting pod "simpletest-rc-to-be-deleted-9vh79" in namespace "gc-5074"
    Jan 29 04:04:00.637: INFO: Deleting pod "simpletest-rc-to-be-deleted-c29cg" in namespace "gc-5074"
    Jan 29 04:04:00.791: INFO: Deleting pod "simpletest-rc-to-be-deleted-cjjbq" in namespace "gc-5074"
    Jan 29 04:04:00.972: INFO: Deleting pod "simpletest-rc-to-be-deleted-cnbrx" in namespace "gc-5074"
    Jan 29 04:04:01.100: INFO: Deleting pod "simpletest-rc-to-be-deleted-cwc2k" in namespace "gc-5074"
    Jan 29 04:04:01.272: INFO: Deleting pod "simpletest-rc-to-be-deleted-dml4n" in namespace "gc-5074"
    Jan 29 04:04:01.378: INFO: Deleting pod "simpletest-rc-to-be-deleted-fjpth" in namespace "gc-5074"
    Jan 29 04:04:01.626: INFO: Deleting pod "simpletest-rc-to-be-deleted-g5gmp" in namespace "gc-5074"
    Jan 29 04:04:01.748: INFO: Deleting pod "simpletest-rc-to-be-deleted-g6k99" in namespace "gc-5074"
    Jan 29 04:04:01.864: INFO: Deleting pod "simpletest-rc-to-be-deleted-g7jcd" in namespace "gc-5074"
    Jan 29 04:04:02.006: INFO: Deleting pod "simpletest-rc-to-be-deleted-g7mth" in namespace "gc-5074"
    Jan 29 04:04:02.121: INFO: Deleting pod "simpletest-rc-to-be-deleted-gktdz" in namespace "gc-5074"
    Jan 29 04:04:02.204: INFO: Deleting pod "simpletest-rc-to-be-deleted-gl2vj" in namespace "gc-5074"
    Jan 29 04:04:02.301: INFO: Deleting pod "simpletest-rc-to-be-deleted-gp6sj" in namespace "gc-5074"
    Jan 29 04:04:02.525: INFO: Deleting pod "simpletest-rc-to-be-deleted-hmwk2" in namespace "gc-5074"
    Jan 29 04:04:02.643: INFO: Deleting pod "simpletest-rc-to-be-deleted-hnk4k" in namespace "gc-5074"
    Jan 29 04:04:02.744: INFO: Deleting pod "simpletest-rc-to-be-deleted-hr9z4" in namespace "gc-5074"
    Jan 29 04:04:02.903: INFO: Deleting pod "simpletest-rc-to-be-deleted-ht26p" in namespace "gc-5074"
    Jan 29 04:04:03.120: INFO: Deleting pod "simpletest-rc-to-be-deleted-j7jvj" in namespace "gc-5074"
    Jan 29 04:04:03.219: INFO: Deleting pod "simpletest-rc-to-be-deleted-jlv8f" in namespace "gc-5074"
    Jan 29 04:04:03.296: INFO: Deleting pod "simpletest-rc-to-be-deleted-jqs69" in namespace "gc-5074"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan 29 04:04:03.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-5074" for this suite. 01/29/23 04:04:03.382
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:04:03.399
Jan 29 04:04:03.400: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename statefulset 01/29/23 04:04:03.401
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:04:03.463
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:04:03.469
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-4996 01/29/23 04:04:03.474
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
STEP: Creating stateful set ss in namespace statefulset-4996 01/29/23 04:04:03.489
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4996 01/29/23 04:04:03.513
Jan 29 04:04:03.519: INFO: Found 0 stateful pods, waiting for 1
Jan 29 04:04:13.528: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 01/29/23 04:04:13.528
Jan 29 04:04:13.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=statefulset-4996 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 29 04:04:13.775: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 29 04:04:13.775: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 29 04:04:13.775: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 29 04:04:13.782: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan 29 04:04:23.791: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 29 04:04:23.791: INFO: Waiting for statefulset status.replicas updated to 0
Jan 29 04:04:23.827: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
Jan 29 04:04:23.827: INFO: ss-0  slave2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:04:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:04:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:04:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:04:03 +0000 UTC  }]
Jan 29 04:04:23.827: INFO: 
Jan 29 04:04:23.827: INFO: StatefulSet ss has not reached scale 3, at 1
Jan 29 04:04:24.836: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.990872438s
Jan 29 04:04:25.843: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.981380033s
Jan 29 04:04:26.851: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.974407646s
Jan 29 04:04:27.859: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.967093777s
Jan 29 04:04:28.866: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.958880361s
Jan 29 04:04:29.873: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.95133401s
Jan 29 04:04:30.880: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.944559084s
Jan 29 04:04:31.888: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.937209434s
Jan 29 04:04:32.896: INFO: Verifying statefulset ss doesn't scale past 3 for another 929.496262ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4996 01/29/23 04:04:33.896
Jan 29 04:04:33.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=statefulset-4996 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 29 04:04:34.141: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 29 04:04:34.141: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 29 04:04:34.141: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 29 04:04:34.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=statefulset-4996 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 29 04:04:34.597: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan 29 04:04:34.598: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 29 04:04:34.598: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 29 04:04:34.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=statefulset-4996 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 29 04:04:34.902: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan 29 04:04:34.903: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 29 04:04:34.903: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 29 04:04:34.909: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Jan 29 04:04:44.919: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 29 04:04:44.919: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 29 04:04:44.919: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 01/29/23 04:04:44.919
Jan 29 04:04:44.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=statefulset-4996 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 29 04:04:45.155: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 29 04:04:45.155: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 29 04:04:45.155: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 29 04:04:45.155: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=statefulset-4996 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 29 04:04:45.412: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 29 04:04:45.412: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 29 04:04:45.412: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 29 04:04:45.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=statefulset-4996 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 29 04:04:45.667: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 29 04:04:45.667: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 29 04:04:45.667: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 29 04:04:45.667: INFO: Waiting for statefulset status.replicas updated to 0
Jan 29 04:04:45.673: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Jan 29 04:04:55.688: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 29 04:04:55.688: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan 29 04:04:55.688: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan 29 04:04:55.709: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Jan 29 04:04:55.709: INFO: ss-0  slave2   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:04:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:04:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:04:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:04:03 +0000 UTC  }]
Jan 29 04:04:55.709: INFO: ss-1  slave1   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:04:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:04:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:04:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:04:23 +0000 UTC  }]
Jan 29 04:04:55.709: INFO: ss-2  master1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:04:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:04:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:04:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:04:23 +0000 UTC  }]
Jan 29 04:04:55.709: INFO: 
Jan 29 04:04:55.709: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 29 04:04:56.716: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Jan 29 04:04:56.716: INFO: ss-0  slave2   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:04:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:04:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:04:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:04:03 +0000 UTC  }]
Jan 29 04:04:56.716: INFO: ss-1  slave1   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:04:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:04:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:04:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:04:23 +0000 UTC  }]
Jan 29 04:04:56.716: INFO: ss-2  master1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:04:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:04:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:04:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:04:23 +0000 UTC  }]
Jan 29 04:04:56.716: INFO: 
Jan 29 04:04:56.716: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 29 04:04:57.723: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.98518774s
Jan 29 04:04:58.740: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.978146453s
Jan 29 04:04:59.746: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.961264136s
Jan 29 04:05:00.752: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.955597138s
Jan 29 04:05:01.771: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.949189235s
Jan 29 04:05:02.779: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.930673428s
Jan 29 04:05:03.784: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.922892255s
Jan 29 04:05:04.791: INFO: Verifying statefulset ss doesn't scale past 0 for another 917.127956ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4996 01/29/23 04:05:05.791
Jan 29 04:05:05.798: INFO: Scaling statefulset ss to 0
Jan 29 04:05:05.817: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 29 04:05:05.824: INFO: Deleting all statefulset in ns statefulset-4996
Jan 29 04:05:05.832: INFO: Scaling statefulset ss to 0
Jan 29 04:05:05.852: INFO: Waiting for statefulset status.replicas updated to 0
Jan 29 04:05:05.859: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan 29 04:05:05.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4996" for this suite. 01/29/23 04:05:05.904
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","completed":246,"skipped":4194,"failed":0}
------------------------------
• [SLOW TEST] [62.514 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:695

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:04:03.399
    Jan 29 04:04:03.400: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename statefulset 01/29/23 04:04:03.401
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:04:03.463
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:04:03.469
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-4996 01/29/23 04:04:03.474
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:695
    STEP: Creating stateful set ss in namespace statefulset-4996 01/29/23 04:04:03.489
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4996 01/29/23 04:04:03.513
    Jan 29 04:04:03.519: INFO: Found 0 stateful pods, waiting for 1
    Jan 29 04:04:13.528: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 01/29/23 04:04:13.528
    Jan 29 04:04:13.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=statefulset-4996 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 29 04:04:13.775: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 29 04:04:13.775: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 29 04:04:13.775: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 29 04:04:13.782: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Jan 29 04:04:23.791: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan 29 04:04:23.791: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 29 04:04:23.827: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
    Jan 29 04:04:23.827: INFO: ss-0  slave2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:04:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:04:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:04:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:04:03 +0000 UTC  }]
    Jan 29 04:04:23.827: INFO: 
    Jan 29 04:04:23.827: INFO: StatefulSet ss has not reached scale 3, at 1
    Jan 29 04:04:24.836: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.990872438s
    Jan 29 04:04:25.843: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.981380033s
    Jan 29 04:04:26.851: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.974407646s
    Jan 29 04:04:27.859: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.967093777s
    Jan 29 04:04:28.866: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.958880361s
    Jan 29 04:04:29.873: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.95133401s
    Jan 29 04:04:30.880: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.944559084s
    Jan 29 04:04:31.888: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.937209434s
    Jan 29 04:04:32.896: INFO: Verifying statefulset ss doesn't scale past 3 for another 929.496262ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4996 01/29/23 04:04:33.896
    Jan 29 04:04:33.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=statefulset-4996 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 29 04:04:34.141: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 29 04:04:34.141: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 29 04:04:34.141: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 29 04:04:34.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=statefulset-4996 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 29 04:04:34.597: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Jan 29 04:04:34.598: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 29 04:04:34.598: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 29 04:04:34.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=statefulset-4996 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 29 04:04:34.902: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Jan 29 04:04:34.903: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 29 04:04:34.903: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 29 04:04:34.909: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
    Jan 29 04:04:44.919: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 29 04:04:44.919: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 29 04:04:44.919: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 01/29/23 04:04:44.919
    Jan 29 04:04:44.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=statefulset-4996 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 29 04:04:45.155: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 29 04:04:45.155: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 29 04:04:45.155: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 29 04:04:45.155: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=statefulset-4996 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 29 04:04:45.412: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 29 04:04:45.412: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 29 04:04:45.412: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 29 04:04:45.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=statefulset-4996 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 29 04:04:45.667: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 29 04:04:45.667: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 29 04:04:45.667: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 29 04:04:45.667: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 29 04:04:45.673: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Jan 29 04:04:55.688: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan 29 04:04:55.688: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Jan 29 04:04:55.688: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Jan 29 04:04:55.709: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
    Jan 29 04:04:55.709: INFO: ss-0  slave2   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:04:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:04:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:04:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:04:03 +0000 UTC  }]
    Jan 29 04:04:55.709: INFO: ss-1  slave1   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:04:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:04:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:04:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:04:23 +0000 UTC  }]
    Jan 29 04:04:55.709: INFO: ss-2  master1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:04:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:04:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:04:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:04:23 +0000 UTC  }]
    Jan 29 04:04:55.709: INFO: 
    Jan 29 04:04:55.709: INFO: StatefulSet ss has not reached scale 0, at 3
    Jan 29 04:04:56.716: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
    Jan 29 04:04:56.716: INFO: ss-0  slave2   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:04:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:04:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:04:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:04:03 +0000 UTC  }]
    Jan 29 04:04:56.716: INFO: ss-1  slave1   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:04:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:04:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:04:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:04:23 +0000 UTC  }]
    Jan 29 04:04:56.716: INFO: ss-2  master1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:04:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:04:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:04:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:04:23 +0000 UTC  }]
    Jan 29 04:04:56.716: INFO: 
    Jan 29 04:04:56.716: INFO: StatefulSet ss has not reached scale 0, at 3
    Jan 29 04:04:57.723: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.98518774s
    Jan 29 04:04:58.740: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.978146453s
    Jan 29 04:04:59.746: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.961264136s
    Jan 29 04:05:00.752: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.955597138s
    Jan 29 04:05:01.771: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.949189235s
    Jan 29 04:05:02.779: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.930673428s
    Jan 29 04:05:03.784: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.922892255s
    Jan 29 04:05:04.791: INFO: Verifying statefulset ss doesn't scale past 0 for another 917.127956ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4996 01/29/23 04:05:05.791
    Jan 29 04:05:05.798: INFO: Scaling statefulset ss to 0
    Jan 29 04:05:05.817: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan 29 04:05:05.824: INFO: Deleting all statefulset in ns statefulset-4996
    Jan 29 04:05:05.832: INFO: Scaling statefulset ss to 0
    Jan 29 04:05:05.852: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 29 04:05:05.859: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan 29 04:05:05.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-4996" for this suite. 01/29/23 04:05:05.904
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:05:05.916
Jan 29 04:05:05.916: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename crd-publish-openapi 01/29/23 04:05:05.917
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:05:05.947
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:05:05.954
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
Jan 29 04:05:05.960: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 01/29/23 04:05:13.6
Jan 29 04:05:13.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=crd-publish-openapi-8280 --namespace=crd-publish-openapi-8280 create -f -'
Jan 29 04:05:15.144: INFO: stderr: ""
Jan 29 04:05:15.144: INFO: stdout: "e2e-test-crd-publish-openapi-5655-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jan 29 04:05:15.144: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=crd-publish-openapi-8280 --namespace=crd-publish-openapi-8280 delete e2e-test-crd-publish-openapi-5655-crds test-foo'
Jan 29 04:05:15.285: INFO: stderr: ""
Jan 29 04:05:15.285: INFO: stdout: "e2e-test-crd-publish-openapi-5655-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Jan 29 04:05:15.285: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=crd-publish-openapi-8280 --namespace=crd-publish-openapi-8280 apply -f -'
Jan 29 04:05:16.573: INFO: stderr: ""
Jan 29 04:05:16.573: INFO: stdout: "e2e-test-crd-publish-openapi-5655-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jan 29 04:05:16.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=crd-publish-openapi-8280 --namespace=crd-publish-openapi-8280 delete e2e-test-crd-publish-openapi-5655-crds test-foo'
Jan 29 04:05:16.715: INFO: stderr: ""
Jan 29 04:05:16.715: INFO: stdout: "e2e-test-crd-publish-openapi-5655-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 01/29/23 04:05:16.715
Jan 29 04:05:16.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=crd-publish-openapi-8280 --namespace=crd-publish-openapi-8280 create -f -'
Jan 29 04:05:17.942: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 01/29/23 04:05:17.942
Jan 29 04:05:17.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=crd-publish-openapi-8280 --namespace=crd-publish-openapi-8280 create -f -'
Jan 29 04:05:18.324: INFO: rc: 1
Jan 29 04:05:18.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=crd-publish-openapi-8280 --namespace=crd-publish-openapi-8280 apply -f -'
Jan 29 04:05:36.367: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 01/29/23 04:05:36.368
Jan 29 04:05:36.368: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=crd-publish-openapi-8280 --namespace=crd-publish-openapi-8280 create -f -'
Jan 29 04:05:36.779: INFO: rc: 1
Jan 29 04:05:36.779: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=crd-publish-openapi-8280 --namespace=crd-publish-openapi-8280 apply -f -'
Jan 29 04:05:37.165: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 01/29/23 04:05:37.166
Jan 29 04:05:37.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=crd-publish-openapi-8280 explain e2e-test-crd-publish-openapi-5655-crds'
Jan 29 04:05:37.525: INFO: stderr: ""
Jan 29 04:05:37.525: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5655-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 01/29/23 04:05:37.526
Jan 29 04:05:37.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=crd-publish-openapi-8280 explain e2e-test-crd-publish-openapi-5655-crds.metadata'
Jan 29 04:05:37.907: INFO: stderr: ""
Jan 29 04:05:37.907: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5655-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Jan 29 04:05:37.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=crd-publish-openapi-8280 explain e2e-test-crd-publish-openapi-5655-crds.spec'
Jan 29 04:05:38.277: INFO: stderr: ""
Jan 29 04:05:38.277: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5655-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Jan 29 04:05:38.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=crd-publish-openapi-8280 explain e2e-test-crd-publish-openapi-5655-crds.spec.bars'
Jan 29 04:05:38.636: INFO: stderr: ""
Jan 29 04:05:38.636: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5655-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 01/29/23 04:05:38.636
Jan 29 04:05:38.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=crd-publish-openapi-8280 explain e2e-test-crd-publish-openapi-5655-crds.spec.bars2'
Jan 29 04:05:39.003: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 29 04:05:43.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8280" for this suite. 01/29/23 04:05:43.828
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","completed":247,"skipped":4218,"failed":0}
------------------------------
• [SLOW TEST] [37.927 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:05:05.916
    Jan 29 04:05:05.916: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename crd-publish-openapi 01/29/23 04:05:05.917
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:05:05.947
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:05:05.954
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:68
    Jan 29 04:05:05.960: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 01/29/23 04:05:13.6
    Jan 29 04:05:13.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=crd-publish-openapi-8280 --namespace=crd-publish-openapi-8280 create -f -'
    Jan 29 04:05:15.144: INFO: stderr: ""
    Jan 29 04:05:15.144: INFO: stdout: "e2e-test-crd-publish-openapi-5655-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Jan 29 04:05:15.144: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=crd-publish-openapi-8280 --namespace=crd-publish-openapi-8280 delete e2e-test-crd-publish-openapi-5655-crds test-foo'
    Jan 29 04:05:15.285: INFO: stderr: ""
    Jan 29 04:05:15.285: INFO: stdout: "e2e-test-crd-publish-openapi-5655-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Jan 29 04:05:15.285: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=crd-publish-openapi-8280 --namespace=crd-publish-openapi-8280 apply -f -'
    Jan 29 04:05:16.573: INFO: stderr: ""
    Jan 29 04:05:16.573: INFO: stdout: "e2e-test-crd-publish-openapi-5655-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Jan 29 04:05:16.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=crd-publish-openapi-8280 --namespace=crd-publish-openapi-8280 delete e2e-test-crd-publish-openapi-5655-crds test-foo'
    Jan 29 04:05:16.715: INFO: stderr: ""
    Jan 29 04:05:16.715: INFO: stdout: "e2e-test-crd-publish-openapi-5655-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 01/29/23 04:05:16.715
    Jan 29 04:05:16.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=crd-publish-openapi-8280 --namespace=crd-publish-openapi-8280 create -f -'
    Jan 29 04:05:17.942: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 01/29/23 04:05:17.942
    Jan 29 04:05:17.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=crd-publish-openapi-8280 --namespace=crd-publish-openapi-8280 create -f -'
    Jan 29 04:05:18.324: INFO: rc: 1
    Jan 29 04:05:18.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=crd-publish-openapi-8280 --namespace=crd-publish-openapi-8280 apply -f -'
    Jan 29 04:05:36.367: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 01/29/23 04:05:36.368
    Jan 29 04:05:36.368: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=crd-publish-openapi-8280 --namespace=crd-publish-openapi-8280 create -f -'
    Jan 29 04:05:36.779: INFO: rc: 1
    Jan 29 04:05:36.779: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=crd-publish-openapi-8280 --namespace=crd-publish-openapi-8280 apply -f -'
    Jan 29 04:05:37.165: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 01/29/23 04:05:37.166
    Jan 29 04:05:37.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=crd-publish-openapi-8280 explain e2e-test-crd-publish-openapi-5655-crds'
    Jan 29 04:05:37.525: INFO: stderr: ""
    Jan 29 04:05:37.525: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5655-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 01/29/23 04:05:37.526
    Jan 29 04:05:37.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=crd-publish-openapi-8280 explain e2e-test-crd-publish-openapi-5655-crds.metadata'
    Jan 29 04:05:37.907: INFO: stderr: ""
    Jan 29 04:05:37.907: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5655-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Jan 29 04:05:37.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=crd-publish-openapi-8280 explain e2e-test-crd-publish-openapi-5655-crds.spec'
    Jan 29 04:05:38.277: INFO: stderr: ""
    Jan 29 04:05:38.277: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5655-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Jan 29 04:05:38.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=crd-publish-openapi-8280 explain e2e-test-crd-publish-openapi-5655-crds.spec.bars'
    Jan 29 04:05:38.636: INFO: stderr: ""
    Jan 29 04:05:38.636: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5655-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 01/29/23 04:05:38.636
    Jan 29 04:05:38.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=crd-publish-openapi-8280 explain e2e-test-crd-publish-openapi-5655-crds.spec.bars2'
    Jan 29 04:05:39.003: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 29 04:05:43.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-8280" for this suite. 01/29/23 04:05:43.828
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:05:43.845
Jan 29 04:05:43.846: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename emptydir 01/29/23 04:05:43.847
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:05:43.872
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:05:43.878
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
STEP: Creating a pod to test emptydir 0777 on node default medium 01/29/23 04:05:43.884
Jan 29 04:05:43.902: INFO: Waiting up to 5m0s for pod "pod-5414fd61-3ddf-48c9-875f-e4e4bc461e80" in namespace "emptydir-7791" to be "Succeeded or Failed"
Jan 29 04:05:43.910: INFO: Pod "pod-5414fd61-3ddf-48c9-875f-e4e4bc461e80": Phase="Pending", Reason="", readiness=false. Elapsed: 7.560873ms
Jan 29 04:05:45.918: INFO: Pod "pod-5414fd61-3ddf-48c9-875f-e4e4bc461e80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015588466s
Jan 29 04:05:47.919: INFO: Pod "pod-5414fd61-3ddf-48c9-875f-e4e4bc461e80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016477929s
STEP: Saw pod success 01/29/23 04:05:47.919
Jan 29 04:05:47.919: INFO: Pod "pod-5414fd61-3ddf-48c9-875f-e4e4bc461e80" satisfied condition "Succeeded or Failed"
Jan 29 04:05:47.926: INFO: Trying to get logs from node slave2 pod pod-5414fd61-3ddf-48c9-875f-e4e4bc461e80 container test-container: <nil>
STEP: delete the pod 01/29/23 04:05:47.958
Jan 29 04:05:48.028: INFO: Waiting for pod pod-5414fd61-3ddf-48c9-875f-e4e4bc461e80 to disappear
Jan 29 04:05:48.034: INFO: Pod pod-5414fd61-3ddf-48c9-875f-e4e4bc461e80 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 29 04:05:48.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7791" for this suite. 01/29/23 04:05:48.044
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":248,"skipped":4260,"failed":0}
------------------------------
• [4.210 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:05:43.845
    Jan 29 04:05:43.846: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename emptydir 01/29/23 04:05:43.847
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:05:43.872
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:05:43.878
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:186
    STEP: Creating a pod to test emptydir 0777 on node default medium 01/29/23 04:05:43.884
    Jan 29 04:05:43.902: INFO: Waiting up to 5m0s for pod "pod-5414fd61-3ddf-48c9-875f-e4e4bc461e80" in namespace "emptydir-7791" to be "Succeeded or Failed"
    Jan 29 04:05:43.910: INFO: Pod "pod-5414fd61-3ddf-48c9-875f-e4e4bc461e80": Phase="Pending", Reason="", readiness=false. Elapsed: 7.560873ms
    Jan 29 04:05:45.918: INFO: Pod "pod-5414fd61-3ddf-48c9-875f-e4e4bc461e80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015588466s
    Jan 29 04:05:47.919: INFO: Pod "pod-5414fd61-3ddf-48c9-875f-e4e4bc461e80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016477929s
    STEP: Saw pod success 01/29/23 04:05:47.919
    Jan 29 04:05:47.919: INFO: Pod "pod-5414fd61-3ddf-48c9-875f-e4e4bc461e80" satisfied condition "Succeeded or Failed"
    Jan 29 04:05:47.926: INFO: Trying to get logs from node slave2 pod pod-5414fd61-3ddf-48c9-875f-e4e4bc461e80 container test-container: <nil>
    STEP: delete the pod 01/29/23 04:05:47.958
    Jan 29 04:05:48.028: INFO: Waiting for pod pod-5414fd61-3ddf-48c9-875f-e4e4bc461e80 to disappear
    Jan 29 04:05:48.034: INFO: Pod pod-5414fd61-3ddf-48c9-875f-e4e4bc461e80 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 29 04:05:48.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-7791" for this suite. 01/29/23 04:05:48.044
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:05:48.059
Jan 29 04:05:48.059: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename security-context-test 01/29/23 04:05:48.06
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:05:48.085
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:05:48.09
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
Jan 29 04:05:48.113: INFO: Waiting up to 5m0s for pod "busybox-user-65534-20ab8ea0-dae7-4208-b05c-2672fb36cf4c" in namespace "security-context-test-6921" to be "Succeeded or Failed"
Jan 29 04:05:48.120: INFO: Pod "busybox-user-65534-20ab8ea0-dae7-4208-b05c-2672fb36cf4c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.846408ms
Jan 29 04:05:50.127: INFO: Pod "busybox-user-65534-20ab8ea0-dae7-4208-b05c-2672fb36cf4c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013261349s
Jan 29 04:05:52.133: INFO: Pod "busybox-user-65534-20ab8ea0-dae7-4208-b05c-2672fb36cf4c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01964345s
Jan 29 04:05:52.133: INFO: Pod "busybox-user-65534-20ab8ea0-dae7-4208-b05c-2672fb36cf4c" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jan 29 04:05:52.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6921" for this suite. 01/29/23 04:05:52.147
{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","completed":249,"skipped":4306,"failed":0}
------------------------------
• [4.106 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:308
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:05:48.059
    Jan 29 04:05:48.059: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename security-context-test 01/29/23 04:05:48.06
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:05:48.085
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:05:48.09
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:346
    Jan 29 04:05:48.113: INFO: Waiting up to 5m0s for pod "busybox-user-65534-20ab8ea0-dae7-4208-b05c-2672fb36cf4c" in namespace "security-context-test-6921" to be "Succeeded or Failed"
    Jan 29 04:05:48.120: INFO: Pod "busybox-user-65534-20ab8ea0-dae7-4208-b05c-2672fb36cf4c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.846408ms
    Jan 29 04:05:50.127: INFO: Pod "busybox-user-65534-20ab8ea0-dae7-4208-b05c-2672fb36cf4c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013261349s
    Jan 29 04:05:52.133: INFO: Pod "busybox-user-65534-20ab8ea0-dae7-4208-b05c-2672fb36cf4c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01964345s
    Jan 29 04:05:52.133: INFO: Pod "busybox-user-65534-20ab8ea0-dae7-4208-b05c-2672fb36cf4c" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jan 29 04:05:52.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-6921" for this suite. 01/29/23 04:05:52.147
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:05:52.167
Jan 29 04:05:52.167: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename resourcequota 01/29/23 04:05:52.169
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:05:52.207
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:05:52.214
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
STEP: Counting existing ResourceQuota 01/29/23 04:05:52.221
STEP: Creating a ResourceQuota 01/29/23 04:05:57.227
STEP: Ensuring resource quota status is calculated 01/29/23 04:05:57.234
STEP: Creating a Pod that fits quota 01/29/23 04:05:59.243
STEP: Ensuring ResourceQuota status captures the pod usage 01/29/23 04:05:59.271
STEP: Not allowing a pod to be created that exceeds remaining quota 01/29/23 04:06:01.279
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 01/29/23 04:06:01.287
STEP: Ensuring a pod cannot update its resource requirements 01/29/23 04:06:01.296
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 01/29/23 04:06:01.303
STEP: Deleting the pod 01/29/23 04:06:03.31
STEP: Ensuring resource quota status released the pod usage 01/29/23 04:06:03.383
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 29 04:06:05.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9670" for this suite. 01/29/23 04:06:05.4
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","completed":250,"skipped":4320,"failed":0}
------------------------------
• [SLOW TEST] [13.242 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:05:52.167
    Jan 29 04:05:52.167: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename resourcequota 01/29/23 04:05:52.169
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:05:52.207
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:05:52.214
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:220
    STEP: Counting existing ResourceQuota 01/29/23 04:05:52.221
    STEP: Creating a ResourceQuota 01/29/23 04:05:57.227
    STEP: Ensuring resource quota status is calculated 01/29/23 04:05:57.234
    STEP: Creating a Pod that fits quota 01/29/23 04:05:59.243
    STEP: Ensuring ResourceQuota status captures the pod usage 01/29/23 04:05:59.271
    STEP: Not allowing a pod to be created that exceeds remaining quota 01/29/23 04:06:01.279
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 01/29/23 04:06:01.287
    STEP: Ensuring a pod cannot update its resource requirements 01/29/23 04:06:01.296
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 01/29/23 04:06:01.303
    STEP: Deleting the pod 01/29/23 04:06:03.31
    STEP: Ensuring resource quota status released the pod usage 01/29/23 04:06:03.383
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 29 04:06:05.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-9670" for this suite. 01/29/23 04:06:05.4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:06:05.41
Jan 29 04:06:05.411: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename svcaccounts 01/29/23 04:06:05.412
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:06:05.433
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:06:05.44
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
Jan 29 04:06:05.478: INFO: created pod pod-service-account-defaultsa
Jan 29 04:06:05.478: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jan 29 04:06:05.489: INFO: created pod pod-service-account-mountsa
Jan 29 04:06:05.489: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jan 29 04:06:05.503: INFO: created pod pod-service-account-nomountsa
Jan 29 04:06:05.503: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jan 29 04:06:05.517: INFO: created pod pod-service-account-defaultsa-mountspec
Jan 29 04:06:05.517: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jan 29 04:06:05.535: INFO: created pod pod-service-account-mountsa-mountspec
Jan 29 04:06:05.535: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jan 29 04:06:05.550: INFO: created pod pod-service-account-nomountsa-mountspec
Jan 29 04:06:05.551: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jan 29 04:06:05.562: INFO: created pod pod-service-account-defaultsa-nomountspec
Jan 29 04:06:05.562: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jan 29 04:06:05.579: INFO: created pod pod-service-account-mountsa-nomountspec
Jan 29 04:06:05.579: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jan 29 04:06:05.592: INFO: created pod pod-service-account-nomountsa-nomountspec
Jan 29 04:06:05.592: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jan 29 04:06:05.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2465" for this suite. 01/29/23 04:06:05.606
{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","completed":251,"skipped":4341,"failed":0}
------------------------------
• [0.208 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:06:05.41
    Jan 29 04:06:05.411: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename svcaccounts 01/29/23 04:06:05.412
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:06:05.433
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:06:05.44
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:158
    Jan 29 04:06:05.478: INFO: created pod pod-service-account-defaultsa
    Jan 29 04:06:05.478: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Jan 29 04:06:05.489: INFO: created pod pod-service-account-mountsa
    Jan 29 04:06:05.489: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Jan 29 04:06:05.503: INFO: created pod pod-service-account-nomountsa
    Jan 29 04:06:05.503: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Jan 29 04:06:05.517: INFO: created pod pod-service-account-defaultsa-mountspec
    Jan 29 04:06:05.517: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Jan 29 04:06:05.535: INFO: created pod pod-service-account-mountsa-mountspec
    Jan 29 04:06:05.535: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Jan 29 04:06:05.550: INFO: created pod pod-service-account-nomountsa-mountspec
    Jan 29 04:06:05.551: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Jan 29 04:06:05.562: INFO: created pod pod-service-account-defaultsa-nomountspec
    Jan 29 04:06:05.562: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Jan 29 04:06:05.579: INFO: created pod pod-service-account-mountsa-nomountspec
    Jan 29 04:06:05.579: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Jan 29 04:06:05.592: INFO: created pod pod-service-account-nomountsa-nomountspec
    Jan 29 04:06:05.592: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jan 29 04:06:05.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-2465" for this suite. 01/29/23 04:06:05.606
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:06:05.622
Jan 29 04:06:05.622: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename resourcequota 01/29/23 04:06:05.624
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:06:05.655
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:06:05.662
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
STEP: Counting existing ResourceQuota 01/29/23 04:06:22.675
STEP: Creating a ResourceQuota 01/29/23 04:06:27.681
STEP: Ensuring resource quota status is calculated 01/29/23 04:06:27.688
STEP: Creating a ConfigMap 01/29/23 04:06:29.708
STEP: Ensuring resource quota status captures configMap creation 01/29/23 04:06:29.745
STEP: Deleting a ConfigMap 01/29/23 04:06:31.754
STEP: Ensuring resource quota status released usage 01/29/23 04:06:31.765
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 29 04:06:33.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7485" for this suite. 01/29/23 04:06:33.782
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","completed":252,"skipped":4375,"failed":0}
------------------------------
• [SLOW TEST] [28.170 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:06:05.622
    Jan 29 04:06:05.622: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename resourcequota 01/29/23 04:06:05.624
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:06:05.655
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:06:05.662
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:316
    STEP: Counting existing ResourceQuota 01/29/23 04:06:22.675
    STEP: Creating a ResourceQuota 01/29/23 04:06:27.681
    STEP: Ensuring resource quota status is calculated 01/29/23 04:06:27.688
    STEP: Creating a ConfigMap 01/29/23 04:06:29.708
    STEP: Ensuring resource quota status captures configMap creation 01/29/23 04:06:29.745
    STEP: Deleting a ConfigMap 01/29/23 04:06:31.754
    STEP: Ensuring resource quota status released usage 01/29/23 04:06:31.765
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 29 04:06:33.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-7485" for this suite. 01/29/23 04:06:33.782
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:06:33.794
Jan 29 04:06:33.794: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename replication-controller 01/29/23 04:06:33.796
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:06:33.823
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:06:33.83
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
STEP: creating a ReplicationController 01/29/23 04:06:33.842
STEP: waiting for RC to be added 01/29/23 04:06:33.85
STEP: waiting for available Replicas 01/29/23 04:06:33.85
STEP: patching ReplicationController 01/29/23 04:06:35.759
STEP: waiting for RC to be modified 01/29/23 04:06:35.768
STEP: patching ReplicationController status 01/29/23 04:06:35.768
STEP: waiting for RC to be modified 01/29/23 04:06:35.778
STEP: waiting for available Replicas 01/29/23 04:06:35.778
STEP: fetching ReplicationController status 01/29/23 04:06:35.791
STEP: patching ReplicationController scale 01/29/23 04:06:35.8
STEP: waiting for RC to be modified 01/29/23 04:06:35.808
STEP: waiting for ReplicationController's scale to be the max amount 01/29/23 04:06:35.808
STEP: fetching ReplicationController; ensuring that it's patched 01/29/23 04:06:38.104
STEP: updating ReplicationController status 01/29/23 04:06:38.111
STEP: waiting for RC to be modified 01/29/23 04:06:38.118
STEP: listing all ReplicationControllers 01/29/23 04:06:38.119
STEP: checking that ReplicationController has expected values 01/29/23 04:06:38.125
STEP: deleting ReplicationControllers by collection 01/29/23 04:06:38.125
STEP: waiting for ReplicationController to have a DELETED watchEvent 01/29/23 04:06:38.139
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jan 29 04:06:38.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7583" for this suite. 01/29/23 04:06:38.234
{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","completed":253,"skipped":4414,"failed":0}
------------------------------
• [4.449 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:06:33.794
    Jan 29 04:06:33.794: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename replication-controller 01/29/23 04:06:33.796
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:06:33.823
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:06:33.83
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:109
    STEP: creating a ReplicationController 01/29/23 04:06:33.842
    STEP: waiting for RC to be added 01/29/23 04:06:33.85
    STEP: waiting for available Replicas 01/29/23 04:06:33.85
    STEP: patching ReplicationController 01/29/23 04:06:35.759
    STEP: waiting for RC to be modified 01/29/23 04:06:35.768
    STEP: patching ReplicationController status 01/29/23 04:06:35.768
    STEP: waiting for RC to be modified 01/29/23 04:06:35.778
    STEP: waiting for available Replicas 01/29/23 04:06:35.778
    STEP: fetching ReplicationController status 01/29/23 04:06:35.791
    STEP: patching ReplicationController scale 01/29/23 04:06:35.8
    STEP: waiting for RC to be modified 01/29/23 04:06:35.808
    STEP: waiting for ReplicationController's scale to be the max amount 01/29/23 04:06:35.808
    STEP: fetching ReplicationController; ensuring that it's patched 01/29/23 04:06:38.104
    STEP: updating ReplicationController status 01/29/23 04:06:38.111
    STEP: waiting for RC to be modified 01/29/23 04:06:38.118
    STEP: listing all ReplicationControllers 01/29/23 04:06:38.119
    STEP: checking that ReplicationController has expected values 01/29/23 04:06:38.125
    STEP: deleting ReplicationControllers by collection 01/29/23 04:06:38.125
    STEP: waiting for ReplicationController to have a DELETED watchEvent 01/29/23 04:06:38.139
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jan 29 04:06:38.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-7583" for this suite. 01/29/23 04:06:38.234
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:06:38.245
Jan 29 04:06:38.245: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename job 01/29/23 04:06:38.247
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:06:38.268
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:06:38.274
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
STEP: Creating a job 01/29/23 04:06:38.279
STEP: Ensuring active pods == parallelism 01/29/23 04:06:38.292
STEP: Orphaning one of the Job's Pods 01/29/23 04:06:40.301
Jan 29 04:06:40.822: INFO: Successfully updated pod "adopt-release-jvpx9"
STEP: Checking that the Job readopts the Pod 01/29/23 04:06:40.822
Jan 29 04:06:40.822: INFO: Waiting up to 15m0s for pod "adopt-release-jvpx9" in namespace "job-1394" to be "adopted"
Jan 29 04:06:40.830: INFO: Pod "adopt-release-jvpx9": Phase="Running", Reason="", readiness=true. Elapsed: 7.970855ms
Jan 29 04:06:42.840: INFO: Pod "adopt-release-jvpx9": Phase="Running", Reason="", readiness=true. Elapsed: 2.017272063s
Jan 29 04:06:42.840: INFO: Pod "adopt-release-jvpx9" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 01/29/23 04:06:42.84
Jan 29 04:06:43.360: INFO: Successfully updated pod "adopt-release-jvpx9"
STEP: Checking that the Job releases the Pod 01/29/23 04:06:43.36
Jan 29 04:06:43.360: INFO: Waiting up to 15m0s for pod "adopt-release-jvpx9" in namespace "job-1394" to be "released"
Jan 29 04:06:43.367: INFO: Pod "adopt-release-jvpx9": Phase="Running", Reason="", readiness=true. Elapsed: 6.751947ms
Jan 29 04:06:45.374: INFO: Pod "adopt-release-jvpx9": Phase="Running", Reason="", readiness=true. Elapsed: 2.013695978s
Jan 29 04:06:45.374: INFO: Pod "adopt-release-jvpx9" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jan 29 04:06:45.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1394" for this suite. 01/29/23 04:06:45.383
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","completed":254,"skipped":4434,"failed":0}
------------------------------
• [SLOW TEST] [7.146 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:06:38.245
    Jan 29 04:06:38.245: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename job 01/29/23 04:06:38.247
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:06:38.268
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:06:38.274
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:335
    STEP: Creating a job 01/29/23 04:06:38.279
    STEP: Ensuring active pods == parallelism 01/29/23 04:06:38.292
    STEP: Orphaning one of the Job's Pods 01/29/23 04:06:40.301
    Jan 29 04:06:40.822: INFO: Successfully updated pod "adopt-release-jvpx9"
    STEP: Checking that the Job readopts the Pod 01/29/23 04:06:40.822
    Jan 29 04:06:40.822: INFO: Waiting up to 15m0s for pod "adopt-release-jvpx9" in namespace "job-1394" to be "adopted"
    Jan 29 04:06:40.830: INFO: Pod "adopt-release-jvpx9": Phase="Running", Reason="", readiness=true. Elapsed: 7.970855ms
    Jan 29 04:06:42.840: INFO: Pod "adopt-release-jvpx9": Phase="Running", Reason="", readiness=true. Elapsed: 2.017272063s
    Jan 29 04:06:42.840: INFO: Pod "adopt-release-jvpx9" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 01/29/23 04:06:42.84
    Jan 29 04:06:43.360: INFO: Successfully updated pod "adopt-release-jvpx9"
    STEP: Checking that the Job releases the Pod 01/29/23 04:06:43.36
    Jan 29 04:06:43.360: INFO: Waiting up to 15m0s for pod "adopt-release-jvpx9" in namespace "job-1394" to be "released"
    Jan 29 04:06:43.367: INFO: Pod "adopt-release-jvpx9": Phase="Running", Reason="", readiness=true. Elapsed: 6.751947ms
    Jan 29 04:06:45.374: INFO: Pod "adopt-release-jvpx9": Phase="Running", Reason="", readiness=true. Elapsed: 2.013695978s
    Jan 29 04:06:45.374: INFO: Pod "adopt-release-jvpx9" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jan 29 04:06:45.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-1394" for this suite. 01/29/23 04:06:45.383
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:06:45.399
Jan 29 04:06:45.399: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename projected 01/29/23 04:06:45.4
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:06:45.422
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:06:45.429
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
STEP: Creating projection with secret that has name projected-secret-test-a11567f7-3568-47fb-862a-e9420e44d1f1 01/29/23 04:06:45.434
STEP: Creating a pod to test consume secrets 01/29/23 04:06:45.441
Jan 29 04:06:45.465: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-79082b58-da6e-44ac-bf66-2b43bc68aa00" in namespace "projected-1900" to be "Succeeded or Failed"
Jan 29 04:06:45.474: INFO: Pod "pod-projected-secrets-79082b58-da6e-44ac-bf66-2b43bc68aa00": Phase="Pending", Reason="", readiness=false. Elapsed: 8.801341ms
Jan 29 04:06:47.481: INFO: Pod "pod-projected-secrets-79082b58-da6e-44ac-bf66-2b43bc68aa00": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015986354s
Jan 29 04:06:49.481: INFO: Pod "pod-projected-secrets-79082b58-da6e-44ac-bf66-2b43bc68aa00": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015010789s
STEP: Saw pod success 01/29/23 04:06:49.481
Jan 29 04:06:49.481: INFO: Pod "pod-projected-secrets-79082b58-da6e-44ac-bf66-2b43bc68aa00" satisfied condition "Succeeded or Failed"
Jan 29 04:06:49.487: INFO: Trying to get logs from node slave1 pod pod-projected-secrets-79082b58-da6e-44ac-bf66-2b43bc68aa00 container projected-secret-volume-test: <nil>
STEP: delete the pod 01/29/23 04:06:49.519
Jan 29 04:06:49.618: INFO: Waiting for pod pod-projected-secrets-79082b58-da6e-44ac-bf66-2b43bc68aa00 to disappear
Jan 29 04:06:49.625: INFO: Pod pod-projected-secrets-79082b58-da6e-44ac-bf66-2b43bc68aa00 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan 29 04:06:49.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1900" for this suite. 01/29/23 04:06:49.634
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","completed":255,"skipped":4556,"failed":0}
------------------------------
• [4.244 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:06:45.399
    Jan 29 04:06:45.399: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename projected 01/29/23 04:06:45.4
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:06:45.422
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:06:45.429
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:45
    STEP: Creating projection with secret that has name projected-secret-test-a11567f7-3568-47fb-862a-e9420e44d1f1 01/29/23 04:06:45.434
    STEP: Creating a pod to test consume secrets 01/29/23 04:06:45.441
    Jan 29 04:06:45.465: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-79082b58-da6e-44ac-bf66-2b43bc68aa00" in namespace "projected-1900" to be "Succeeded or Failed"
    Jan 29 04:06:45.474: INFO: Pod "pod-projected-secrets-79082b58-da6e-44ac-bf66-2b43bc68aa00": Phase="Pending", Reason="", readiness=false. Elapsed: 8.801341ms
    Jan 29 04:06:47.481: INFO: Pod "pod-projected-secrets-79082b58-da6e-44ac-bf66-2b43bc68aa00": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015986354s
    Jan 29 04:06:49.481: INFO: Pod "pod-projected-secrets-79082b58-da6e-44ac-bf66-2b43bc68aa00": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015010789s
    STEP: Saw pod success 01/29/23 04:06:49.481
    Jan 29 04:06:49.481: INFO: Pod "pod-projected-secrets-79082b58-da6e-44ac-bf66-2b43bc68aa00" satisfied condition "Succeeded or Failed"
    Jan 29 04:06:49.487: INFO: Trying to get logs from node slave1 pod pod-projected-secrets-79082b58-da6e-44ac-bf66-2b43bc68aa00 container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/29/23 04:06:49.519
    Jan 29 04:06:49.618: INFO: Waiting for pod pod-projected-secrets-79082b58-da6e-44ac-bf66-2b43bc68aa00 to disappear
    Jan 29 04:06:49.625: INFO: Pod pod-projected-secrets-79082b58-da6e-44ac-bf66-2b43bc68aa00 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan 29 04:06:49.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1900" for this suite. 01/29/23 04:06:49.634
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:06:49.644
Jan 29 04:06:49.644: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename cronjob 01/29/23 04:06:49.645
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:06:49.668
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:06:49.673
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 01/29/23 04:06:49.678
STEP: Ensuring more than one job is running at a time 01/29/23 04:06:49.692
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 01/29/23 04:08:01.7
STEP: Removing cronjob 01/29/23 04:08:01.707
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jan 29 04:08:01.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-3549" for this suite. 01/29/23 04:08:01.727
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","completed":256,"skipped":4565,"failed":0}
------------------------------
• [SLOW TEST] [72.095 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:06:49.644
    Jan 29 04:06:49.644: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename cronjob 01/29/23 04:06:49.645
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:06:49.668
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:06:49.673
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 01/29/23 04:06:49.678
    STEP: Ensuring more than one job is running at a time 01/29/23 04:06:49.692
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 01/29/23 04:08:01.7
    STEP: Removing cronjob 01/29/23 04:08:01.707
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jan 29 04:08:01.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-3549" for this suite. 01/29/23 04:08:01.727
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:08:01.743
Jan 29 04:08:01.743: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename downward-api 01/29/23 04:08:01.744
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:08:01.779
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:08:01.786
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
STEP: Creating the pod 01/29/23 04:08:01.794
Jan 29 04:08:01.845: INFO: Waiting up to 5m0s for pod "labelsupdatedc3fd829-2023-4345-8cb6-ddc312dd6868" in namespace "downward-api-483" to be "running and ready"
Jan 29 04:08:01.911: INFO: Pod "labelsupdatedc3fd829-2023-4345-8cb6-ddc312dd6868": Phase="Pending", Reason="", readiness=false. Elapsed: 65.241036ms
Jan 29 04:08:01.911: INFO: The phase of Pod labelsupdatedc3fd829-2023-4345-8cb6-ddc312dd6868 is Pending, waiting for it to be Running (with Ready = true)
Jan 29 04:08:03.918: INFO: Pod "labelsupdatedc3fd829-2023-4345-8cb6-ddc312dd6868": Phase="Running", Reason="", readiness=true. Elapsed: 2.072459589s
Jan 29 04:08:03.918: INFO: The phase of Pod labelsupdatedc3fd829-2023-4345-8cb6-ddc312dd6868 is Running (Ready = true)
Jan 29 04:08:03.918: INFO: Pod "labelsupdatedc3fd829-2023-4345-8cb6-ddc312dd6868" satisfied condition "running and ready"
Jan 29 04:08:04.492: INFO: Successfully updated pod "labelsupdatedc3fd829-2023-4345-8cb6-ddc312dd6868"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 29 04:08:08.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-483" for this suite. 01/29/23 04:08:08.563
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","completed":257,"skipped":4601,"failed":0}
------------------------------
• [SLOW TEST] [6.830 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:08:01.743
    Jan 29 04:08:01.743: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename downward-api 01/29/23 04:08:01.744
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:08:01.779
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:08:01.786
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:129
    STEP: Creating the pod 01/29/23 04:08:01.794
    Jan 29 04:08:01.845: INFO: Waiting up to 5m0s for pod "labelsupdatedc3fd829-2023-4345-8cb6-ddc312dd6868" in namespace "downward-api-483" to be "running and ready"
    Jan 29 04:08:01.911: INFO: Pod "labelsupdatedc3fd829-2023-4345-8cb6-ddc312dd6868": Phase="Pending", Reason="", readiness=false. Elapsed: 65.241036ms
    Jan 29 04:08:01.911: INFO: The phase of Pod labelsupdatedc3fd829-2023-4345-8cb6-ddc312dd6868 is Pending, waiting for it to be Running (with Ready = true)
    Jan 29 04:08:03.918: INFO: Pod "labelsupdatedc3fd829-2023-4345-8cb6-ddc312dd6868": Phase="Running", Reason="", readiness=true. Elapsed: 2.072459589s
    Jan 29 04:08:03.918: INFO: The phase of Pod labelsupdatedc3fd829-2023-4345-8cb6-ddc312dd6868 is Running (Ready = true)
    Jan 29 04:08:03.918: INFO: Pod "labelsupdatedc3fd829-2023-4345-8cb6-ddc312dd6868" satisfied condition "running and ready"
    Jan 29 04:08:04.492: INFO: Successfully updated pod "labelsupdatedc3fd829-2023-4345-8cb6-ddc312dd6868"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 29 04:08:08.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-483" for this suite. 01/29/23 04:08:08.563
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:08:08.574
Jan 29 04:08:08.574: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename proxy 01/29/23 04:08:08.575
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:08:08.598
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:08:08.604
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Jan 29 04:08:08.610: INFO: Creating pod...
Jan 29 04:08:08.641: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-723" to be "running"
Jan 29 04:08:08.650: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 9.385526ms
Jan 29 04:08:10.659: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.017933208s
Jan 29 04:08:10.659: INFO: Pod "agnhost" satisfied condition "running"
Jan 29 04:08:10.659: INFO: Creating service...
Jan 29 04:08:10.676: INFO: Starting http.Client for https://100.105.0.1:443/api/v1/namespaces/proxy-723/pods/agnhost/proxy/some/path/with/DELETE
Jan 29 04:08:10.693: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 29 04:08:10.693: INFO: Starting http.Client for https://100.105.0.1:443/api/v1/namespaces/proxy-723/pods/agnhost/proxy/some/path/with/GET
Jan 29 04:08:10.713: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jan 29 04:08:10.713: INFO: Starting http.Client for https://100.105.0.1:443/api/v1/namespaces/proxy-723/pods/agnhost/proxy/some/path/with/HEAD
Jan 29 04:08:10.730: INFO: http.Client request:HEAD | StatusCode:200
Jan 29 04:08:10.730: INFO: Starting http.Client for https://100.105.0.1:443/api/v1/namespaces/proxy-723/pods/agnhost/proxy/some/path/with/OPTIONS
Jan 29 04:08:10.760: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 29 04:08:10.760: INFO: Starting http.Client for https://100.105.0.1:443/api/v1/namespaces/proxy-723/pods/agnhost/proxy/some/path/with/PATCH
Jan 29 04:08:10.769: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 29 04:08:10.769: INFO: Starting http.Client for https://100.105.0.1:443/api/v1/namespaces/proxy-723/pods/agnhost/proxy/some/path/with/POST
Jan 29 04:08:10.785: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 29 04:08:10.785: INFO: Starting http.Client for https://100.105.0.1:443/api/v1/namespaces/proxy-723/pods/agnhost/proxy/some/path/with/PUT
Jan 29 04:08:10.805: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan 29 04:08:10.805: INFO: Starting http.Client for https://100.105.0.1:443/api/v1/namespaces/proxy-723/services/test-service/proxy/some/path/with/DELETE
Jan 29 04:08:10.820: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 29 04:08:10.820: INFO: Starting http.Client for https://100.105.0.1:443/api/v1/namespaces/proxy-723/services/test-service/proxy/some/path/with/GET
Jan 29 04:08:10.834: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jan 29 04:08:10.835: INFO: Starting http.Client for https://100.105.0.1:443/api/v1/namespaces/proxy-723/services/test-service/proxy/some/path/with/HEAD
Jan 29 04:08:10.852: INFO: http.Client request:HEAD | StatusCode:200
Jan 29 04:08:10.852: INFO: Starting http.Client for https://100.105.0.1:443/api/v1/namespaces/proxy-723/services/test-service/proxy/some/path/with/OPTIONS
Jan 29 04:08:10.869: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 29 04:08:10.869: INFO: Starting http.Client for https://100.105.0.1:443/api/v1/namespaces/proxy-723/services/test-service/proxy/some/path/with/PATCH
Jan 29 04:08:10.898: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 29 04:08:10.898: INFO: Starting http.Client for https://100.105.0.1:443/api/v1/namespaces/proxy-723/services/test-service/proxy/some/path/with/POST
Jan 29 04:08:10.910: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 29 04:08:10.910: INFO: Starting http.Client for https://100.105.0.1:443/api/v1/namespaces/proxy-723/services/test-service/proxy/some/path/with/PUT
Jan 29 04:08:10.919: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Jan 29 04:08:10.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-723" for this suite. 01/29/23 04:08:10.929
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","completed":258,"skipped":4614,"failed":0}
------------------------------
• [2.364 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:08:08.574
    Jan 29 04:08:08.574: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename proxy 01/29/23 04:08:08.575
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:08:08.598
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:08:08.604
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Jan 29 04:08:08.610: INFO: Creating pod...
    Jan 29 04:08:08.641: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-723" to be "running"
    Jan 29 04:08:08.650: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 9.385526ms
    Jan 29 04:08:10.659: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.017933208s
    Jan 29 04:08:10.659: INFO: Pod "agnhost" satisfied condition "running"
    Jan 29 04:08:10.659: INFO: Creating service...
    Jan 29 04:08:10.676: INFO: Starting http.Client for https://100.105.0.1:443/api/v1/namespaces/proxy-723/pods/agnhost/proxy/some/path/with/DELETE
    Jan 29 04:08:10.693: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan 29 04:08:10.693: INFO: Starting http.Client for https://100.105.0.1:443/api/v1/namespaces/proxy-723/pods/agnhost/proxy/some/path/with/GET
    Jan 29 04:08:10.713: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Jan 29 04:08:10.713: INFO: Starting http.Client for https://100.105.0.1:443/api/v1/namespaces/proxy-723/pods/agnhost/proxy/some/path/with/HEAD
    Jan 29 04:08:10.730: INFO: http.Client request:HEAD | StatusCode:200
    Jan 29 04:08:10.730: INFO: Starting http.Client for https://100.105.0.1:443/api/v1/namespaces/proxy-723/pods/agnhost/proxy/some/path/with/OPTIONS
    Jan 29 04:08:10.760: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan 29 04:08:10.760: INFO: Starting http.Client for https://100.105.0.1:443/api/v1/namespaces/proxy-723/pods/agnhost/proxy/some/path/with/PATCH
    Jan 29 04:08:10.769: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan 29 04:08:10.769: INFO: Starting http.Client for https://100.105.0.1:443/api/v1/namespaces/proxy-723/pods/agnhost/proxy/some/path/with/POST
    Jan 29 04:08:10.785: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan 29 04:08:10.785: INFO: Starting http.Client for https://100.105.0.1:443/api/v1/namespaces/proxy-723/pods/agnhost/proxy/some/path/with/PUT
    Jan 29 04:08:10.805: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jan 29 04:08:10.805: INFO: Starting http.Client for https://100.105.0.1:443/api/v1/namespaces/proxy-723/services/test-service/proxy/some/path/with/DELETE
    Jan 29 04:08:10.820: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan 29 04:08:10.820: INFO: Starting http.Client for https://100.105.0.1:443/api/v1/namespaces/proxy-723/services/test-service/proxy/some/path/with/GET
    Jan 29 04:08:10.834: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Jan 29 04:08:10.835: INFO: Starting http.Client for https://100.105.0.1:443/api/v1/namespaces/proxy-723/services/test-service/proxy/some/path/with/HEAD
    Jan 29 04:08:10.852: INFO: http.Client request:HEAD | StatusCode:200
    Jan 29 04:08:10.852: INFO: Starting http.Client for https://100.105.0.1:443/api/v1/namespaces/proxy-723/services/test-service/proxy/some/path/with/OPTIONS
    Jan 29 04:08:10.869: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan 29 04:08:10.869: INFO: Starting http.Client for https://100.105.0.1:443/api/v1/namespaces/proxy-723/services/test-service/proxy/some/path/with/PATCH
    Jan 29 04:08:10.898: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan 29 04:08:10.898: INFO: Starting http.Client for https://100.105.0.1:443/api/v1/namespaces/proxy-723/services/test-service/proxy/some/path/with/POST
    Jan 29 04:08:10.910: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan 29 04:08:10.910: INFO: Starting http.Client for https://100.105.0.1:443/api/v1/namespaces/proxy-723/services/test-service/proxy/some/path/with/PUT
    Jan 29 04:08:10.919: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Jan 29 04:08:10.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-723" for this suite. 01/29/23 04:08:10.929
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:08:10.939
Jan 29 04:08:10.939: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename pods 01/29/23 04:08:10.94
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:08:10.973
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:08:10.979
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
STEP: Create a pod 01/29/23 04:08:10.985
Jan 29 04:08:11.004: INFO: Waiting up to 5m0s for pod "pod-wxnv4" in namespace "pods-3226" to be "running"
Jan 29 04:08:11.010: INFO: Pod "pod-wxnv4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.478445ms
Jan 29 04:08:13.019: INFO: Pod "pod-wxnv4": Phase="Running", Reason="", readiness=true. Elapsed: 2.015663892s
Jan 29 04:08:13.019: INFO: Pod "pod-wxnv4" satisfied condition "running"
STEP: patching /status 01/29/23 04:08:13.02
Jan 29 04:08:13.031: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 29 04:08:13.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3226" for this suite. 01/29/23 04:08:13.041
{"msg":"PASSED [sig-node] Pods should patch a pod status [Conformance]","completed":259,"skipped":4625,"failed":0}
------------------------------
• [2.112 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:08:10.939
    Jan 29 04:08:10.939: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename pods 01/29/23 04:08:10.94
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:08:10.973
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:08:10.979
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1082
    STEP: Create a pod 01/29/23 04:08:10.985
    Jan 29 04:08:11.004: INFO: Waiting up to 5m0s for pod "pod-wxnv4" in namespace "pods-3226" to be "running"
    Jan 29 04:08:11.010: INFO: Pod "pod-wxnv4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.478445ms
    Jan 29 04:08:13.019: INFO: Pod "pod-wxnv4": Phase="Running", Reason="", readiness=true. Elapsed: 2.015663892s
    Jan 29 04:08:13.019: INFO: Pod "pod-wxnv4" satisfied condition "running"
    STEP: patching /status 01/29/23 04:08:13.02
    Jan 29 04:08:13.031: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 29 04:08:13.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-3226" for this suite. 01/29/23 04:08:13.041
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:08:13.053
Jan 29 04:08:13.053: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename kubectl 01/29/23 04:08:13.054
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:08:13.078
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:08:13.085
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
STEP: creating Agnhost RC 01/29/23 04:08:13.092
Jan 29 04:08:13.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-2 create -f -'
Jan 29 04:08:14.321: INFO: stderr: ""
Jan 29 04:08:14.321: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 01/29/23 04:08:14.321
Jan 29 04:08:15.328: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 29 04:08:15.328: INFO: Found 0 / 1
Jan 29 04:08:16.329: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 29 04:08:16.329: INFO: Found 1 / 1
Jan 29 04:08:16.329: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 01/29/23 04:08:16.329
Jan 29 04:08:16.335: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 29 04:08:16.335: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 29 04:08:16.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-2 patch pod agnhost-primary-smdmg -p {"metadata":{"annotations":{"x":"y"}}}'
Jan 29 04:08:16.501: INFO: stderr: ""
Jan 29 04:08:16.501: INFO: stdout: "pod/agnhost-primary-smdmg patched\n"
STEP: checking annotations 01/29/23 04:08:16.501
Jan 29 04:08:16.507: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 29 04:08:16.508: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 29 04:08:16.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2" for this suite. 01/29/23 04:08:16.517
{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","completed":260,"skipped":4645,"failed":0}
------------------------------
• [3.472 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1644
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:08:13.053
    Jan 29 04:08:13.053: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename kubectl 01/29/23 04:08:13.054
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:08:13.078
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:08:13.085
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1650
    STEP: creating Agnhost RC 01/29/23 04:08:13.092
    Jan 29 04:08:13.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-2 create -f -'
    Jan 29 04:08:14.321: INFO: stderr: ""
    Jan 29 04:08:14.321: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 01/29/23 04:08:14.321
    Jan 29 04:08:15.328: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 29 04:08:15.328: INFO: Found 0 / 1
    Jan 29 04:08:16.329: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 29 04:08:16.329: INFO: Found 1 / 1
    Jan 29 04:08:16.329: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 01/29/23 04:08:16.329
    Jan 29 04:08:16.335: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 29 04:08:16.335: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jan 29 04:08:16.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-2 patch pod agnhost-primary-smdmg -p {"metadata":{"annotations":{"x":"y"}}}'
    Jan 29 04:08:16.501: INFO: stderr: ""
    Jan 29 04:08:16.501: INFO: stdout: "pod/agnhost-primary-smdmg patched\n"
    STEP: checking annotations 01/29/23 04:08:16.501
    Jan 29 04:08:16.507: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 29 04:08:16.508: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 29 04:08:16.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-2" for this suite. 01/29/23 04:08:16.517
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:08:16.526
Jan 29 04:08:16.527: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename gc 01/29/23 04:08:16.528
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:08:16.547
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:08:16.555
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Jan 29 04:08:16.625: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"24b0ccb2-3675-4bad-8086-5d57fa8ca388", Controller:(*bool)(0x4002c5045a), BlockOwnerDeletion:(*bool)(0x4002c5045b)}}
Jan 29 04:08:16.643: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"59a7a5f5-3bc1-43fa-8bfe-d738f4e30dde", Controller:(*bool)(0x400446bf72), BlockOwnerDeletion:(*bool)(0x400446bf73)}}
Jan 29 04:08:16.662: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"0e770413-3d0e-42dd-959f-e100448e1a3f", Controller:(*bool)(0x400308421a), BlockOwnerDeletion:(*bool)(0x400308421b)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan 29 04:08:21.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5242" for this suite. 01/29/23 04:08:21.717
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","completed":261,"skipped":4651,"failed":0}
------------------------------
• [SLOW TEST] [5.203 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:08:16.526
    Jan 29 04:08:16.527: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename gc 01/29/23 04:08:16.528
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:08:16.547
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:08:16.555
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Jan 29 04:08:16.625: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"24b0ccb2-3675-4bad-8086-5d57fa8ca388", Controller:(*bool)(0x4002c5045a), BlockOwnerDeletion:(*bool)(0x4002c5045b)}}
    Jan 29 04:08:16.643: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"59a7a5f5-3bc1-43fa-8bfe-d738f4e30dde", Controller:(*bool)(0x400446bf72), BlockOwnerDeletion:(*bool)(0x400446bf73)}}
    Jan 29 04:08:16.662: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"0e770413-3d0e-42dd-959f-e100448e1a3f", Controller:(*bool)(0x400308421a), BlockOwnerDeletion:(*bool)(0x400308421b)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan 29 04:08:21.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-5242" for this suite. 01/29/23 04:08:21.717
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:08:21.733
Jan 29 04:08:21.733: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename services 01/29/23 04:08:21.734
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:08:21.77
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:08:21.777
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
STEP: creating service nodeport-test with type=NodePort in namespace services-7314 01/29/23 04:08:21.785
STEP: creating replication controller nodeport-test in namespace services-7314 01/29/23 04:08:21.813
I0129 04:08:21.830401      22 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-7314, replica count: 2
I0129 04:08:24.883235      22 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 29 04:08:24.883: INFO: Creating new exec pod
Jan 29 04:08:24.897: INFO: Waiting up to 5m0s for pod "execpodz7zvq" in namespace "services-7314" to be "running"
Jan 29 04:08:24.903: INFO: Pod "execpodz7zvq": Phase="Pending", Reason="", readiness=false. Elapsed: 6.529966ms
Jan 29 04:08:26.910: INFO: Pod "execpodz7zvq": Phase="Running", Reason="", readiness=true. Elapsed: 2.013199551s
Jan 29 04:08:26.910: INFO: Pod "execpodz7zvq" satisfied condition "running"
Jan 29 04:08:27.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-7314 exec execpodz7zvq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 29 04:08:28.166: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 29 04:08:28.166: INFO: stdout: "nodeport-test-w48vd"
Jan 29 04:08:28.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-7314 exec execpodz7zvq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.105.27.155 80'
Jan 29 04:08:28.402: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.105.27.155 80\nConnection to 100.105.27.155 80 port [tcp/http] succeeded!\n"
Jan 29 04:08:28.402: INFO: stdout: "nodeport-test-w48vd"
Jan 29 04:08:28.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-7314 exec execpodz7zvq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.122.241 30735'
Jan 29 04:08:28.630: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.122.241 30735\nConnection to 192.168.122.241 30735 port [tcp/*] succeeded!\n"
Jan 29 04:08:28.630: INFO: stdout: ""
Jan 29 04:08:29.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-7314 exec execpodz7zvq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.122.241 30735'
Jan 29 04:08:29.867: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.122.241 30735\nConnection to 192.168.122.241 30735 port [tcp/*] succeeded!\n"
Jan 29 04:08:29.867: INFO: stdout: "nodeport-test-xsrlq"
Jan 29 04:08:29.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-7314 exec execpodz7zvq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.122.242 30735'
Jan 29 04:08:30.111: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.122.242 30735\nConnection to 192.168.122.242 30735 port [tcp/*] succeeded!\n"
Jan 29 04:08:30.111: INFO: stdout: "nodeport-test-xsrlq"
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 29 04:08:30.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7314" for this suite. 01/29/23 04:08:30.12
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","completed":262,"skipped":4692,"failed":0}
------------------------------
• [SLOW TEST] [8.398 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:08:21.733
    Jan 29 04:08:21.733: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename services 01/29/23 04:08:21.734
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:08:21.77
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:08:21.777
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1268
    STEP: creating service nodeport-test with type=NodePort in namespace services-7314 01/29/23 04:08:21.785
    STEP: creating replication controller nodeport-test in namespace services-7314 01/29/23 04:08:21.813
    I0129 04:08:21.830401      22 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-7314, replica count: 2
    I0129 04:08:24.883235      22 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 29 04:08:24.883: INFO: Creating new exec pod
    Jan 29 04:08:24.897: INFO: Waiting up to 5m0s for pod "execpodz7zvq" in namespace "services-7314" to be "running"
    Jan 29 04:08:24.903: INFO: Pod "execpodz7zvq": Phase="Pending", Reason="", readiness=false. Elapsed: 6.529966ms
    Jan 29 04:08:26.910: INFO: Pod "execpodz7zvq": Phase="Running", Reason="", readiness=true. Elapsed: 2.013199551s
    Jan 29 04:08:26.910: INFO: Pod "execpodz7zvq" satisfied condition "running"
    Jan 29 04:08:27.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-7314 exec execpodz7zvq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 29 04:08:28.166: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 29 04:08:28.166: INFO: stdout: "nodeport-test-w48vd"
    Jan 29 04:08:28.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-7314 exec execpodz7zvq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.105.27.155 80'
    Jan 29 04:08:28.402: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.105.27.155 80\nConnection to 100.105.27.155 80 port [tcp/http] succeeded!\n"
    Jan 29 04:08:28.402: INFO: stdout: "nodeport-test-w48vd"
    Jan 29 04:08:28.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-7314 exec execpodz7zvq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.122.241 30735'
    Jan 29 04:08:28.630: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.122.241 30735\nConnection to 192.168.122.241 30735 port [tcp/*] succeeded!\n"
    Jan 29 04:08:28.630: INFO: stdout: ""
    Jan 29 04:08:29.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-7314 exec execpodz7zvq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.122.241 30735'
    Jan 29 04:08:29.867: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.122.241 30735\nConnection to 192.168.122.241 30735 port [tcp/*] succeeded!\n"
    Jan 29 04:08:29.867: INFO: stdout: "nodeport-test-xsrlq"
    Jan 29 04:08:29.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-7314 exec execpodz7zvq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.122.242 30735'
    Jan 29 04:08:30.111: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.122.242 30735\nConnection to 192.168.122.242 30735 port [tcp/*] succeeded!\n"
    Jan 29 04:08:30.111: INFO: stdout: "nodeport-test-xsrlq"
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 29 04:08:30.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-7314" for this suite. 01/29/23 04:08:30.12
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:08:30.135
Jan 29 04:08:30.135: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename services 01/29/23 04:08:30.136
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:08:30.16
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:08:30.166
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173
STEP: creating service in namespace services-9543 01/29/23 04:08:30.171
Jan 29 04:08:30.191: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-9543" to be "running and ready"
Jan 29 04:08:30.198: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 7.17671ms
Jan 29 04:08:30.198: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Jan 29 04:08:32.215: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.024315448s
Jan 29 04:08:32.216: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
Jan 29 04:08:32.216: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
Jan 29 04:08:32.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-9543 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jan 29 04:08:32.502: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Jan 29 04:08:32.502: INFO: stdout: "iptables"
Jan 29 04:08:32.502: INFO: proxyMode: iptables
Jan 29 04:08:32.571: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jan 29 04:08:32.579: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-9543 01/29/23 04:08:32.579
STEP: creating replication controller affinity-clusterip-timeout in namespace services-9543 01/29/23 04:08:32.597
I0129 04:08:32.622169      22 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-9543, replica count: 3
I0129 04:08:35.673203      22 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 29 04:08:35.689: INFO: Creating new exec pod
Jan 29 04:08:35.704: INFO: Waiting up to 5m0s for pod "execpod-affinity54cmw" in namespace "services-9543" to be "running"
Jan 29 04:08:35.711: INFO: Pod "execpod-affinity54cmw": Phase="Pending", Reason="", readiness=false. Elapsed: 7.479433ms
Jan 29 04:08:37.719: INFO: Pod "execpod-affinity54cmw": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015455707s
Jan 29 04:08:39.720: INFO: Pod "execpod-affinity54cmw": Phase="Running", Reason="", readiness=true. Elapsed: 4.01610387s
Jan 29 04:08:39.720: INFO: Pod "execpod-affinity54cmw" satisfied condition "running"
Jan 29 04:08:40.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-9543 exec execpod-affinity54cmw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
Jan 29 04:08:40.951: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Jan 29 04:08:40.951: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 29 04:08:40.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-9543 exec execpod-affinity54cmw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.105.4.215 80'
Jan 29 04:08:41.194: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.105.4.215 80\nConnection to 100.105.4.215 80 port [tcp/http] succeeded!\n"
Jan 29 04:08:41.195: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 29 04:08:41.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-9543 exec execpod-affinity54cmw -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.105.4.215:80/ ; done'
Jan 29 04:08:41.530: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.4.215:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.4.215:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.4.215:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.4.215:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.4.215:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.4.215:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.4.215:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.4.215:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.4.215:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.4.215:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.4.215:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.4.215:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.4.215:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.4.215:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.4.215:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.4.215:80/\n"
Jan 29 04:08:41.530: INFO: stdout: "\naffinity-clusterip-timeout-r848t\naffinity-clusterip-timeout-r848t\naffinity-clusterip-timeout-r848t\naffinity-clusterip-timeout-r848t\naffinity-clusterip-timeout-r848t\naffinity-clusterip-timeout-r848t\naffinity-clusterip-timeout-r848t\naffinity-clusterip-timeout-r848t\naffinity-clusterip-timeout-r848t\naffinity-clusterip-timeout-r848t\naffinity-clusterip-timeout-r848t\naffinity-clusterip-timeout-r848t\naffinity-clusterip-timeout-r848t\naffinity-clusterip-timeout-r848t\naffinity-clusterip-timeout-r848t\naffinity-clusterip-timeout-r848t"
Jan 29 04:08:41.530: INFO: Received response from host: affinity-clusterip-timeout-r848t
Jan 29 04:08:41.530: INFO: Received response from host: affinity-clusterip-timeout-r848t
Jan 29 04:08:41.530: INFO: Received response from host: affinity-clusterip-timeout-r848t
Jan 29 04:08:41.530: INFO: Received response from host: affinity-clusterip-timeout-r848t
Jan 29 04:08:41.530: INFO: Received response from host: affinity-clusterip-timeout-r848t
Jan 29 04:08:41.530: INFO: Received response from host: affinity-clusterip-timeout-r848t
Jan 29 04:08:41.530: INFO: Received response from host: affinity-clusterip-timeout-r848t
Jan 29 04:08:41.530: INFO: Received response from host: affinity-clusterip-timeout-r848t
Jan 29 04:08:41.530: INFO: Received response from host: affinity-clusterip-timeout-r848t
Jan 29 04:08:41.530: INFO: Received response from host: affinity-clusterip-timeout-r848t
Jan 29 04:08:41.530: INFO: Received response from host: affinity-clusterip-timeout-r848t
Jan 29 04:08:41.530: INFO: Received response from host: affinity-clusterip-timeout-r848t
Jan 29 04:08:41.530: INFO: Received response from host: affinity-clusterip-timeout-r848t
Jan 29 04:08:41.530: INFO: Received response from host: affinity-clusterip-timeout-r848t
Jan 29 04:08:41.530: INFO: Received response from host: affinity-clusterip-timeout-r848t
Jan 29 04:08:41.530: INFO: Received response from host: affinity-clusterip-timeout-r848t
Jan 29 04:08:41.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-9543 exec execpod-affinity54cmw -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://100.105.4.215:80/'
Jan 29 04:08:41.784: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://100.105.4.215:80/\n"
Jan 29 04:08:41.784: INFO: stdout: "affinity-clusterip-timeout-r848t"
Jan 29 04:09:01.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-9543 exec execpod-affinity54cmw -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://100.105.4.215:80/'
Jan 29 04:09:02.054: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://100.105.4.215:80/\n"
Jan 29 04:09:02.054: INFO: stdout: "affinity-clusterip-timeout-fdhql"
Jan 29 04:09:02.054: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-9543, will wait for the garbage collector to delete the pods 01/29/23 04:09:02.167
Jan 29 04:09:02.238: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 12.344146ms
Jan 29 04:09:02.339: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 100.695544ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 29 04:09:05.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9543" for this suite. 01/29/23 04:09:05.176
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","completed":263,"skipped":4755,"failed":0}
------------------------------
• [SLOW TEST] [35.050 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:08:30.135
    Jan 29 04:08:30.135: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename services 01/29/23 04:08:30.136
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:08:30.16
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:08:30.166
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2173
    STEP: creating service in namespace services-9543 01/29/23 04:08:30.171
    Jan 29 04:08:30.191: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-9543" to be "running and ready"
    Jan 29 04:08:30.198: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 7.17671ms
    Jan 29 04:08:30.198: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
    Jan 29 04:08:32.215: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.024315448s
    Jan 29 04:08:32.216: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
    Jan 29 04:08:32.216: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
    Jan 29 04:08:32.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-9543 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
    Jan 29 04:08:32.502: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
    Jan 29 04:08:32.502: INFO: stdout: "iptables"
    Jan 29 04:08:32.502: INFO: proxyMode: iptables
    Jan 29 04:08:32.571: INFO: Waiting for pod kube-proxy-mode-detector to disappear
    Jan 29 04:08:32.579: INFO: Pod kube-proxy-mode-detector no longer exists
    STEP: creating service affinity-clusterip-timeout in namespace services-9543 01/29/23 04:08:32.579
    STEP: creating replication controller affinity-clusterip-timeout in namespace services-9543 01/29/23 04:08:32.597
    I0129 04:08:32.622169      22 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-9543, replica count: 3
    I0129 04:08:35.673203      22 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 29 04:08:35.689: INFO: Creating new exec pod
    Jan 29 04:08:35.704: INFO: Waiting up to 5m0s for pod "execpod-affinity54cmw" in namespace "services-9543" to be "running"
    Jan 29 04:08:35.711: INFO: Pod "execpod-affinity54cmw": Phase="Pending", Reason="", readiness=false. Elapsed: 7.479433ms
    Jan 29 04:08:37.719: INFO: Pod "execpod-affinity54cmw": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015455707s
    Jan 29 04:08:39.720: INFO: Pod "execpod-affinity54cmw": Phase="Running", Reason="", readiness=true. Elapsed: 4.01610387s
    Jan 29 04:08:39.720: INFO: Pod "execpod-affinity54cmw" satisfied condition "running"
    Jan 29 04:08:40.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-9543 exec execpod-affinity54cmw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
    Jan 29 04:08:40.951: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
    Jan 29 04:08:40.951: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 29 04:08:40.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-9543 exec execpod-affinity54cmw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.105.4.215 80'
    Jan 29 04:08:41.194: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.105.4.215 80\nConnection to 100.105.4.215 80 port [tcp/http] succeeded!\n"
    Jan 29 04:08:41.195: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 29 04:08:41.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-9543 exec execpod-affinity54cmw -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.105.4.215:80/ ; done'
    Jan 29 04:08:41.530: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.4.215:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.4.215:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.4.215:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.4.215:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.4.215:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.4.215:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.4.215:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.4.215:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.4.215:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.4.215:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.4.215:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.4.215:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.4.215:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.4.215:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.4.215:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.4.215:80/\n"
    Jan 29 04:08:41.530: INFO: stdout: "\naffinity-clusterip-timeout-r848t\naffinity-clusterip-timeout-r848t\naffinity-clusterip-timeout-r848t\naffinity-clusterip-timeout-r848t\naffinity-clusterip-timeout-r848t\naffinity-clusterip-timeout-r848t\naffinity-clusterip-timeout-r848t\naffinity-clusterip-timeout-r848t\naffinity-clusterip-timeout-r848t\naffinity-clusterip-timeout-r848t\naffinity-clusterip-timeout-r848t\naffinity-clusterip-timeout-r848t\naffinity-clusterip-timeout-r848t\naffinity-clusterip-timeout-r848t\naffinity-clusterip-timeout-r848t\naffinity-clusterip-timeout-r848t"
    Jan 29 04:08:41.530: INFO: Received response from host: affinity-clusterip-timeout-r848t
    Jan 29 04:08:41.530: INFO: Received response from host: affinity-clusterip-timeout-r848t
    Jan 29 04:08:41.530: INFO: Received response from host: affinity-clusterip-timeout-r848t
    Jan 29 04:08:41.530: INFO: Received response from host: affinity-clusterip-timeout-r848t
    Jan 29 04:08:41.530: INFO: Received response from host: affinity-clusterip-timeout-r848t
    Jan 29 04:08:41.530: INFO: Received response from host: affinity-clusterip-timeout-r848t
    Jan 29 04:08:41.530: INFO: Received response from host: affinity-clusterip-timeout-r848t
    Jan 29 04:08:41.530: INFO: Received response from host: affinity-clusterip-timeout-r848t
    Jan 29 04:08:41.530: INFO: Received response from host: affinity-clusterip-timeout-r848t
    Jan 29 04:08:41.530: INFO: Received response from host: affinity-clusterip-timeout-r848t
    Jan 29 04:08:41.530: INFO: Received response from host: affinity-clusterip-timeout-r848t
    Jan 29 04:08:41.530: INFO: Received response from host: affinity-clusterip-timeout-r848t
    Jan 29 04:08:41.530: INFO: Received response from host: affinity-clusterip-timeout-r848t
    Jan 29 04:08:41.530: INFO: Received response from host: affinity-clusterip-timeout-r848t
    Jan 29 04:08:41.530: INFO: Received response from host: affinity-clusterip-timeout-r848t
    Jan 29 04:08:41.530: INFO: Received response from host: affinity-clusterip-timeout-r848t
    Jan 29 04:08:41.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-9543 exec execpod-affinity54cmw -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://100.105.4.215:80/'
    Jan 29 04:08:41.784: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://100.105.4.215:80/\n"
    Jan 29 04:08:41.784: INFO: stdout: "affinity-clusterip-timeout-r848t"
    Jan 29 04:09:01.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-9543 exec execpod-affinity54cmw -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://100.105.4.215:80/'
    Jan 29 04:09:02.054: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://100.105.4.215:80/\n"
    Jan 29 04:09:02.054: INFO: stdout: "affinity-clusterip-timeout-fdhql"
    Jan 29 04:09:02.054: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-9543, will wait for the garbage collector to delete the pods 01/29/23 04:09:02.167
    Jan 29 04:09:02.238: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 12.344146ms
    Jan 29 04:09:02.339: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 100.695544ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 29 04:09:05.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-9543" for this suite. 01/29/23 04:09:05.176
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:09:05.186
Jan 29 04:09:05.186: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename container-probe 01/29/23 04:09:05.187
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:09:05.211
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:09:05.217
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
STEP: Creating pod busybox-520e7a93-3046-4029-98c3-5ebbd2ced04f in namespace container-probe-184 01/29/23 04:09:05.223
Jan 29 04:09:05.243: INFO: Waiting up to 5m0s for pod "busybox-520e7a93-3046-4029-98c3-5ebbd2ced04f" in namespace "container-probe-184" to be "not pending"
Jan 29 04:09:05.252: INFO: Pod "busybox-520e7a93-3046-4029-98c3-5ebbd2ced04f": Phase="Pending", Reason="", readiness=false. Elapsed: 9.117503ms
Jan 29 04:09:07.260: INFO: Pod "busybox-520e7a93-3046-4029-98c3-5ebbd2ced04f": Phase="Running", Reason="", readiness=true. Elapsed: 2.016996077s
Jan 29 04:09:07.260: INFO: Pod "busybox-520e7a93-3046-4029-98c3-5ebbd2ced04f" satisfied condition "not pending"
Jan 29 04:09:07.260: INFO: Started pod busybox-520e7a93-3046-4029-98c3-5ebbd2ced04f in namespace container-probe-184
STEP: checking the pod's current state and verifying that restartCount is present 01/29/23 04:09:07.26
Jan 29 04:09:07.267: INFO: Initial restart count of pod busybox-520e7a93-3046-4029-98c3-5ebbd2ced04f is 0
Jan 29 04:09:57.485: INFO: Restart count of pod container-probe-184/busybox-520e7a93-3046-4029-98c3-5ebbd2ced04f is now 1 (50.217412808s elapsed)
STEP: deleting the pod 01/29/23 04:09:57.485
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan 29 04:09:57.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-184" for this suite. 01/29/23 04:09:57.586
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":264,"skipped":4764,"failed":0}
------------------------------
• [SLOW TEST] [52.410 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:09:05.186
    Jan 29 04:09:05.186: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename container-probe 01/29/23 04:09:05.187
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:09:05.211
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:09:05.217
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:131
    STEP: Creating pod busybox-520e7a93-3046-4029-98c3-5ebbd2ced04f in namespace container-probe-184 01/29/23 04:09:05.223
    Jan 29 04:09:05.243: INFO: Waiting up to 5m0s for pod "busybox-520e7a93-3046-4029-98c3-5ebbd2ced04f" in namespace "container-probe-184" to be "not pending"
    Jan 29 04:09:05.252: INFO: Pod "busybox-520e7a93-3046-4029-98c3-5ebbd2ced04f": Phase="Pending", Reason="", readiness=false. Elapsed: 9.117503ms
    Jan 29 04:09:07.260: INFO: Pod "busybox-520e7a93-3046-4029-98c3-5ebbd2ced04f": Phase="Running", Reason="", readiness=true. Elapsed: 2.016996077s
    Jan 29 04:09:07.260: INFO: Pod "busybox-520e7a93-3046-4029-98c3-5ebbd2ced04f" satisfied condition "not pending"
    Jan 29 04:09:07.260: INFO: Started pod busybox-520e7a93-3046-4029-98c3-5ebbd2ced04f in namespace container-probe-184
    STEP: checking the pod's current state and verifying that restartCount is present 01/29/23 04:09:07.26
    Jan 29 04:09:07.267: INFO: Initial restart count of pod busybox-520e7a93-3046-4029-98c3-5ebbd2ced04f is 0
    Jan 29 04:09:57.485: INFO: Restart count of pod container-probe-184/busybox-520e7a93-3046-4029-98c3-5ebbd2ced04f is now 1 (50.217412808s elapsed)
    STEP: deleting the pod 01/29/23 04:09:57.485
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan 29 04:09:57.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-184" for this suite. 01/29/23 04:09:57.586
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:09:57.598
Jan 29 04:09:57.598: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename pods 01/29/23 04:09:57.599
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:09:57.623
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:09:57.628
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
STEP: creating the pod 01/29/23 04:09:57.634
STEP: submitting the pod to kubernetes 01/29/23 04:09:57.634
Jan 29 04:09:57.652: INFO: Waiting up to 5m0s for pod "pod-update-2867be30-fded-49f6-a601-3ee9073a6d9f" in namespace "pods-3722" to be "running and ready"
Jan 29 04:09:57.659: INFO: Pod "pod-update-2867be30-fded-49f6-a601-3ee9073a6d9f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.485385ms
Jan 29 04:09:57.659: INFO: The phase of Pod pod-update-2867be30-fded-49f6-a601-3ee9073a6d9f is Pending, waiting for it to be Running (with Ready = true)
Jan 29 04:09:59.666: INFO: Pod "pod-update-2867be30-fded-49f6-a601-3ee9073a6d9f": Phase="Running", Reason="", readiness=true. Elapsed: 2.013952976s
Jan 29 04:09:59.667: INFO: The phase of Pod pod-update-2867be30-fded-49f6-a601-3ee9073a6d9f is Running (Ready = true)
Jan 29 04:09:59.667: INFO: Pod "pod-update-2867be30-fded-49f6-a601-3ee9073a6d9f" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 01/29/23 04:09:59.674
STEP: updating the pod 01/29/23 04:09:59.68
Jan 29 04:10:00.196: INFO: Successfully updated pod "pod-update-2867be30-fded-49f6-a601-3ee9073a6d9f"
Jan 29 04:10:00.196: INFO: Waiting up to 5m0s for pod "pod-update-2867be30-fded-49f6-a601-3ee9073a6d9f" in namespace "pods-3722" to be "running"
Jan 29 04:10:00.202: INFO: Pod "pod-update-2867be30-fded-49f6-a601-3ee9073a6d9f": Phase="Running", Reason="", readiness=true. Elapsed: 5.986062ms
Jan 29 04:10:00.202: INFO: Pod "pod-update-2867be30-fded-49f6-a601-3ee9073a6d9f" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 01/29/23 04:10:00.202
Jan 29 04:10:00.211: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 29 04:10:00.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3722" for this suite. 01/29/23 04:10:00.22
{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","completed":265,"skipped":4779,"failed":0}
------------------------------
• [2.632 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:09:57.598
    Jan 29 04:09:57.598: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename pods 01/29/23 04:09:57.599
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:09:57.623
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:09:57.628
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:343
    STEP: creating the pod 01/29/23 04:09:57.634
    STEP: submitting the pod to kubernetes 01/29/23 04:09:57.634
    Jan 29 04:09:57.652: INFO: Waiting up to 5m0s for pod "pod-update-2867be30-fded-49f6-a601-3ee9073a6d9f" in namespace "pods-3722" to be "running and ready"
    Jan 29 04:09:57.659: INFO: Pod "pod-update-2867be30-fded-49f6-a601-3ee9073a6d9f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.485385ms
    Jan 29 04:09:57.659: INFO: The phase of Pod pod-update-2867be30-fded-49f6-a601-3ee9073a6d9f is Pending, waiting for it to be Running (with Ready = true)
    Jan 29 04:09:59.666: INFO: Pod "pod-update-2867be30-fded-49f6-a601-3ee9073a6d9f": Phase="Running", Reason="", readiness=true. Elapsed: 2.013952976s
    Jan 29 04:09:59.667: INFO: The phase of Pod pod-update-2867be30-fded-49f6-a601-3ee9073a6d9f is Running (Ready = true)
    Jan 29 04:09:59.667: INFO: Pod "pod-update-2867be30-fded-49f6-a601-3ee9073a6d9f" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 01/29/23 04:09:59.674
    STEP: updating the pod 01/29/23 04:09:59.68
    Jan 29 04:10:00.196: INFO: Successfully updated pod "pod-update-2867be30-fded-49f6-a601-3ee9073a6d9f"
    Jan 29 04:10:00.196: INFO: Waiting up to 5m0s for pod "pod-update-2867be30-fded-49f6-a601-3ee9073a6d9f" in namespace "pods-3722" to be "running"
    Jan 29 04:10:00.202: INFO: Pod "pod-update-2867be30-fded-49f6-a601-3ee9073a6d9f": Phase="Running", Reason="", readiness=true. Elapsed: 5.986062ms
    Jan 29 04:10:00.202: INFO: Pod "pod-update-2867be30-fded-49f6-a601-3ee9073a6d9f" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 01/29/23 04:10:00.202
    Jan 29 04:10:00.211: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 29 04:10:00.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-3722" for this suite. 01/29/23 04:10:00.22
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:10:00.231
Jan 29 04:10:00.231: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename configmap 01/29/23 04:10:00.233
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:10:00.256
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:10:00.263
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
STEP: Creating configMap with name configmap-test-volume-map-52b91698-45ca-4e28-bde0-14cb2455d525 01/29/23 04:10:00.27
STEP: Creating a pod to test consume configMaps 01/29/23 04:10:00.281
Jan 29 04:10:00.300: INFO: Waiting up to 5m0s for pod "pod-configmaps-d9ab0707-cdae-45ad-aacb-6fd13789e056" in namespace "configmap-2924" to be "Succeeded or Failed"
Jan 29 04:10:00.308: INFO: Pod "pod-configmaps-d9ab0707-cdae-45ad-aacb-6fd13789e056": Phase="Pending", Reason="", readiness=false. Elapsed: 8.035696ms
Jan 29 04:10:02.326: INFO: Pod "pod-configmaps-d9ab0707-cdae-45ad-aacb-6fd13789e056": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025638458s
Jan 29 04:10:04.322: INFO: Pod "pod-configmaps-d9ab0707-cdae-45ad-aacb-6fd13789e056": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022013491s
STEP: Saw pod success 01/29/23 04:10:04.322
Jan 29 04:10:04.323: INFO: Pod "pod-configmaps-d9ab0707-cdae-45ad-aacb-6fd13789e056" satisfied condition "Succeeded or Failed"
Jan 29 04:10:04.335: INFO: Trying to get logs from node slave2 pod pod-configmaps-d9ab0707-cdae-45ad-aacb-6fd13789e056 container agnhost-container: <nil>
STEP: delete the pod 01/29/23 04:10:04.397
Jan 29 04:10:04.565: INFO: Waiting for pod pod-configmaps-d9ab0707-cdae-45ad-aacb-6fd13789e056 to disappear
Jan 29 04:10:04.576: INFO: Pod pod-configmaps-d9ab0707-cdae-45ad-aacb-6fd13789e056 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 29 04:10:04.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2924" for this suite. 01/29/23 04:10:04.593
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":266,"skipped":4792,"failed":0}
------------------------------
• [4.385 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:10:00.231
    Jan 29 04:10:00.231: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename configmap 01/29/23 04:10:00.233
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:10:00.256
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:10:00.263
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:88
    STEP: Creating configMap with name configmap-test-volume-map-52b91698-45ca-4e28-bde0-14cb2455d525 01/29/23 04:10:00.27
    STEP: Creating a pod to test consume configMaps 01/29/23 04:10:00.281
    Jan 29 04:10:00.300: INFO: Waiting up to 5m0s for pod "pod-configmaps-d9ab0707-cdae-45ad-aacb-6fd13789e056" in namespace "configmap-2924" to be "Succeeded or Failed"
    Jan 29 04:10:00.308: INFO: Pod "pod-configmaps-d9ab0707-cdae-45ad-aacb-6fd13789e056": Phase="Pending", Reason="", readiness=false. Elapsed: 8.035696ms
    Jan 29 04:10:02.326: INFO: Pod "pod-configmaps-d9ab0707-cdae-45ad-aacb-6fd13789e056": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025638458s
    Jan 29 04:10:04.322: INFO: Pod "pod-configmaps-d9ab0707-cdae-45ad-aacb-6fd13789e056": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022013491s
    STEP: Saw pod success 01/29/23 04:10:04.322
    Jan 29 04:10:04.323: INFO: Pod "pod-configmaps-d9ab0707-cdae-45ad-aacb-6fd13789e056" satisfied condition "Succeeded or Failed"
    Jan 29 04:10:04.335: INFO: Trying to get logs from node slave2 pod pod-configmaps-d9ab0707-cdae-45ad-aacb-6fd13789e056 container agnhost-container: <nil>
    STEP: delete the pod 01/29/23 04:10:04.397
    Jan 29 04:10:04.565: INFO: Waiting for pod pod-configmaps-d9ab0707-cdae-45ad-aacb-6fd13789e056 to disappear
    Jan 29 04:10:04.576: INFO: Pod pod-configmaps-d9ab0707-cdae-45ad-aacb-6fd13789e056 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 29 04:10:04.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-2924" for this suite. 01/29/23 04:10:04.593
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:10:04.618
Jan 29 04:10:04.618: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename subpath 01/29/23 04:10:04.621
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:10:04.67
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:10:04.677
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/29/23 04:10:04.685
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-7bzs 01/29/23 04:10:04.711
STEP: Creating a pod to test atomic-volume-subpath 01/29/23 04:10:04.711
Jan 29 04:10:04.734: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-7bzs" in namespace "subpath-231" to be "Succeeded or Failed"
Jan 29 04:10:04.742: INFO: Pod "pod-subpath-test-downwardapi-7bzs": Phase="Pending", Reason="", readiness=false. Elapsed: 8.077456ms
Jan 29 04:10:06.750: INFO: Pod "pod-subpath-test-downwardapi-7bzs": Phase="Running", Reason="", readiness=true. Elapsed: 2.015417726s
Jan 29 04:10:08.750: INFO: Pod "pod-subpath-test-downwardapi-7bzs": Phase="Running", Reason="", readiness=true. Elapsed: 4.015940009s
Jan 29 04:10:10.749: INFO: Pod "pod-subpath-test-downwardapi-7bzs": Phase="Running", Reason="", readiness=true. Elapsed: 6.015128402s
Jan 29 04:10:12.750: INFO: Pod "pod-subpath-test-downwardapi-7bzs": Phase="Running", Reason="", readiness=true. Elapsed: 8.015987886s
Jan 29 04:10:14.751: INFO: Pod "pod-subpath-test-downwardapi-7bzs": Phase="Running", Reason="", readiness=true. Elapsed: 10.016512129s
Jan 29 04:10:16.751: INFO: Pod "pod-subpath-test-downwardapi-7bzs": Phase="Running", Reason="", readiness=true. Elapsed: 12.016969931s
Jan 29 04:10:18.762: INFO: Pod "pod-subpath-test-downwardapi-7bzs": Phase="Running", Reason="", readiness=true. Elapsed: 14.027396022s
Jan 29 04:10:20.751: INFO: Pod "pod-subpath-test-downwardapi-7bzs": Phase="Running", Reason="", readiness=true. Elapsed: 16.016672786s
Jan 29 04:10:22.752: INFO: Pod "pod-subpath-test-downwardapi-7bzs": Phase="Running", Reason="", readiness=true. Elapsed: 18.017594231s
Jan 29 04:10:24.752: INFO: Pod "pod-subpath-test-downwardapi-7bzs": Phase="Running", Reason="", readiness=true. Elapsed: 20.018011733s
Jan 29 04:10:26.750: INFO: Pod "pod-subpath-test-downwardapi-7bzs": Phase="Running", Reason="", readiness=true. Elapsed: 22.015699476s
Jan 29 04:10:28.751: INFO: Pod "pod-subpath-test-downwardapi-7bzs": Phase="Running", Reason="", readiness=false. Elapsed: 24.016665041s
Jan 29 04:10:30.751: INFO: Pod "pod-subpath-test-downwardapi-7bzs": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.016369904s
STEP: Saw pod success 01/29/23 04:10:30.751
Jan 29 04:10:30.751: INFO: Pod "pod-subpath-test-downwardapi-7bzs" satisfied condition "Succeeded or Failed"
Jan 29 04:10:30.758: INFO: Trying to get logs from node slave2 pod pod-subpath-test-downwardapi-7bzs container test-container-subpath-downwardapi-7bzs: <nil>
STEP: delete the pod 01/29/23 04:10:30.776
Jan 29 04:10:30.891: INFO: Waiting for pod pod-subpath-test-downwardapi-7bzs to disappear
Jan 29 04:10:30.897: INFO: Pod pod-subpath-test-downwardapi-7bzs no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-7bzs 01/29/23 04:10:30.897
Jan 29 04:10:30.898: INFO: Deleting pod "pod-subpath-test-downwardapi-7bzs" in namespace "subpath-231"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jan 29 04:10:30.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-231" for this suite. 01/29/23 04:10:30.914
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]","completed":267,"skipped":4799,"failed":0}
------------------------------
• [SLOW TEST] [26.305 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:10:04.618
    Jan 29 04:10:04.618: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename subpath 01/29/23 04:10:04.621
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:10:04.67
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:10:04.677
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/29/23 04:10:04.685
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-7bzs 01/29/23 04:10:04.711
    STEP: Creating a pod to test atomic-volume-subpath 01/29/23 04:10:04.711
    Jan 29 04:10:04.734: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-7bzs" in namespace "subpath-231" to be "Succeeded or Failed"
    Jan 29 04:10:04.742: INFO: Pod "pod-subpath-test-downwardapi-7bzs": Phase="Pending", Reason="", readiness=false. Elapsed: 8.077456ms
    Jan 29 04:10:06.750: INFO: Pod "pod-subpath-test-downwardapi-7bzs": Phase="Running", Reason="", readiness=true. Elapsed: 2.015417726s
    Jan 29 04:10:08.750: INFO: Pod "pod-subpath-test-downwardapi-7bzs": Phase="Running", Reason="", readiness=true. Elapsed: 4.015940009s
    Jan 29 04:10:10.749: INFO: Pod "pod-subpath-test-downwardapi-7bzs": Phase="Running", Reason="", readiness=true. Elapsed: 6.015128402s
    Jan 29 04:10:12.750: INFO: Pod "pod-subpath-test-downwardapi-7bzs": Phase="Running", Reason="", readiness=true. Elapsed: 8.015987886s
    Jan 29 04:10:14.751: INFO: Pod "pod-subpath-test-downwardapi-7bzs": Phase="Running", Reason="", readiness=true. Elapsed: 10.016512129s
    Jan 29 04:10:16.751: INFO: Pod "pod-subpath-test-downwardapi-7bzs": Phase="Running", Reason="", readiness=true. Elapsed: 12.016969931s
    Jan 29 04:10:18.762: INFO: Pod "pod-subpath-test-downwardapi-7bzs": Phase="Running", Reason="", readiness=true. Elapsed: 14.027396022s
    Jan 29 04:10:20.751: INFO: Pod "pod-subpath-test-downwardapi-7bzs": Phase="Running", Reason="", readiness=true. Elapsed: 16.016672786s
    Jan 29 04:10:22.752: INFO: Pod "pod-subpath-test-downwardapi-7bzs": Phase="Running", Reason="", readiness=true. Elapsed: 18.017594231s
    Jan 29 04:10:24.752: INFO: Pod "pod-subpath-test-downwardapi-7bzs": Phase="Running", Reason="", readiness=true. Elapsed: 20.018011733s
    Jan 29 04:10:26.750: INFO: Pod "pod-subpath-test-downwardapi-7bzs": Phase="Running", Reason="", readiness=true. Elapsed: 22.015699476s
    Jan 29 04:10:28.751: INFO: Pod "pod-subpath-test-downwardapi-7bzs": Phase="Running", Reason="", readiness=false. Elapsed: 24.016665041s
    Jan 29 04:10:30.751: INFO: Pod "pod-subpath-test-downwardapi-7bzs": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.016369904s
    STEP: Saw pod success 01/29/23 04:10:30.751
    Jan 29 04:10:30.751: INFO: Pod "pod-subpath-test-downwardapi-7bzs" satisfied condition "Succeeded or Failed"
    Jan 29 04:10:30.758: INFO: Trying to get logs from node slave2 pod pod-subpath-test-downwardapi-7bzs container test-container-subpath-downwardapi-7bzs: <nil>
    STEP: delete the pod 01/29/23 04:10:30.776
    Jan 29 04:10:30.891: INFO: Waiting for pod pod-subpath-test-downwardapi-7bzs to disappear
    Jan 29 04:10:30.897: INFO: Pod pod-subpath-test-downwardapi-7bzs no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-7bzs 01/29/23 04:10:30.897
    Jan 29 04:10:30.898: INFO: Deleting pod "pod-subpath-test-downwardapi-7bzs" in namespace "subpath-231"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jan 29 04:10:30.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-231" for this suite. 01/29/23 04:10:30.914
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:10:30.928
Jan 29 04:10:30.928: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename kubectl 01/29/23 04:10:30.929
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:10:30.964
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:10:30.97
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/29/23 04:10:30.977
Jan 29 04:10:30.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8381 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jan 29 04:10:31.121: INFO: stderr: ""
Jan 29 04:10:31.121: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 01/29/23 04:10:31.121
Jan 29 04:10:31.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8381 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
Jan 29 04:10:32.212: INFO: stderr: ""
Jan 29 04:10:32.212: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/29/23 04:10:32.212
Jan 29 04:10:32.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8381 delete pods e2e-test-httpd-pod'
Jan 29 04:10:35.709: INFO: stderr: ""
Jan 29 04:10:35.709: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 29 04:10:35.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8381" for this suite. 01/29/23 04:10:35.72
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","completed":268,"skipped":4871,"failed":0}
------------------------------
• [4.802 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:954
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:960

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:10:30.928
    Jan 29 04:10:30.928: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename kubectl 01/29/23 04:10:30.929
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:10:30.964
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:10:30.97
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:960
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/29/23 04:10:30.977
    Jan 29 04:10:30.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8381 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Jan 29 04:10:31.121: INFO: stderr: ""
    Jan 29 04:10:31.121: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 01/29/23 04:10:31.121
    Jan 29 04:10:31.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8381 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
    Jan 29 04:10:32.212: INFO: stderr: ""
    Jan 29 04:10:32.212: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/29/23 04:10:32.212
    Jan 29 04:10:32.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-8381 delete pods e2e-test-httpd-pod'
    Jan 29 04:10:35.709: INFO: stderr: ""
    Jan 29 04:10:35.709: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 29 04:10:35.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-8381" for this suite. 01/29/23 04:10:35.72
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:10:35.732
Jan 29 04:10:35.732: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename secrets 01/29/23 04:10:35.733
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:10:35.756
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:10:35.762
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
STEP: creating a secret 01/29/23 04:10:35.768
STEP: listing secrets in all namespaces to ensure that there are more than zero 01/29/23 04:10:35.777
STEP: patching the secret 01/29/23 04:10:35.786
STEP: deleting the secret using a LabelSelector 01/29/23 04:10:35.802
STEP: listing secrets in all namespaces, searching for label name and value in patch 01/29/23 04:10:35.82
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Jan 29 04:10:35.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3634" for this suite. 01/29/23 04:10:35.84
{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","completed":269,"skipped":4886,"failed":0}
------------------------------
• [0.118 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:10:35.732
    Jan 29 04:10:35.732: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename secrets 01/29/23 04:10:35.733
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:10:35.756
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:10:35.762
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:153
    STEP: creating a secret 01/29/23 04:10:35.768
    STEP: listing secrets in all namespaces to ensure that there are more than zero 01/29/23 04:10:35.777
    STEP: patching the secret 01/29/23 04:10:35.786
    STEP: deleting the secret using a LabelSelector 01/29/23 04:10:35.802
    STEP: listing secrets in all namespaces, searching for label name and value in patch 01/29/23 04:10:35.82
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Jan 29 04:10:35.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-3634" for this suite. 01/29/23 04:10:35.84
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:10:35.851
Jan 29 04:10:35.851: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename secrets 01/29/23 04:10:35.852
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:10:35.874
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:10:35.88
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
STEP: Creating secret with name secret-test-14d6054f-9576-4822-b865-a3372b4b203f 01/29/23 04:10:35.885
STEP: Creating a pod to test consume secrets 01/29/23 04:10:35.893
Jan 29 04:10:35.914: INFO: Waiting up to 5m0s for pod "pod-secrets-f923e047-0534-4615-86e3-5accd39b6599" in namespace "secrets-7454" to be "Succeeded or Failed"
Jan 29 04:10:35.921: INFO: Pod "pod-secrets-f923e047-0534-4615-86e3-5accd39b6599": Phase="Pending", Reason="", readiness=false. Elapsed: 7.314031ms
Jan 29 04:10:37.929: INFO: Pod "pod-secrets-f923e047-0534-4615-86e3-5accd39b6599": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014926609s
Jan 29 04:10:39.928: INFO: Pod "pod-secrets-f923e047-0534-4615-86e3-5accd39b6599": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014532931s
STEP: Saw pod success 01/29/23 04:10:39.928
Jan 29 04:10:39.929: INFO: Pod "pod-secrets-f923e047-0534-4615-86e3-5accd39b6599" satisfied condition "Succeeded or Failed"
Jan 29 04:10:39.936: INFO: Trying to get logs from node slave2 pod pod-secrets-f923e047-0534-4615-86e3-5accd39b6599 container secret-volume-test: <nil>
STEP: delete the pod 01/29/23 04:10:39.95
Jan 29 04:10:40.046: INFO: Waiting for pod pod-secrets-f923e047-0534-4615-86e3-5accd39b6599 to disappear
Jan 29 04:10:40.052: INFO: Pod pod-secrets-f923e047-0534-4615-86e3-5accd39b6599 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 29 04:10:40.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7454" for this suite. 01/29/23 04:10:40.064
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":270,"skipped":4896,"failed":0}
------------------------------
• [4.225 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:10:35.851
    Jan 29 04:10:35.851: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename secrets 01/29/23 04:10:35.852
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:10:35.874
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:10:35.88
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:56
    STEP: Creating secret with name secret-test-14d6054f-9576-4822-b865-a3372b4b203f 01/29/23 04:10:35.885
    STEP: Creating a pod to test consume secrets 01/29/23 04:10:35.893
    Jan 29 04:10:35.914: INFO: Waiting up to 5m0s for pod "pod-secrets-f923e047-0534-4615-86e3-5accd39b6599" in namespace "secrets-7454" to be "Succeeded or Failed"
    Jan 29 04:10:35.921: INFO: Pod "pod-secrets-f923e047-0534-4615-86e3-5accd39b6599": Phase="Pending", Reason="", readiness=false. Elapsed: 7.314031ms
    Jan 29 04:10:37.929: INFO: Pod "pod-secrets-f923e047-0534-4615-86e3-5accd39b6599": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014926609s
    Jan 29 04:10:39.928: INFO: Pod "pod-secrets-f923e047-0534-4615-86e3-5accd39b6599": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014532931s
    STEP: Saw pod success 01/29/23 04:10:39.928
    Jan 29 04:10:39.929: INFO: Pod "pod-secrets-f923e047-0534-4615-86e3-5accd39b6599" satisfied condition "Succeeded or Failed"
    Jan 29 04:10:39.936: INFO: Trying to get logs from node slave2 pod pod-secrets-f923e047-0534-4615-86e3-5accd39b6599 container secret-volume-test: <nil>
    STEP: delete the pod 01/29/23 04:10:39.95
    Jan 29 04:10:40.046: INFO: Waiting for pod pod-secrets-f923e047-0534-4615-86e3-5accd39b6599 to disappear
    Jan 29 04:10:40.052: INFO: Pod pod-secrets-f923e047-0534-4615-86e3-5accd39b6599 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 29 04:10:40.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-7454" for this suite. 01/29/23 04:10:40.064
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:10:40.076
Jan 29 04:10:40.077: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename tables 01/29/23 04:10:40.078
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:10:40.102
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:10:40.108
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:187
Jan 29 04:10:40.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-1736" for this suite. 01/29/23 04:10:40.127
{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","completed":271,"skipped":4899,"failed":0}
------------------------------
• [0.061 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:10:40.076
    Jan 29 04:10:40.077: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename tables 01/29/23 04:10:40.078
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:10:40.102
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:10:40.108
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:187
    Jan 29 04:10:40.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "tables-1736" for this suite. 01/29/23 04:10:40.127
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:10:40.138
Jan 29 04:10:40.138: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename job 01/29/23 04:10:40.14
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:10:40.162
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:10:40.168
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
STEP: Creating a job 01/29/23 04:10:40.173
STEP: Ensure pods equal to paralellism count is attached to the job 01/29/23 04:10:40.186
STEP: patching /status 01/29/23 04:10:42.196
STEP: updating /status 01/29/23 04:10:42.205
STEP: get /status 01/29/23 04:10:42.243
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jan 29 04:10:42.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7014" for this suite. 01/29/23 04:10:42.262
{"msg":"PASSED [sig-apps] Job should apply changes to a job status [Conformance]","completed":272,"skipped":4907,"failed":0}
------------------------------
• [2.134 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:10:40.138
    Jan 29 04:10:40.138: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename job 01/29/23 04:10:40.14
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:10:40.162
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:10:40.168
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:464
    STEP: Creating a job 01/29/23 04:10:40.173
    STEP: Ensure pods equal to paralellism count is attached to the job 01/29/23 04:10:40.186
    STEP: patching /status 01/29/23 04:10:42.196
    STEP: updating /status 01/29/23 04:10:42.205
    STEP: get /status 01/29/23 04:10:42.243
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jan 29 04:10:42.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-7014" for this suite. 01/29/23 04:10:42.262
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:10:42.273
Jan 29 04:10:42.273: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename init-container 01/29/23 04:10:42.275
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:10:42.3
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:10:42.306
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
STEP: creating the pod 01/29/23 04:10:42.314
Jan 29 04:10:42.314: INFO: PodSpec: initContainers in spec.initContainers
Jan 29 04:11:24.105: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-68cee7f2-7dad-4b37-938b-599cc914b380", GenerateName:"", Namespace:"init-container-6434", SelfLink:"", UID:"02dad7f5-1ebe-4db7-b4b3-a33abfbc1421", ResourceVersion:"5978974", Generation:0, CreationTimestamp:time.Date(2023, time.January, 29, 4, 10, 42, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"314401292"}, Annotations:map[string]string{"k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"\",\n    \"ips\": [\n        \"100.101.49.143\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]", "k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"\",\n    \"ips\": [\n        \"100.101.49.143\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-gh62g", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0x40034ae020), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-gh62g", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-gh62g", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-gh62g", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0x40052de0c8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"slave2", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0x4000b2a070), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0x40052de160)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0x40052de180)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"priority-class-apps", Priority:(*int32)(0x40052de188), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0x40052de18c), PreemptionPolicy:(*v1.PreemptionPolicy)(0x4004ef21b0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 29, 4, 10, 42, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 29, 4, 10, 42, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 29, 4, 10, 42, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 29, 4, 10, 42, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.122.245", PodIP:"100.101.49.143", PodIPs:[]v1.PodIP{v1.PodIP{IP:"100.101.49.143"}}, StartTime:time.Date(2023, time.January, 29, 4, 10, 42, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0x4000b2a150)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0x4000b2a1c0)}, Ready:false, RestartCount:3, Image:"registry-jinan-lab.inspurcloud.cn/library/cke/busybox-arm64:1.29-2", ImageID:"docker-pullable://registry-jinan-lab.inspurcloud.cn/library/cke/busybox-arm64@sha256:dfb92c23d197b42f2912b269650531443222998fcf9c71df7415ed3234d47106", ContainerID:"docker://91fe53d44036ec2e42677c4484009426b5653a29d2ffb96e3321fdfeb744a0cd", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0x40034ae0a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0x40034ae080), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0x40052de214)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Jan 29 04:11:24.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6434" for this suite. 01/29/23 04:11:24.121
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","completed":273,"skipped":4909,"failed":0}
------------------------------
• [SLOW TEST] [41.860 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:10:42.273
    Jan 29 04:10:42.273: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename init-container 01/29/23 04:10:42.275
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:10:42.3
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:10:42.306
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:333
    STEP: creating the pod 01/29/23 04:10:42.314
    Jan 29 04:10:42.314: INFO: PodSpec: initContainers in spec.initContainers
    Jan 29 04:11:24.105: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-68cee7f2-7dad-4b37-938b-599cc914b380", GenerateName:"", Namespace:"init-container-6434", SelfLink:"", UID:"02dad7f5-1ebe-4db7-b4b3-a33abfbc1421", ResourceVersion:"5978974", Generation:0, CreationTimestamp:time.Date(2023, time.January, 29, 4, 10, 42, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"314401292"}, Annotations:map[string]string{"k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"\",\n    \"ips\": [\n        \"100.101.49.143\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]", "k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"\",\n    \"ips\": [\n        \"100.101.49.143\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-gh62g", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0x40034ae020), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-gh62g", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-gh62g", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-gh62g", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0x40052de0c8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"slave2", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0x4000b2a070), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0x40052de160)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0x40052de180)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"priority-class-apps", Priority:(*int32)(0x40052de188), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0x40052de18c), PreemptionPolicy:(*v1.PreemptionPolicy)(0x4004ef21b0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 29, 4, 10, 42, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 29, 4, 10, 42, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 29, 4, 10, 42, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 29, 4, 10, 42, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.122.245", PodIP:"100.101.49.143", PodIPs:[]v1.PodIP{v1.PodIP{IP:"100.101.49.143"}}, StartTime:time.Date(2023, time.January, 29, 4, 10, 42, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0x4000b2a150)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0x4000b2a1c0)}, Ready:false, RestartCount:3, Image:"registry-jinan-lab.inspurcloud.cn/library/cke/busybox-arm64:1.29-2", ImageID:"docker-pullable://registry-jinan-lab.inspurcloud.cn/library/cke/busybox-arm64@sha256:dfb92c23d197b42f2912b269650531443222998fcf9c71df7415ed3234d47106", ContainerID:"docker://91fe53d44036ec2e42677c4484009426b5653a29d2ffb96e3321fdfeb744a0cd", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0x40034ae0a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0x40034ae080), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0x40052de214)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan 29 04:11:24.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-6434" for this suite. 01/29/23 04:11:24.121
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:11:24.135
Jan 29 04:11:24.135: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename kubectl 01/29/23 04:11:24.137
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:11:24.169
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:11:24.175
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
STEP: creating all guestbook components 01/29/23 04:11:24.183
Jan 29 04:11:24.183: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Jan 29 04:11:24.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-324 create -f -'
Jan 29 04:11:24.584: INFO: stderr: ""
Jan 29 04:11:24.584: INFO: stdout: "service/agnhost-replica created\n"
Jan 29 04:11:24.584: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Jan 29 04:11:24.584: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-324 create -f -'
Jan 29 04:11:24.998: INFO: stderr: ""
Jan 29 04:11:24.998: INFO: stdout: "service/agnhost-primary created\n"
Jan 29 04:11:24.998: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jan 29 04:11:24.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-324 create -f -'
Jan 29 04:11:25.382: INFO: stderr: ""
Jan 29 04:11:25.382: INFO: stdout: "service/frontend created\n"
Jan 29 04:11:25.382: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Jan 29 04:11:25.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-324 create -f -'
Jan 29 04:11:25.780: INFO: stderr: ""
Jan 29 04:11:25.780: INFO: stdout: "deployment.apps/frontend created\n"
Jan 29 04:11:25.780: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan 29 04:11:25.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-324 create -f -'
Jan 29 04:11:26.200: INFO: stderr: ""
Jan 29 04:11:26.200: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Jan 29 04:11:26.200: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan 29 04:11:26.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-324 create -f -'
Jan 29 04:11:26.694: INFO: stderr: ""
Jan 29 04:11:26.694: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 01/29/23 04:11:26.694
Jan 29 04:11:26.694: INFO: Waiting for all frontend pods to be Running.
Jan 29 04:11:31.745: INFO: Waiting for frontend to serve content.
Jan 29 04:11:31.763: INFO: Trying to add a new entry to the guestbook.
Jan 29 04:11:31.782: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 01/29/23 04:11:31.798
Jan 29 04:11:31.798: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-324 delete --grace-period=0 --force -f -'
Jan 29 04:11:31.955: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 29 04:11:31.955: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 01/29/23 04:11:31.955
Jan 29 04:11:31.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-324 delete --grace-period=0 --force -f -'
Jan 29 04:11:32.173: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 29 04:11:32.173: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 01/29/23 04:11:32.173
Jan 29 04:11:32.173: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-324 delete --grace-period=0 --force -f -'
Jan 29 04:11:32.324: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 29 04:11:32.324: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 01/29/23 04:11:32.324
Jan 29 04:11:32.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-324 delete --grace-period=0 --force -f -'
Jan 29 04:11:32.439: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 29 04:11:32.440: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 01/29/23 04:11:32.44
Jan 29 04:11:32.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-324 delete --grace-period=0 --force -f -'
Jan 29 04:11:32.555: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 29 04:11:32.555: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 01/29/23 04:11:32.555
Jan 29 04:11:32.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-324 delete --grace-period=0 --force -f -'
Jan 29 04:11:32.686: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 29 04:11:32.686: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 29 04:11:32.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-324" for this suite. 01/29/23 04:11:32.696
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","completed":274,"skipped":4917,"failed":0}
------------------------------
• [SLOW TEST] [8.570 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:367
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:11:24.135
    Jan 29 04:11:24.135: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename kubectl 01/29/23 04:11:24.137
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:11:24.169
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:11:24.175
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:392
    STEP: creating all guestbook components 01/29/23 04:11:24.183
    Jan 29 04:11:24.183: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Jan 29 04:11:24.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-324 create -f -'
    Jan 29 04:11:24.584: INFO: stderr: ""
    Jan 29 04:11:24.584: INFO: stdout: "service/agnhost-replica created\n"
    Jan 29 04:11:24.584: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Jan 29 04:11:24.584: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-324 create -f -'
    Jan 29 04:11:24.998: INFO: stderr: ""
    Jan 29 04:11:24.998: INFO: stdout: "service/agnhost-primary created\n"
    Jan 29 04:11:24.998: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Jan 29 04:11:24.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-324 create -f -'
    Jan 29 04:11:25.382: INFO: stderr: ""
    Jan 29 04:11:25.382: INFO: stdout: "service/frontend created\n"
    Jan 29 04:11:25.382: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Jan 29 04:11:25.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-324 create -f -'
    Jan 29 04:11:25.780: INFO: stderr: ""
    Jan 29 04:11:25.780: INFO: stdout: "deployment.apps/frontend created\n"
    Jan 29 04:11:25.780: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Jan 29 04:11:25.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-324 create -f -'
    Jan 29 04:11:26.200: INFO: stderr: ""
    Jan 29 04:11:26.200: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Jan 29 04:11:26.200: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Jan 29 04:11:26.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-324 create -f -'
    Jan 29 04:11:26.694: INFO: stderr: ""
    Jan 29 04:11:26.694: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 01/29/23 04:11:26.694
    Jan 29 04:11:26.694: INFO: Waiting for all frontend pods to be Running.
    Jan 29 04:11:31.745: INFO: Waiting for frontend to serve content.
    Jan 29 04:11:31.763: INFO: Trying to add a new entry to the guestbook.
    Jan 29 04:11:31.782: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 01/29/23 04:11:31.798
    Jan 29 04:11:31.798: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-324 delete --grace-period=0 --force -f -'
    Jan 29 04:11:31.955: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 29 04:11:31.955: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 01/29/23 04:11:31.955
    Jan 29 04:11:31.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-324 delete --grace-period=0 --force -f -'
    Jan 29 04:11:32.173: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 29 04:11:32.173: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 01/29/23 04:11:32.173
    Jan 29 04:11:32.173: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-324 delete --grace-period=0 --force -f -'
    Jan 29 04:11:32.324: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 29 04:11:32.324: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 01/29/23 04:11:32.324
    Jan 29 04:11:32.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-324 delete --grace-period=0 --force -f -'
    Jan 29 04:11:32.439: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 29 04:11:32.440: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 01/29/23 04:11:32.44
    Jan 29 04:11:32.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-324 delete --grace-period=0 --force -f -'
    Jan 29 04:11:32.555: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 29 04:11:32.555: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 01/29/23 04:11:32.555
    Jan 29 04:11:32.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-324 delete --grace-period=0 --force -f -'
    Jan 29 04:11:32.686: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 29 04:11:32.686: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 29 04:11:32.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-324" for this suite. 01/29/23 04:11:32.696
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:11:32.708
Jan 29 04:11:32.708: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename replication-controller 01/29/23 04:11:32.709
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:11:32.731
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:11:32.737
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
STEP: Given a ReplicationController is created 01/29/23 04:11:32.745
STEP: When the matched label of one of its pods change 01/29/23 04:11:32.754
Jan 29 04:11:32.761: INFO: Pod name pod-release: Found 0 pods out of 1
Jan 29 04:11:37.769: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 01/29/23 04:11:37.79
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jan 29 04:11:38.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3745" for this suite. 01/29/23 04:11:38.822
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","completed":275,"skipped":4944,"failed":0}
------------------------------
• [SLOW TEST] [6.129 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:11:32.708
    Jan 29 04:11:32.708: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename replication-controller 01/29/23 04:11:32.709
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:11:32.731
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:11:32.737
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:100
    STEP: Given a ReplicationController is created 01/29/23 04:11:32.745
    STEP: When the matched label of one of its pods change 01/29/23 04:11:32.754
    Jan 29 04:11:32.761: INFO: Pod name pod-release: Found 0 pods out of 1
    Jan 29 04:11:37.769: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 01/29/23 04:11:37.79
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jan 29 04:11:38.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-3745" for this suite. 01/29/23 04:11:38.822
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:11:38.838
Jan 29 04:11:38.838: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename emptydir 01/29/23 04:11:38.84
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:11:38.875
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:11:38.883
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
STEP: Creating a pod to test emptydir volume type on node default medium 01/29/23 04:11:38.888
Jan 29 04:11:38.910: INFO: Waiting up to 5m0s for pod "pod-86518dda-8fc6-4c48-9355-389b4fd03a53" in namespace "emptydir-8312" to be "Succeeded or Failed"
Jan 29 04:11:38.918: INFO: Pod "pod-86518dda-8fc6-4c48-9355-389b4fd03a53": Phase="Pending", Reason="", readiness=false. Elapsed: 8.032396ms
Jan 29 04:11:40.925: INFO: Pod "pod-86518dda-8fc6-4c48-9355-389b4fd03a53": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015635013s
Jan 29 04:11:42.925: INFO: Pod "pod-86518dda-8fc6-4c48-9355-389b4fd03a53": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015798339s
STEP: Saw pod success 01/29/23 04:11:42.925
Jan 29 04:11:42.926: INFO: Pod "pod-86518dda-8fc6-4c48-9355-389b4fd03a53" satisfied condition "Succeeded or Failed"
Jan 29 04:11:42.932: INFO: Trying to get logs from node slave2 pod pod-86518dda-8fc6-4c48-9355-389b4fd03a53 container test-container: <nil>
STEP: delete the pod 01/29/23 04:11:42.951
Jan 29 04:11:43.023: INFO: Waiting for pod pod-86518dda-8fc6-4c48-9355-389b4fd03a53 to disappear
Jan 29 04:11:43.029: INFO: Pod pod-86518dda-8fc6-4c48-9355-389b4fd03a53 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 29 04:11:43.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8312" for this suite. 01/29/23 04:11:43.04
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":276,"skipped":4956,"failed":0}
------------------------------
• [4.212 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:11:38.838
    Jan 29 04:11:38.838: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename emptydir 01/29/23 04:11:38.84
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:11:38.875
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:11:38.883
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:156
    STEP: Creating a pod to test emptydir volume type on node default medium 01/29/23 04:11:38.888
    Jan 29 04:11:38.910: INFO: Waiting up to 5m0s for pod "pod-86518dda-8fc6-4c48-9355-389b4fd03a53" in namespace "emptydir-8312" to be "Succeeded or Failed"
    Jan 29 04:11:38.918: INFO: Pod "pod-86518dda-8fc6-4c48-9355-389b4fd03a53": Phase="Pending", Reason="", readiness=false. Elapsed: 8.032396ms
    Jan 29 04:11:40.925: INFO: Pod "pod-86518dda-8fc6-4c48-9355-389b4fd03a53": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015635013s
    Jan 29 04:11:42.925: INFO: Pod "pod-86518dda-8fc6-4c48-9355-389b4fd03a53": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015798339s
    STEP: Saw pod success 01/29/23 04:11:42.925
    Jan 29 04:11:42.926: INFO: Pod "pod-86518dda-8fc6-4c48-9355-389b4fd03a53" satisfied condition "Succeeded or Failed"
    Jan 29 04:11:42.932: INFO: Trying to get logs from node slave2 pod pod-86518dda-8fc6-4c48-9355-389b4fd03a53 container test-container: <nil>
    STEP: delete the pod 01/29/23 04:11:42.951
    Jan 29 04:11:43.023: INFO: Waiting for pod pod-86518dda-8fc6-4c48-9355-389b4fd03a53 to disappear
    Jan 29 04:11:43.029: INFO: Pod pod-86518dda-8fc6-4c48-9355-389b4fd03a53 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 29 04:11:43.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-8312" for this suite. 01/29/23 04:11:43.04
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:11:43.051
Jan 29 04:11:43.052: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename container-runtime 01/29/23 04:11:43.053
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:11:43.076
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:11:43.082
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
STEP: create the container 01/29/23 04:11:43.088
STEP: wait for the container to reach Failed 01/29/23 04:11:43.107
STEP: get the container status 01/29/23 04:11:47.149
STEP: the container should be terminated 01/29/23 04:11:47.156
STEP: the termination message should be set 01/29/23 04:11:47.156
Jan 29 04:11:47.156: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 01/29/23 04:11:47.156
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jan 29 04:11:47.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5001" for this suite. 01/29/23 04:11:47.276
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":277,"skipped":4968,"failed":0}
------------------------------
• [4.236 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:11:43.051
    Jan 29 04:11:43.052: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename container-runtime 01/29/23 04:11:43.053
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:11:43.076
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:11:43.082
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215
    STEP: create the container 01/29/23 04:11:43.088
    STEP: wait for the container to reach Failed 01/29/23 04:11:43.107
    STEP: get the container status 01/29/23 04:11:47.149
    STEP: the container should be terminated 01/29/23 04:11:47.156
    STEP: the termination message should be set 01/29/23 04:11:47.156
    Jan 29 04:11:47.156: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 01/29/23 04:11:47.156
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jan 29 04:11:47.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-5001" for this suite. 01/29/23 04:11:47.276
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:11:47.288
Jan 29 04:11:47.289: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename replicaset 01/29/23 04:11:47.29
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:11:47.313
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:11:47.318
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 01/29/23 04:11:47.324
Jan 29 04:11:47.342: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-5413" to be "running and ready"
Jan 29 04:11:47.350: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 7.290871ms
Jan 29 04:11:47.350: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jan 29 04:11:49.358: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.015324091s
Jan 29 04:11:49.358: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Jan 29 04:11:49.358: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 01/29/23 04:11:49.364
STEP: Then the orphan pod is adopted 01/29/23 04:11:49.377
STEP: When the matched label of one of its pods change 01/29/23 04:11:50.391
Jan 29 04:11:50.398: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 01/29/23 04:11:50.413
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jan 29 04:11:51.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5413" for this suite. 01/29/23 04:11:51.44
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","completed":278,"skipped":4976,"failed":0}
------------------------------
• [4.164 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:11:47.288
    Jan 29 04:11:47.289: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename replicaset 01/29/23 04:11:47.29
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:11:47.313
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:11:47.318
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 01/29/23 04:11:47.324
    Jan 29 04:11:47.342: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-5413" to be "running and ready"
    Jan 29 04:11:47.350: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 7.290871ms
    Jan 29 04:11:47.350: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Jan 29 04:11:49.358: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.015324091s
    Jan 29 04:11:49.358: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Jan 29 04:11:49.358: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 01/29/23 04:11:49.364
    STEP: Then the orphan pod is adopted 01/29/23 04:11:49.377
    STEP: When the matched label of one of its pods change 01/29/23 04:11:50.391
    Jan 29 04:11:50.398: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 01/29/23 04:11:50.413
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jan 29 04:11:51.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-5413" for this suite. 01/29/23 04:11:51.44
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:11:51.452
Jan 29 04:11:51.453: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename webhook 01/29/23 04:11:51.454
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:11:51.487
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:11:51.504
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/29/23 04:11:51.549
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/29/23 04:11:52.708
STEP: Deploying the webhook pod 01/29/23 04:11:52.721
STEP: Wait for the deployment to be ready 01/29/23 04:11:52.75
Jan 29 04:11:52.766: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 29 04:11:54.788: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 29, 4, 11, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 4, 11, 52, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 29, 4, 11, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 4, 11, 52, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/29/23 04:11:56.796
STEP: Verifying the service has paired with the endpoint 01/29/23 04:11:56.811
Jan 29 04:11:57.812: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/29/23 04:11:57.82
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/29/23 04:11:57.846
STEP: Creating a dummy validating-webhook-configuration object 01/29/23 04:11:57.869
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 01/29/23 04:11:57.882
STEP: Creating a dummy mutating-webhook-configuration object 01/29/23 04:11:57.893
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 01/29/23 04:11:57.907
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 29 04:11:57.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8368" for this suite. 01/29/23 04:11:57.942
STEP: Destroying namespace "webhook-8368-markers" for this suite. 01/29/23 04:11:57.951
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","completed":279,"skipped":4977,"failed":0}
------------------------------
• [SLOW TEST] [6.591 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:11:51.452
    Jan 29 04:11:51.453: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename webhook 01/29/23 04:11:51.454
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:11:51.487
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:11:51.504
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/29/23 04:11:51.549
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/29/23 04:11:52.708
    STEP: Deploying the webhook pod 01/29/23 04:11:52.721
    STEP: Wait for the deployment to be ready 01/29/23 04:11:52.75
    Jan 29 04:11:52.766: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 29 04:11:54.788: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 29, 4, 11, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 4, 11, 52, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 29, 4, 11, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 4, 11, 52, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/29/23 04:11:56.796
    STEP: Verifying the service has paired with the endpoint 01/29/23 04:11:56.811
    Jan 29 04:11:57.812: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:276
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/29/23 04:11:57.82
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/29/23 04:11:57.846
    STEP: Creating a dummy validating-webhook-configuration object 01/29/23 04:11:57.869
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 01/29/23 04:11:57.882
    STEP: Creating a dummy mutating-webhook-configuration object 01/29/23 04:11:57.893
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 01/29/23 04:11:57.907
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 29 04:11:57.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-8368" for this suite. 01/29/23 04:11:57.942
    STEP: Destroying namespace "webhook-8368-markers" for this suite. 01/29/23 04:11:57.951
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:11:58.046
Jan 29 04:11:58.047: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename svcaccounts 01/29/23 04:11:58.048
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:11:58.076
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:11:58.084
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
STEP: creating a ServiceAccount 01/29/23 04:11:58.089
STEP: watching for the ServiceAccount to be added 01/29/23 04:11:58.104
STEP: patching the ServiceAccount 01/29/23 04:11:58.107
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 01/29/23 04:11:58.114
STEP: deleting the ServiceAccount 01/29/23 04:11:58.121
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jan 29 04:11:58.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9818" for this suite. 01/29/23 04:11:58.153
{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","completed":280,"skipped":4988,"failed":0}
------------------------------
• [0.115 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:11:58.046
    Jan 29 04:11:58.047: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename svcaccounts 01/29/23 04:11:58.048
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:11:58.076
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:11:58.084
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:646
    STEP: creating a ServiceAccount 01/29/23 04:11:58.089
    STEP: watching for the ServiceAccount to be added 01/29/23 04:11:58.104
    STEP: patching the ServiceAccount 01/29/23 04:11:58.107
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 01/29/23 04:11:58.114
    STEP: deleting the ServiceAccount 01/29/23 04:11:58.121
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jan 29 04:11:58.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-9818" for this suite. 01/29/23 04:11:58.153
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:11:58.162
Jan 29 04:11:58.162: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename container-probe 01/29/23 04:11:58.163
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:11:58.184
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:11:58.19
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
STEP: Creating pod busybox-3008e63c-7990-4c03-bcb8-07fa4348cfc0 in namespace container-probe-3061 01/29/23 04:11:58.195
Jan 29 04:11:58.214: INFO: Waiting up to 5m0s for pod "busybox-3008e63c-7990-4c03-bcb8-07fa4348cfc0" in namespace "container-probe-3061" to be "not pending"
Jan 29 04:11:58.222: INFO: Pod "busybox-3008e63c-7990-4c03-bcb8-07fa4348cfc0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.17715ms
Jan 29 04:12:00.230: INFO: Pod "busybox-3008e63c-7990-4c03-bcb8-07fa4348cfc0": Phase="Running", Reason="", readiness=true. Elapsed: 2.015650053s
Jan 29 04:12:00.230: INFO: Pod "busybox-3008e63c-7990-4c03-bcb8-07fa4348cfc0" satisfied condition "not pending"
Jan 29 04:12:00.230: INFO: Started pod busybox-3008e63c-7990-4c03-bcb8-07fa4348cfc0 in namespace container-probe-3061
STEP: checking the pod's current state and verifying that restartCount is present 01/29/23 04:12:00.23
Jan 29 04:12:00.237: INFO: Initial restart count of pod busybox-3008e63c-7990-4c03-bcb8-07fa4348cfc0 is 0
STEP: deleting the pod 01/29/23 04:16:01.309
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan 29 04:16:01.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3061" for this suite. 01/29/23 04:16:01.419
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":281,"skipped":4988,"failed":0}
------------------------------
• [SLOW TEST] [243.272 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:11:58.162
    Jan 29 04:11:58.162: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename container-probe 01/29/23 04:11:58.163
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:11:58.184
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:11:58.19
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:148
    STEP: Creating pod busybox-3008e63c-7990-4c03-bcb8-07fa4348cfc0 in namespace container-probe-3061 01/29/23 04:11:58.195
    Jan 29 04:11:58.214: INFO: Waiting up to 5m0s for pod "busybox-3008e63c-7990-4c03-bcb8-07fa4348cfc0" in namespace "container-probe-3061" to be "not pending"
    Jan 29 04:11:58.222: INFO: Pod "busybox-3008e63c-7990-4c03-bcb8-07fa4348cfc0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.17715ms
    Jan 29 04:12:00.230: INFO: Pod "busybox-3008e63c-7990-4c03-bcb8-07fa4348cfc0": Phase="Running", Reason="", readiness=true. Elapsed: 2.015650053s
    Jan 29 04:12:00.230: INFO: Pod "busybox-3008e63c-7990-4c03-bcb8-07fa4348cfc0" satisfied condition "not pending"
    Jan 29 04:12:00.230: INFO: Started pod busybox-3008e63c-7990-4c03-bcb8-07fa4348cfc0 in namespace container-probe-3061
    STEP: checking the pod's current state and verifying that restartCount is present 01/29/23 04:12:00.23
    Jan 29 04:12:00.237: INFO: Initial restart count of pod busybox-3008e63c-7990-4c03-bcb8-07fa4348cfc0 is 0
    STEP: deleting the pod 01/29/23 04:16:01.309
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan 29 04:16:01.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-3061" for this suite. 01/29/23 04:16:01.419
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:16:01.435
Jan 29 04:16:01.435: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename pods 01/29/23 04:16:01.437
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:16:01.47
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:16:01.476
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
STEP: creating a Pod with a static label 01/29/23 04:16:01.494
STEP: watching for Pod to be ready 01/29/23 04:16:01.513
Jan 29 04:16:01.516: INFO: observed Pod pod-test in namespace pods-8835 in phase Pending with labels: map[test-pod-static:true] & conditions []
Jan 29 04:16:01.519: INFO: observed Pod pod-test in namespace pods-8835 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:16:01 +0000 UTC  }]
Jan 29 04:16:01.535: INFO: observed Pod pod-test in namespace pods-8835 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:16:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:16:01 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:16:01 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:16:01 +0000 UTC  }]
Jan 29 04:16:02.417: INFO: observed Pod pod-test in namespace pods-8835 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:16:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:16:01 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:16:01 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:16:01 +0000 UTC  }]
Jan 29 04:16:02.771: INFO: observed Pod pod-test in namespace pods-8835 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:16:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:16:01 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:16:01 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:16:01 +0000 UTC  }]
Jan 29 04:16:03.171: INFO: Found Pod pod-test in namespace pods-8835 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:16:01 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:16:03 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:16:03 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:16:01 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 01/29/23 04:16:03.178
STEP: getting the Pod and ensuring that it's patched 01/29/23 04:16:03.192
STEP: replacing the Pod's status Ready condition to False 01/29/23 04:16:03.199
STEP: check the Pod again to ensure its Ready conditions are False 01/29/23 04:16:03.214
STEP: deleting the Pod via a Collection with a LabelSelector 01/29/23 04:16:03.214
STEP: watching for the Pod to be deleted 01/29/23 04:16:03.255
Jan 29 04:16:03.258: INFO: observed event type MODIFIED
Jan 29 04:16:05.256: INFO: observed event type MODIFIED
Jan 29 04:16:06.309: INFO: observed event type MODIFIED
Jan 29 04:16:06.347: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 29 04:16:06.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8835" for this suite. 01/29/23 04:16:06.421
{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","completed":282,"skipped":5002,"failed":0}
------------------------------
• [4.997 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:16:01.435
    Jan 29 04:16:01.435: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename pods 01/29/23 04:16:01.437
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:16:01.47
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:16:01.476
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:895
    STEP: creating a Pod with a static label 01/29/23 04:16:01.494
    STEP: watching for Pod to be ready 01/29/23 04:16:01.513
    Jan 29 04:16:01.516: INFO: observed Pod pod-test in namespace pods-8835 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Jan 29 04:16:01.519: INFO: observed Pod pod-test in namespace pods-8835 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:16:01 +0000 UTC  }]
    Jan 29 04:16:01.535: INFO: observed Pod pod-test in namespace pods-8835 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:16:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:16:01 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:16:01 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:16:01 +0000 UTC  }]
    Jan 29 04:16:02.417: INFO: observed Pod pod-test in namespace pods-8835 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:16:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:16:01 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:16:01 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:16:01 +0000 UTC  }]
    Jan 29 04:16:02.771: INFO: observed Pod pod-test in namespace pods-8835 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:16:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:16:01 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:16:01 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:16:01 +0000 UTC  }]
    Jan 29 04:16:03.171: INFO: Found Pod pod-test in namespace pods-8835 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:16:01 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:16:03 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:16:03 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-29 04:16:01 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 01/29/23 04:16:03.178
    STEP: getting the Pod and ensuring that it's patched 01/29/23 04:16:03.192
    STEP: replacing the Pod's status Ready condition to False 01/29/23 04:16:03.199
    STEP: check the Pod again to ensure its Ready conditions are False 01/29/23 04:16:03.214
    STEP: deleting the Pod via a Collection with a LabelSelector 01/29/23 04:16:03.214
    STEP: watching for the Pod to be deleted 01/29/23 04:16:03.255
    Jan 29 04:16:03.258: INFO: observed event type MODIFIED
    Jan 29 04:16:05.256: INFO: observed event type MODIFIED
    Jan 29 04:16:06.309: INFO: observed event type MODIFIED
    Jan 29 04:16:06.347: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 29 04:16:06.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-8835" for this suite. 01/29/23 04:16:06.421
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:16:06.432
Jan 29 04:16:06.433: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename downward-api 01/29/23 04:16:06.434
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:16:06.463
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:16:06.47
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
STEP: Creating a pod to test downward API volume plugin 01/29/23 04:16:06.478
Jan 29 04:16:06.507: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b331fc47-ec77-4750-bda6-dc162a227618" in namespace "downward-api-3082" to be "Succeeded or Failed"
Jan 29 04:16:06.524: INFO: Pod "downwardapi-volume-b331fc47-ec77-4750-bda6-dc162a227618": Phase="Pending", Reason="", readiness=false. Elapsed: 16.914059ms
Jan 29 04:16:08.532: INFO: Pod "downwardapi-volume-b331fc47-ec77-4750-bda6-dc162a227618": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024616278s
Jan 29 04:16:10.535: INFO: Pod "downwardapi-volume-b331fc47-ec77-4750-bda6-dc162a227618": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027591785s
Jan 29 04:16:12.532: INFO: Pod "downwardapi-volume-b331fc47-ec77-4750-bda6-dc162a227618": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024028026s
STEP: Saw pod success 01/29/23 04:16:12.532
Jan 29 04:16:12.532: INFO: Pod "downwardapi-volume-b331fc47-ec77-4750-bda6-dc162a227618" satisfied condition "Succeeded or Failed"
Jan 29 04:16:12.537: INFO: Trying to get logs from node slave2 pod downwardapi-volume-b331fc47-ec77-4750-bda6-dc162a227618 container client-container: <nil>
STEP: delete the pod 01/29/23 04:16:12.566
Jan 29 04:16:12.660: INFO: Waiting for pod downwardapi-volume-b331fc47-ec77-4750-bda6-dc162a227618 to disappear
Jan 29 04:16:12.665: INFO: Pod downwardapi-volume-b331fc47-ec77-4750-bda6-dc162a227618 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 29 04:16:12.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3082" for this suite. 01/29/23 04:16:12.674
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":283,"skipped":5004,"failed":0}
------------------------------
• [SLOW TEST] [6.252 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:16:06.432
    Jan 29 04:16:06.433: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename downward-api 01/29/23 04:16:06.434
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:16:06.463
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:16:06.47
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:248
    STEP: Creating a pod to test downward API volume plugin 01/29/23 04:16:06.478
    Jan 29 04:16:06.507: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b331fc47-ec77-4750-bda6-dc162a227618" in namespace "downward-api-3082" to be "Succeeded or Failed"
    Jan 29 04:16:06.524: INFO: Pod "downwardapi-volume-b331fc47-ec77-4750-bda6-dc162a227618": Phase="Pending", Reason="", readiness=false. Elapsed: 16.914059ms
    Jan 29 04:16:08.532: INFO: Pod "downwardapi-volume-b331fc47-ec77-4750-bda6-dc162a227618": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024616278s
    Jan 29 04:16:10.535: INFO: Pod "downwardapi-volume-b331fc47-ec77-4750-bda6-dc162a227618": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027591785s
    Jan 29 04:16:12.532: INFO: Pod "downwardapi-volume-b331fc47-ec77-4750-bda6-dc162a227618": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024028026s
    STEP: Saw pod success 01/29/23 04:16:12.532
    Jan 29 04:16:12.532: INFO: Pod "downwardapi-volume-b331fc47-ec77-4750-bda6-dc162a227618" satisfied condition "Succeeded or Failed"
    Jan 29 04:16:12.537: INFO: Trying to get logs from node slave2 pod downwardapi-volume-b331fc47-ec77-4750-bda6-dc162a227618 container client-container: <nil>
    STEP: delete the pod 01/29/23 04:16:12.566
    Jan 29 04:16:12.660: INFO: Waiting for pod downwardapi-volume-b331fc47-ec77-4750-bda6-dc162a227618 to disappear
    Jan 29 04:16:12.665: INFO: Pod downwardapi-volume-b331fc47-ec77-4750-bda6-dc162a227618 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 29 04:16:12.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-3082" for this suite. 01/29/23 04:16:12.674
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:16:12.687
Jan 29 04:16:12.687: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename sched-preemption 01/29/23 04:16:12.688
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:16:12.711
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:16:12.716
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jan 29 04:16:12.741: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 29 04:17:12.801: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
STEP: Create pods that use 4/5 of node resources. 01/29/23 04:17:12.808
Jan 29 04:17:12.855: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jan 29 04:17:12.872: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jan 29 04:17:12.903: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jan 29 04:17:12.916: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Jan 29 04:17:12.945: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Jan 29 04:17:12.961: INFO: Created pod: pod2-1-sched-preemption-medium-priority
Jan 29 04:17:12.988: INFO: Created pod: pod3-0-sched-preemption-medium-priority
Jan 29 04:17:13.004: INFO: Created pod: pod3-1-sched-preemption-medium-priority
Jan 29 04:17:13.032: INFO: Created pod: pod4-0-sched-preemption-medium-priority
Jan 29 04:17:13.045: INFO: Created pod: pod4-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 01/29/23 04:17:13.045
Jan 29 04:17:13.045: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-1398" to be "running"
Jan 29 04:17:13.053: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 7.999717ms
Jan 29 04:17:15.061: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016157641s
Jan 29 04:17:17.061: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015793487s
Jan 29 04:17:19.062: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.017379746s
Jan 29 04:17:21.062: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.01691563s
Jan 29 04:17:23.063: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 10.018495389s
Jan 29 04:17:25.061: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 12.016530783s
Jan 29 04:17:25.061: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Jan 29 04:17:25.062: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-1398" to be "running"
Jan 29 04:17:25.068: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.658067ms
Jan 29 04:17:25.068: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Jan 29 04:17:25.068: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-1398" to be "running"
Jan 29 04:17:25.074: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.068683ms
Jan 29 04:17:25.074: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Jan 29 04:17:25.074: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-1398" to be "running"
Jan 29 04:17:25.081: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.220323ms
Jan 29 04:17:25.081: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Jan 29 04:17:25.081: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-1398" to be "running"
Jan 29 04:17:25.087: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.983782ms
Jan 29 04:17:25.087: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Jan 29 04:17:25.087: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-1398" to be "running"
Jan 29 04:17:25.093: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.902082ms
Jan 29 04:17:25.093: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
Jan 29 04:17:25.093: INFO: Waiting up to 5m0s for pod "pod3-0-sched-preemption-medium-priority" in namespace "sched-preemption-1398" to be "running"
Jan 29 04:17:25.099: INFO: Pod "pod3-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.977842ms
Jan 29 04:17:25.099: INFO: Pod "pod3-0-sched-preemption-medium-priority" satisfied condition "running"
Jan 29 04:17:25.099: INFO: Waiting up to 5m0s for pod "pod3-1-sched-preemption-medium-priority" in namespace "sched-preemption-1398" to be "running"
Jan 29 04:17:25.105: INFO: Pod "pod3-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.729661ms
Jan 29 04:17:25.105: INFO: Pod "pod3-1-sched-preemption-medium-priority" satisfied condition "running"
Jan 29 04:17:25.105: INFO: Waiting up to 5m0s for pod "pod4-0-sched-preemption-medium-priority" in namespace "sched-preemption-1398" to be "running"
Jan 29 04:17:25.110: INFO: Pod "pod4-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.846281ms
Jan 29 04:17:25.111: INFO: Pod "pod4-0-sched-preemption-medium-priority" satisfied condition "running"
Jan 29 04:17:25.111: INFO: Waiting up to 5m0s for pod "pod4-1-sched-preemption-medium-priority" in namespace "sched-preemption-1398" to be "running"
Jan 29 04:17:25.116: INFO: Pod "pod4-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.337197ms
Jan 29 04:17:25.116: INFO: Pod "pod4-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 01/29/23 04:17:25.116
Jan 29 04:17:25.129: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-1398" to be "running"
Jan 29 04:17:25.135: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.574946ms
Jan 29 04:17:27.143: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014108007s
Jan 29 04:17:29.143: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014405777s
Jan 29 04:17:31.145: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.016260997s
Jan 29 04:17:31.145: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Jan 29 04:17:31.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-1398" for this suite. 01/29/23 04:17:31.218
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","completed":284,"skipped":5018,"failed":0}
------------------------------
• [SLOW TEST] [78.618 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:16:12.687
    Jan 29 04:16:12.687: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename sched-preemption 01/29/23 04:16:12.688
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:16:12.711
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:16:12.716
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Jan 29 04:16:12.741: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 29 04:17:12.801: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:125
    STEP: Create pods that use 4/5 of node resources. 01/29/23 04:17:12.808
    Jan 29 04:17:12.855: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Jan 29 04:17:12.872: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Jan 29 04:17:12.903: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Jan 29 04:17:12.916: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Jan 29 04:17:12.945: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Jan 29 04:17:12.961: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    Jan 29 04:17:12.988: INFO: Created pod: pod3-0-sched-preemption-medium-priority
    Jan 29 04:17:13.004: INFO: Created pod: pod3-1-sched-preemption-medium-priority
    Jan 29 04:17:13.032: INFO: Created pod: pod4-0-sched-preemption-medium-priority
    Jan 29 04:17:13.045: INFO: Created pod: pod4-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 01/29/23 04:17:13.045
    Jan 29 04:17:13.045: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-1398" to be "running"
    Jan 29 04:17:13.053: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 7.999717ms
    Jan 29 04:17:15.061: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016157641s
    Jan 29 04:17:17.061: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015793487s
    Jan 29 04:17:19.062: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.017379746s
    Jan 29 04:17:21.062: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.01691563s
    Jan 29 04:17:23.063: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 10.018495389s
    Jan 29 04:17:25.061: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 12.016530783s
    Jan 29 04:17:25.061: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Jan 29 04:17:25.062: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-1398" to be "running"
    Jan 29 04:17:25.068: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.658067ms
    Jan 29 04:17:25.068: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan 29 04:17:25.068: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-1398" to be "running"
    Jan 29 04:17:25.074: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.068683ms
    Jan 29 04:17:25.074: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan 29 04:17:25.074: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-1398" to be "running"
    Jan 29 04:17:25.081: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.220323ms
    Jan 29 04:17:25.081: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan 29 04:17:25.081: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-1398" to be "running"
    Jan 29 04:17:25.087: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.983782ms
    Jan 29 04:17:25.087: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan 29 04:17:25.087: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-1398" to be "running"
    Jan 29 04:17:25.093: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.902082ms
    Jan 29 04:17:25.093: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan 29 04:17:25.093: INFO: Waiting up to 5m0s for pod "pod3-0-sched-preemption-medium-priority" in namespace "sched-preemption-1398" to be "running"
    Jan 29 04:17:25.099: INFO: Pod "pod3-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.977842ms
    Jan 29 04:17:25.099: INFO: Pod "pod3-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan 29 04:17:25.099: INFO: Waiting up to 5m0s for pod "pod3-1-sched-preemption-medium-priority" in namespace "sched-preemption-1398" to be "running"
    Jan 29 04:17:25.105: INFO: Pod "pod3-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.729661ms
    Jan 29 04:17:25.105: INFO: Pod "pod3-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan 29 04:17:25.105: INFO: Waiting up to 5m0s for pod "pod4-0-sched-preemption-medium-priority" in namespace "sched-preemption-1398" to be "running"
    Jan 29 04:17:25.110: INFO: Pod "pod4-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.846281ms
    Jan 29 04:17:25.111: INFO: Pod "pod4-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan 29 04:17:25.111: INFO: Waiting up to 5m0s for pod "pod4-1-sched-preemption-medium-priority" in namespace "sched-preemption-1398" to be "running"
    Jan 29 04:17:25.116: INFO: Pod "pod4-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.337197ms
    Jan 29 04:17:25.116: INFO: Pod "pod4-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 01/29/23 04:17:25.116
    Jan 29 04:17:25.129: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-1398" to be "running"
    Jan 29 04:17:25.135: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.574946ms
    Jan 29 04:17:27.143: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014108007s
    Jan 29 04:17:29.143: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014405777s
    Jan 29 04:17:31.145: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.016260997s
    Jan 29 04:17:31.145: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Jan 29 04:17:31.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-1398" for this suite. 01/29/23 04:17:31.218
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:17:31.305
Jan 29 04:17:31.305: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename emptydir 01/29/23 04:17:31.306
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:17:31.327
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:17:31.335
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
STEP: Creating a pod to test emptydir 0666 on node default medium 01/29/23 04:17:31.34
Jan 29 04:17:31.371: INFO: Waiting up to 5m0s for pod "pod-d45a30b3-1fe4-4429-ac03-629c68bf0d4a" in namespace "emptydir-8726" to be "Succeeded or Failed"
Jan 29 04:17:31.381: INFO: Pod "pod-d45a30b3-1fe4-4429-ac03-629c68bf0d4a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.097051ms
Jan 29 04:17:33.390: INFO: Pod "pod-d45a30b3-1fe4-4429-ac03-629c68bf0d4a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019227343s
Jan 29 04:17:35.389: INFO: Pod "pod-d45a30b3-1fe4-4429-ac03-629c68bf0d4a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018560846s
STEP: Saw pod success 01/29/23 04:17:35.39
Jan 29 04:17:35.390: INFO: Pod "pod-d45a30b3-1fe4-4429-ac03-629c68bf0d4a" satisfied condition "Succeeded or Failed"
Jan 29 04:17:35.396: INFO: Trying to get logs from node slave2 pod pod-d45a30b3-1fe4-4429-ac03-629c68bf0d4a container test-container: <nil>
STEP: delete the pod 01/29/23 04:17:35.412
Jan 29 04:17:35.476: INFO: Waiting for pod pod-d45a30b3-1fe4-4429-ac03-629c68bf0d4a to disappear
Jan 29 04:17:35.482: INFO: Pod pod-d45a30b3-1fe4-4429-ac03-629c68bf0d4a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 29 04:17:35.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8726" for this suite. 01/29/23 04:17:35.492
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":285,"skipped":5020,"failed":0}
------------------------------
• [4.197 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:17:31.305
    Jan 29 04:17:31.305: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename emptydir 01/29/23 04:17:31.306
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:17:31.327
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:17:31.335
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:176
    STEP: Creating a pod to test emptydir 0666 on node default medium 01/29/23 04:17:31.34
    Jan 29 04:17:31.371: INFO: Waiting up to 5m0s for pod "pod-d45a30b3-1fe4-4429-ac03-629c68bf0d4a" in namespace "emptydir-8726" to be "Succeeded or Failed"
    Jan 29 04:17:31.381: INFO: Pod "pod-d45a30b3-1fe4-4429-ac03-629c68bf0d4a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.097051ms
    Jan 29 04:17:33.390: INFO: Pod "pod-d45a30b3-1fe4-4429-ac03-629c68bf0d4a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019227343s
    Jan 29 04:17:35.389: INFO: Pod "pod-d45a30b3-1fe4-4429-ac03-629c68bf0d4a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018560846s
    STEP: Saw pod success 01/29/23 04:17:35.39
    Jan 29 04:17:35.390: INFO: Pod "pod-d45a30b3-1fe4-4429-ac03-629c68bf0d4a" satisfied condition "Succeeded or Failed"
    Jan 29 04:17:35.396: INFO: Trying to get logs from node slave2 pod pod-d45a30b3-1fe4-4429-ac03-629c68bf0d4a container test-container: <nil>
    STEP: delete the pod 01/29/23 04:17:35.412
    Jan 29 04:17:35.476: INFO: Waiting for pod pod-d45a30b3-1fe4-4429-ac03-629c68bf0d4a to disappear
    Jan 29 04:17:35.482: INFO: Pod pod-d45a30b3-1fe4-4429-ac03-629c68bf0d4a no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 29 04:17:35.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-8726" for this suite. 01/29/23 04:17:35.492
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:17:35.503
Jan 29 04:17:35.503: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename endpointslice 01/29/23 04:17:35.505
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:17:35.531
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:17:35.536
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
STEP: referencing a single matching pod 01/29/23 04:17:40.68
STEP: referencing matching pods with named port 01/29/23 04:17:45.695
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 01/29/23 04:17:50.709
STEP: recreating EndpointSlices after they've been deleted 01/29/23 04:17:55.726
Jan 29 04:17:55.766: INFO: EndpointSlice for Service endpointslice-9249/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Jan 29 04:18:05.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-9249" for this suite. 01/29/23 04:18:05.795
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","completed":286,"skipped":5026,"failed":0}
------------------------------
• [SLOW TEST] [30.303 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:17:35.503
    Jan 29 04:17:35.503: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename endpointslice 01/29/23 04:17:35.505
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:17:35.531
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:17:35.536
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:204
    STEP: referencing a single matching pod 01/29/23 04:17:40.68
    STEP: referencing matching pods with named port 01/29/23 04:17:45.695
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 01/29/23 04:17:50.709
    STEP: recreating EndpointSlices after they've been deleted 01/29/23 04:17:55.726
    Jan 29 04:17:55.766: INFO: EndpointSlice for Service endpointslice-9249/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Jan 29 04:18:05.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-9249" for this suite. 01/29/23 04:18:05.795
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:18:05.807
Jan 29 04:18:05.807: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename sched-preemption 01/29/23 04:18:05.809
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:18:05.834
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:18:05.839
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jan 29 04:18:05.873: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 29 04:19:05.932: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:19:05.938
Jan 29 04:19:05.938: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename sched-preemption-path 01/29/23 04:19:05.939
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:19:05.982
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:19:05.988
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:690
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
Jan 29 04:19:06.015: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Jan 29 04:19:06.021: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/framework.go:187
Jan 29 04:19:06.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-8747" for this suite. 01/29/23 04:19:06.061
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:706
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Jan 29 04:19:06.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-9432" for this suite. 01/29/23 04:19:06.1
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","completed":287,"skipped":5042,"failed":0}
------------------------------
• [SLOW TEST] [60.404 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:683
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:733

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:18:05.807
    Jan 29 04:18:05.807: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename sched-preemption 01/29/23 04:18:05.809
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:18:05.834
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:18:05.839
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Jan 29 04:18:05.873: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 29 04:19:05.932: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:19:05.938
    Jan 29 04:19:05.938: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename sched-preemption-path 01/29/23 04:19:05.939
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:19:05.982
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:19:05.988
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:690
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:733
    Jan 29 04:19:06.015: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Jan 29 04:19:06.021: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/framework.go:187
    Jan 29 04:19:06.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-8747" for this suite. 01/29/23 04:19:06.061
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:706
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Jan 29 04:19:06.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-9432" for this suite. 01/29/23 04:19:06.1
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:19:06.212
Jan 29 04:19:06.212: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename projected 01/29/23 04:19:06.214
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:19:06.24
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:19:06.245
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
STEP: Creating configMap with name projected-configmap-test-volume-83d55bc8-fd72-4a63-bd94-b3cb62dc66c4 01/29/23 04:19:06.25
STEP: Creating a pod to test consume configMaps 01/29/23 04:19:06.259
Jan 29 04:19:06.280: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a9aefd03-9263-4d26-ac3e-aa879ca90c53" in namespace "projected-7676" to be "Succeeded or Failed"
Jan 29 04:19:06.286: INFO: Pod "pod-projected-configmaps-a9aefd03-9263-4d26-ac3e-aa879ca90c53": Phase="Pending", Reason="", readiness=false. Elapsed: 6.753067ms
Jan 29 04:19:08.297: INFO: Pod "pod-projected-configmaps-a9aefd03-9263-4d26-ac3e-aa879ca90c53": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017247402s
Jan 29 04:19:10.295: INFO: Pod "pod-projected-configmaps-a9aefd03-9263-4d26-ac3e-aa879ca90c53": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015579671s
Jan 29 04:19:12.295: INFO: Pod "pod-projected-configmaps-a9aefd03-9263-4d26-ac3e-aa879ca90c53": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01514539s
STEP: Saw pod success 01/29/23 04:19:12.295
Jan 29 04:19:12.295: INFO: Pod "pod-projected-configmaps-a9aefd03-9263-4d26-ac3e-aa879ca90c53" satisfied condition "Succeeded or Failed"
Jan 29 04:19:12.303: INFO: Trying to get logs from node slave2 pod pod-projected-configmaps-a9aefd03-9263-4d26-ac3e-aa879ca90c53 container agnhost-container: <nil>
STEP: delete the pod 01/29/23 04:19:12.336
Jan 29 04:19:12.417: INFO: Waiting for pod pod-projected-configmaps-a9aefd03-9263-4d26-ac3e-aa879ca90c53 to disappear
Jan 29 04:19:12.423: INFO: Pod pod-projected-configmaps-a9aefd03-9263-4d26-ac3e-aa879ca90c53 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 29 04:19:12.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7676" for this suite. 01/29/23 04:19:12.433
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":288,"skipped":5054,"failed":0}
------------------------------
• [SLOW TEST] [6.230 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:19:06.212
    Jan 29 04:19:06.212: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename projected 01/29/23 04:19:06.214
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:19:06.24
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:19:06.245
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:73
    STEP: Creating configMap with name projected-configmap-test-volume-83d55bc8-fd72-4a63-bd94-b3cb62dc66c4 01/29/23 04:19:06.25
    STEP: Creating a pod to test consume configMaps 01/29/23 04:19:06.259
    Jan 29 04:19:06.280: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a9aefd03-9263-4d26-ac3e-aa879ca90c53" in namespace "projected-7676" to be "Succeeded or Failed"
    Jan 29 04:19:06.286: INFO: Pod "pod-projected-configmaps-a9aefd03-9263-4d26-ac3e-aa879ca90c53": Phase="Pending", Reason="", readiness=false. Elapsed: 6.753067ms
    Jan 29 04:19:08.297: INFO: Pod "pod-projected-configmaps-a9aefd03-9263-4d26-ac3e-aa879ca90c53": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017247402s
    Jan 29 04:19:10.295: INFO: Pod "pod-projected-configmaps-a9aefd03-9263-4d26-ac3e-aa879ca90c53": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015579671s
    Jan 29 04:19:12.295: INFO: Pod "pod-projected-configmaps-a9aefd03-9263-4d26-ac3e-aa879ca90c53": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01514539s
    STEP: Saw pod success 01/29/23 04:19:12.295
    Jan 29 04:19:12.295: INFO: Pod "pod-projected-configmaps-a9aefd03-9263-4d26-ac3e-aa879ca90c53" satisfied condition "Succeeded or Failed"
    Jan 29 04:19:12.303: INFO: Trying to get logs from node slave2 pod pod-projected-configmaps-a9aefd03-9263-4d26-ac3e-aa879ca90c53 container agnhost-container: <nil>
    STEP: delete the pod 01/29/23 04:19:12.336
    Jan 29 04:19:12.417: INFO: Waiting for pod pod-projected-configmaps-a9aefd03-9263-4d26-ac3e-aa879ca90c53 to disappear
    Jan 29 04:19:12.423: INFO: Pod pod-projected-configmaps-a9aefd03-9263-4d26-ac3e-aa879ca90c53 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 29 04:19:12.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7676" for this suite. 01/29/23 04:19:12.433
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:19:12.444
Jan 29 04:19:12.444: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename daemonsets 01/29/23 04:19:12.445
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:19:12.468
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:19:12.473
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
Jan 29 04:19:12.528: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 01/29/23 04:19:12.541
Jan 29 04:19:12.556: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 29 04:19:12.556: INFO: Node master1 is running 0 daemon pod, expected 1
Jan 29 04:19:13.574: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 29 04:19:13.574: INFO: Node master1 is running 0 daemon pod, expected 1
Jan 29 04:19:14.577: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan 29 04:19:14.577: INFO: Node slave1 is running 0 daemon pod, expected 1
Jan 29 04:19:15.574: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Jan 29 04:19:15.575: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
STEP: Update daemon pods image. 01/29/23 04:19:15.599
STEP: Check that daemon pods images are updated. 01/29/23 04:19:15.616
Jan 29 04:19:15.623: INFO: Wrong image for pod: daemon-set-dztv4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 29 04:19:15.623: INFO: Wrong image for pod: daemon-set-jphxn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 29 04:19:15.623: INFO: Wrong image for pod: daemon-set-kfkc5. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 29 04:19:15.623: INFO: Wrong image for pod: daemon-set-qjf4f. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 29 04:19:15.623: INFO: Wrong image for pod: daemon-set-zk7hr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 29 04:19:16.641: INFO: Wrong image for pod: daemon-set-jphxn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 29 04:19:16.641: INFO: Wrong image for pod: daemon-set-kfkc5. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 29 04:19:16.641: INFO: Wrong image for pod: daemon-set-qjf4f. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 29 04:19:16.641: INFO: Wrong image for pod: daemon-set-zk7hr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 29 04:19:17.640: INFO: Wrong image for pod: daemon-set-jphxn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 29 04:19:17.640: INFO: Wrong image for pod: daemon-set-kfkc5. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 29 04:19:17.640: INFO: Wrong image for pod: daemon-set-qjf4f. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 29 04:19:17.640: INFO: Wrong image for pod: daemon-set-zk7hr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 29 04:19:18.643: INFO: Wrong image for pod: daemon-set-jphxn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 29 04:19:18.643: INFO: Wrong image for pod: daemon-set-kfkc5. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 29 04:19:18.643: INFO: Pod daemon-set-lz66l is not available
Jan 29 04:19:18.643: INFO: Wrong image for pod: daemon-set-qjf4f. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 29 04:19:18.643: INFO: Wrong image for pod: daemon-set-zk7hr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 29 04:19:19.640: INFO: Wrong image for pod: daemon-set-jphxn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 29 04:19:19.640: INFO: Wrong image for pod: daemon-set-kfkc5. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 29 04:19:19.640: INFO: Pod daemon-set-lz66l is not available
Jan 29 04:19:19.640: INFO: Wrong image for pod: daemon-set-qjf4f. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 29 04:19:19.640: INFO: Wrong image for pod: daemon-set-zk7hr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 29 04:19:20.641: INFO: Wrong image for pod: daemon-set-kfkc5. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 29 04:19:20.641: INFO: Wrong image for pod: daemon-set-qjf4f. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 29 04:19:20.641: INFO: Wrong image for pod: daemon-set-zk7hr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 29 04:19:21.641: INFO: Wrong image for pod: daemon-set-kfkc5. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 29 04:19:21.641: INFO: Wrong image for pod: daemon-set-qjf4f. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 29 04:19:21.641: INFO: Wrong image for pod: daemon-set-zk7hr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 29 04:19:22.642: INFO: Pod daemon-set-bl497 is not available
Jan 29 04:19:22.642: INFO: Wrong image for pod: daemon-set-kfkc5. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 29 04:19:22.643: INFO: Wrong image for pod: daemon-set-qjf4f. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 29 04:19:22.643: INFO: Wrong image for pod: daemon-set-zk7hr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 29 04:19:23.642: INFO: Pod daemon-set-bl497 is not available
Jan 29 04:19:23.642: INFO: Wrong image for pod: daemon-set-kfkc5. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 29 04:19:23.642: INFO: Wrong image for pod: daemon-set-qjf4f. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 29 04:19:23.642: INFO: Wrong image for pod: daemon-set-zk7hr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 29 04:19:24.647: INFO: Wrong image for pod: daemon-set-kfkc5. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 29 04:19:24.647: INFO: Wrong image for pod: daemon-set-qjf4f. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 29 04:19:25.642: INFO: Wrong image for pod: daemon-set-kfkc5. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 29 04:19:25.642: INFO: Pod daemon-set-lv8cf is not available
Jan 29 04:19:25.642: INFO: Wrong image for pod: daemon-set-qjf4f. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 29 04:19:26.642: INFO: Wrong image for pod: daemon-set-kfkc5. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 29 04:19:26.642: INFO: Pod daemon-set-lv8cf is not available
Jan 29 04:19:26.642: INFO: Wrong image for pod: daemon-set-qjf4f. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 29 04:19:27.642: INFO: Wrong image for pod: daemon-set-kfkc5. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 29 04:19:28.642: INFO: Wrong image for pod: daemon-set-kfkc5. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 29 04:19:28.642: INFO: Pod daemon-set-vc8w7 is not available
Jan 29 04:19:29.643: INFO: Wrong image for pod: daemon-set-kfkc5. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 29 04:19:29.643: INFO: Pod daemon-set-vc8w7 is not available
Jan 29 04:19:32.641: INFO: Pod daemon-set-jljd7 is not available
STEP: Check that daemon pods are still running on every node of the cluster. 01/29/23 04:19:32.652
Jan 29 04:19:32.668: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Jan 29 04:19:32.668: INFO: Node master2 is running 0 daemon pod, expected 1
Jan 29 04:19:33.687: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Jan 29 04:19:33.687: INFO: Node master2 is running 0 daemon pod, expected 1
Jan 29 04:19:34.685: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Jan 29 04:19:34.685: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 01/29/23 04:19:34.716
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6783, will wait for the garbage collector to delete the pods 01/29/23 04:19:34.716
Jan 29 04:19:34.785: INFO: Deleting DaemonSet.extensions daemon-set took: 11.614121ms
Jan 29 04:19:34.986: INFO: Terminating DaemonSet.extensions daemon-set pods took: 201.058108ms
Jan 29 04:19:37.995: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 29 04:19:37.995: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 29 04:19:38.002: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"5982320"},"items":null}

Jan 29 04:19:38.009: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"5982320"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan 29 04:19:38.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6783" for this suite. 01/29/23 04:19:38.055
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","completed":289,"skipped":5067,"failed":0}
------------------------------
• [SLOW TEST] [25.620 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:19:12.444
    Jan 29 04:19:12.444: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename daemonsets 01/29/23 04:19:12.445
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:19:12.468
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:19:12.473
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:373
    Jan 29 04:19:12.528: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 01/29/23 04:19:12.541
    Jan 29 04:19:12.556: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 29 04:19:12.556: INFO: Node master1 is running 0 daemon pod, expected 1
    Jan 29 04:19:13.574: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 29 04:19:13.574: INFO: Node master1 is running 0 daemon pod, expected 1
    Jan 29 04:19:14.577: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan 29 04:19:14.577: INFO: Node slave1 is running 0 daemon pod, expected 1
    Jan 29 04:19:15.574: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
    Jan 29 04:19:15.575: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
    STEP: Update daemon pods image. 01/29/23 04:19:15.599
    STEP: Check that daemon pods images are updated. 01/29/23 04:19:15.616
    Jan 29 04:19:15.623: INFO: Wrong image for pod: daemon-set-dztv4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 29 04:19:15.623: INFO: Wrong image for pod: daemon-set-jphxn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 29 04:19:15.623: INFO: Wrong image for pod: daemon-set-kfkc5. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 29 04:19:15.623: INFO: Wrong image for pod: daemon-set-qjf4f. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 29 04:19:15.623: INFO: Wrong image for pod: daemon-set-zk7hr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 29 04:19:16.641: INFO: Wrong image for pod: daemon-set-jphxn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 29 04:19:16.641: INFO: Wrong image for pod: daemon-set-kfkc5. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 29 04:19:16.641: INFO: Wrong image for pod: daemon-set-qjf4f. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 29 04:19:16.641: INFO: Wrong image for pod: daemon-set-zk7hr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 29 04:19:17.640: INFO: Wrong image for pod: daemon-set-jphxn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 29 04:19:17.640: INFO: Wrong image for pod: daemon-set-kfkc5. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 29 04:19:17.640: INFO: Wrong image for pod: daemon-set-qjf4f. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 29 04:19:17.640: INFO: Wrong image for pod: daemon-set-zk7hr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 29 04:19:18.643: INFO: Wrong image for pod: daemon-set-jphxn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 29 04:19:18.643: INFO: Wrong image for pod: daemon-set-kfkc5. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 29 04:19:18.643: INFO: Pod daemon-set-lz66l is not available
    Jan 29 04:19:18.643: INFO: Wrong image for pod: daemon-set-qjf4f. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 29 04:19:18.643: INFO: Wrong image for pod: daemon-set-zk7hr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 29 04:19:19.640: INFO: Wrong image for pod: daemon-set-jphxn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 29 04:19:19.640: INFO: Wrong image for pod: daemon-set-kfkc5. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 29 04:19:19.640: INFO: Pod daemon-set-lz66l is not available
    Jan 29 04:19:19.640: INFO: Wrong image for pod: daemon-set-qjf4f. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 29 04:19:19.640: INFO: Wrong image for pod: daemon-set-zk7hr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 29 04:19:20.641: INFO: Wrong image for pod: daemon-set-kfkc5. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 29 04:19:20.641: INFO: Wrong image for pod: daemon-set-qjf4f. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 29 04:19:20.641: INFO: Wrong image for pod: daemon-set-zk7hr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 29 04:19:21.641: INFO: Wrong image for pod: daemon-set-kfkc5. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 29 04:19:21.641: INFO: Wrong image for pod: daemon-set-qjf4f. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 29 04:19:21.641: INFO: Wrong image for pod: daemon-set-zk7hr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 29 04:19:22.642: INFO: Pod daemon-set-bl497 is not available
    Jan 29 04:19:22.642: INFO: Wrong image for pod: daemon-set-kfkc5. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 29 04:19:22.643: INFO: Wrong image for pod: daemon-set-qjf4f. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 29 04:19:22.643: INFO: Wrong image for pod: daemon-set-zk7hr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 29 04:19:23.642: INFO: Pod daemon-set-bl497 is not available
    Jan 29 04:19:23.642: INFO: Wrong image for pod: daemon-set-kfkc5. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 29 04:19:23.642: INFO: Wrong image for pod: daemon-set-qjf4f. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 29 04:19:23.642: INFO: Wrong image for pod: daemon-set-zk7hr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 29 04:19:24.647: INFO: Wrong image for pod: daemon-set-kfkc5. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 29 04:19:24.647: INFO: Wrong image for pod: daemon-set-qjf4f. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 29 04:19:25.642: INFO: Wrong image for pod: daemon-set-kfkc5. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 29 04:19:25.642: INFO: Pod daemon-set-lv8cf is not available
    Jan 29 04:19:25.642: INFO: Wrong image for pod: daemon-set-qjf4f. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 29 04:19:26.642: INFO: Wrong image for pod: daemon-set-kfkc5. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 29 04:19:26.642: INFO: Pod daemon-set-lv8cf is not available
    Jan 29 04:19:26.642: INFO: Wrong image for pod: daemon-set-qjf4f. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 29 04:19:27.642: INFO: Wrong image for pod: daemon-set-kfkc5. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 29 04:19:28.642: INFO: Wrong image for pod: daemon-set-kfkc5. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 29 04:19:28.642: INFO: Pod daemon-set-vc8w7 is not available
    Jan 29 04:19:29.643: INFO: Wrong image for pod: daemon-set-kfkc5. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 29 04:19:29.643: INFO: Pod daemon-set-vc8w7 is not available
    Jan 29 04:19:32.641: INFO: Pod daemon-set-jljd7 is not available
    STEP: Check that daemon pods are still running on every node of the cluster. 01/29/23 04:19:32.652
    Jan 29 04:19:32.668: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Jan 29 04:19:32.668: INFO: Node master2 is running 0 daemon pod, expected 1
    Jan 29 04:19:33.687: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Jan 29 04:19:33.687: INFO: Node master2 is running 0 daemon pod, expected 1
    Jan 29 04:19:34.685: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
    Jan 29 04:19:34.685: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 01/29/23 04:19:34.716
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6783, will wait for the garbage collector to delete the pods 01/29/23 04:19:34.716
    Jan 29 04:19:34.785: INFO: Deleting DaemonSet.extensions daemon-set took: 11.614121ms
    Jan 29 04:19:34.986: INFO: Terminating DaemonSet.extensions daemon-set pods took: 201.058108ms
    Jan 29 04:19:37.995: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 29 04:19:37.995: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 29 04:19:38.002: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"5982320"},"items":null}

    Jan 29 04:19:38.009: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"5982320"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan 29 04:19:38.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-6783" for this suite. 01/29/23 04:19:38.055
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:19:38.065
Jan 29 04:19:38.065: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename containers 01/29/23 04:19:38.066
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:19:38.089
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:19:38.095
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
Jan 29 04:19:38.119: INFO: Waiting up to 5m0s for pod "client-containers-ffc1c2a9-18ec-4dd7-8636-637a324c27fd" in namespace "containers-7884" to be "running"
Jan 29 04:19:38.125: INFO: Pod "client-containers-ffc1c2a9-18ec-4dd7-8636-637a324c27fd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.673707ms
Jan 29 04:19:40.134: INFO: Pod "client-containers-ffc1c2a9-18ec-4dd7-8636-637a324c27fd": Phase="Running", Reason="", readiness=true. Elapsed: 2.015440109s
Jan 29 04:19:40.134: INFO: Pod "client-containers-ffc1c2a9-18ec-4dd7-8636-637a324c27fd" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Jan 29 04:19:40.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7884" for this suite. 01/29/23 04:19:40.158
{"msg":"PASSED [sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","completed":290,"skipped":5075,"failed":0}
------------------------------
• [2.103 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:19:38.065
    Jan 29 04:19:38.065: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename containers 01/29/23 04:19:38.066
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:19:38.089
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:19:38.095
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:38
    Jan 29 04:19:38.119: INFO: Waiting up to 5m0s for pod "client-containers-ffc1c2a9-18ec-4dd7-8636-637a324c27fd" in namespace "containers-7884" to be "running"
    Jan 29 04:19:38.125: INFO: Pod "client-containers-ffc1c2a9-18ec-4dd7-8636-637a324c27fd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.673707ms
    Jan 29 04:19:40.134: INFO: Pod "client-containers-ffc1c2a9-18ec-4dd7-8636-637a324c27fd": Phase="Running", Reason="", readiness=true. Elapsed: 2.015440109s
    Jan 29 04:19:40.134: INFO: Pod "client-containers-ffc1c2a9-18ec-4dd7-8636-637a324c27fd" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Jan 29 04:19:40.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-7884" for this suite. 01/29/23 04:19:40.158
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:19:40.17
Jan 29 04:19:40.170: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename webhook 01/29/23 04:19:40.171
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:19:40.195
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:19:40.2
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/29/23 04:19:40.226
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/29/23 04:19:41.861
STEP: Deploying the webhook pod 01/29/23 04:19:41.875
STEP: Wait for the deployment to be ready 01/29/23 04:19:41.902
Jan 29 04:19:41.934: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jan 29 04:19:43.954: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 29, 4, 19, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 4, 19, 41, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 29, 4, 19, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 4, 19, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/29/23 04:19:45.959
STEP: Verifying the service has paired with the endpoint 01/29/23 04:19:45.98
Jan 29 04:19:46.981: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
STEP: Creating a mutating webhook configuration 01/29/23 04:19:46.988
STEP: Updating a mutating webhook configuration's rules to not include the create operation 01/29/23 04:19:47.018
STEP: Creating a configMap that should not be mutated 01/29/23 04:19:47.027
STEP: Patching a mutating webhook configuration's rules to include the create operation 01/29/23 04:19:47.045
STEP: Creating a configMap that should be mutated 01/29/23 04:19:47.053
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 29 04:19:47.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9383" for this suite. 01/29/23 04:19:47.118
STEP: Destroying namespace "webhook-9383-markers" for this suite. 01/29/23 04:19:47.127
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","completed":291,"skipped":5103,"failed":0}
------------------------------
• [SLOW TEST] [7.054 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:19:40.17
    Jan 29 04:19:40.170: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename webhook 01/29/23 04:19:40.171
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:19:40.195
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:19:40.2
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/29/23 04:19:40.226
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/29/23 04:19:41.861
    STEP: Deploying the webhook pod 01/29/23 04:19:41.875
    STEP: Wait for the deployment to be ready 01/29/23 04:19:41.902
    Jan 29 04:19:41.934: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    Jan 29 04:19:43.954: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 29, 4, 19, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 4, 19, 41, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 29, 4, 19, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 4, 19, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/29/23 04:19:45.959
    STEP: Verifying the service has paired with the endpoint 01/29/23 04:19:45.98
    Jan 29 04:19:46.981: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:507
    STEP: Creating a mutating webhook configuration 01/29/23 04:19:46.988
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 01/29/23 04:19:47.018
    STEP: Creating a configMap that should not be mutated 01/29/23 04:19:47.027
    STEP: Patching a mutating webhook configuration's rules to include the create operation 01/29/23 04:19:47.045
    STEP: Creating a configMap that should be mutated 01/29/23 04:19:47.053
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 29 04:19:47.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-9383" for this suite. 01/29/23 04:19:47.118
    STEP: Destroying namespace "webhook-9383-markers" for this suite. 01/29/23 04:19:47.127
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:19:47.228
Jan 29 04:19:47.228: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename downward-api 01/29/23 04:19:47.229
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:19:47.256
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:19:47.263
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
STEP: Creating a pod to test downward api env vars 01/29/23 04:19:47.268
Jan 29 04:19:47.289: INFO: Waiting up to 5m0s for pod "downward-api-bac3f965-1180-4e85-b3ea-c8b81647b75d" in namespace "downward-api-2824" to be "Succeeded or Failed"
Jan 29 04:19:47.296: INFO: Pod "downward-api-bac3f965-1180-4e85-b3ea-c8b81647b75d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.993309ms
Jan 29 04:19:49.310: INFO: Pod "downward-api-bac3f965-1180-4e85-b3ea-c8b81647b75d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021144729s
Jan 29 04:19:51.304: INFO: Pod "downward-api-bac3f965-1180-4e85-b3ea-c8b81647b75d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015373169s
Jan 29 04:19:53.304: INFO: Pod "downward-api-bac3f965-1180-4e85-b3ea-c8b81647b75d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015009908s
STEP: Saw pod success 01/29/23 04:19:53.304
Jan 29 04:19:53.304: INFO: Pod "downward-api-bac3f965-1180-4e85-b3ea-c8b81647b75d" satisfied condition "Succeeded or Failed"
Jan 29 04:19:53.311: INFO: Trying to get logs from node slave2 pod downward-api-bac3f965-1180-4e85-b3ea-c8b81647b75d container dapi-container: <nil>
STEP: delete the pod 01/29/23 04:19:53.327
Jan 29 04:19:53.426: INFO: Waiting for pod downward-api-bac3f965-1180-4e85-b3ea-c8b81647b75d to disappear
Jan 29 04:19:53.431: INFO: Pod downward-api-bac3f965-1180-4e85-b3ea-c8b81647b75d no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jan 29 04:19:53.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2824" for this suite. 01/29/23 04:19:53.441
{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","completed":292,"skipped":5154,"failed":0}
------------------------------
• [SLOW TEST] [6.223 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:19:47.228
    Jan 29 04:19:47.228: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename downward-api 01/29/23 04:19:47.229
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:19:47.256
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:19:47.263
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:89
    STEP: Creating a pod to test downward api env vars 01/29/23 04:19:47.268
    Jan 29 04:19:47.289: INFO: Waiting up to 5m0s for pod "downward-api-bac3f965-1180-4e85-b3ea-c8b81647b75d" in namespace "downward-api-2824" to be "Succeeded or Failed"
    Jan 29 04:19:47.296: INFO: Pod "downward-api-bac3f965-1180-4e85-b3ea-c8b81647b75d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.993309ms
    Jan 29 04:19:49.310: INFO: Pod "downward-api-bac3f965-1180-4e85-b3ea-c8b81647b75d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021144729s
    Jan 29 04:19:51.304: INFO: Pod "downward-api-bac3f965-1180-4e85-b3ea-c8b81647b75d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015373169s
    Jan 29 04:19:53.304: INFO: Pod "downward-api-bac3f965-1180-4e85-b3ea-c8b81647b75d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015009908s
    STEP: Saw pod success 01/29/23 04:19:53.304
    Jan 29 04:19:53.304: INFO: Pod "downward-api-bac3f965-1180-4e85-b3ea-c8b81647b75d" satisfied condition "Succeeded or Failed"
    Jan 29 04:19:53.311: INFO: Trying to get logs from node slave2 pod downward-api-bac3f965-1180-4e85-b3ea-c8b81647b75d container dapi-container: <nil>
    STEP: delete the pod 01/29/23 04:19:53.327
    Jan 29 04:19:53.426: INFO: Waiting for pod downward-api-bac3f965-1180-4e85-b3ea-c8b81647b75d to disappear
    Jan 29 04:19:53.431: INFO: Pod downward-api-bac3f965-1180-4e85-b3ea-c8b81647b75d no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jan 29 04:19:53.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-2824" for this suite. 01/29/23 04:19:53.441
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:19:53.454
Jan 29 04:19:53.454: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename dns 01/29/23 04:19:53.455
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:19:53.481
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:19:53.487
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8679.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-8679.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 01/29/23 04:19:53.492
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8679.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-8679.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 01/29/23 04:19:53.492
STEP: creating a pod to probe /etc/hosts 01/29/23 04:19:53.492
STEP: submitting the pod to kubernetes 01/29/23 04:19:53.492
Jan 29 04:19:53.514: INFO: Waiting up to 15m0s for pod "dns-test-f0a904ce-f52b-43f0-8a97-3166ee54de68" in namespace "dns-8679" to be "running"
Jan 29 04:19:53.522: INFO: Pod "dns-test-f0a904ce-f52b-43f0-8a97-3166ee54de68": Phase="Pending", Reason="", readiness=false. Elapsed: 7.530613ms
Jan 29 04:19:55.529: INFO: Pod "dns-test-f0a904ce-f52b-43f0-8a97-3166ee54de68": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014639944s
Jan 29 04:19:57.530: INFO: Pod "dns-test-f0a904ce-f52b-43f0-8a97-3166ee54de68": Phase="Running", Reason="", readiness=true. Elapsed: 4.016006074s
Jan 29 04:19:57.531: INFO: Pod "dns-test-f0a904ce-f52b-43f0-8a97-3166ee54de68" satisfied condition "running"
STEP: retrieving the pod 01/29/23 04:19:57.531
STEP: looking for the results for each expected name from probers 01/29/23 04:19:57.536
Jan 29 04:19:57.565: INFO: DNS probes using dns-8679/dns-test-f0a904ce-f52b-43f0-8a97-3166ee54de68 succeeded

STEP: deleting the pod 01/29/23 04:19:57.565
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan 29 04:19:57.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8679" for this suite. 01/29/23 04:19:57.684
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]","completed":293,"skipped":5194,"failed":0}
------------------------------
• [4.239 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:19:53.454
    Jan 29 04:19:53.454: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename dns 01/29/23 04:19:53.455
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:19:53.481
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:19:53.487
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8679.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-8679.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     01/29/23 04:19:53.492
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8679.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-8679.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     01/29/23 04:19:53.492
    STEP: creating a pod to probe /etc/hosts 01/29/23 04:19:53.492
    STEP: submitting the pod to kubernetes 01/29/23 04:19:53.492
    Jan 29 04:19:53.514: INFO: Waiting up to 15m0s for pod "dns-test-f0a904ce-f52b-43f0-8a97-3166ee54de68" in namespace "dns-8679" to be "running"
    Jan 29 04:19:53.522: INFO: Pod "dns-test-f0a904ce-f52b-43f0-8a97-3166ee54de68": Phase="Pending", Reason="", readiness=false. Elapsed: 7.530613ms
    Jan 29 04:19:55.529: INFO: Pod "dns-test-f0a904ce-f52b-43f0-8a97-3166ee54de68": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014639944s
    Jan 29 04:19:57.530: INFO: Pod "dns-test-f0a904ce-f52b-43f0-8a97-3166ee54de68": Phase="Running", Reason="", readiness=true. Elapsed: 4.016006074s
    Jan 29 04:19:57.531: INFO: Pod "dns-test-f0a904ce-f52b-43f0-8a97-3166ee54de68" satisfied condition "running"
    STEP: retrieving the pod 01/29/23 04:19:57.531
    STEP: looking for the results for each expected name from probers 01/29/23 04:19:57.536
    Jan 29 04:19:57.565: INFO: DNS probes using dns-8679/dns-test-f0a904ce-f52b-43f0-8a97-3166ee54de68 succeeded

    STEP: deleting the pod 01/29/23 04:19:57.565
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan 29 04:19:57.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-8679" for this suite. 01/29/23 04:19:57.684
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:19:57.694
Jan 29 04:19:57.694: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename dns 01/29/23 04:19:57.696
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:19:57.72
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:19:57.727
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 01/29/23 04:19:57.733
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7321.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7321.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7321.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7321.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7321.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-7321.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7321.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-7321.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7321.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-7321.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7321.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-7321.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 1.241.105.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.105.241.1_udp@PTR;check="$$(dig +tcp +noall +answer +search 1.241.105.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.105.241.1_tcp@PTR;sleep 1; done
 01/29/23 04:19:57.762
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7321.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7321.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7321.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7321.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7321.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-7321.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7321.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-7321.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7321.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-7321.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7321.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-7321.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 1.241.105.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.105.241.1_udp@PTR;check="$$(dig +tcp +noall +answer +search 1.241.105.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.105.241.1_tcp@PTR;sleep 1; done
 01/29/23 04:19:57.762
STEP: creating a pod to probe DNS 01/29/23 04:19:57.762
STEP: submitting the pod to kubernetes 01/29/23 04:19:57.762
Jan 29 04:19:57.801: INFO: Waiting up to 15m0s for pod "dns-test-26ff95fc-7e81-41fb-80c6-2840508d36d8" in namespace "dns-7321" to be "running"
Jan 29 04:19:57.819: INFO: Pod "dns-test-26ff95fc-7e81-41fb-80c6-2840508d36d8": Phase="Pending", Reason="", readiness=false. Elapsed: 17.490583ms
Jan 29 04:19:59.828: INFO: Pod "dns-test-26ff95fc-7e81-41fb-80c6-2840508d36d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025967163s
Jan 29 04:20:01.847: INFO: Pod "dns-test-26ff95fc-7e81-41fb-80c6-2840508d36d8": Phase="Running", Reason="", readiness=true. Elapsed: 4.0453861s
Jan 29 04:20:01.847: INFO: Pod "dns-test-26ff95fc-7e81-41fb-80c6-2840508d36d8" satisfied condition "running"
STEP: retrieving the pod 01/29/23 04:20:01.847
STEP: looking for the results for each expected name from probers 01/29/23 04:20:01.861
Jan 29 04:20:01.893: INFO: Unable to read wheezy_udp@dns-test-service.dns-7321.svc.cluster.local from pod dns-7321/dns-test-26ff95fc-7e81-41fb-80c6-2840508d36d8: the server could not find the requested resource (get pods dns-test-26ff95fc-7e81-41fb-80c6-2840508d36d8)
Jan 29 04:20:01.915: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7321.svc.cluster.local from pod dns-7321/dns-test-26ff95fc-7e81-41fb-80c6-2840508d36d8: the server could not find the requested resource (get pods dns-test-26ff95fc-7e81-41fb-80c6-2840508d36d8)
Jan 29 04:20:01.972: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7321.svc.cluster.local from pod dns-7321/dns-test-26ff95fc-7e81-41fb-80c6-2840508d36d8: the server could not find the requested resource (get pods dns-test-26ff95fc-7e81-41fb-80c6-2840508d36d8)
Jan 29 04:20:02.090: INFO: Unable to read jessie_udp@dns-test-service.dns-7321.svc.cluster.local from pod dns-7321/dns-test-26ff95fc-7e81-41fb-80c6-2840508d36d8: the server could not find the requested resource (get pods dns-test-26ff95fc-7e81-41fb-80c6-2840508d36d8)
Jan 29 04:20:02.136: INFO: Unable to read jessie_tcp@dns-test-service.dns-7321.svc.cluster.local from pod dns-7321/dns-test-26ff95fc-7e81-41fb-80c6-2840508d36d8: the server could not find the requested resource (get pods dns-test-26ff95fc-7e81-41fb-80c6-2840508d36d8)
Jan 29 04:20:02.155: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7321.svc.cluster.local from pod dns-7321/dns-test-26ff95fc-7e81-41fb-80c6-2840508d36d8: the server could not find the requested resource (get pods dns-test-26ff95fc-7e81-41fb-80c6-2840508d36d8)
Jan 29 04:20:02.207: INFO: Lookups using dns-7321/dns-test-26ff95fc-7e81-41fb-80c6-2840508d36d8 failed for: [wheezy_udp@dns-test-service.dns-7321.svc.cluster.local wheezy_tcp@dns-test-service.dns-7321.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7321.svc.cluster.local jessie_udp@dns-test-service.dns-7321.svc.cluster.local jessie_tcp@dns-test-service.dns-7321.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7321.svc.cluster.local]

Jan 29 04:20:07.316: INFO: DNS probes using dns-7321/dns-test-26ff95fc-7e81-41fb-80c6-2840508d36d8 succeeded

STEP: deleting the pod 01/29/23 04:20:07.316
STEP: deleting the test service 01/29/23 04:20:07.394
STEP: deleting the test headless service 01/29/23 04:20:07.438
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan 29 04:20:07.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7321" for this suite. 01/29/23 04:20:07.485
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","completed":294,"skipped":5205,"failed":0}
------------------------------
• [SLOW TEST] [9.809 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:19:57.694
    Jan 29 04:19:57.694: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename dns 01/29/23 04:19:57.696
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:19:57.72
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:19:57.727
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 01/29/23 04:19:57.733
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7321.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7321.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7321.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7321.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7321.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-7321.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7321.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-7321.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7321.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-7321.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7321.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-7321.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 1.241.105.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.105.241.1_udp@PTR;check="$$(dig +tcp +noall +answer +search 1.241.105.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.105.241.1_tcp@PTR;sleep 1; done
     01/29/23 04:19:57.762
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7321.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7321.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7321.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7321.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7321.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-7321.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7321.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-7321.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7321.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-7321.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7321.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-7321.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 1.241.105.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.105.241.1_udp@PTR;check="$$(dig +tcp +noall +answer +search 1.241.105.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.105.241.1_tcp@PTR;sleep 1; done
     01/29/23 04:19:57.762
    STEP: creating a pod to probe DNS 01/29/23 04:19:57.762
    STEP: submitting the pod to kubernetes 01/29/23 04:19:57.762
    Jan 29 04:19:57.801: INFO: Waiting up to 15m0s for pod "dns-test-26ff95fc-7e81-41fb-80c6-2840508d36d8" in namespace "dns-7321" to be "running"
    Jan 29 04:19:57.819: INFO: Pod "dns-test-26ff95fc-7e81-41fb-80c6-2840508d36d8": Phase="Pending", Reason="", readiness=false. Elapsed: 17.490583ms
    Jan 29 04:19:59.828: INFO: Pod "dns-test-26ff95fc-7e81-41fb-80c6-2840508d36d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025967163s
    Jan 29 04:20:01.847: INFO: Pod "dns-test-26ff95fc-7e81-41fb-80c6-2840508d36d8": Phase="Running", Reason="", readiness=true. Elapsed: 4.0453861s
    Jan 29 04:20:01.847: INFO: Pod "dns-test-26ff95fc-7e81-41fb-80c6-2840508d36d8" satisfied condition "running"
    STEP: retrieving the pod 01/29/23 04:20:01.847
    STEP: looking for the results for each expected name from probers 01/29/23 04:20:01.861
    Jan 29 04:20:01.893: INFO: Unable to read wheezy_udp@dns-test-service.dns-7321.svc.cluster.local from pod dns-7321/dns-test-26ff95fc-7e81-41fb-80c6-2840508d36d8: the server could not find the requested resource (get pods dns-test-26ff95fc-7e81-41fb-80c6-2840508d36d8)
    Jan 29 04:20:01.915: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7321.svc.cluster.local from pod dns-7321/dns-test-26ff95fc-7e81-41fb-80c6-2840508d36d8: the server could not find the requested resource (get pods dns-test-26ff95fc-7e81-41fb-80c6-2840508d36d8)
    Jan 29 04:20:01.972: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7321.svc.cluster.local from pod dns-7321/dns-test-26ff95fc-7e81-41fb-80c6-2840508d36d8: the server could not find the requested resource (get pods dns-test-26ff95fc-7e81-41fb-80c6-2840508d36d8)
    Jan 29 04:20:02.090: INFO: Unable to read jessie_udp@dns-test-service.dns-7321.svc.cluster.local from pod dns-7321/dns-test-26ff95fc-7e81-41fb-80c6-2840508d36d8: the server could not find the requested resource (get pods dns-test-26ff95fc-7e81-41fb-80c6-2840508d36d8)
    Jan 29 04:20:02.136: INFO: Unable to read jessie_tcp@dns-test-service.dns-7321.svc.cluster.local from pod dns-7321/dns-test-26ff95fc-7e81-41fb-80c6-2840508d36d8: the server could not find the requested resource (get pods dns-test-26ff95fc-7e81-41fb-80c6-2840508d36d8)
    Jan 29 04:20:02.155: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7321.svc.cluster.local from pod dns-7321/dns-test-26ff95fc-7e81-41fb-80c6-2840508d36d8: the server could not find the requested resource (get pods dns-test-26ff95fc-7e81-41fb-80c6-2840508d36d8)
    Jan 29 04:20:02.207: INFO: Lookups using dns-7321/dns-test-26ff95fc-7e81-41fb-80c6-2840508d36d8 failed for: [wheezy_udp@dns-test-service.dns-7321.svc.cluster.local wheezy_tcp@dns-test-service.dns-7321.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7321.svc.cluster.local jessie_udp@dns-test-service.dns-7321.svc.cluster.local jessie_tcp@dns-test-service.dns-7321.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7321.svc.cluster.local]

    Jan 29 04:20:07.316: INFO: DNS probes using dns-7321/dns-test-26ff95fc-7e81-41fb-80c6-2840508d36d8 succeeded

    STEP: deleting the pod 01/29/23 04:20:07.316
    STEP: deleting the test service 01/29/23 04:20:07.394
    STEP: deleting the test headless service 01/29/23 04:20:07.438
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan 29 04:20:07.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-7321" for this suite. 01/29/23 04:20:07.485
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:20:07.503
Jan 29 04:20:07.504: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename sched-pred 01/29/23 04:20:07.505
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:20:07.534
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:20:07.541
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jan 29 04:20:07.547: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 29 04:20:07.565: INFO: Waiting for terminating namespaces to be deleted...
Jan 29 04:20:07.571: INFO: 
Logging pods the apiserver thinks is on node master1 before test
Jan 29 04:20:07.588: INFO: calico-kube-controllers-5c6d4b68d6-6p9cm from kube-system started at 2023-01-11 07:48:50 +0000 UTC (1 container statuses recorded)
Jan 29 04:20:07.588: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jan 29 04:20:07.588: INFO: calico-node-j4qnr from kube-system started at 2023-01-11 07:49:07 +0000 UTC (1 container statuses recorded)
Jan 29 04:20:07.588: INFO: 	Container calico-node ready: true, restart count 2
Jan 29 04:20:07.588: INFO: cke-admission-daemonset-g5mvj from kube-system started at 2023-01-11 07:51:31 +0000 UTC (1 container statuses recorded)
Jan 29 04:20:07.588: INFO: 	Container cke-admission ready: true, restart count 0
Jan 29 04:20:07.588: INFO: cke-controller-manager-master1 from kube-system started at 2023-01-11 07:49:37 +0000 UTC (1 container statuses recorded)
Jan 29 04:20:07.588: INFO: 	Container cke-controller-manager ready: true, restart count 0
Jan 29 04:20:07.588: INFO: component-controller-manager-master1 from kube-system started at 2023-01-11 07:49:18 +0000 UTC (1 container statuses recorded)
Jan 29 04:20:07.588: INFO: 	Container component-controller-manager ready: true, restart count 0
Jan 29 04:20:07.588: INFO: coredns-tntlt from kube-system started at 2023-01-11 07:49:05 +0000 UTC (1 container statuses recorded)
Jan 29 04:20:07.588: INFO: 	Container coredns ready: true, restart count 0
Jan 29 04:20:07.588: INFO: keepalived-master1 from kube-system started at 2023-01-11 07:48:19 +0000 UTC (1 container statuses recorded)
Jan 29 04:20:07.588: INFO: 	Container keepalived ready: true, restart count 0
Jan 29 04:20:07.588: INFO: kube-apiserver-master1 from kube-system started at 2023-01-11 07:48:12 +0000 UTC (1 container statuses recorded)
Jan 29 04:20:07.588: INFO: 	Container kube-apiserver ready: true, restart count 0
Jan 29 04:20:07.588: INFO: kube-controller-manager-master1 from kube-system started at 2023-01-11 07:48:20 +0000 UTC (1 container statuses recorded)
Jan 29 04:20:07.588: INFO: 	Container kube-controller-manager ready: true, restart count 5
Jan 29 04:20:07.588: INFO: kube-multus-ds-8b89r from kube-system started at 2023-01-11 07:51:31 +0000 UTC (1 container statuses recorded)
Jan 29 04:20:07.588: INFO: 	Container kube-multus ready: true, restart count 0
Jan 29 04:20:07.588: INFO: kube-proxy-master1 from kube-system started at 2023-01-11 07:48:15 +0000 UTC (1 container statuses recorded)
Jan 29 04:20:07.588: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 29 04:20:07.588: INFO: kube-scheduler-master1 from kube-system started at 2023-01-11 07:48:21 +0000 UTC (1 container statuses recorded)
Jan 29 04:20:07.588: INFO: 	Container kube-scheduler ready: true, restart count 9
Jan 29 04:20:07.588: INFO: nginx-proxy-master1 from kube-system started at 2023-01-11 07:48:26 +0000 UTC (2 container statuses recorded)
Jan 29 04:20:07.588: INFO: 	Container kube-apiserver-healthcheck ready: true, restart count 0
Jan 29 04:20:07.588: INFO: 	Container nginx-proxy ready: true, restart count 0
Jan 29 04:20:07.588: INFO: node-problem-detector-dgrmp from kube-system started at 2023-01-11 07:51:49 +0000 UTC (1 container statuses recorded)
Jan 29 04:20:07.588: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan 29 04:20:07.588: INFO: 
Logging pods the apiserver thinks is on node master2 before test
Jan 29 04:20:07.603: INFO: calico-node-sx6hm from kube-system started at 2023-01-11 07:49:07 +0000 UTC (1 container statuses recorded)
Jan 29 04:20:07.603: INFO: 	Container calico-node ready: true, restart count 2
Jan 29 04:20:07.603: INFO: cke-admission-daemonset-6v8vg from kube-system started at 2023-01-11 07:51:31 +0000 UTC (1 container statuses recorded)
Jan 29 04:20:07.603: INFO: 	Container cke-admission ready: true, restart count 0
Jan 29 04:20:07.603: INFO: cke-controller-manager-master2 from kube-system started at 2023-01-11 07:49:04 +0000 UTC (1 container statuses recorded)
Jan 29 04:20:07.603: INFO: 	Container cke-controller-manager ready: true, restart count 1
Jan 29 04:20:07.603: INFO: component-controller-manager-master2 from kube-system started at 2023-01-11 07:49:02 +0000 UTC (1 container statuses recorded)
Jan 29 04:20:07.603: INFO: 	Container component-controller-manager ready: true, restart count 1
Jan 29 04:20:07.603: INFO: coredns-m9lv7 from kube-system started at 2023-01-11 07:50:16 +0000 UTC (1 container statuses recorded)
Jan 29 04:20:07.603: INFO: 	Container coredns ready: true, restart count 0
Jan 29 04:20:07.603: INFO: keepalived-master2 from kube-system started at 2023-01-11 07:47:57 +0000 UTC (1 container statuses recorded)
Jan 29 04:20:07.603: INFO: 	Container keepalived ready: true, restart count 0
Jan 29 04:20:07.603: INFO: kube-apiserver-master2 from kube-system started at 2023-01-11 07:47:49 +0000 UTC (1 container statuses recorded)
Jan 29 04:20:07.603: INFO: 	Container kube-apiserver ready: true, restart count 0
Jan 29 04:20:07.603: INFO: kube-controller-manager-master2 from kube-system started at 2023-01-11 07:47:51 +0000 UTC (1 container statuses recorded)
Jan 29 04:20:07.603: INFO: 	Container kube-controller-manager ready: true, restart count 4
Jan 29 04:20:07.603: INFO: kube-multus-ds-qq7kz from kube-system started at 2023-01-11 07:51:31 +0000 UTC (1 container statuses recorded)
Jan 29 04:20:07.603: INFO: 	Container kube-multus ready: true, restart count 0
Jan 29 04:20:07.604: INFO: kube-proxy-master2 from kube-system started at 2023-01-11 07:47:55 +0000 UTC (1 container statuses recorded)
Jan 29 04:20:07.604: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 29 04:20:07.604: INFO: kube-scheduler-master2 from kube-system started at 2023-01-11 07:48:09 +0000 UTC (1 container statuses recorded)
Jan 29 04:20:07.604: INFO: 	Container kube-scheduler ready: true, restart count 9
Jan 29 04:20:07.604: INFO: nginx-proxy-master2 from kube-system started at 2023-01-11 07:48:01 +0000 UTC (2 container statuses recorded)
Jan 29 04:20:07.604: INFO: 	Container kube-apiserver-healthcheck ready: true, restart count 0
Jan 29 04:20:07.604: INFO: 	Container nginx-proxy ready: true, restart count 0
Jan 29 04:20:07.604: INFO: node-problem-detector-w69xv from kube-system started at 2023-01-11 07:51:49 +0000 UTC (1 container statuses recorded)
Jan 29 04:20:07.604: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan 29 04:20:07.604: INFO: 
Logging pods the apiserver thinks is on node master3 before test
Jan 29 04:20:07.618: INFO: calico-node-9b26s from kube-system started at 2023-01-11 07:49:07 +0000 UTC (1 container statuses recorded)
Jan 29 04:20:07.618: INFO: 	Container calico-node ready: true, restart count 0
Jan 29 04:20:07.618: INFO: cke-admission-daemonset-mqt4k from kube-system started at 2023-01-11 07:51:31 +0000 UTC (1 container statuses recorded)
Jan 29 04:20:07.618: INFO: 	Container cke-admission ready: true, restart count 0
Jan 29 04:20:07.618: INFO: cke-controller-manager-master3 from kube-system started at 2023-01-11 07:49:08 +0000 UTC (1 container statuses recorded)
Jan 29 04:20:07.618: INFO: 	Container cke-controller-manager ready: true, restart count 0
Jan 29 04:20:07.618: INFO: component-controller-manager-master3 from kube-system started at 2023-01-11 07:49:17 +0000 UTC (1 container statuses recorded)
Jan 29 04:20:07.618: INFO: 	Container component-controller-manager ready: true, restart count 0
Jan 29 04:20:07.618: INFO: coredns-hwvn8 from kube-system started at 2023-01-11 07:49:26 +0000 UTC (1 container statuses recorded)
Jan 29 04:20:07.618: INFO: 	Container coredns ready: true, restart count 0
Jan 29 04:20:07.618: INFO: keepalived-master3 from kube-system started at 2023-01-11 07:48:26 +0000 UTC (1 container statuses recorded)
Jan 29 04:20:07.618: INFO: 	Container keepalived ready: true, restart count 0
Jan 29 04:20:07.618: INFO: kube-apiserver-master3 from kube-system started at 2023-01-11 07:48:44 +0000 UTC (1 container statuses recorded)
Jan 29 04:20:07.618: INFO: 	Container kube-apiserver ready: true, restart count 0
Jan 29 04:20:07.618: INFO: kube-controller-manager-master3 from kube-system started at 2023-01-11 07:48:27 +0000 UTC (1 container statuses recorded)
Jan 29 04:20:07.618: INFO: 	Container kube-controller-manager ready: true, restart count 4
Jan 29 04:20:07.618: INFO: kube-multus-ds-wrhjf from kube-system started at 2023-01-11 07:51:31 +0000 UTC (1 container statuses recorded)
Jan 29 04:20:07.618: INFO: 	Container kube-multus ready: true, restart count 0
Jan 29 04:20:07.618: INFO: kube-proxy-master3 from kube-system started at 2023-01-11 07:48:37 +0000 UTC (1 container statuses recorded)
Jan 29 04:20:07.618: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 29 04:20:07.618: INFO: kube-scheduler-master3 from kube-system started at 2023-01-11 07:48:30 +0000 UTC (1 container statuses recorded)
Jan 29 04:20:07.618: INFO: 	Container kube-scheduler ready: true, restart count 5
Jan 29 04:20:07.618: INFO: metrics-server-5584f68fbf-l689z from kube-system started at 2023-01-11 07:50:55 +0000 UTC (1 container statuses recorded)
Jan 29 04:20:07.618: INFO: 	Container metrics-server ready: true, restart count 0
Jan 29 04:20:07.618: INFO: nginx-proxy-master3 from kube-system started at 2023-01-11 07:48:32 +0000 UTC (2 container statuses recorded)
Jan 29 04:20:07.618: INFO: 	Container kube-apiserver-healthcheck ready: true, restart count 0
Jan 29 04:20:07.618: INFO: 	Container nginx-proxy ready: true, restart count 0
Jan 29 04:20:07.618: INFO: node-problem-detector-9987m from kube-system started at 2023-01-11 07:51:49 +0000 UTC (1 container statuses recorded)
Jan 29 04:20:07.618: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan 29 04:20:07.618: INFO: 
Logging pods the apiserver thinks is on node slave1 before test
Jan 29 04:20:07.633: INFO: calico-node-scr7m from kube-system started at 2023-01-11 07:49:07 +0000 UTC (1 container statuses recorded)
Jan 29 04:20:07.633: INFO: 	Container calico-node ready: true, restart count 0
Jan 29 04:20:07.633: INFO: kube-multus-ds-ng7xc from kube-system started at 2023-01-29 02:51:08 +0000 UTC (1 container statuses recorded)
Jan 29 04:20:07.633: INFO: 	Container kube-multus ready: true, restart count 0
Jan 29 04:20:07.633: INFO: kube-proxy-slave1 from kube-system started at 2023-01-11 07:48:42 +0000 UTC (1 container statuses recorded)
Jan 29 04:20:07.633: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 29 04:20:07.633: INFO: node-problem-detector-8bg82 from kube-system started at 2023-01-11 07:51:49 +0000 UTC (1 container statuses recorded)
Jan 29 04:20:07.633: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan 29 04:20:07.633: INFO: sonobuoy from sonobuoy started at 2023-01-29 02:56:33 +0000 UTC (1 container statuses recorded)
Jan 29 04:20:07.633: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 29 04:20:07.633: INFO: sonobuoy-e2e-job-3f32079945944ddf from sonobuoy started at 2023-01-29 02:56:35 +0000 UTC (2 container statuses recorded)
Jan 29 04:20:07.633: INFO: 	Container e2e ready: true, restart count 0
Jan 29 04:20:07.633: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 29 04:20:07.633: INFO: 
Logging pods the apiserver thinks is on node slave2 before test
Jan 29 04:20:07.648: INFO: calico-node-qhb5r from kube-system started at 2023-01-11 07:58:46 +0000 UTC (1 container statuses recorded)
Jan 29 04:20:07.648: INFO: 	Container calico-node ready: true, restart count 2
Jan 29 04:20:07.648: INFO: kube-multus-ds-fbdgp from kube-system started at 2023-01-29 03:58:55 +0000 UTC (1 container statuses recorded)
Jan 29 04:20:07.648: INFO: 	Container kube-multus ready: true, restart count 0
Jan 29 04:20:07.648: INFO: kube-proxy-slave2 from kube-system started at 2023-01-11 07:58:39 +0000 UTC (1 container statuses recorded)
Jan 29 04:20:07.648: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 29 04:20:07.648: INFO: node-problem-detector-m8cck from kube-system started at 2023-01-11 07:58:46 +0000 UTC (1 container statuses recorded)
Jan 29 04:20:07.648: INFO: 	Container node-problem-detector ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
STEP: verifying the node has the label node master1 01/29/23 04:20:07.696
STEP: verifying the node has the label node master2 01/29/23 04:20:07.717
STEP: verifying the node has the label node master3 01/29/23 04:20:07.737
STEP: verifying the node has the label node slave1 01/29/23 04:20:07.757
STEP: verifying the node has the label node slave2 01/29/23 04:20:07.793
Jan 29 04:20:07.855: INFO: Pod calico-kube-controllers-5c6d4b68d6-6p9cm requesting resource cpu=30m on Node master1
Jan 29 04:20:07.855: INFO: Pod calico-node-9b26s requesting resource cpu=300m on Node master3
Jan 29 04:20:07.855: INFO: Pod calico-node-j4qnr requesting resource cpu=300m on Node master1
Jan 29 04:20:07.855: INFO: Pod calico-node-qhb5r requesting resource cpu=300m on Node slave2
Jan 29 04:20:07.855: INFO: Pod calico-node-scr7m requesting resource cpu=300m on Node slave1
Jan 29 04:20:07.855: INFO: Pod calico-node-sx6hm requesting resource cpu=300m on Node master2
Jan 29 04:20:07.855: INFO: Pod cke-admission-daemonset-6v8vg requesting resource cpu=0m on Node master2
Jan 29 04:20:07.855: INFO: Pod cke-admission-daemonset-g5mvj requesting resource cpu=0m on Node master1
Jan 29 04:20:07.855: INFO: Pod cke-admission-daemonset-mqt4k requesting resource cpu=0m on Node master3
Jan 29 04:20:07.855: INFO: Pod cke-controller-manager-master1 requesting resource cpu=100m on Node master1
Jan 29 04:20:07.855: INFO: Pod cke-controller-manager-master2 requesting resource cpu=100m on Node master2
Jan 29 04:20:07.855: INFO: Pod cke-controller-manager-master3 requesting resource cpu=100m on Node master3
Jan 29 04:20:07.855: INFO: Pod component-controller-manager-master1 requesting resource cpu=100m on Node master1
Jan 29 04:20:07.855: INFO: Pod component-controller-manager-master2 requesting resource cpu=100m on Node master2
Jan 29 04:20:07.855: INFO: Pod component-controller-manager-master3 requesting resource cpu=100m on Node master3
Jan 29 04:20:07.855: INFO: Pod coredns-hwvn8 requesting resource cpu=300m on Node master3
Jan 29 04:20:07.855: INFO: Pod coredns-m9lv7 requesting resource cpu=300m on Node master2
Jan 29 04:20:07.855: INFO: Pod coredns-tntlt requesting resource cpu=300m on Node master1
Jan 29 04:20:07.855: INFO: Pod keepalived-master1 requesting resource cpu=500m on Node master1
Jan 29 04:20:07.855: INFO: Pod keepalived-master2 requesting resource cpu=500m on Node master2
Jan 29 04:20:07.855: INFO: Pod keepalived-master3 requesting resource cpu=500m on Node master3
Jan 29 04:20:07.855: INFO: Pod kube-apiserver-master1 requesting resource cpu=500m on Node master1
Jan 29 04:20:07.855: INFO: Pod kube-apiserver-master2 requesting resource cpu=500m on Node master2
Jan 29 04:20:07.855: INFO: Pod kube-apiserver-master3 requesting resource cpu=500m on Node master3
Jan 29 04:20:07.855: INFO: Pod kube-controller-manager-master1 requesting resource cpu=100m on Node master1
Jan 29 04:20:07.855: INFO: Pod kube-controller-manager-master2 requesting resource cpu=100m on Node master2
Jan 29 04:20:07.855: INFO: Pod kube-controller-manager-master3 requesting resource cpu=100m on Node master3
Jan 29 04:20:07.855: INFO: Pod kube-multus-ds-8b89r requesting resource cpu=100m on Node master1
Jan 29 04:20:07.855: INFO: Pod kube-multus-ds-fbdgp requesting resource cpu=100m on Node slave2
Jan 29 04:20:07.855: INFO: Pod kube-multus-ds-ng7xc requesting resource cpu=100m on Node slave1
Jan 29 04:20:07.855: INFO: Pod kube-multus-ds-qq7kz requesting resource cpu=100m on Node master2
Jan 29 04:20:07.855: INFO: Pod kube-multus-ds-wrhjf requesting resource cpu=100m on Node master3
Jan 29 04:20:07.855: INFO: Pod kube-proxy-master1 requesting resource cpu=500m on Node master1
Jan 29 04:20:07.855: INFO: Pod kube-proxy-master2 requesting resource cpu=500m on Node master2
Jan 29 04:20:07.855: INFO: Pod kube-proxy-master3 requesting resource cpu=500m on Node master3
Jan 29 04:20:07.855: INFO: Pod kube-proxy-slave1 requesting resource cpu=500m on Node slave1
Jan 29 04:20:07.855: INFO: Pod kube-proxy-slave2 requesting resource cpu=500m on Node slave2
Jan 29 04:20:07.855: INFO: Pod kube-scheduler-master1 requesting resource cpu=80m on Node master1
Jan 29 04:20:07.855: INFO: Pod kube-scheduler-master2 requesting resource cpu=80m on Node master2
Jan 29 04:20:07.855: INFO: Pod kube-scheduler-master3 requesting resource cpu=80m on Node master3
Jan 29 04:20:07.855: INFO: Pod metrics-server-5584f68fbf-l689z requesting resource cpu=0m on Node master3
Jan 29 04:20:07.855: INFO: Pod nginx-proxy-master1 requesting resource cpu=150m on Node master1
Jan 29 04:20:07.855: INFO: Pod nginx-proxy-master2 requesting resource cpu=150m on Node master2
Jan 29 04:20:07.855: INFO: Pod nginx-proxy-master3 requesting resource cpu=150m on Node master3
Jan 29 04:20:07.855: INFO: Pod node-problem-detector-8bg82 requesting resource cpu=50m on Node slave1
Jan 29 04:20:07.855: INFO: Pod node-problem-detector-9987m requesting resource cpu=50m on Node master3
Jan 29 04:20:07.856: INFO: Pod node-problem-detector-dgrmp requesting resource cpu=50m on Node master1
Jan 29 04:20:07.856: INFO: Pod node-problem-detector-m8cck requesting resource cpu=50m on Node slave2
Jan 29 04:20:07.856: INFO: Pod node-problem-detector-w69xv requesting resource cpu=50m on Node master2
Jan 29 04:20:07.856: INFO: Pod sonobuoy requesting resource cpu=0m on Node slave1
Jan 29 04:20:07.856: INFO: Pod sonobuoy-e2e-job-3f32079945944ddf requesting resource cpu=0m on Node slave1
STEP: Starting Pods to consume most of the cluster CPU. 01/29/23 04:20:07.856
Jan 29 04:20:07.856: INFO: Creating a pod which consumes cpu=1715m on Node slave2
Jan 29 04:20:07.877: INFO: Creating a pod which consumes cpu=133m on Node master1
Jan 29 04:20:07.894: INFO: Creating a pod which consumes cpu=154m on Node master2
Jan 29 04:20:07.910: INFO: Creating a pod which consumes cpu=154m on Node master3
Jan 29 04:20:07.928: INFO: Creating a pod which consumes cpu=1715m on Node slave1
Jan 29 04:20:07.949: INFO: Waiting up to 5m0s for pod "filler-pod-b1db2123-7f85-4f8b-9580-1746d91a5b67" in namespace "sched-pred-7101" to be "running"
Jan 29 04:20:07.964: INFO: Pod "filler-pod-b1db2123-7f85-4f8b-9580-1746d91a5b67": Phase="Pending", Reason="", readiness=false. Elapsed: 14.847504ms
Jan 29 04:20:09.972: INFO: Pod "filler-pod-b1db2123-7f85-4f8b-9580-1746d91a5b67": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022826421s
Jan 29 04:20:11.983: INFO: Pod "filler-pod-b1db2123-7f85-4f8b-9580-1746d91a5b67": Phase="Running", Reason="", readiness=true. Elapsed: 4.0339744s
Jan 29 04:20:11.983: INFO: Pod "filler-pod-b1db2123-7f85-4f8b-9580-1746d91a5b67" satisfied condition "running"
Jan 29 04:20:11.984: INFO: Waiting up to 5m0s for pod "filler-pod-b405214b-26cc-4a73-bab3-d9a2deb7b9a1" in namespace "sched-pred-7101" to be "running"
Jan 29 04:20:11.991: INFO: Pod "filler-pod-b405214b-26cc-4a73-bab3-d9a2deb7b9a1": Phase="Running", Reason="", readiness=true. Elapsed: 7.855075ms
Jan 29 04:20:11.991: INFO: Pod "filler-pod-b405214b-26cc-4a73-bab3-d9a2deb7b9a1" satisfied condition "running"
Jan 29 04:20:11.992: INFO: Waiting up to 5m0s for pod "filler-pod-e17309cd-a18e-4b68-9b4c-346ec1fbcf3d" in namespace "sched-pred-7101" to be "running"
Jan 29 04:20:12.002: INFO: Pod "filler-pod-e17309cd-a18e-4b68-9b4c-346ec1fbcf3d": Phase="Running", Reason="", readiness=true. Elapsed: 10.392032ms
Jan 29 04:20:12.002: INFO: Pod "filler-pod-e17309cd-a18e-4b68-9b4c-346ec1fbcf3d" satisfied condition "running"
Jan 29 04:20:12.002: INFO: Waiting up to 5m0s for pod "filler-pod-fa1f2a30-96be-4f95-8d5d-afb6c87e0f32" in namespace "sched-pred-7101" to be "running"
Jan 29 04:20:12.009: INFO: Pod "filler-pod-fa1f2a30-96be-4f95-8d5d-afb6c87e0f32": Phase="Running", Reason="", readiness=true. Elapsed: 6.978609ms
Jan 29 04:20:12.009: INFO: Pod "filler-pod-fa1f2a30-96be-4f95-8d5d-afb6c87e0f32" satisfied condition "running"
Jan 29 04:20:12.009: INFO: Waiting up to 5m0s for pod "filler-pod-8279b861-3d31-4f7d-8e16-62c0485e8330" in namespace "sched-pred-7101" to be "running"
Jan 29 04:20:12.016: INFO: Pod "filler-pod-8279b861-3d31-4f7d-8e16-62c0485e8330": Phase="Running", Reason="", readiness=true. Elapsed: 7.020729ms
Jan 29 04:20:12.016: INFO: Pod "filler-pod-8279b861-3d31-4f7d-8e16-62c0485e8330" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 01/29/23 04:20:12.016
I0129 04:20:24.524063      22 trace.go:205] Trace[504397417]: "Reflector ListAndWatch" name:test/e2e/scheduling/events.go:98 (29-Jan-2023 04:20:12.017) (total time: 12506ms):
Trace[504397417]: ---"Objects listed" error:<nil> 12506ms (04:20:24.523)
Trace[504397417]: [12.506876075s] [12.506876075s] END
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8279b861-3d31-4f7d-8e16-62c0485e8330.173eacb5bb858ee1], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7101/filler-pod-8279b861-3d31-4f7d-8e16-62c0485e8330 to slave1] 01/29/23 04:20:24.524
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b1db2123-7f85-4f8b-9580-1746d91a5b67.173eacb5b6d12cf4], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7101/filler-pod-b1db2123-7f85-4f8b-9580-1746d91a5b67 to slave2] 01/29/23 04:20:24.524
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b405214b-26cc-4a73-bab3-d9a2deb7b9a1.173eacb5b7b6786d], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7101/filler-pod-b405214b-26cc-4a73-bab3-d9a2deb7b9a1 to master1] 01/29/23 04:20:24.525
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b405214b-26cc-4a73-bab3-d9a2deb7b9a1.173eacb60454f01a], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 01/29/23 04:20:24.525
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b405214b-26cc-4a73-bab3-d9a2deb7b9a1.173eacb60888dfd7], Reason = [Created], Message = [Created container filler-pod-b405214b-26cc-4a73-bab3-d9a2deb7b9a1] 01/29/23 04:20:24.525
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b405214b-26cc-4a73-bab3-d9a2deb7b9a1.173eacb6198c7071], Reason = [Started], Message = [Started container filler-pod-b405214b-26cc-4a73-bab3-d9a2deb7b9a1] 01/29/23 04:20:24.525
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e17309cd-a18e-4b68-9b4c-346ec1fbcf3d.173eacb5b8e04d42], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7101/filler-pod-e17309cd-a18e-4b68-9b4c-346ec1fbcf3d to master2] 01/29/23 04:20:24.525
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e17309cd-a18e-4b68-9b4c-346ec1fbcf3d.173eacb600ac7b19], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 01/29/23 04:20:24.525
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e17309cd-a18e-4b68-9b4c-346ec1fbcf3d.173eacb6046d1cf9], Reason = [Created], Message = [Created container filler-pod-e17309cd-a18e-4b68-9b4c-346ec1fbcf3d] 01/29/23 04:20:24.526
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e17309cd-a18e-4b68-9b4c-346ec1fbcf3d.173eacb60e7fe0a6], Reason = [Started], Message = [Started container filler-pod-e17309cd-a18e-4b68-9b4c-346ec1fbcf3d] 01/29/23 04:20:24.526
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-fa1f2a30-96be-4f95-8d5d-afb6c87e0f32.173eacb5b9fea9d1], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7101/filler-pod-fa1f2a30-96be-4f95-8d5d-afb6c87e0f32 to master3] 01/29/23 04:20:24.526
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-fa1f2a30-96be-4f95-8d5d-afb6c87e0f32.173eacb5fe97b119], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 01/29/23 04:20:24.526
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-fa1f2a30-96be-4f95-8d5d-afb6c87e0f32.173eacb60b7b7d84], Reason = [Created], Message = [Created container filler-pod-fa1f2a30-96be-4f95-8d5d-afb6c87e0f32] 01/29/23 04:20:24.526
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-fa1f2a30-96be-4f95-8d5d-afb6c87e0f32.173eacb61907cd4c], Reason = [Started], Message = [Started container filler-pod-fa1f2a30-96be-4f95-8d5d-afb6c87e0f32] 01/29/23 04:20:24.526
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8279b861-3d31-4f7d-8e16-62c0485e8330.173eacb5ff539703], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 01/29/23 04:20:24.608
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8279b861-3d31-4f7d-8e16-62c0485e8330.173eacb602df67db], Reason = [Created], Message = [Created container filler-pod-8279b861-3d31-4f7d-8e16-62c0485e8330] 01/29/23 04:20:24.608
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b1db2123-7f85-4f8b-9580-1746d91a5b67.173eacb5fbc6b723], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 01/29/23 04:20:24.608
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8279b861-3d31-4f7d-8e16-62c0485e8330.173eacb60b64faac], Reason = [Started], Message = [Started container filler-pod-8279b861-3d31-4f7d-8e16-62c0485e8330] 01/29/23 04:20:24.609
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b1db2123-7f85-4f8b-9580-1746d91a5b67.173eacb600e7f9d5], Reason = [Created], Message = [Created container filler-pod-b1db2123-7f85-4f8b-9580-1746d91a5b67] 01/29/23 04:20:24.609
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.173eacb997ec9c02], Reason = [FailedScheduling], Message = [0/5 nodes are available: 5 Insufficient cpu. preemption: 0/5 nodes are available: 5 No preemption victims found for incoming pod.] 01/29/23 04:20:24.609
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b1db2123-7f85-4f8b-9580-1746d91a5b67.173eacb609421eae], Reason = [Started], Message = [Started container filler-pod-b1db2123-7f85-4f8b-9580-1746d91a5b67] 01/29/23 04:20:24.609
STEP: removing the label node off the node master1 01/29/23 04:20:25.559
STEP: verifying the node doesn't have the label node 01/29/23 04:20:25.582
STEP: removing the label node off the node master2 01/29/23 04:20:25.594
STEP: verifying the node doesn't have the label node 01/29/23 04:20:25.614
STEP: removing the label node off the node master3 01/29/23 04:20:25.621
STEP: verifying the node doesn't have the label node 01/29/23 04:20:25.642
STEP: removing the label node off the node slave1 01/29/23 04:20:25.65
STEP: verifying the node doesn't have the label node 01/29/23 04:20:25.667
STEP: removing the label node off the node slave2 01/29/23 04:20:25.675
STEP: verifying the node doesn't have the label node 01/29/23 04:20:25.695
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Jan 29 04:20:25.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7101" for this suite. 01/29/23 04:20:25.712
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","completed":295,"skipped":5206,"failed":0}
------------------------------
• [SLOW TEST] [18.221 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:20:07.503
    Jan 29 04:20:07.504: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename sched-pred 01/29/23 04:20:07.505
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:20:07.534
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:20:07.541
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Jan 29 04:20:07.547: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan 29 04:20:07.565: INFO: Waiting for terminating namespaces to be deleted...
    Jan 29 04:20:07.571: INFO: 
    Logging pods the apiserver thinks is on node master1 before test
    Jan 29 04:20:07.588: INFO: calico-kube-controllers-5c6d4b68d6-6p9cm from kube-system started at 2023-01-11 07:48:50 +0000 UTC (1 container statuses recorded)
    Jan 29 04:20:07.588: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Jan 29 04:20:07.588: INFO: calico-node-j4qnr from kube-system started at 2023-01-11 07:49:07 +0000 UTC (1 container statuses recorded)
    Jan 29 04:20:07.588: INFO: 	Container calico-node ready: true, restart count 2
    Jan 29 04:20:07.588: INFO: cke-admission-daemonset-g5mvj from kube-system started at 2023-01-11 07:51:31 +0000 UTC (1 container statuses recorded)
    Jan 29 04:20:07.588: INFO: 	Container cke-admission ready: true, restart count 0
    Jan 29 04:20:07.588: INFO: cke-controller-manager-master1 from kube-system started at 2023-01-11 07:49:37 +0000 UTC (1 container statuses recorded)
    Jan 29 04:20:07.588: INFO: 	Container cke-controller-manager ready: true, restart count 0
    Jan 29 04:20:07.588: INFO: component-controller-manager-master1 from kube-system started at 2023-01-11 07:49:18 +0000 UTC (1 container statuses recorded)
    Jan 29 04:20:07.588: INFO: 	Container component-controller-manager ready: true, restart count 0
    Jan 29 04:20:07.588: INFO: coredns-tntlt from kube-system started at 2023-01-11 07:49:05 +0000 UTC (1 container statuses recorded)
    Jan 29 04:20:07.588: INFO: 	Container coredns ready: true, restart count 0
    Jan 29 04:20:07.588: INFO: keepalived-master1 from kube-system started at 2023-01-11 07:48:19 +0000 UTC (1 container statuses recorded)
    Jan 29 04:20:07.588: INFO: 	Container keepalived ready: true, restart count 0
    Jan 29 04:20:07.588: INFO: kube-apiserver-master1 from kube-system started at 2023-01-11 07:48:12 +0000 UTC (1 container statuses recorded)
    Jan 29 04:20:07.588: INFO: 	Container kube-apiserver ready: true, restart count 0
    Jan 29 04:20:07.588: INFO: kube-controller-manager-master1 from kube-system started at 2023-01-11 07:48:20 +0000 UTC (1 container statuses recorded)
    Jan 29 04:20:07.588: INFO: 	Container kube-controller-manager ready: true, restart count 5
    Jan 29 04:20:07.588: INFO: kube-multus-ds-8b89r from kube-system started at 2023-01-11 07:51:31 +0000 UTC (1 container statuses recorded)
    Jan 29 04:20:07.588: INFO: 	Container kube-multus ready: true, restart count 0
    Jan 29 04:20:07.588: INFO: kube-proxy-master1 from kube-system started at 2023-01-11 07:48:15 +0000 UTC (1 container statuses recorded)
    Jan 29 04:20:07.588: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan 29 04:20:07.588: INFO: kube-scheduler-master1 from kube-system started at 2023-01-11 07:48:21 +0000 UTC (1 container statuses recorded)
    Jan 29 04:20:07.588: INFO: 	Container kube-scheduler ready: true, restart count 9
    Jan 29 04:20:07.588: INFO: nginx-proxy-master1 from kube-system started at 2023-01-11 07:48:26 +0000 UTC (2 container statuses recorded)
    Jan 29 04:20:07.588: INFO: 	Container kube-apiserver-healthcheck ready: true, restart count 0
    Jan 29 04:20:07.588: INFO: 	Container nginx-proxy ready: true, restart count 0
    Jan 29 04:20:07.588: INFO: node-problem-detector-dgrmp from kube-system started at 2023-01-11 07:51:49 +0000 UTC (1 container statuses recorded)
    Jan 29 04:20:07.588: INFO: 	Container node-problem-detector ready: true, restart count 0
    Jan 29 04:20:07.588: INFO: 
    Logging pods the apiserver thinks is on node master2 before test
    Jan 29 04:20:07.603: INFO: calico-node-sx6hm from kube-system started at 2023-01-11 07:49:07 +0000 UTC (1 container statuses recorded)
    Jan 29 04:20:07.603: INFO: 	Container calico-node ready: true, restart count 2
    Jan 29 04:20:07.603: INFO: cke-admission-daemonset-6v8vg from kube-system started at 2023-01-11 07:51:31 +0000 UTC (1 container statuses recorded)
    Jan 29 04:20:07.603: INFO: 	Container cke-admission ready: true, restart count 0
    Jan 29 04:20:07.603: INFO: cke-controller-manager-master2 from kube-system started at 2023-01-11 07:49:04 +0000 UTC (1 container statuses recorded)
    Jan 29 04:20:07.603: INFO: 	Container cke-controller-manager ready: true, restart count 1
    Jan 29 04:20:07.603: INFO: component-controller-manager-master2 from kube-system started at 2023-01-11 07:49:02 +0000 UTC (1 container statuses recorded)
    Jan 29 04:20:07.603: INFO: 	Container component-controller-manager ready: true, restart count 1
    Jan 29 04:20:07.603: INFO: coredns-m9lv7 from kube-system started at 2023-01-11 07:50:16 +0000 UTC (1 container statuses recorded)
    Jan 29 04:20:07.603: INFO: 	Container coredns ready: true, restart count 0
    Jan 29 04:20:07.603: INFO: keepalived-master2 from kube-system started at 2023-01-11 07:47:57 +0000 UTC (1 container statuses recorded)
    Jan 29 04:20:07.603: INFO: 	Container keepalived ready: true, restart count 0
    Jan 29 04:20:07.603: INFO: kube-apiserver-master2 from kube-system started at 2023-01-11 07:47:49 +0000 UTC (1 container statuses recorded)
    Jan 29 04:20:07.603: INFO: 	Container kube-apiserver ready: true, restart count 0
    Jan 29 04:20:07.603: INFO: kube-controller-manager-master2 from kube-system started at 2023-01-11 07:47:51 +0000 UTC (1 container statuses recorded)
    Jan 29 04:20:07.603: INFO: 	Container kube-controller-manager ready: true, restart count 4
    Jan 29 04:20:07.603: INFO: kube-multus-ds-qq7kz from kube-system started at 2023-01-11 07:51:31 +0000 UTC (1 container statuses recorded)
    Jan 29 04:20:07.603: INFO: 	Container kube-multus ready: true, restart count 0
    Jan 29 04:20:07.604: INFO: kube-proxy-master2 from kube-system started at 2023-01-11 07:47:55 +0000 UTC (1 container statuses recorded)
    Jan 29 04:20:07.604: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan 29 04:20:07.604: INFO: kube-scheduler-master2 from kube-system started at 2023-01-11 07:48:09 +0000 UTC (1 container statuses recorded)
    Jan 29 04:20:07.604: INFO: 	Container kube-scheduler ready: true, restart count 9
    Jan 29 04:20:07.604: INFO: nginx-proxy-master2 from kube-system started at 2023-01-11 07:48:01 +0000 UTC (2 container statuses recorded)
    Jan 29 04:20:07.604: INFO: 	Container kube-apiserver-healthcheck ready: true, restart count 0
    Jan 29 04:20:07.604: INFO: 	Container nginx-proxy ready: true, restart count 0
    Jan 29 04:20:07.604: INFO: node-problem-detector-w69xv from kube-system started at 2023-01-11 07:51:49 +0000 UTC (1 container statuses recorded)
    Jan 29 04:20:07.604: INFO: 	Container node-problem-detector ready: true, restart count 0
    Jan 29 04:20:07.604: INFO: 
    Logging pods the apiserver thinks is on node master3 before test
    Jan 29 04:20:07.618: INFO: calico-node-9b26s from kube-system started at 2023-01-11 07:49:07 +0000 UTC (1 container statuses recorded)
    Jan 29 04:20:07.618: INFO: 	Container calico-node ready: true, restart count 0
    Jan 29 04:20:07.618: INFO: cke-admission-daemonset-mqt4k from kube-system started at 2023-01-11 07:51:31 +0000 UTC (1 container statuses recorded)
    Jan 29 04:20:07.618: INFO: 	Container cke-admission ready: true, restart count 0
    Jan 29 04:20:07.618: INFO: cke-controller-manager-master3 from kube-system started at 2023-01-11 07:49:08 +0000 UTC (1 container statuses recorded)
    Jan 29 04:20:07.618: INFO: 	Container cke-controller-manager ready: true, restart count 0
    Jan 29 04:20:07.618: INFO: component-controller-manager-master3 from kube-system started at 2023-01-11 07:49:17 +0000 UTC (1 container statuses recorded)
    Jan 29 04:20:07.618: INFO: 	Container component-controller-manager ready: true, restart count 0
    Jan 29 04:20:07.618: INFO: coredns-hwvn8 from kube-system started at 2023-01-11 07:49:26 +0000 UTC (1 container statuses recorded)
    Jan 29 04:20:07.618: INFO: 	Container coredns ready: true, restart count 0
    Jan 29 04:20:07.618: INFO: keepalived-master3 from kube-system started at 2023-01-11 07:48:26 +0000 UTC (1 container statuses recorded)
    Jan 29 04:20:07.618: INFO: 	Container keepalived ready: true, restart count 0
    Jan 29 04:20:07.618: INFO: kube-apiserver-master3 from kube-system started at 2023-01-11 07:48:44 +0000 UTC (1 container statuses recorded)
    Jan 29 04:20:07.618: INFO: 	Container kube-apiserver ready: true, restart count 0
    Jan 29 04:20:07.618: INFO: kube-controller-manager-master3 from kube-system started at 2023-01-11 07:48:27 +0000 UTC (1 container statuses recorded)
    Jan 29 04:20:07.618: INFO: 	Container kube-controller-manager ready: true, restart count 4
    Jan 29 04:20:07.618: INFO: kube-multus-ds-wrhjf from kube-system started at 2023-01-11 07:51:31 +0000 UTC (1 container statuses recorded)
    Jan 29 04:20:07.618: INFO: 	Container kube-multus ready: true, restart count 0
    Jan 29 04:20:07.618: INFO: kube-proxy-master3 from kube-system started at 2023-01-11 07:48:37 +0000 UTC (1 container statuses recorded)
    Jan 29 04:20:07.618: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan 29 04:20:07.618: INFO: kube-scheduler-master3 from kube-system started at 2023-01-11 07:48:30 +0000 UTC (1 container statuses recorded)
    Jan 29 04:20:07.618: INFO: 	Container kube-scheduler ready: true, restart count 5
    Jan 29 04:20:07.618: INFO: metrics-server-5584f68fbf-l689z from kube-system started at 2023-01-11 07:50:55 +0000 UTC (1 container statuses recorded)
    Jan 29 04:20:07.618: INFO: 	Container metrics-server ready: true, restart count 0
    Jan 29 04:20:07.618: INFO: nginx-proxy-master3 from kube-system started at 2023-01-11 07:48:32 +0000 UTC (2 container statuses recorded)
    Jan 29 04:20:07.618: INFO: 	Container kube-apiserver-healthcheck ready: true, restart count 0
    Jan 29 04:20:07.618: INFO: 	Container nginx-proxy ready: true, restart count 0
    Jan 29 04:20:07.618: INFO: node-problem-detector-9987m from kube-system started at 2023-01-11 07:51:49 +0000 UTC (1 container statuses recorded)
    Jan 29 04:20:07.618: INFO: 	Container node-problem-detector ready: true, restart count 0
    Jan 29 04:20:07.618: INFO: 
    Logging pods the apiserver thinks is on node slave1 before test
    Jan 29 04:20:07.633: INFO: calico-node-scr7m from kube-system started at 2023-01-11 07:49:07 +0000 UTC (1 container statuses recorded)
    Jan 29 04:20:07.633: INFO: 	Container calico-node ready: true, restart count 0
    Jan 29 04:20:07.633: INFO: kube-multus-ds-ng7xc from kube-system started at 2023-01-29 02:51:08 +0000 UTC (1 container statuses recorded)
    Jan 29 04:20:07.633: INFO: 	Container kube-multus ready: true, restart count 0
    Jan 29 04:20:07.633: INFO: kube-proxy-slave1 from kube-system started at 2023-01-11 07:48:42 +0000 UTC (1 container statuses recorded)
    Jan 29 04:20:07.633: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan 29 04:20:07.633: INFO: node-problem-detector-8bg82 from kube-system started at 2023-01-11 07:51:49 +0000 UTC (1 container statuses recorded)
    Jan 29 04:20:07.633: INFO: 	Container node-problem-detector ready: true, restart count 0
    Jan 29 04:20:07.633: INFO: sonobuoy from sonobuoy started at 2023-01-29 02:56:33 +0000 UTC (1 container statuses recorded)
    Jan 29 04:20:07.633: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan 29 04:20:07.633: INFO: sonobuoy-e2e-job-3f32079945944ddf from sonobuoy started at 2023-01-29 02:56:35 +0000 UTC (2 container statuses recorded)
    Jan 29 04:20:07.633: INFO: 	Container e2e ready: true, restart count 0
    Jan 29 04:20:07.633: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 29 04:20:07.633: INFO: 
    Logging pods the apiserver thinks is on node slave2 before test
    Jan 29 04:20:07.648: INFO: calico-node-qhb5r from kube-system started at 2023-01-11 07:58:46 +0000 UTC (1 container statuses recorded)
    Jan 29 04:20:07.648: INFO: 	Container calico-node ready: true, restart count 2
    Jan 29 04:20:07.648: INFO: kube-multus-ds-fbdgp from kube-system started at 2023-01-29 03:58:55 +0000 UTC (1 container statuses recorded)
    Jan 29 04:20:07.648: INFO: 	Container kube-multus ready: true, restart count 0
    Jan 29 04:20:07.648: INFO: kube-proxy-slave2 from kube-system started at 2023-01-11 07:58:39 +0000 UTC (1 container statuses recorded)
    Jan 29 04:20:07.648: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan 29 04:20:07.648: INFO: node-problem-detector-m8cck from kube-system started at 2023-01-11 07:58:46 +0000 UTC (1 container statuses recorded)
    Jan 29 04:20:07.648: INFO: 	Container node-problem-detector ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:326
    STEP: verifying the node has the label node master1 01/29/23 04:20:07.696
    STEP: verifying the node has the label node master2 01/29/23 04:20:07.717
    STEP: verifying the node has the label node master3 01/29/23 04:20:07.737
    STEP: verifying the node has the label node slave1 01/29/23 04:20:07.757
    STEP: verifying the node has the label node slave2 01/29/23 04:20:07.793
    Jan 29 04:20:07.855: INFO: Pod calico-kube-controllers-5c6d4b68d6-6p9cm requesting resource cpu=30m on Node master1
    Jan 29 04:20:07.855: INFO: Pod calico-node-9b26s requesting resource cpu=300m on Node master3
    Jan 29 04:20:07.855: INFO: Pod calico-node-j4qnr requesting resource cpu=300m on Node master1
    Jan 29 04:20:07.855: INFO: Pod calico-node-qhb5r requesting resource cpu=300m on Node slave2
    Jan 29 04:20:07.855: INFO: Pod calico-node-scr7m requesting resource cpu=300m on Node slave1
    Jan 29 04:20:07.855: INFO: Pod calico-node-sx6hm requesting resource cpu=300m on Node master2
    Jan 29 04:20:07.855: INFO: Pod cke-admission-daemonset-6v8vg requesting resource cpu=0m on Node master2
    Jan 29 04:20:07.855: INFO: Pod cke-admission-daemonset-g5mvj requesting resource cpu=0m on Node master1
    Jan 29 04:20:07.855: INFO: Pod cke-admission-daemonset-mqt4k requesting resource cpu=0m on Node master3
    Jan 29 04:20:07.855: INFO: Pod cke-controller-manager-master1 requesting resource cpu=100m on Node master1
    Jan 29 04:20:07.855: INFO: Pod cke-controller-manager-master2 requesting resource cpu=100m on Node master2
    Jan 29 04:20:07.855: INFO: Pod cke-controller-manager-master3 requesting resource cpu=100m on Node master3
    Jan 29 04:20:07.855: INFO: Pod component-controller-manager-master1 requesting resource cpu=100m on Node master1
    Jan 29 04:20:07.855: INFO: Pod component-controller-manager-master2 requesting resource cpu=100m on Node master2
    Jan 29 04:20:07.855: INFO: Pod component-controller-manager-master3 requesting resource cpu=100m on Node master3
    Jan 29 04:20:07.855: INFO: Pod coredns-hwvn8 requesting resource cpu=300m on Node master3
    Jan 29 04:20:07.855: INFO: Pod coredns-m9lv7 requesting resource cpu=300m on Node master2
    Jan 29 04:20:07.855: INFO: Pod coredns-tntlt requesting resource cpu=300m on Node master1
    Jan 29 04:20:07.855: INFO: Pod keepalived-master1 requesting resource cpu=500m on Node master1
    Jan 29 04:20:07.855: INFO: Pod keepalived-master2 requesting resource cpu=500m on Node master2
    Jan 29 04:20:07.855: INFO: Pod keepalived-master3 requesting resource cpu=500m on Node master3
    Jan 29 04:20:07.855: INFO: Pod kube-apiserver-master1 requesting resource cpu=500m on Node master1
    Jan 29 04:20:07.855: INFO: Pod kube-apiserver-master2 requesting resource cpu=500m on Node master2
    Jan 29 04:20:07.855: INFO: Pod kube-apiserver-master3 requesting resource cpu=500m on Node master3
    Jan 29 04:20:07.855: INFO: Pod kube-controller-manager-master1 requesting resource cpu=100m on Node master1
    Jan 29 04:20:07.855: INFO: Pod kube-controller-manager-master2 requesting resource cpu=100m on Node master2
    Jan 29 04:20:07.855: INFO: Pod kube-controller-manager-master3 requesting resource cpu=100m on Node master3
    Jan 29 04:20:07.855: INFO: Pod kube-multus-ds-8b89r requesting resource cpu=100m on Node master1
    Jan 29 04:20:07.855: INFO: Pod kube-multus-ds-fbdgp requesting resource cpu=100m on Node slave2
    Jan 29 04:20:07.855: INFO: Pod kube-multus-ds-ng7xc requesting resource cpu=100m on Node slave1
    Jan 29 04:20:07.855: INFO: Pod kube-multus-ds-qq7kz requesting resource cpu=100m on Node master2
    Jan 29 04:20:07.855: INFO: Pod kube-multus-ds-wrhjf requesting resource cpu=100m on Node master3
    Jan 29 04:20:07.855: INFO: Pod kube-proxy-master1 requesting resource cpu=500m on Node master1
    Jan 29 04:20:07.855: INFO: Pod kube-proxy-master2 requesting resource cpu=500m on Node master2
    Jan 29 04:20:07.855: INFO: Pod kube-proxy-master3 requesting resource cpu=500m on Node master3
    Jan 29 04:20:07.855: INFO: Pod kube-proxy-slave1 requesting resource cpu=500m on Node slave1
    Jan 29 04:20:07.855: INFO: Pod kube-proxy-slave2 requesting resource cpu=500m on Node slave2
    Jan 29 04:20:07.855: INFO: Pod kube-scheduler-master1 requesting resource cpu=80m on Node master1
    Jan 29 04:20:07.855: INFO: Pod kube-scheduler-master2 requesting resource cpu=80m on Node master2
    Jan 29 04:20:07.855: INFO: Pod kube-scheduler-master3 requesting resource cpu=80m on Node master3
    Jan 29 04:20:07.855: INFO: Pod metrics-server-5584f68fbf-l689z requesting resource cpu=0m on Node master3
    Jan 29 04:20:07.855: INFO: Pod nginx-proxy-master1 requesting resource cpu=150m on Node master1
    Jan 29 04:20:07.855: INFO: Pod nginx-proxy-master2 requesting resource cpu=150m on Node master2
    Jan 29 04:20:07.855: INFO: Pod nginx-proxy-master3 requesting resource cpu=150m on Node master3
    Jan 29 04:20:07.855: INFO: Pod node-problem-detector-8bg82 requesting resource cpu=50m on Node slave1
    Jan 29 04:20:07.855: INFO: Pod node-problem-detector-9987m requesting resource cpu=50m on Node master3
    Jan 29 04:20:07.856: INFO: Pod node-problem-detector-dgrmp requesting resource cpu=50m on Node master1
    Jan 29 04:20:07.856: INFO: Pod node-problem-detector-m8cck requesting resource cpu=50m on Node slave2
    Jan 29 04:20:07.856: INFO: Pod node-problem-detector-w69xv requesting resource cpu=50m on Node master2
    Jan 29 04:20:07.856: INFO: Pod sonobuoy requesting resource cpu=0m on Node slave1
    Jan 29 04:20:07.856: INFO: Pod sonobuoy-e2e-job-3f32079945944ddf requesting resource cpu=0m on Node slave1
    STEP: Starting Pods to consume most of the cluster CPU. 01/29/23 04:20:07.856
    Jan 29 04:20:07.856: INFO: Creating a pod which consumes cpu=1715m on Node slave2
    Jan 29 04:20:07.877: INFO: Creating a pod which consumes cpu=133m on Node master1
    Jan 29 04:20:07.894: INFO: Creating a pod which consumes cpu=154m on Node master2
    Jan 29 04:20:07.910: INFO: Creating a pod which consumes cpu=154m on Node master3
    Jan 29 04:20:07.928: INFO: Creating a pod which consumes cpu=1715m on Node slave1
    Jan 29 04:20:07.949: INFO: Waiting up to 5m0s for pod "filler-pod-b1db2123-7f85-4f8b-9580-1746d91a5b67" in namespace "sched-pred-7101" to be "running"
    Jan 29 04:20:07.964: INFO: Pod "filler-pod-b1db2123-7f85-4f8b-9580-1746d91a5b67": Phase="Pending", Reason="", readiness=false. Elapsed: 14.847504ms
    Jan 29 04:20:09.972: INFO: Pod "filler-pod-b1db2123-7f85-4f8b-9580-1746d91a5b67": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022826421s
    Jan 29 04:20:11.983: INFO: Pod "filler-pod-b1db2123-7f85-4f8b-9580-1746d91a5b67": Phase="Running", Reason="", readiness=true. Elapsed: 4.0339744s
    Jan 29 04:20:11.983: INFO: Pod "filler-pod-b1db2123-7f85-4f8b-9580-1746d91a5b67" satisfied condition "running"
    Jan 29 04:20:11.984: INFO: Waiting up to 5m0s for pod "filler-pod-b405214b-26cc-4a73-bab3-d9a2deb7b9a1" in namespace "sched-pred-7101" to be "running"
    Jan 29 04:20:11.991: INFO: Pod "filler-pod-b405214b-26cc-4a73-bab3-d9a2deb7b9a1": Phase="Running", Reason="", readiness=true. Elapsed: 7.855075ms
    Jan 29 04:20:11.991: INFO: Pod "filler-pod-b405214b-26cc-4a73-bab3-d9a2deb7b9a1" satisfied condition "running"
    Jan 29 04:20:11.992: INFO: Waiting up to 5m0s for pod "filler-pod-e17309cd-a18e-4b68-9b4c-346ec1fbcf3d" in namespace "sched-pred-7101" to be "running"
    Jan 29 04:20:12.002: INFO: Pod "filler-pod-e17309cd-a18e-4b68-9b4c-346ec1fbcf3d": Phase="Running", Reason="", readiness=true. Elapsed: 10.392032ms
    Jan 29 04:20:12.002: INFO: Pod "filler-pod-e17309cd-a18e-4b68-9b4c-346ec1fbcf3d" satisfied condition "running"
    Jan 29 04:20:12.002: INFO: Waiting up to 5m0s for pod "filler-pod-fa1f2a30-96be-4f95-8d5d-afb6c87e0f32" in namespace "sched-pred-7101" to be "running"
    Jan 29 04:20:12.009: INFO: Pod "filler-pod-fa1f2a30-96be-4f95-8d5d-afb6c87e0f32": Phase="Running", Reason="", readiness=true. Elapsed: 6.978609ms
    Jan 29 04:20:12.009: INFO: Pod "filler-pod-fa1f2a30-96be-4f95-8d5d-afb6c87e0f32" satisfied condition "running"
    Jan 29 04:20:12.009: INFO: Waiting up to 5m0s for pod "filler-pod-8279b861-3d31-4f7d-8e16-62c0485e8330" in namespace "sched-pred-7101" to be "running"
    Jan 29 04:20:12.016: INFO: Pod "filler-pod-8279b861-3d31-4f7d-8e16-62c0485e8330": Phase="Running", Reason="", readiness=true. Elapsed: 7.020729ms
    Jan 29 04:20:12.016: INFO: Pod "filler-pod-8279b861-3d31-4f7d-8e16-62c0485e8330" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 01/29/23 04:20:12.016
    I0129 04:20:24.524063      22 trace.go:205] Trace[504397417]: "Reflector ListAndWatch" name:test/e2e/scheduling/events.go:98 (29-Jan-2023 04:20:12.017) (total time: 12506ms):
    Trace[504397417]: ---"Objects listed" error:<nil> 12506ms (04:20:24.523)
    Trace[504397417]: [12.506876075s] [12.506876075s] END
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-8279b861-3d31-4f7d-8e16-62c0485e8330.173eacb5bb858ee1], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7101/filler-pod-8279b861-3d31-4f7d-8e16-62c0485e8330 to slave1] 01/29/23 04:20:24.524
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-b1db2123-7f85-4f8b-9580-1746d91a5b67.173eacb5b6d12cf4], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7101/filler-pod-b1db2123-7f85-4f8b-9580-1746d91a5b67 to slave2] 01/29/23 04:20:24.524
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-b405214b-26cc-4a73-bab3-d9a2deb7b9a1.173eacb5b7b6786d], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7101/filler-pod-b405214b-26cc-4a73-bab3-d9a2deb7b9a1 to master1] 01/29/23 04:20:24.525
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-b405214b-26cc-4a73-bab3-d9a2deb7b9a1.173eacb60454f01a], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 01/29/23 04:20:24.525
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-b405214b-26cc-4a73-bab3-d9a2deb7b9a1.173eacb60888dfd7], Reason = [Created], Message = [Created container filler-pod-b405214b-26cc-4a73-bab3-d9a2deb7b9a1] 01/29/23 04:20:24.525
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-b405214b-26cc-4a73-bab3-d9a2deb7b9a1.173eacb6198c7071], Reason = [Started], Message = [Started container filler-pod-b405214b-26cc-4a73-bab3-d9a2deb7b9a1] 01/29/23 04:20:24.525
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-e17309cd-a18e-4b68-9b4c-346ec1fbcf3d.173eacb5b8e04d42], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7101/filler-pod-e17309cd-a18e-4b68-9b4c-346ec1fbcf3d to master2] 01/29/23 04:20:24.525
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-e17309cd-a18e-4b68-9b4c-346ec1fbcf3d.173eacb600ac7b19], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 01/29/23 04:20:24.525
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-e17309cd-a18e-4b68-9b4c-346ec1fbcf3d.173eacb6046d1cf9], Reason = [Created], Message = [Created container filler-pod-e17309cd-a18e-4b68-9b4c-346ec1fbcf3d] 01/29/23 04:20:24.526
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-e17309cd-a18e-4b68-9b4c-346ec1fbcf3d.173eacb60e7fe0a6], Reason = [Started], Message = [Started container filler-pod-e17309cd-a18e-4b68-9b4c-346ec1fbcf3d] 01/29/23 04:20:24.526
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-fa1f2a30-96be-4f95-8d5d-afb6c87e0f32.173eacb5b9fea9d1], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7101/filler-pod-fa1f2a30-96be-4f95-8d5d-afb6c87e0f32 to master3] 01/29/23 04:20:24.526
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-fa1f2a30-96be-4f95-8d5d-afb6c87e0f32.173eacb5fe97b119], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 01/29/23 04:20:24.526
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-fa1f2a30-96be-4f95-8d5d-afb6c87e0f32.173eacb60b7b7d84], Reason = [Created], Message = [Created container filler-pod-fa1f2a30-96be-4f95-8d5d-afb6c87e0f32] 01/29/23 04:20:24.526
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-fa1f2a30-96be-4f95-8d5d-afb6c87e0f32.173eacb61907cd4c], Reason = [Started], Message = [Started container filler-pod-fa1f2a30-96be-4f95-8d5d-afb6c87e0f32] 01/29/23 04:20:24.526
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-8279b861-3d31-4f7d-8e16-62c0485e8330.173eacb5ff539703], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 01/29/23 04:20:24.608
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-8279b861-3d31-4f7d-8e16-62c0485e8330.173eacb602df67db], Reason = [Created], Message = [Created container filler-pod-8279b861-3d31-4f7d-8e16-62c0485e8330] 01/29/23 04:20:24.608
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-b1db2123-7f85-4f8b-9580-1746d91a5b67.173eacb5fbc6b723], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 01/29/23 04:20:24.608
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-8279b861-3d31-4f7d-8e16-62c0485e8330.173eacb60b64faac], Reason = [Started], Message = [Started container filler-pod-8279b861-3d31-4f7d-8e16-62c0485e8330] 01/29/23 04:20:24.609
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-b1db2123-7f85-4f8b-9580-1746d91a5b67.173eacb600e7f9d5], Reason = [Created], Message = [Created container filler-pod-b1db2123-7f85-4f8b-9580-1746d91a5b67] 01/29/23 04:20:24.609
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.173eacb997ec9c02], Reason = [FailedScheduling], Message = [0/5 nodes are available: 5 Insufficient cpu. preemption: 0/5 nodes are available: 5 No preemption victims found for incoming pod.] 01/29/23 04:20:24.609
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-b1db2123-7f85-4f8b-9580-1746d91a5b67.173eacb609421eae], Reason = [Started], Message = [Started container filler-pod-b1db2123-7f85-4f8b-9580-1746d91a5b67] 01/29/23 04:20:24.609
    STEP: removing the label node off the node master1 01/29/23 04:20:25.559
    STEP: verifying the node doesn't have the label node 01/29/23 04:20:25.582
    STEP: removing the label node off the node master2 01/29/23 04:20:25.594
    STEP: verifying the node doesn't have the label node 01/29/23 04:20:25.614
    STEP: removing the label node off the node master3 01/29/23 04:20:25.621
    STEP: verifying the node doesn't have the label node 01/29/23 04:20:25.642
    STEP: removing the label node off the node slave1 01/29/23 04:20:25.65
    STEP: verifying the node doesn't have the label node 01/29/23 04:20:25.667
    STEP: removing the label node off the node slave2 01/29/23 04:20:25.675
    STEP: verifying the node doesn't have the label node 01/29/23 04:20:25.695
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Jan 29 04:20:25.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-7101" for this suite. 01/29/23 04:20:25.712
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:20:25.726
Jan 29 04:20:25.726: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename container-runtime 01/29/23 04:20:25.728
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:20:25.752
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:20:25.758
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 01/29/23 04:20:25.781
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 01/29/23 04:20:40.922
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 01/29/23 04:20:40.94
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 01/29/23 04:20:40.962
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 01/29/23 04:20:40.963
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 01/29/23 04:20:41.091
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 01/29/23 04:20:45.13
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 01/29/23 04:20:47.15
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 01/29/23 04:20:47.163
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 01/29/23 04:20:47.163
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 01/29/23 04:20:47.28
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 01/29/23 04:20:48.294
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 01/29/23 04:20:51.327
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 01/29/23 04:20:51.339
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 01/29/23 04:20:51.339
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jan 29 04:20:51.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7795" for this suite. 01/29/23 04:20:51.465
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","completed":296,"skipped":5228,"failed":0}
------------------------------
• [SLOW TEST] [25.747 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    when starting a container that exits
    test/e2e/common/node/runtime.go:44
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:20:25.726
    Jan 29 04:20:25.726: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename container-runtime 01/29/23 04:20:25.728
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:20:25.752
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:20:25.758
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 01/29/23 04:20:25.781
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 01/29/23 04:20:40.922
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 01/29/23 04:20:40.94
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 01/29/23 04:20:40.962
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 01/29/23 04:20:40.963
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 01/29/23 04:20:41.091
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 01/29/23 04:20:45.13
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 01/29/23 04:20:47.15
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 01/29/23 04:20:47.163
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 01/29/23 04:20:47.163
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 01/29/23 04:20:47.28
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 01/29/23 04:20:48.294
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 01/29/23 04:20:51.327
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 01/29/23 04:20:51.339
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 01/29/23 04:20:51.339
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jan 29 04:20:51.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-7795" for this suite. 01/29/23 04:20:51.465
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:20:51.476
Jan 29 04:20:51.476: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename resourcequota 01/29/23 04:20:51.478
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:20:51.499
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:20:51.507
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
STEP: Counting existing ResourceQuota 01/29/23 04:20:51.511
STEP: Creating a ResourceQuota 01/29/23 04:20:56.518
STEP: Ensuring resource quota status is calculated 01/29/23 04:20:56.526
STEP: Creating a Service 01/29/23 04:20:58.534
STEP: Creating a NodePort Service 01/29/23 04:20:58.555
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 01/29/23 04:20:58.596
STEP: Ensuring resource quota status captures service creation 01/29/23 04:20:58.647
STEP: Deleting Services 01/29/23 04:21:00.654
STEP: Ensuring resource quota status released usage 01/29/23 04:21:00.745
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 29 04:21:02.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6591" for this suite. 01/29/23 04:21:02.764
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","completed":297,"skipped":5261,"failed":0}
------------------------------
• [SLOW TEST] [11.305 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:20:51.476
    Jan 29 04:20:51.476: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename resourcequota 01/29/23 04:20:51.478
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:20:51.499
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:20:51.507
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:90
    STEP: Counting existing ResourceQuota 01/29/23 04:20:51.511
    STEP: Creating a ResourceQuota 01/29/23 04:20:56.518
    STEP: Ensuring resource quota status is calculated 01/29/23 04:20:56.526
    STEP: Creating a Service 01/29/23 04:20:58.534
    STEP: Creating a NodePort Service 01/29/23 04:20:58.555
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 01/29/23 04:20:58.596
    STEP: Ensuring resource quota status captures service creation 01/29/23 04:20:58.647
    STEP: Deleting Services 01/29/23 04:21:00.654
    STEP: Ensuring resource quota status released usage 01/29/23 04:21:00.745
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 29 04:21:02.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-6591" for this suite. 01/29/23 04:21:02.764
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:21:02.789
Jan 29 04:21:02.789: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename watch 01/29/23 04:21:02.79
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:21:02.821
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:21:02.827
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 01/29/23 04:21:02.833
STEP: creating a new configmap 01/29/23 04:21:02.835
STEP: modifying the configmap once 01/29/23 04:21:02.847
STEP: closing the watch once it receives two notifications 01/29/23 04:21:02.867
Jan 29 04:21:02.867: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5201  b65c7484-ef15-4889-80be-43deb5643ac3 5983266 0 2023-01-29 04:21:02 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] []},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 29 04:21:02.867: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5201  b65c7484-ef15-4889-80be-43deb5643ac3 5983268 0 2023-01-29 04:21:02 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 01/29/23 04:21:02.867
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 01/29/23 04:21:02.881
STEP: deleting the configmap 01/29/23 04:21:02.884
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 01/29/23 04:21:02.896
Jan 29 04:21:02.896: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5201  b65c7484-ef15-4889-80be-43deb5643ac3 5983269 0 2023-01-29 04:21:02 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 29 04:21:02.896: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5201  b65c7484-ef15-4889-80be-43deb5643ac3 5983270 0 2023-01-29 04:21:02 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jan 29 04:21:02.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5201" for this suite. 01/29/23 04:21:02.904
{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","completed":298,"skipped":5328,"failed":0}
------------------------------
• [0.124 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:21:02.789
    Jan 29 04:21:02.789: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename watch 01/29/23 04:21:02.79
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:21:02.821
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:21:02.827
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 01/29/23 04:21:02.833
    STEP: creating a new configmap 01/29/23 04:21:02.835
    STEP: modifying the configmap once 01/29/23 04:21:02.847
    STEP: closing the watch once it receives two notifications 01/29/23 04:21:02.867
    Jan 29 04:21:02.867: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5201  b65c7484-ef15-4889-80be-43deb5643ac3 5983266 0 2023-01-29 04:21:02 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] []},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 29 04:21:02.867: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5201  b65c7484-ef15-4889-80be-43deb5643ac3 5983268 0 2023-01-29 04:21:02 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 01/29/23 04:21:02.867
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 01/29/23 04:21:02.881
    STEP: deleting the configmap 01/29/23 04:21:02.884
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 01/29/23 04:21:02.896
    Jan 29 04:21:02.896: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5201  b65c7484-ef15-4889-80be-43deb5643ac3 5983269 0 2023-01-29 04:21:02 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 29 04:21:02.896: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5201  b65c7484-ef15-4889-80be-43deb5643ac3 5983270 0 2023-01-29 04:21:02 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jan 29 04:21:02.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-5201" for this suite. 01/29/23 04:21:02.904
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:21:02.914
Jan 29 04:21:02.914: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename watch 01/29/23 04:21:02.916
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:21:02.937
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:21:02.942
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 01/29/23 04:21:02.947
STEP: starting a background goroutine to produce watch events 01/29/23 04:21:02.954
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 01/29/23 04:21:02.954
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jan 29 04:21:05.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-577" for this suite. 01/29/23 04:21:05.776
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","completed":299,"skipped":5347,"failed":0}
------------------------------
• [2.911 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:21:02.914
    Jan 29 04:21:02.914: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename watch 01/29/23 04:21:02.916
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:21:02.937
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:21:02.942
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 01/29/23 04:21:02.947
    STEP: starting a background goroutine to produce watch events 01/29/23 04:21:02.954
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 01/29/23 04:21:02.954
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jan 29 04:21:05.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-577" for this suite. 01/29/23 04:21:05.776
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:21:05.826
Jan 29 04:21:05.826: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename configmap 01/29/23 04:21:05.827
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:21:05.849
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:21:05.854
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
STEP: Creating configMap configmap-6861/configmap-test-6c1c1001-6dcf-4e38-a8ed-51f83da4ac39 01/29/23 04:21:05.859
STEP: Creating a pod to test consume configMaps 01/29/23 04:21:05.866
Jan 29 04:21:05.884: INFO: Waiting up to 5m0s for pod "pod-configmaps-b910b3ae-6894-40cc-b1d1-32200f48eb65" in namespace "configmap-6861" to be "Succeeded or Failed"
Jan 29 04:21:05.890: INFO: Pod "pod-configmaps-b910b3ae-6894-40cc-b1d1-32200f48eb65": Phase="Pending", Reason="", readiness=false. Elapsed: 6.665107ms
Jan 29 04:21:07.897: INFO: Pod "pod-configmaps-b910b3ae-6894-40cc-b1d1-32200f48eb65": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013428835s
Jan 29 04:21:09.899: INFO: Pod "pod-configmaps-b910b3ae-6894-40cc-b1d1-32200f48eb65": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015563051s
STEP: Saw pod success 01/29/23 04:21:09.899
Jan 29 04:21:09.899: INFO: Pod "pod-configmaps-b910b3ae-6894-40cc-b1d1-32200f48eb65" satisfied condition "Succeeded or Failed"
Jan 29 04:21:09.906: INFO: Trying to get logs from node slave2 pod pod-configmaps-b910b3ae-6894-40cc-b1d1-32200f48eb65 container env-test: <nil>
STEP: delete the pod 01/29/23 04:21:09.92
Jan 29 04:21:10.015: INFO: Waiting for pod pod-configmaps-b910b3ae-6894-40cc-b1d1-32200f48eb65 to disappear
Jan 29 04:21:10.021: INFO: Pod pod-configmaps-b910b3ae-6894-40cc-b1d1-32200f48eb65 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Jan 29 04:21:10.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6861" for this suite. 01/29/23 04:21:10.031
{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","completed":300,"skipped":5348,"failed":0}
------------------------------
• [4.215 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:21:05.826
    Jan 29 04:21:05.826: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename configmap 01/29/23 04:21:05.827
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:21:05.849
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:21:05.854
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:44
    STEP: Creating configMap configmap-6861/configmap-test-6c1c1001-6dcf-4e38-a8ed-51f83da4ac39 01/29/23 04:21:05.859
    STEP: Creating a pod to test consume configMaps 01/29/23 04:21:05.866
    Jan 29 04:21:05.884: INFO: Waiting up to 5m0s for pod "pod-configmaps-b910b3ae-6894-40cc-b1d1-32200f48eb65" in namespace "configmap-6861" to be "Succeeded or Failed"
    Jan 29 04:21:05.890: INFO: Pod "pod-configmaps-b910b3ae-6894-40cc-b1d1-32200f48eb65": Phase="Pending", Reason="", readiness=false. Elapsed: 6.665107ms
    Jan 29 04:21:07.897: INFO: Pod "pod-configmaps-b910b3ae-6894-40cc-b1d1-32200f48eb65": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013428835s
    Jan 29 04:21:09.899: INFO: Pod "pod-configmaps-b910b3ae-6894-40cc-b1d1-32200f48eb65": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015563051s
    STEP: Saw pod success 01/29/23 04:21:09.899
    Jan 29 04:21:09.899: INFO: Pod "pod-configmaps-b910b3ae-6894-40cc-b1d1-32200f48eb65" satisfied condition "Succeeded or Failed"
    Jan 29 04:21:09.906: INFO: Trying to get logs from node slave2 pod pod-configmaps-b910b3ae-6894-40cc-b1d1-32200f48eb65 container env-test: <nil>
    STEP: delete the pod 01/29/23 04:21:09.92
    Jan 29 04:21:10.015: INFO: Waiting for pod pod-configmaps-b910b3ae-6894-40cc-b1d1-32200f48eb65 to disappear
    Jan 29 04:21:10.021: INFO: Pod pod-configmaps-b910b3ae-6894-40cc-b1d1-32200f48eb65 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 29 04:21:10.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-6861" for this suite. 01/29/23 04:21:10.031
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:21:10.042
Jan 29 04:21:10.043: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename services 01/29/23 04:21:10.044
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:21:10.067
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:21:10.072
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221
STEP: creating service in namespace services-6304 01/29/23 04:21:10.078
Jan 29 04:21:10.098: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-6304" to be "running and ready"
Jan 29 04:21:10.106: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 7.798675ms
Jan 29 04:21:10.106: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Jan 29 04:21:12.115: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.016052914s
Jan 29 04:21:12.115: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
Jan 29 04:21:12.115: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
Jan 29 04:21:12.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-6304 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jan 29 04:21:12.344: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Jan 29 04:21:12.344: INFO: stdout: "iptables"
Jan 29 04:21:12.344: INFO: proxyMode: iptables
Jan 29 04:21:12.440: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jan 29 04:21:12.446: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-6304 01/29/23 04:21:12.446
STEP: creating replication controller affinity-nodeport-timeout in namespace services-6304 01/29/23 04:21:12.467
I0129 04:21:12.481399      22 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-6304, replica count: 3
I0129 04:21:15.532983      22 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 29 04:21:15.554: INFO: Creating new exec pod
Jan 29 04:21:15.567: INFO: Waiting up to 5m0s for pod "execpod-affinity2mvhg" in namespace "services-6304" to be "running"
Jan 29 04:21:15.578: INFO: Pod "execpod-affinity2mvhg": Phase="Pending", Reason="", readiness=false. Elapsed: 10.665275ms
Jan 29 04:21:17.585: INFO: Pod "execpod-affinity2mvhg": Phase="Running", Reason="", readiness=true. Elapsed: 2.018214685s
Jan 29 04:21:17.586: INFO: Pod "execpod-affinity2mvhg" satisfied condition "running"
Jan 29 04:21:18.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-6304 exec execpod-affinity2mvhg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
Jan 29 04:21:18.834: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Jan 29 04:21:18.834: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 29 04:21:18.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-6304 exec execpod-affinity2mvhg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.105.60.184 80'
Jan 29 04:21:19.061: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.105.60.184 80\nConnection to 100.105.60.184 80 port [tcp/http] succeeded!\n"
Jan 29 04:21:19.061: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 29 04:21:19.061: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-6304 exec execpod-affinity2mvhg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.122.245 32584'
Jan 29 04:21:19.286: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.122.245 32584\nConnection to 192.168.122.245 32584 port [tcp/*] succeeded!\n"
Jan 29 04:21:19.286: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 29 04:21:19.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-6304 exec execpod-affinity2mvhg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.122.243 32584'
Jan 29 04:21:19.513: INFO: stderr: "+ + echonc hostName -v\n -t -w 2 192.168.122.243 32584\nConnection to 192.168.122.243 32584 port [tcp/*] succeeded!\n"
Jan 29 04:21:19.513: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 29 04:21:19.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-6304 exec execpod-affinity2mvhg -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.122.241:32584/ ; done'
Jan 29 04:21:19.838: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:32584/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:32584/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:32584/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:32584/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:32584/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:32584/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:32584/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:32584/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:32584/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:32584/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:32584/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:32584/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:32584/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:32584/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:32584/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:32584/\n"
Jan 29 04:21:19.839: INFO: stdout: "\naffinity-nodeport-timeout-gxztd\naffinity-nodeport-timeout-gxztd\naffinity-nodeport-timeout-gxztd\naffinity-nodeport-timeout-gxztd\naffinity-nodeport-timeout-gxztd\naffinity-nodeport-timeout-gxztd\naffinity-nodeport-timeout-gxztd\naffinity-nodeport-timeout-gxztd\naffinity-nodeport-timeout-gxztd\naffinity-nodeport-timeout-gxztd\naffinity-nodeport-timeout-gxztd\naffinity-nodeport-timeout-gxztd\naffinity-nodeport-timeout-gxztd\naffinity-nodeport-timeout-gxztd\naffinity-nodeport-timeout-gxztd\naffinity-nodeport-timeout-gxztd"
Jan 29 04:21:19.839: INFO: Received response from host: affinity-nodeport-timeout-gxztd
Jan 29 04:21:19.839: INFO: Received response from host: affinity-nodeport-timeout-gxztd
Jan 29 04:21:19.839: INFO: Received response from host: affinity-nodeport-timeout-gxztd
Jan 29 04:21:19.839: INFO: Received response from host: affinity-nodeport-timeout-gxztd
Jan 29 04:21:19.839: INFO: Received response from host: affinity-nodeport-timeout-gxztd
Jan 29 04:21:19.839: INFO: Received response from host: affinity-nodeport-timeout-gxztd
Jan 29 04:21:19.839: INFO: Received response from host: affinity-nodeport-timeout-gxztd
Jan 29 04:21:19.839: INFO: Received response from host: affinity-nodeport-timeout-gxztd
Jan 29 04:21:19.839: INFO: Received response from host: affinity-nodeport-timeout-gxztd
Jan 29 04:21:19.839: INFO: Received response from host: affinity-nodeport-timeout-gxztd
Jan 29 04:21:19.839: INFO: Received response from host: affinity-nodeport-timeout-gxztd
Jan 29 04:21:19.839: INFO: Received response from host: affinity-nodeport-timeout-gxztd
Jan 29 04:21:19.839: INFO: Received response from host: affinity-nodeport-timeout-gxztd
Jan 29 04:21:19.839: INFO: Received response from host: affinity-nodeport-timeout-gxztd
Jan 29 04:21:19.839: INFO: Received response from host: affinity-nodeport-timeout-gxztd
Jan 29 04:21:19.839: INFO: Received response from host: affinity-nodeport-timeout-gxztd
Jan 29 04:21:19.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-6304 exec execpod-affinity2mvhg -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.122.241:32584/'
Jan 29 04:21:20.072: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.122.241:32584/\n"
Jan 29 04:21:20.072: INFO: stdout: "affinity-nodeport-timeout-gxztd"
Jan 29 04:21:40.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-6304 exec execpod-affinity2mvhg -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.122.241:32584/'
Jan 29 04:21:40.316: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.122.241:32584/\n"
Jan 29 04:21:40.316: INFO: stdout: "affinity-nodeport-timeout-vdqz9"
Jan 29 04:21:40.316: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-6304, will wait for the garbage collector to delete the pods 01/29/23 04:21:40.388
Jan 29 04:21:40.456: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 10.532414ms
Jan 29 04:21:40.556: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 100.543342ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 29 04:21:43.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6304" for this suite. 01/29/23 04:21:43.618
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","completed":301,"skipped":5368,"failed":0}
------------------------------
• [SLOW TEST] [33.588 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:21:10.042
    Jan 29 04:21:10.043: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename services 01/29/23 04:21:10.044
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:21:10.067
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:21:10.072
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2221
    STEP: creating service in namespace services-6304 01/29/23 04:21:10.078
    Jan 29 04:21:10.098: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-6304" to be "running and ready"
    Jan 29 04:21:10.106: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 7.798675ms
    Jan 29 04:21:10.106: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
    Jan 29 04:21:12.115: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.016052914s
    Jan 29 04:21:12.115: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
    Jan 29 04:21:12.115: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
    Jan 29 04:21:12.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-6304 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
    Jan 29 04:21:12.344: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
    Jan 29 04:21:12.344: INFO: stdout: "iptables"
    Jan 29 04:21:12.344: INFO: proxyMode: iptables
    Jan 29 04:21:12.440: INFO: Waiting for pod kube-proxy-mode-detector to disappear
    Jan 29 04:21:12.446: INFO: Pod kube-proxy-mode-detector no longer exists
    STEP: creating service affinity-nodeport-timeout in namespace services-6304 01/29/23 04:21:12.446
    STEP: creating replication controller affinity-nodeport-timeout in namespace services-6304 01/29/23 04:21:12.467
    I0129 04:21:12.481399      22 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-6304, replica count: 3
    I0129 04:21:15.532983      22 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 29 04:21:15.554: INFO: Creating new exec pod
    Jan 29 04:21:15.567: INFO: Waiting up to 5m0s for pod "execpod-affinity2mvhg" in namespace "services-6304" to be "running"
    Jan 29 04:21:15.578: INFO: Pod "execpod-affinity2mvhg": Phase="Pending", Reason="", readiness=false. Elapsed: 10.665275ms
    Jan 29 04:21:17.585: INFO: Pod "execpod-affinity2mvhg": Phase="Running", Reason="", readiness=true. Elapsed: 2.018214685s
    Jan 29 04:21:17.586: INFO: Pod "execpod-affinity2mvhg" satisfied condition "running"
    Jan 29 04:21:18.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-6304 exec execpod-affinity2mvhg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
    Jan 29 04:21:18.834: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
    Jan 29 04:21:18.834: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 29 04:21:18.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-6304 exec execpod-affinity2mvhg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.105.60.184 80'
    Jan 29 04:21:19.061: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.105.60.184 80\nConnection to 100.105.60.184 80 port [tcp/http] succeeded!\n"
    Jan 29 04:21:19.061: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 29 04:21:19.061: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-6304 exec execpod-affinity2mvhg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.122.245 32584'
    Jan 29 04:21:19.286: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.122.245 32584\nConnection to 192.168.122.245 32584 port [tcp/*] succeeded!\n"
    Jan 29 04:21:19.286: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 29 04:21:19.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-6304 exec execpod-affinity2mvhg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.122.243 32584'
    Jan 29 04:21:19.513: INFO: stderr: "+ + echonc hostName -v\n -t -w 2 192.168.122.243 32584\nConnection to 192.168.122.243 32584 port [tcp/*] succeeded!\n"
    Jan 29 04:21:19.513: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 29 04:21:19.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-6304 exec execpod-affinity2mvhg -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.122.241:32584/ ; done'
    Jan 29 04:21:19.838: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:32584/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:32584/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:32584/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:32584/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:32584/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:32584/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:32584/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:32584/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:32584/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:32584/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:32584/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:32584/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:32584/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:32584/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:32584/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:32584/\n"
    Jan 29 04:21:19.839: INFO: stdout: "\naffinity-nodeport-timeout-gxztd\naffinity-nodeport-timeout-gxztd\naffinity-nodeport-timeout-gxztd\naffinity-nodeport-timeout-gxztd\naffinity-nodeport-timeout-gxztd\naffinity-nodeport-timeout-gxztd\naffinity-nodeport-timeout-gxztd\naffinity-nodeport-timeout-gxztd\naffinity-nodeport-timeout-gxztd\naffinity-nodeport-timeout-gxztd\naffinity-nodeport-timeout-gxztd\naffinity-nodeport-timeout-gxztd\naffinity-nodeport-timeout-gxztd\naffinity-nodeport-timeout-gxztd\naffinity-nodeport-timeout-gxztd\naffinity-nodeport-timeout-gxztd"
    Jan 29 04:21:19.839: INFO: Received response from host: affinity-nodeport-timeout-gxztd
    Jan 29 04:21:19.839: INFO: Received response from host: affinity-nodeport-timeout-gxztd
    Jan 29 04:21:19.839: INFO: Received response from host: affinity-nodeport-timeout-gxztd
    Jan 29 04:21:19.839: INFO: Received response from host: affinity-nodeport-timeout-gxztd
    Jan 29 04:21:19.839: INFO: Received response from host: affinity-nodeport-timeout-gxztd
    Jan 29 04:21:19.839: INFO: Received response from host: affinity-nodeport-timeout-gxztd
    Jan 29 04:21:19.839: INFO: Received response from host: affinity-nodeport-timeout-gxztd
    Jan 29 04:21:19.839: INFO: Received response from host: affinity-nodeport-timeout-gxztd
    Jan 29 04:21:19.839: INFO: Received response from host: affinity-nodeport-timeout-gxztd
    Jan 29 04:21:19.839: INFO: Received response from host: affinity-nodeport-timeout-gxztd
    Jan 29 04:21:19.839: INFO: Received response from host: affinity-nodeport-timeout-gxztd
    Jan 29 04:21:19.839: INFO: Received response from host: affinity-nodeport-timeout-gxztd
    Jan 29 04:21:19.839: INFO: Received response from host: affinity-nodeport-timeout-gxztd
    Jan 29 04:21:19.839: INFO: Received response from host: affinity-nodeport-timeout-gxztd
    Jan 29 04:21:19.839: INFO: Received response from host: affinity-nodeport-timeout-gxztd
    Jan 29 04:21:19.839: INFO: Received response from host: affinity-nodeport-timeout-gxztd
    Jan 29 04:21:19.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-6304 exec execpod-affinity2mvhg -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.122.241:32584/'
    Jan 29 04:21:20.072: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.122.241:32584/\n"
    Jan 29 04:21:20.072: INFO: stdout: "affinity-nodeport-timeout-gxztd"
    Jan 29 04:21:40.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-6304 exec execpod-affinity2mvhg -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.122.241:32584/'
    Jan 29 04:21:40.316: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.122.241:32584/\n"
    Jan 29 04:21:40.316: INFO: stdout: "affinity-nodeport-timeout-vdqz9"
    Jan 29 04:21:40.316: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-6304, will wait for the garbage collector to delete the pods 01/29/23 04:21:40.388
    Jan 29 04:21:40.456: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 10.532414ms
    Jan 29 04:21:40.556: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 100.543342ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 29 04:21:43.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-6304" for this suite. 01/29/23 04:21:43.618
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:21:43.635
Jan 29 04:21:43.635: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename runtimeclass 01/29/23 04:21:43.637
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:21:43.663
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:21:43.67
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-9921-delete-me 01/29/23 04:21:43.682
STEP: Waiting for the RuntimeClass to disappear 01/29/23 04:21:43.695
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jan 29 04:21:43.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-9921" for this suite. 01/29/23 04:21:43.726
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]","completed":302,"skipped":5410,"failed":0}
------------------------------
• [0.099 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:21:43.635
    Jan 29 04:21:43.635: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename runtimeclass 01/29/23 04:21:43.637
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:21:43.663
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:21:43.67
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-9921-delete-me 01/29/23 04:21:43.682
    STEP: Waiting for the RuntimeClass to disappear 01/29/23 04:21:43.695
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jan 29 04:21:43.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-9921" for this suite. 01/29/23 04:21:43.726
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:21:43.735
Jan 29 04:21:43.735: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename crd-webhook 01/29/23 04:21:43.736
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:21:43.759
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:21:43.764
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 01/29/23 04:21:43.769
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/29/23 04:21:44.944
STEP: Deploying the custom resource conversion webhook pod 01/29/23 04:21:44.955
STEP: Wait for the deployment to be ready 01/29/23 04:21:44.979
Jan 29 04:21:44.992: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/29/23 04:21:47.013
STEP: Verifying the service has paired with the endpoint 01/29/23 04:21:47.033
Jan 29 04:21:48.034: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Jan 29 04:21:48.050: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Creating a v1 custom resource 01/29/23 04:21:50.664
STEP: v2 custom resource should be converted 01/29/23 04:21:50.673
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 29 04:21:51.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-4496" for this suite. 01/29/23 04:21:51.212
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","completed":303,"skipped":5413,"failed":0}
------------------------------
• [SLOW TEST] [7.586 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:21:43.735
    Jan 29 04:21:43.735: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename crd-webhook 01/29/23 04:21:43.736
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:21:43.759
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:21:43.764
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 01/29/23 04:21:43.769
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/29/23 04:21:44.944
    STEP: Deploying the custom resource conversion webhook pod 01/29/23 04:21:44.955
    STEP: Wait for the deployment to be ready 01/29/23 04:21:44.979
    Jan 29 04:21:44.992: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/29/23 04:21:47.013
    STEP: Verifying the service has paired with the endpoint 01/29/23 04:21:47.033
    Jan 29 04:21:48.034: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Jan 29 04:21:48.050: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Creating a v1 custom resource 01/29/23 04:21:50.664
    STEP: v2 custom resource should be converted 01/29/23 04:21:50.673
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 29 04:21:51.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-4496" for this suite. 01/29/23 04:21:51.212
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:21:51.323
Jan 29 04:21:51.323: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename job 01/29/23 04:21:51.325
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:21:51.382
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:21:51.392
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
STEP: Creating a job 01/29/23 04:21:51.414
STEP: Ensuring job reaches completions 01/29/23 04:21:51.447
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jan 29 04:22:03.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1373" for this suite. 01/29/23 04:22:03.465
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","completed":304,"skipped":5425,"failed":0}
------------------------------
• [SLOW TEST] [12.150 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:21:51.323
    Jan 29 04:21:51.323: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename job 01/29/23 04:21:51.325
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:21:51.382
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:21:51.392
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:254
    STEP: Creating a job 01/29/23 04:21:51.414
    STEP: Ensuring job reaches completions 01/29/23 04:21:51.447
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jan 29 04:22:03.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-1373" for this suite. 01/29/23 04:22:03.465
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:22:03.474
Jan 29 04:22:03.474: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename projected 01/29/23 04:22:03.476
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:22:03.496
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:22:03.501
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
STEP: Creating a pod to test downward API volume plugin 01/29/23 04:22:03.506
Jan 29 04:22:03.524: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0d34bd86-764d-4a61-b07f-83e7e2224b6d" in namespace "projected-5040" to be "Succeeded or Failed"
Jan 29 04:22:03.531: INFO: Pod "downwardapi-volume-0d34bd86-764d-4a61-b07f-83e7e2224b6d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.533746ms
Jan 29 04:22:05.538: INFO: Pod "downwardapi-volume-0d34bd86-764d-4a61-b07f-83e7e2224b6d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013825194s
Jan 29 04:22:07.539: INFO: Pod "downwardapi-volume-0d34bd86-764d-4a61-b07f-83e7e2224b6d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014860258s
STEP: Saw pod success 01/29/23 04:22:07.539
Jan 29 04:22:07.540: INFO: Pod "downwardapi-volume-0d34bd86-764d-4a61-b07f-83e7e2224b6d" satisfied condition "Succeeded or Failed"
Jan 29 04:22:07.546: INFO: Trying to get logs from node slave2 pod downwardapi-volume-0d34bd86-764d-4a61-b07f-83e7e2224b6d container client-container: <nil>
STEP: delete the pod 01/29/23 04:22:07.562
Jan 29 04:22:07.636: INFO: Waiting for pod downwardapi-volume-0d34bd86-764d-4a61-b07f-83e7e2224b6d to disappear
Jan 29 04:22:07.642: INFO: Pod downwardapi-volume-0d34bd86-764d-4a61-b07f-83e7e2224b6d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 29 04:22:07.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5040" for this suite. 01/29/23 04:22:07.652
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","completed":305,"skipped":5434,"failed":0}
------------------------------
• [4.188 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:22:03.474
    Jan 29 04:22:03.474: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename projected 01/29/23 04:22:03.476
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:22:03.496
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:22:03.501
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:234
    STEP: Creating a pod to test downward API volume plugin 01/29/23 04:22:03.506
    Jan 29 04:22:03.524: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0d34bd86-764d-4a61-b07f-83e7e2224b6d" in namespace "projected-5040" to be "Succeeded or Failed"
    Jan 29 04:22:03.531: INFO: Pod "downwardapi-volume-0d34bd86-764d-4a61-b07f-83e7e2224b6d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.533746ms
    Jan 29 04:22:05.538: INFO: Pod "downwardapi-volume-0d34bd86-764d-4a61-b07f-83e7e2224b6d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013825194s
    Jan 29 04:22:07.539: INFO: Pod "downwardapi-volume-0d34bd86-764d-4a61-b07f-83e7e2224b6d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014860258s
    STEP: Saw pod success 01/29/23 04:22:07.539
    Jan 29 04:22:07.540: INFO: Pod "downwardapi-volume-0d34bd86-764d-4a61-b07f-83e7e2224b6d" satisfied condition "Succeeded or Failed"
    Jan 29 04:22:07.546: INFO: Trying to get logs from node slave2 pod downwardapi-volume-0d34bd86-764d-4a61-b07f-83e7e2224b6d container client-container: <nil>
    STEP: delete the pod 01/29/23 04:22:07.562
    Jan 29 04:22:07.636: INFO: Waiting for pod downwardapi-volume-0d34bd86-764d-4a61-b07f-83e7e2224b6d to disappear
    Jan 29 04:22:07.642: INFO: Pod downwardapi-volume-0d34bd86-764d-4a61-b07f-83e7e2224b6d no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 29 04:22:07.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5040" for this suite. 01/29/23 04:22:07.652
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:309
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:22:07.663
Jan 29 04:22:07.663: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename job 01/29/23 04:22:07.664
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:22:07.687
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:22:07.692
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:309
STEP: Creating a job 01/29/23 04:22:07.697
STEP: Ensuring active pods == parallelism 01/29/23 04:22:07.713
STEP: delete a job 01/29/23 04:22:09.721
STEP: deleting Job.batch foo in namespace job-204, will wait for the garbage collector to delete the pods 01/29/23 04:22:09.721
Jan 29 04:22:09.789: INFO: Deleting Job.batch foo took: 10.662994ms
Jan 29 04:22:09.890: INFO: Terminating Job.batch foo pods took: 101.235368ms
STEP: Ensuring job was deleted 01/29/23 04:22:43.691
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jan 29 04:22:43.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-204" for this suite. 01/29/23 04:22:43.736
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","completed":306,"skipped":5437,"failed":0}
------------------------------
• [SLOW TEST] [36.091 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:22:07.663
    Jan 29 04:22:07.663: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename job 01/29/23 04:22:07.664
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:22:07.687
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:22:07.692
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:309
    STEP: Creating a job 01/29/23 04:22:07.697
    STEP: Ensuring active pods == parallelism 01/29/23 04:22:07.713
    STEP: delete a job 01/29/23 04:22:09.721
    STEP: deleting Job.batch foo in namespace job-204, will wait for the garbage collector to delete the pods 01/29/23 04:22:09.721
    Jan 29 04:22:09.789: INFO: Deleting Job.batch foo took: 10.662994ms
    Jan 29 04:22:09.890: INFO: Terminating Job.batch foo pods took: 101.235368ms
    STEP: Ensuring job was deleted 01/29/23 04:22:43.691
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jan 29 04:22:43.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-204" for this suite. 01/29/23 04:22:43.736
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:22:43.759
Jan 29 04:22:43.759: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename custom-resource-definition 01/29/23 04:22:43.761
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:22:43.813
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:22:43.818
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Jan 29 04:22:43.824: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 29 04:22:44.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3061" for this suite. 01/29/23 04:22:44.88
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","completed":307,"skipped":5513,"failed":0}
------------------------------
• [1.129 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:22:43.759
    Jan 29 04:22:43.759: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename custom-resource-definition 01/29/23 04:22:43.761
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:22:43.813
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:22:43.818
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Jan 29 04:22:43.824: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 29 04:22:44.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-3061" for this suite. 01/29/23 04:22:44.88
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:22:44.89
Jan 29 04:22:44.890: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename projected 01/29/23 04:22:44.891
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:22:44.913
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:22:44.918
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
STEP: Creating a pod to test downward API volume plugin 01/29/23 04:22:44.922
Jan 29 04:22:44.951: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4b637169-06d3-413f-9aac-ce9555f5802e" in namespace "projected-9044" to be "Succeeded or Failed"
Jan 29 04:22:44.959: INFO: Pod "downwardapi-volume-4b637169-06d3-413f-9aac-ce9555f5802e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.023936ms
Jan 29 04:22:46.966: INFO: Pod "downwardapi-volume-4b637169-06d3-413f-9aac-ce9555f5802e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015523046s
Jan 29 04:22:48.967: INFO: Pod "downwardapi-volume-4b637169-06d3-413f-9aac-ce9555f5802e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015736984s
STEP: Saw pod success 01/29/23 04:22:48.967
Jan 29 04:22:48.967: INFO: Pod "downwardapi-volume-4b637169-06d3-413f-9aac-ce9555f5802e" satisfied condition "Succeeded or Failed"
Jan 29 04:22:48.973: INFO: Trying to get logs from node slave2 pod downwardapi-volume-4b637169-06d3-413f-9aac-ce9555f5802e container client-container: <nil>
STEP: delete the pod 01/29/23 04:22:48.991
Jan 29 04:22:49.062: INFO: Waiting for pod downwardapi-volume-4b637169-06d3-413f-9aac-ce9555f5802e to disappear
Jan 29 04:22:49.068: INFO: Pod downwardapi-volume-4b637169-06d3-413f-9aac-ce9555f5802e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 29 04:22:49.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9044" for this suite. 01/29/23 04:22:49.078
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","completed":308,"skipped":5518,"failed":0}
------------------------------
• [4.202 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:22:44.89
    Jan 29 04:22:44.890: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename projected 01/29/23 04:22:44.891
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:22:44.913
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:22:44.918
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:52
    STEP: Creating a pod to test downward API volume plugin 01/29/23 04:22:44.922
    Jan 29 04:22:44.951: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4b637169-06d3-413f-9aac-ce9555f5802e" in namespace "projected-9044" to be "Succeeded or Failed"
    Jan 29 04:22:44.959: INFO: Pod "downwardapi-volume-4b637169-06d3-413f-9aac-ce9555f5802e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.023936ms
    Jan 29 04:22:46.966: INFO: Pod "downwardapi-volume-4b637169-06d3-413f-9aac-ce9555f5802e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015523046s
    Jan 29 04:22:48.967: INFO: Pod "downwardapi-volume-4b637169-06d3-413f-9aac-ce9555f5802e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015736984s
    STEP: Saw pod success 01/29/23 04:22:48.967
    Jan 29 04:22:48.967: INFO: Pod "downwardapi-volume-4b637169-06d3-413f-9aac-ce9555f5802e" satisfied condition "Succeeded or Failed"
    Jan 29 04:22:48.973: INFO: Trying to get logs from node slave2 pod downwardapi-volume-4b637169-06d3-413f-9aac-ce9555f5802e container client-container: <nil>
    STEP: delete the pod 01/29/23 04:22:48.991
    Jan 29 04:22:49.062: INFO: Waiting for pod downwardapi-volume-4b637169-06d3-413f-9aac-ce9555f5802e to disappear
    Jan 29 04:22:49.068: INFO: Pod downwardapi-volume-4b637169-06d3-413f-9aac-ce9555f5802e no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 29 04:22:49.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9044" for this suite. 01/29/23 04:22:49.078
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:22:49.094
Jan 29 04:22:49.094: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename webhook 01/29/23 04:22:49.095
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:22:49.122
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:22:49.13
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/29/23 04:22:49.162
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/29/23 04:22:50.495
STEP: Deploying the webhook pod 01/29/23 04:22:50.513
STEP: Wait for the deployment to be ready 01/29/23 04:22:50.538
Jan 29 04:22:50.560: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/29/23 04:22:52.581
STEP: Verifying the service has paired with the endpoint 01/29/23 04:22:52.596
Jan 29 04:22:53.597: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
Jan 29 04:22:53.604: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Registering the custom resource webhook via the AdmissionRegistration API 01/29/23 04:22:54.118
STEP: Creating a custom resource that should be denied by the webhook 01/29/23 04:22:54.147
STEP: Creating a custom resource whose deletion would be denied by the webhook 01/29/23 04:22:56.185
STEP: Updating the custom resource with disallowed data should be denied 01/29/23 04:22:56.201
STEP: Deleting the custom resource should be denied 01/29/23 04:22:56.22
STEP: Remove the offending key and value from the custom resource data 01/29/23 04:22:56.233
STEP: Deleting the updated custom resource should be successful 01/29/23 04:22:56.251
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 29 04:22:56.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3983" for this suite. 01/29/23 04:22:56.805
STEP: Destroying namespace "webhook-3983-markers" for this suite. 01/29/23 04:22:56.821
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","completed":309,"skipped":5536,"failed":0}
------------------------------
• [SLOW TEST] [7.902 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:22:49.094
    Jan 29 04:22:49.094: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename webhook 01/29/23 04:22:49.095
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:22:49.122
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:22:49.13
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/29/23 04:22:49.162
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/29/23 04:22:50.495
    STEP: Deploying the webhook pod 01/29/23 04:22:50.513
    STEP: Wait for the deployment to be ready 01/29/23 04:22:50.538
    Jan 29 04:22:50.560: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/29/23 04:22:52.581
    STEP: Verifying the service has paired with the endpoint 01/29/23 04:22:52.596
    Jan 29 04:22:53.597: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:220
    Jan 29 04:22:53.604: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 01/29/23 04:22:54.118
    STEP: Creating a custom resource that should be denied by the webhook 01/29/23 04:22:54.147
    STEP: Creating a custom resource whose deletion would be denied by the webhook 01/29/23 04:22:56.185
    STEP: Updating the custom resource with disallowed data should be denied 01/29/23 04:22:56.201
    STEP: Deleting the custom resource should be denied 01/29/23 04:22:56.22
    STEP: Remove the offending key and value from the custom resource data 01/29/23 04:22:56.233
    STEP: Deleting the updated custom resource should be successful 01/29/23 04:22:56.251
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 29 04:22:56.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-3983" for this suite. 01/29/23 04:22:56.805
    STEP: Destroying namespace "webhook-3983-markers" for this suite. 01/29/23 04:22:56.821
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:22:56.996
Jan 29 04:22:56.996: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename webhook 01/29/23 04:22:56.998
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:22:57.032
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:22:57.042
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/29/23 04:22:57.102
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/29/23 04:22:57.899
STEP: Deploying the webhook pod 01/29/23 04:22:57.908
STEP: Wait for the deployment to be ready 01/29/23 04:22:57.931
Jan 29 04:22:57.944: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/29/23 04:22:59.966
STEP: Verifying the service has paired with the endpoint 01/29/23 04:22:59.982
Jan 29 04:23:00.983: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
STEP: Registering the webhook via the AdmissionRegistration API 01/29/23 04:23:00.992
STEP: create a pod that should be denied by the webhook 01/29/23 04:23:01.02
STEP: create a pod that causes the webhook to hang 01/29/23 04:23:01.043
STEP: create a configmap that should be denied by the webhook 01/29/23 04:23:11.059
STEP: create a configmap that should be admitted by the webhook 01/29/23 04:23:11.075
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 01/29/23 04:23:11.101
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 01/29/23 04:23:11.118
STEP: create a namespace that bypass the webhook 01/29/23 04:23:11.128
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 01/29/23 04:23:11.143
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 29 04:23:11.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8510" for this suite. 01/29/23 04:23:11.2
STEP: Destroying namespace "webhook-8510-markers" for this suite. 01/29/23 04:23:11.21
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","completed":310,"skipped":5540,"failed":0}
------------------------------
• [SLOW TEST] [14.329 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:22:56.996
    Jan 29 04:22:56.996: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename webhook 01/29/23 04:22:56.998
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:22:57.032
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:22:57.042
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/29/23 04:22:57.102
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/29/23 04:22:57.899
    STEP: Deploying the webhook pod 01/29/23 04:22:57.908
    STEP: Wait for the deployment to be ready 01/29/23 04:22:57.931
    Jan 29 04:22:57.944: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/29/23 04:22:59.966
    STEP: Verifying the service has paired with the endpoint 01/29/23 04:22:59.982
    Jan 29 04:23:00.983: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:196
    STEP: Registering the webhook via the AdmissionRegistration API 01/29/23 04:23:00.992
    STEP: create a pod that should be denied by the webhook 01/29/23 04:23:01.02
    STEP: create a pod that causes the webhook to hang 01/29/23 04:23:01.043
    STEP: create a configmap that should be denied by the webhook 01/29/23 04:23:11.059
    STEP: create a configmap that should be admitted by the webhook 01/29/23 04:23:11.075
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 01/29/23 04:23:11.101
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 01/29/23 04:23:11.118
    STEP: create a namespace that bypass the webhook 01/29/23 04:23:11.128
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 01/29/23 04:23:11.143
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 29 04:23:11.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-8510" for this suite. 01/29/23 04:23:11.2
    STEP: Destroying namespace "webhook-8510-markers" for this suite. 01/29/23 04:23:11.21
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:23:11.342
Jan 29 04:23:11.342: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename downward-api 01/29/23 04:23:11.344
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:23:11.374
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:23:11.381
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
STEP: Creating a pod to test downward API volume plugin 01/29/23 04:23:11.39
Jan 29 04:23:11.423: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9c5f07f5-d19b-45f6-a0b1-0b639ea0148f" in namespace "downward-api-7554" to be "Succeeded or Failed"
Jan 29 04:23:11.433: INFO: Pod "downwardapi-volume-9c5f07f5-d19b-45f6-a0b1-0b639ea0148f": Phase="Pending", Reason="", readiness=false. Elapsed: 9.619267ms
Jan 29 04:23:13.441: INFO: Pod "downwardapi-volume-9c5f07f5-d19b-45f6-a0b1-0b639ea0148f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017939683s
Jan 29 04:23:15.442: INFO: Pod "downwardapi-volume-9c5f07f5-d19b-45f6-a0b1-0b639ea0148f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018265442s
Jan 29 04:23:17.442: INFO: Pod "downwardapi-volume-9c5f07f5-d19b-45f6-a0b1-0b639ea0148f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018289279s
STEP: Saw pod success 01/29/23 04:23:17.442
Jan 29 04:23:17.442: INFO: Pod "downwardapi-volume-9c5f07f5-d19b-45f6-a0b1-0b639ea0148f" satisfied condition "Succeeded or Failed"
Jan 29 04:23:17.448: INFO: Trying to get logs from node slave2 pod downwardapi-volume-9c5f07f5-d19b-45f6-a0b1-0b639ea0148f container client-container: <nil>
STEP: delete the pod 01/29/23 04:23:17.476
Jan 29 04:23:17.581: INFO: Waiting for pod downwardapi-volume-9c5f07f5-d19b-45f6-a0b1-0b639ea0148f to disappear
Jan 29 04:23:17.587: INFO: Pod downwardapi-volume-9c5f07f5-d19b-45f6-a0b1-0b639ea0148f no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 29 04:23:17.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7554" for this suite. 01/29/23 04:23:17.597
{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":311,"skipped":5615,"failed":0}
------------------------------
• [SLOW TEST] [6.267 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:23:11.342
    Jan 29 04:23:11.342: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename downward-api 01/29/23 04:23:11.344
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:23:11.374
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:23:11.381
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:67
    STEP: Creating a pod to test downward API volume plugin 01/29/23 04:23:11.39
    Jan 29 04:23:11.423: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9c5f07f5-d19b-45f6-a0b1-0b639ea0148f" in namespace "downward-api-7554" to be "Succeeded or Failed"
    Jan 29 04:23:11.433: INFO: Pod "downwardapi-volume-9c5f07f5-d19b-45f6-a0b1-0b639ea0148f": Phase="Pending", Reason="", readiness=false. Elapsed: 9.619267ms
    Jan 29 04:23:13.441: INFO: Pod "downwardapi-volume-9c5f07f5-d19b-45f6-a0b1-0b639ea0148f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017939683s
    Jan 29 04:23:15.442: INFO: Pod "downwardapi-volume-9c5f07f5-d19b-45f6-a0b1-0b639ea0148f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018265442s
    Jan 29 04:23:17.442: INFO: Pod "downwardapi-volume-9c5f07f5-d19b-45f6-a0b1-0b639ea0148f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018289279s
    STEP: Saw pod success 01/29/23 04:23:17.442
    Jan 29 04:23:17.442: INFO: Pod "downwardapi-volume-9c5f07f5-d19b-45f6-a0b1-0b639ea0148f" satisfied condition "Succeeded or Failed"
    Jan 29 04:23:17.448: INFO: Trying to get logs from node slave2 pod downwardapi-volume-9c5f07f5-d19b-45f6-a0b1-0b639ea0148f container client-container: <nil>
    STEP: delete the pod 01/29/23 04:23:17.476
    Jan 29 04:23:17.581: INFO: Waiting for pod downwardapi-volume-9c5f07f5-d19b-45f6-a0b1-0b639ea0148f to disappear
    Jan 29 04:23:17.587: INFO: Pod downwardapi-volume-9c5f07f5-d19b-45f6-a0b1-0b639ea0148f no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 29 04:23:17.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-7554" for this suite. 01/29/23 04:23:17.597
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:23:17.61
Jan 29 04:23:17.610: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename cronjob 01/29/23 04:23:17.612
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:23:17.633
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:23:17.639
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 01/29/23 04:23:17.645
STEP: Ensuring no jobs are scheduled 01/29/23 04:23:17.66
STEP: Ensuring no job exists by listing jobs explicitly 01/29/23 04:28:17.675
STEP: Removing cronjob 01/29/23 04:28:17.681
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jan 29 04:28:17.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-5391" for this suite. 01/29/23 04:28:17.716
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","completed":312,"skipped":5630,"failed":0}
------------------------------
• [SLOW TEST] [300.118 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:23:17.61
    Jan 29 04:23:17.610: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename cronjob 01/29/23 04:23:17.612
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:23:17.633
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:23:17.639
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 01/29/23 04:23:17.645
    STEP: Ensuring no jobs are scheduled 01/29/23 04:23:17.66
    STEP: Ensuring no job exists by listing jobs explicitly 01/29/23 04:28:17.675
    STEP: Removing cronjob 01/29/23 04:28:17.681
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jan 29 04:28:17.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-5391" for this suite. 01/29/23 04:28:17.716
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:28:17.734
Jan 29 04:28:17.734: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename projected 01/29/23 04:28:17.735
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:28:17.773
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:28:17.779
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
STEP: Creating a pod to test downward API volume plugin 01/29/23 04:28:17.785
Jan 29 04:28:17.815: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2c015ccd-41c2-4940-be3d-d87e15e1aa80" in namespace "projected-7182" to be "Succeeded or Failed"
Jan 29 04:28:17.823: INFO: Pod "downwardapi-volume-2c015ccd-41c2-4940-be3d-d87e15e1aa80": Phase="Pending", Reason="", readiness=false. Elapsed: 7.13697ms
Jan 29 04:28:19.831: INFO: Pod "downwardapi-volume-2c015ccd-41c2-4940-be3d-d87e15e1aa80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015869969s
Jan 29 04:28:21.832: INFO: Pod "downwardapi-volume-2c015ccd-41c2-4940-be3d-d87e15e1aa80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016884075s
STEP: Saw pod success 01/29/23 04:28:21.833
Jan 29 04:28:21.833: INFO: Pod "downwardapi-volume-2c015ccd-41c2-4940-be3d-d87e15e1aa80" satisfied condition "Succeeded or Failed"
Jan 29 04:28:21.842: INFO: Trying to get logs from node slave2 pod downwardapi-volume-2c015ccd-41c2-4940-be3d-d87e15e1aa80 container client-container: <nil>
STEP: delete the pod 01/29/23 04:28:21.878
Jan 29 04:28:21.995: INFO: Waiting for pod downwardapi-volume-2c015ccd-41c2-4940-be3d-d87e15e1aa80 to disappear
Jan 29 04:28:22.011: INFO: Pod downwardapi-volume-2c015ccd-41c2-4940-be3d-d87e15e1aa80 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 29 04:28:22.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7182" for this suite. 01/29/23 04:28:22.037
{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":313,"skipped":5711,"failed":0}
------------------------------
• [4.326 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:28:17.734
    Jan 29 04:28:17.734: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename projected 01/29/23 04:28:17.735
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:28:17.773
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:28:17.779
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:67
    STEP: Creating a pod to test downward API volume plugin 01/29/23 04:28:17.785
    Jan 29 04:28:17.815: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2c015ccd-41c2-4940-be3d-d87e15e1aa80" in namespace "projected-7182" to be "Succeeded or Failed"
    Jan 29 04:28:17.823: INFO: Pod "downwardapi-volume-2c015ccd-41c2-4940-be3d-d87e15e1aa80": Phase="Pending", Reason="", readiness=false. Elapsed: 7.13697ms
    Jan 29 04:28:19.831: INFO: Pod "downwardapi-volume-2c015ccd-41c2-4940-be3d-d87e15e1aa80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015869969s
    Jan 29 04:28:21.832: INFO: Pod "downwardapi-volume-2c015ccd-41c2-4940-be3d-d87e15e1aa80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016884075s
    STEP: Saw pod success 01/29/23 04:28:21.833
    Jan 29 04:28:21.833: INFO: Pod "downwardapi-volume-2c015ccd-41c2-4940-be3d-d87e15e1aa80" satisfied condition "Succeeded or Failed"
    Jan 29 04:28:21.842: INFO: Trying to get logs from node slave2 pod downwardapi-volume-2c015ccd-41c2-4940-be3d-d87e15e1aa80 container client-container: <nil>
    STEP: delete the pod 01/29/23 04:28:21.878
    Jan 29 04:28:21.995: INFO: Waiting for pod downwardapi-volume-2c015ccd-41c2-4940-be3d-d87e15e1aa80 to disappear
    Jan 29 04:28:22.011: INFO: Pod downwardapi-volume-2c015ccd-41c2-4940-be3d-d87e15e1aa80 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 29 04:28:22.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7182" for this suite. 01/29/23 04:28:22.037
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:28:22.061
Jan 29 04:28:22.061: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename projected 01/29/23 04:28:22.063
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:28:22.112
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:28:22.12
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
STEP: Creating a pod to test downward API volume plugin 01/29/23 04:28:22.132
Jan 29 04:28:22.159: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c38d5891-985a-46ca-bb81-5f2350f7e6c2" in namespace "projected-9103" to be "Succeeded or Failed"
Jan 29 04:28:22.166: INFO: Pod "downwardapi-volume-c38d5891-985a-46ca-bb81-5f2350f7e6c2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.633067ms
Jan 29 04:28:24.174: INFO: Pod "downwardapi-volume-c38d5891-985a-46ca-bb81-5f2350f7e6c2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015088824s
Jan 29 04:28:26.175: INFO: Pod "downwardapi-volume-c38d5891-985a-46ca-bb81-5f2350f7e6c2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015603166s
STEP: Saw pod success 01/29/23 04:28:26.175
Jan 29 04:28:26.175: INFO: Pod "downwardapi-volume-c38d5891-985a-46ca-bb81-5f2350f7e6c2" satisfied condition "Succeeded or Failed"
Jan 29 04:28:26.181: INFO: Trying to get logs from node slave2 pod downwardapi-volume-c38d5891-985a-46ca-bb81-5f2350f7e6c2 container client-container: <nil>
STEP: delete the pod 01/29/23 04:28:26.197
Jan 29 04:28:26.290: INFO: Waiting for pod downwardapi-volume-c38d5891-985a-46ca-bb81-5f2350f7e6c2 to disappear
Jan 29 04:28:26.297: INFO: Pod downwardapi-volume-c38d5891-985a-46ca-bb81-5f2350f7e6c2 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 29 04:28:26.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9103" for this suite. 01/29/23 04:28:26.307
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","completed":314,"skipped":5714,"failed":0}
------------------------------
• [4.255 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:28:22.061
    Jan 29 04:28:22.061: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename projected 01/29/23 04:28:22.063
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:28:22.112
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:28:22.12
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:192
    STEP: Creating a pod to test downward API volume plugin 01/29/23 04:28:22.132
    Jan 29 04:28:22.159: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c38d5891-985a-46ca-bb81-5f2350f7e6c2" in namespace "projected-9103" to be "Succeeded or Failed"
    Jan 29 04:28:22.166: INFO: Pod "downwardapi-volume-c38d5891-985a-46ca-bb81-5f2350f7e6c2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.633067ms
    Jan 29 04:28:24.174: INFO: Pod "downwardapi-volume-c38d5891-985a-46ca-bb81-5f2350f7e6c2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015088824s
    Jan 29 04:28:26.175: INFO: Pod "downwardapi-volume-c38d5891-985a-46ca-bb81-5f2350f7e6c2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015603166s
    STEP: Saw pod success 01/29/23 04:28:26.175
    Jan 29 04:28:26.175: INFO: Pod "downwardapi-volume-c38d5891-985a-46ca-bb81-5f2350f7e6c2" satisfied condition "Succeeded or Failed"
    Jan 29 04:28:26.181: INFO: Trying to get logs from node slave2 pod downwardapi-volume-c38d5891-985a-46ca-bb81-5f2350f7e6c2 container client-container: <nil>
    STEP: delete the pod 01/29/23 04:28:26.197
    Jan 29 04:28:26.290: INFO: Waiting for pod downwardapi-volume-c38d5891-985a-46ca-bb81-5f2350f7e6c2 to disappear
    Jan 29 04:28:26.297: INFO: Pod downwardapi-volume-c38d5891-985a-46ca-bb81-5f2350f7e6c2 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 29 04:28:26.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9103" for this suite. 01/29/23 04:28:26.307
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:28:26.317
Jan 29 04:28:26.317: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename resourcequota 01/29/23 04:28:26.319
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:28:26.359
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:28:26.365
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
STEP: Discovering how many secrets are in namespace by default 01/29/23 04:28:26.372
STEP: Counting existing ResourceQuota 01/29/23 04:28:31.379
STEP: Creating a ResourceQuota 01/29/23 04:28:36.387
STEP: Ensuring resource quota status is calculated 01/29/23 04:28:36.394
STEP: Creating a Secret 01/29/23 04:28:38.402
STEP: Ensuring resource quota status captures secret creation 01/29/23 04:28:38.417
STEP: Deleting a secret 01/29/23 04:28:40.424
STEP: Ensuring resource quota status released usage 01/29/23 04:28:40.435
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 29 04:28:42.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3317" for this suite. 01/29/23 04:28:42.452
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","completed":315,"skipped":5721,"failed":0}
------------------------------
• [SLOW TEST] [16.145 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:28:26.317
    Jan 29 04:28:26.317: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename resourcequota 01/29/23 04:28:26.319
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:28:26.359
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:28:26.365
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:150
    STEP: Discovering how many secrets are in namespace by default 01/29/23 04:28:26.372
    STEP: Counting existing ResourceQuota 01/29/23 04:28:31.379
    STEP: Creating a ResourceQuota 01/29/23 04:28:36.387
    STEP: Ensuring resource quota status is calculated 01/29/23 04:28:36.394
    STEP: Creating a Secret 01/29/23 04:28:38.402
    STEP: Ensuring resource quota status captures secret creation 01/29/23 04:28:38.417
    STEP: Deleting a secret 01/29/23 04:28:40.424
    STEP: Ensuring resource quota status released usage 01/29/23 04:28:40.435
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 29 04:28:42.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-3317" for this suite. 01/29/23 04:28:42.452
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:28:42.466
Jan 29 04:28:42.466: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename replicaset 01/29/23 04:28:42.467
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:28:42.491
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:28:42.496
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Jan 29 04:28:42.533: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 29 04:28:47.540: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/29/23 04:28:47.54
STEP: Scaling up "test-rs" replicaset  01/29/23 04:28:47.541
Jan 29 04:28:47.553: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 01/29/23 04:28:47.553
W0129 04:28:47.562493      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jan 29 04:28:47.573: INFO: observed ReplicaSet test-rs in namespace replicaset-1819 with ReadyReplicas 1, AvailableReplicas 1
Jan 29 04:28:47.588: INFO: observed ReplicaSet test-rs in namespace replicaset-1819 with ReadyReplicas 1, AvailableReplicas 1
Jan 29 04:28:47.609: INFO: observed ReplicaSet test-rs in namespace replicaset-1819 with ReadyReplicas 1, AvailableReplicas 1
Jan 29 04:28:47.617: INFO: observed ReplicaSet test-rs in namespace replicaset-1819 with ReadyReplicas 1, AvailableReplicas 1
Jan 29 04:28:49.397: INFO: observed ReplicaSet test-rs in namespace replicaset-1819 with ReadyReplicas 2, AvailableReplicas 2
Jan 29 04:28:49.863: INFO: observed Replicaset test-rs in namespace replicaset-1819 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jan 29 04:28:49.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1819" for this suite. 01/29/23 04:28:49.873
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","completed":316,"skipped":5772,"failed":0}
------------------------------
• [SLOW TEST] [7.416 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:28:42.466
    Jan 29 04:28:42.466: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename replicaset 01/29/23 04:28:42.467
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:28:42.491
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:28:42.496
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Jan 29 04:28:42.533: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan 29 04:28:47.540: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/29/23 04:28:47.54
    STEP: Scaling up "test-rs" replicaset  01/29/23 04:28:47.541
    Jan 29 04:28:47.553: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 01/29/23 04:28:47.553
    W0129 04:28:47.562493      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jan 29 04:28:47.573: INFO: observed ReplicaSet test-rs in namespace replicaset-1819 with ReadyReplicas 1, AvailableReplicas 1
    Jan 29 04:28:47.588: INFO: observed ReplicaSet test-rs in namespace replicaset-1819 with ReadyReplicas 1, AvailableReplicas 1
    Jan 29 04:28:47.609: INFO: observed ReplicaSet test-rs in namespace replicaset-1819 with ReadyReplicas 1, AvailableReplicas 1
    Jan 29 04:28:47.617: INFO: observed ReplicaSet test-rs in namespace replicaset-1819 with ReadyReplicas 1, AvailableReplicas 1
    Jan 29 04:28:49.397: INFO: observed ReplicaSet test-rs in namespace replicaset-1819 with ReadyReplicas 2, AvailableReplicas 2
    Jan 29 04:28:49.863: INFO: observed Replicaset test-rs in namespace replicaset-1819 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jan 29 04:28:49.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-1819" for this suite. 01/29/23 04:28:49.873
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:28:49.884
Jan 29 04:28:49.884: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename resourcequota 01/29/23 04:28:49.885
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:28:49.911
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:28:49.917
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
STEP: Creating a ResourceQuota 01/29/23 04:28:49.923
STEP: Getting a ResourceQuota 01/29/23 04:28:49.93
STEP: Listing all ResourceQuotas with LabelSelector 01/29/23 04:28:49.937
STEP: Patching the ResourceQuota 01/29/23 04:28:49.943
STEP: Deleting a Collection of ResourceQuotas 01/29/23 04:28:49.952
STEP: Verifying the deleted ResourceQuota 01/29/23 04:28:49.963
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 29 04:28:49.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1777" for this suite. 01/29/23 04:28:49.985
{"msg":"PASSED [sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]","completed":317,"skipped":5786,"failed":0}
------------------------------
• [0.140 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:28:49.884
    Jan 29 04:28:49.884: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename resourcequota 01/29/23 04:28:49.885
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:28:49.911
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:28:49.917
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:933
    STEP: Creating a ResourceQuota 01/29/23 04:28:49.923
    STEP: Getting a ResourceQuota 01/29/23 04:28:49.93
    STEP: Listing all ResourceQuotas with LabelSelector 01/29/23 04:28:49.937
    STEP: Patching the ResourceQuota 01/29/23 04:28:49.943
    STEP: Deleting a Collection of ResourceQuotas 01/29/23 04:28:49.952
    STEP: Verifying the deleted ResourceQuota 01/29/23 04:28:49.963
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 29 04:28:49.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-1777" for this suite. 01/29/23 04:28:49.985
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:28:50.029
Jan 29 04:28:50.029: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename security-context-test 01/29/23 04:28:50.03
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:28:50.127
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:28:50.133
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
Jan 29 04:28:50.157: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-e282c8c8-fb8e-4066-939d-46d8ed66a09d" in namespace "security-context-test-5028" to be "Succeeded or Failed"
Jan 29 04:28:50.165: INFO: Pod "busybox-readonly-false-e282c8c8-fb8e-4066-939d-46d8ed66a09d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.579113ms
Jan 29 04:28:52.172: INFO: Pod "busybox-readonly-false-e282c8c8-fb8e-4066-939d-46d8ed66a09d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014731601s
Jan 29 04:28:54.173: INFO: Pod "busybox-readonly-false-e282c8c8-fb8e-4066-939d-46d8ed66a09d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016162729s
Jan 29 04:28:54.174: INFO: Pod "busybox-readonly-false-e282c8c8-fb8e-4066-939d-46d8ed66a09d" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jan 29 04:28:54.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5028" for this suite. 01/29/23 04:28:54.183
{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","completed":318,"skipped":5839,"failed":0}
------------------------------
• [4.164 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:429
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:485

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:28:50.029
    Jan 29 04:28:50.029: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename security-context-test 01/29/23 04:28:50.03
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:28:50.127
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:28:50.133
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:485
    Jan 29 04:28:50.157: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-e282c8c8-fb8e-4066-939d-46d8ed66a09d" in namespace "security-context-test-5028" to be "Succeeded or Failed"
    Jan 29 04:28:50.165: INFO: Pod "busybox-readonly-false-e282c8c8-fb8e-4066-939d-46d8ed66a09d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.579113ms
    Jan 29 04:28:52.172: INFO: Pod "busybox-readonly-false-e282c8c8-fb8e-4066-939d-46d8ed66a09d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014731601s
    Jan 29 04:28:54.173: INFO: Pod "busybox-readonly-false-e282c8c8-fb8e-4066-939d-46d8ed66a09d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016162729s
    Jan 29 04:28:54.174: INFO: Pod "busybox-readonly-false-e282c8c8-fb8e-4066-939d-46d8ed66a09d" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jan 29 04:28:54.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-5028" for this suite. 01/29/23 04:28:54.183
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:28:54.194
Jan 29 04:28:54.194: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename pod-network-test 01/29/23 04:28:54.195
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:28:54.219
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:28:54.227
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-6175 01/29/23 04:28:54.232
STEP: creating a selector 01/29/23 04:28:54.232
STEP: Creating the service pods in kubernetes 01/29/23 04:28:54.232
Jan 29 04:28:54.232: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 29 04:28:54.340: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-6175" to be "running and ready"
Jan 29 04:28:54.349: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.58228ms
Jan 29 04:28:54.349: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 29 04:28:56.356: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015892869s
Jan 29 04:28:56.356: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 29 04:28:58.357: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.017018295s
Jan 29 04:28:58.357: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 29 04:29:00.357: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.01659195s
Jan 29 04:29:00.357: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 29 04:29:02.356: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.016268806s
Jan 29 04:29:02.356: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 29 04:29:04.371: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.030991367s
Jan 29 04:29:04.371: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 29 04:29:06.356: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.015520737s
Jan 29 04:29:06.356: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 29 04:29:08.358: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.017805191s
Jan 29 04:29:08.358: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 29 04:29:10.357: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.016935663s
Jan 29 04:29:10.357: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 29 04:29:12.368: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.028025318s
Jan 29 04:29:12.368: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 29 04:29:14.357: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.016396615s
Jan 29 04:29:14.357: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 29 04:29:16.356: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.015836709s
Jan 29 04:29:16.356: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan 29 04:29:16.356: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan 29 04:29:16.362: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-6175" to be "running and ready"
Jan 29 04:29:16.369: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 6.688007ms
Jan 29 04:29:16.369: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan 29 04:29:16.369: INFO: Pod "netserver-1" satisfied condition "running and ready"
Jan 29 04:29:16.375: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-6175" to be "running and ready"
Jan 29 04:29:16.382: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 7.202651ms
Jan 29 04:29:16.382: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Jan 29 04:29:16.382: INFO: Pod "netserver-2" satisfied condition "running and ready"
Jan 29 04:29:16.389: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-6175" to be "running and ready"
Jan 29 04:29:16.395: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 6.421545ms
Jan 29 04:29:16.395: INFO: The phase of Pod netserver-3 is Running (Ready = true)
Jan 29 04:29:16.395: INFO: Pod "netserver-3" satisfied condition "running and ready"
Jan 29 04:29:16.402: INFO: Waiting up to 5m0s for pod "netserver-4" in namespace "pod-network-test-6175" to be "running and ready"
Jan 29 04:29:16.408: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=true. Elapsed: 6.203304ms
Jan 29 04:29:16.408: INFO: The phase of Pod netserver-4 is Running (Ready = true)
Jan 29 04:29:16.408: INFO: Pod "netserver-4" satisfied condition "running and ready"
STEP: Creating test pods 01/29/23 04:29:16.414
Jan 29 04:29:16.441: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-6175" to be "running"
Jan 29 04:29:16.458: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 17.443462ms
Jan 29 04:29:18.467: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.025746598s
Jan 29 04:29:18.467: INFO: Pod "test-container-pod" satisfied condition "running"
Jan 29 04:29:18.473: INFO: Setting MaxTries for pod polling to 55 for networking test based on endpoint count 5
Jan 29 04:29:18.473: INFO: Breadth first check of 100.101.161.246 on host 192.168.122.241...
Jan 29 04:29:18.479: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.101.49.187:9080/dial?request=hostname&protocol=http&host=100.101.161.246&port=8083&tries=1'] Namespace:pod-network-test-6175 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 29 04:29:18.479: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
Jan 29 04:29:18.480: INFO: ExecWithOptions: Clientset creation
Jan 29 04:29:18.480: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/pod-network-test-6175/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F100.101.49.187%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D100.101.161.246%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 29 04:29:18.614: INFO: Waiting for responses: map[]
Jan 29 04:29:18.614: INFO: reached 100.101.161.246 after 0/1 tries
Jan 29 04:29:18.614: INFO: Breadth first check of 100.101.208.9 on host 192.168.122.242...
Jan 29 04:29:18.621: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.101.49.187:9080/dial?request=hostname&protocol=http&host=100.101.208.9&port=8083&tries=1'] Namespace:pod-network-test-6175 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 29 04:29:18.621: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
Jan 29 04:29:18.622: INFO: ExecWithOptions: Clientset creation
Jan 29 04:29:18.622: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/pod-network-test-6175/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F100.101.49.187%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D100.101.208.9%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 29 04:29:18.778: INFO: Waiting for responses: map[]
Jan 29 04:29:18.778: INFO: reached 100.101.208.9 after 0/1 tries
Jan 29 04:29:18.778: INFO: Breadth first check of 100.101.32.152 on host 192.168.122.243...
Jan 29 04:29:18.787: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.101.49.187:9080/dial?request=hostname&protocol=http&host=100.101.32.152&port=8083&tries=1'] Namespace:pod-network-test-6175 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 29 04:29:18.787: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
Jan 29 04:29:18.788: INFO: ExecWithOptions: Clientset creation
Jan 29 04:29:18.788: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/pod-network-test-6175/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F100.101.49.187%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D100.101.32.152%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 29 04:29:18.935: INFO: Waiting for responses: map[]
Jan 29 04:29:18.935: INFO: reached 100.101.32.152 after 0/1 tries
Jan 29 04:29:18.935: INFO: Breadth first check of 100.101.51.118 on host 192.168.122.244...
Jan 29 04:29:18.941: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.101.49.187:9080/dial?request=hostname&protocol=http&host=100.101.51.118&port=8083&tries=1'] Namespace:pod-network-test-6175 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 29 04:29:18.941: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
Jan 29 04:29:18.942: INFO: ExecWithOptions: Clientset creation
Jan 29 04:29:18.942: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/pod-network-test-6175/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F100.101.49.187%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D100.101.51.118%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 29 04:29:19.087: INFO: Waiting for responses: map[]
Jan 29 04:29:19.087: INFO: reached 100.101.51.118 after 0/1 tries
Jan 29 04:29:19.087: INFO: Breadth first check of 100.101.49.197 on host 192.168.122.245...
Jan 29 04:29:19.094: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.101.49.187:9080/dial?request=hostname&protocol=http&host=100.101.49.197&port=8083&tries=1'] Namespace:pod-network-test-6175 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 29 04:29:19.094: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
Jan 29 04:29:19.095: INFO: ExecWithOptions: Clientset creation
Jan 29 04:29:19.095: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/pod-network-test-6175/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F100.101.49.187%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D100.101.49.197%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 29 04:29:19.239: INFO: Waiting for responses: map[]
Jan 29 04:29:19.239: INFO: reached 100.101.49.197 after 0/1 tries
Jan 29 04:29:19.239: INFO: Going to retry 0 out of 5 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Jan 29 04:29:19.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6175" for this suite. 01/29/23 04:29:19.249
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","completed":319,"skipped":5846,"failed":0}
------------------------------
• [SLOW TEST] [25.066 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:28:54.194
    Jan 29 04:28:54.194: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename pod-network-test 01/29/23 04:28:54.195
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:28:54.219
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:28:54.227
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-6175 01/29/23 04:28:54.232
    STEP: creating a selector 01/29/23 04:28:54.232
    STEP: Creating the service pods in kubernetes 01/29/23 04:28:54.232
    Jan 29 04:28:54.232: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan 29 04:28:54.340: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-6175" to be "running and ready"
    Jan 29 04:28:54.349: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.58228ms
    Jan 29 04:28:54.349: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 29 04:28:56.356: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015892869s
    Jan 29 04:28:56.356: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 29 04:28:58.357: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.017018295s
    Jan 29 04:28:58.357: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 29 04:29:00.357: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.01659195s
    Jan 29 04:29:00.357: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 29 04:29:02.356: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.016268806s
    Jan 29 04:29:02.356: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 29 04:29:04.371: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.030991367s
    Jan 29 04:29:04.371: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 29 04:29:06.356: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.015520737s
    Jan 29 04:29:06.356: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 29 04:29:08.358: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.017805191s
    Jan 29 04:29:08.358: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 29 04:29:10.357: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.016935663s
    Jan 29 04:29:10.357: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 29 04:29:12.368: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.028025318s
    Jan 29 04:29:12.368: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 29 04:29:14.357: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.016396615s
    Jan 29 04:29:14.357: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 29 04:29:16.356: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.015836709s
    Jan 29 04:29:16.356: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan 29 04:29:16.356: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan 29 04:29:16.362: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-6175" to be "running and ready"
    Jan 29 04:29:16.369: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 6.688007ms
    Jan 29 04:29:16.369: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan 29 04:29:16.369: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Jan 29 04:29:16.375: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-6175" to be "running and ready"
    Jan 29 04:29:16.382: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 7.202651ms
    Jan 29 04:29:16.382: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Jan 29 04:29:16.382: INFO: Pod "netserver-2" satisfied condition "running and ready"
    Jan 29 04:29:16.389: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-6175" to be "running and ready"
    Jan 29 04:29:16.395: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 6.421545ms
    Jan 29 04:29:16.395: INFO: The phase of Pod netserver-3 is Running (Ready = true)
    Jan 29 04:29:16.395: INFO: Pod "netserver-3" satisfied condition "running and ready"
    Jan 29 04:29:16.402: INFO: Waiting up to 5m0s for pod "netserver-4" in namespace "pod-network-test-6175" to be "running and ready"
    Jan 29 04:29:16.408: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=true. Elapsed: 6.203304ms
    Jan 29 04:29:16.408: INFO: The phase of Pod netserver-4 is Running (Ready = true)
    Jan 29 04:29:16.408: INFO: Pod "netserver-4" satisfied condition "running and ready"
    STEP: Creating test pods 01/29/23 04:29:16.414
    Jan 29 04:29:16.441: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-6175" to be "running"
    Jan 29 04:29:16.458: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 17.443462ms
    Jan 29 04:29:18.467: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.025746598s
    Jan 29 04:29:18.467: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan 29 04:29:18.473: INFO: Setting MaxTries for pod polling to 55 for networking test based on endpoint count 5
    Jan 29 04:29:18.473: INFO: Breadth first check of 100.101.161.246 on host 192.168.122.241...
    Jan 29 04:29:18.479: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.101.49.187:9080/dial?request=hostname&protocol=http&host=100.101.161.246&port=8083&tries=1'] Namespace:pod-network-test-6175 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 29 04:29:18.479: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    Jan 29 04:29:18.480: INFO: ExecWithOptions: Clientset creation
    Jan 29 04:29:18.480: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/pod-network-test-6175/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F100.101.49.187%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D100.101.161.246%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 29 04:29:18.614: INFO: Waiting for responses: map[]
    Jan 29 04:29:18.614: INFO: reached 100.101.161.246 after 0/1 tries
    Jan 29 04:29:18.614: INFO: Breadth first check of 100.101.208.9 on host 192.168.122.242...
    Jan 29 04:29:18.621: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.101.49.187:9080/dial?request=hostname&protocol=http&host=100.101.208.9&port=8083&tries=1'] Namespace:pod-network-test-6175 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 29 04:29:18.621: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    Jan 29 04:29:18.622: INFO: ExecWithOptions: Clientset creation
    Jan 29 04:29:18.622: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/pod-network-test-6175/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F100.101.49.187%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D100.101.208.9%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 29 04:29:18.778: INFO: Waiting for responses: map[]
    Jan 29 04:29:18.778: INFO: reached 100.101.208.9 after 0/1 tries
    Jan 29 04:29:18.778: INFO: Breadth first check of 100.101.32.152 on host 192.168.122.243...
    Jan 29 04:29:18.787: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.101.49.187:9080/dial?request=hostname&protocol=http&host=100.101.32.152&port=8083&tries=1'] Namespace:pod-network-test-6175 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 29 04:29:18.787: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    Jan 29 04:29:18.788: INFO: ExecWithOptions: Clientset creation
    Jan 29 04:29:18.788: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/pod-network-test-6175/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F100.101.49.187%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D100.101.32.152%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 29 04:29:18.935: INFO: Waiting for responses: map[]
    Jan 29 04:29:18.935: INFO: reached 100.101.32.152 after 0/1 tries
    Jan 29 04:29:18.935: INFO: Breadth first check of 100.101.51.118 on host 192.168.122.244...
    Jan 29 04:29:18.941: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.101.49.187:9080/dial?request=hostname&protocol=http&host=100.101.51.118&port=8083&tries=1'] Namespace:pod-network-test-6175 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 29 04:29:18.941: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    Jan 29 04:29:18.942: INFO: ExecWithOptions: Clientset creation
    Jan 29 04:29:18.942: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/pod-network-test-6175/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F100.101.49.187%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D100.101.51.118%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 29 04:29:19.087: INFO: Waiting for responses: map[]
    Jan 29 04:29:19.087: INFO: reached 100.101.51.118 after 0/1 tries
    Jan 29 04:29:19.087: INFO: Breadth first check of 100.101.49.197 on host 192.168.122.245...
    Jan 29 04:29:19.094: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.101.49.187:9080/dial?request=hostname&protocol=http&host=100.101.49.197&port=8083&tries=1'] Namespace:pod-network-test-6175 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 29 04:29:19.094: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    Jan 29 04:29:19.095: INFO: ExecWithOptions: Clientset creation
    Jan 29 04:29:19.095: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/pod-network-test-6175/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F100.101.49.187%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D100.101.49.197%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 29 04:29:19.239: INFO: Waiting for responses: map[]
    Jan 29 04:29:19.239: INFO: reached 100.101.49.197 after 0/1 tries
    Jan 29 04:29:19.239: INFO: Going to retry 0 out of 5 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Jan 29 04:29:19.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-6175" for this suite. 01/29/23 04:29:19.249
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:29:19.26
Jan 29 04:29:19.261: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename services 01/29/23 04:29:19.262
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:29:19.284
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:29:19.289
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
STEP: creating service in namespace services-2515 01/29/23 04:29:19.295
STEP: creating service affinity-clusterip in namespace services-2515 01/29/23 04:29:19.295
STEP: creating replication controller affinity-clusterip in namespace services-2515 01/29/23 04:29:19.311
I0129 04:29:19.324543      22 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-2515, replica count: 3
I0129 04:29:22.375482      22 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 29 04:29:22.388: INFO: Creating new exec pod
Jan 29 04:29:22.401: INFO: Waiting up to 5m0s for pod "execpod-affinityc4vxk" in namespace "services-2515" to be "running"
Jan 29 04:29:22.408: INFO: Pod "execpod-affinityc4vxk": Phase="Pending", Reason="", readiness=false. Elapsed: 6.944028ms
Jan 29 04:29:24.416: INFO: Pod "execpod-affinityc4vxk": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015029563s
Jan 29 04:29:26.417: INFO: Pod "execpod-affinityc4vxk": Phase="Running", Reason="", readiness=true. Elapsed: 4.016052728s
Jan 29 04:29:26.417: INFO: Pod "execpod-affinityc4vxk" satisfied condition "running"
Jan 29 04:29:27.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-2515 exec execpod-affinityc4vxk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Jan 29 04:29:27.654: INFO: stderr: "+ + echonc hostName -v\n -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Jan 29 04:29:27.655: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 29 04:29:27.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-2515 exec execpod-affinityc4vxk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.105.123.107 80'
Jan 29 04:29:27.893: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.105.123.107 80\nConnection to 100.105.123.107 80 port [tcp/http] succeeded!\n"
Jan 29 04:29:27.893: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 29 04:29:27.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-2515 exec execpod-affinityc4vxk -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.105.123.107:80/ ; done'
Jan 29 04:29:28.206: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.123.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.123.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.123.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.123.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.123.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.123.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.123.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.123.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.123.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.123.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.123.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.123.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.123.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.123.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.123.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.123.107:80/\n"
Jan 29 04:29:28.206: INFO: stdout: "\naffinity-clusterip-qml94\naffinity-clusterip-qml94\naffinity-clusterip-qml94\naffinity-clusterip-qml94\naffinity-clusterip-qml94\naffinity-clusterip-qml94\naffinity-clusterip-qml94\naffinity-clusterip-qml94\naffinity-clusterip-qml94\naffinity-clusterip-qml94\naffinity-clusterip-qml94\naffinity-clusterip-qml94\naffinity-clusterip-qml94\naffinity-clusterip-qml94\naffinity-clusterip-qml94\naffinity-clusterip-qml94"
Jan 29 04:29:28.206: INFO: Received response from host: affinity-clusterip-qml94
Jan 29 04:29:28.206: INFO: Received response from host: affinity-clusterip-qml94
Jan 29 04:29:28.206: INFO: Received response from host: affinity-clusterip-qml94
Jan 29 04:29:28.206: INFO: Received response from host: affinity-clusterip-qml94
Jan 29 04:29:28.206: INFO: Received response from host: affinity-clusterip-qml94
Jan 29 04:29:28.206: INFO: Received response from host: affinity-clusterip-qml94
Jan 29 04:29:28.206: INFO: Received response from host: affinity-clusterip-qml94
Jan 29 04:29:28.206: INFO: Received response from host: affinity-clusterip-qml94
Jan 29 04:29:28.206: INFO: Received response from host: affinity-clusterip-qml94
Jan 29 04:29:28.206: INFO: Received response from host: affinity-clusterip-qml94
Jan 29 04:29:28.206: INFO: Received response from host: affinity-clusterip-qml94
Jan 29 04:29:28.206: INFO: Received response from host: affinity-clusterip-qml94
Jan 29 04:29:28.206: INFO: Received response from host: affinity-clusterip-qml94
Jan 29 04:29:28.206: INFO: Received response from host: affinity-clusterip-qml94
Jan 29 04:29:28.206: INFO: Received response from host: affinity-clusterip-qml94
Jan 29 04:29:28.206: INFO: Received response from host: affinity-clusterip-qml94
Jan 29 04:29:28.206: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-2515, will wait for the garbage collector to delete the pods 01/29/23 04:29:28.275
Jan 29 04:29:28.343: INFO: Deleting ReplicationController affinity-clusterip took: 10.090991ms
Jan 29 04:29:28.444: INFO: Terminating ReplicationController affinity-clusterip pods took: 101.262269ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 29 04:29:31.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2515" for this suite. 01/29/23 04:29:31.407
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","completed":320,"skipped":5850,"failed":0}
------------------------------
• [SLOW TEST] [12.155 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:29:19.26
    Jan 29 04:29:19.261: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename services 01/29/23 04:29:19.262
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:29:19.284
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:29:19.289
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2157
    STEP: creating service in namespace services-2515 01/29/23 04:29:19.295
    STEP: creating service affinity-clusterip in namespace services-2515 01/29/23 04:29:19.295
    STEP: creating replication controller affinity-clusterip in namespace services-2515 01/29/23 04:29:19.311
    I0129 04:29:19.324543      22 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-2515, replica count: 3
    I0129 04:29:22.375482      22 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 29 04:29:22.388: INFO: Creating new exec pod
    Jan 29 04:29:22.401: INFO: Waiting up to 5m0s for pod "execpod-affinityc4vxk" in namespace "services-2515" to be "running"
    Jan 29 04:29:22.408: INFO: Pod "execpod-affinityc4vxk": Phase="Pending", Reason="", readiness=false. Elapsed: 6.944028ms
    Jan 29 04:29:24.416: INFO: Pod "execpod-affinityc4vxk": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015029563s
    Jan 29 04:29:26.417: INFO: Pod "execpod-affinityc4vxk": Phase="Running", Reason="", readiness=true. Elapsed: 4.016052728s
    Jan 29 04:29:26.417: INFO: Pod "execpod-affinityc4vxk" satisfied condition "running"
    Jan 29 04:29:27.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-2515 exec execpod-affinityc4vxk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
    Jan 29 04:29:27.654: INFO: stderr: "+ + echonc hostName -v\n -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Jan 29 04:29:27.655: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 29 04:29:27.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-2515 exec execpod-affinityc4vxk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.105.123.107 80'
    Jan 29 04:29:27.893: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.105.123.107 80\nConnection to 100.105.123.107 80 port [tcp/http] succeeded!\n"
    Jan 29 04:29:27.893: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 29 04:29:27.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-2515 exec execpod-affinityc4vxk -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.105.123.107:80/ ; done'
    Jan 29 04:29:28.206: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.123.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.123.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.123.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.123.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.123.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.123.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.123.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.123.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.123.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.123.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.123.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.123.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.123.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.123.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.123.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.105.123.107:80/\n"
    Jan 29 04:29:28.206: INFO: stdout: "\naffinity-clusterip-qml94\naffinity-clusterip-qml94\naffinity-clusterip-qml94\naffinity-clusterip-qml94\naffinity-clusterip-qml94\naffinity-clusterip-qml94\naffinity-clusterip-qml94\naffinity-clusterip-qml94\naffinity-clusterip-qml94\naffinity-clusterip-qml94\naffinity-clusterip-qml94\naffinity-clusterip-qml94\naffinity-clusterip-qml94\naffinity-clusterip-qml94\naffinity-clusterip-qml94\naffinity-clusterip-qml94"
    Jan 29 04:29:28.206: INFO: Received response from host: affinity-clusterip-qml94
    Jan 29 04:29:28.206: INFO: Received response from host: affinity-clusterip-qml94
    Jan 29 04:29:28.206: INFO: Received response from host: affinity-clusterip-qml94
    Jan 29 04:29:28.206: INFO: Received response from host: affinity-clusterip-qml94
    Jan 29 04:29:28.206: INFO: Received response from host: affinity-clusterip-qml94
    Jan 29 04:29:28.206: INFO: Received response from host: affinity-clusterip-qml94
    Jan 29 04:29:28.206: INFO: Received response from host: affinity-clusterip-qml94
    Jan 29 04:29:28.206: INFO: Received response from host: affinity-clusterip-qml94
    Jan 29 04:29:28.206: INFO: Received response from host: affinity-clusterip-qml94
    Jan 29 04:29:28.206: INFO: Received response from host: affinity-clusterip-qml94
    Jan 29 04:29:28.206: INFO: Received response from host: affinity-clusterip-qml94
    Jan 29 04:29:28.206: INFO: Received response from host: affinity-clusterip-qml94
    Jan 29 04:29:28.206: INFO: Received response from host: affinity-clusterip-qml94
    Jan 29 04:29:28.206: INFO: Received response from host: affinity-clusterip-qml94
    Jan 29 04:29:28.206: INFO: Received response from host: affinity-clusterip-qml94
    Jan 29 04:29:28.206: INFO: Received response from host: affinity-clusterip-qml94
    Jan 29 04:29:28.206: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-2515, will wait for the garbage collector to delete the pods 01/29/23 04:29:28.275
    Jan 29 04:29:28.343: INFO: Deleting ReplicationController affinity-clusterip took: 10.090991ms
    Jan 29 04:29:28.444: INFO: Terminating ReplicationController affinity-clusterip pods took: 101.262269ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 29 04:29:31.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-2515" for this suite. 01/29/23 04:29:31.407
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:29:31.421
Jan 29 04:29:31.421: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename crd-webhook 01/29/23 04:29:31.422
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:29:31.446
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:29:31.452
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 01/29/23 04:29:31.457
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/29/23 04:29:32.456
STEP: Deploying the custom resource conversion webhook pod 01/29/23 04:29:32.47
STEP: Wait for the deployment to be ready 01/29/23 04:29:32.493
Jan 29 04:29:32.508: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/29/23 04:29:34.547
STEP: Verifying the service has paired with the endpoint 01/29/23 04:29:34.598
Jan 29 04:29:35.598: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Jan 29 04:29:35.607: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Creating a v1 custom resource 01/29/23 04:29:38.222
STEP: Create a v2 custom resource 01/29/23 04:29:38.245
STEP: List CRs in v1 01/29/23 04:29:38.308
STEP: List CRs in v2 01/29/23 04:29:38.317
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 29 04:29:38.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-8834" for this suite. 01/29/23 04:29:38.854
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","completed":321,"skipped":5915,"failed":0}
------------------------------
• [SLOW TEST] [7.575 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:29:31.421
    Jan 29 04:29:31.421: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename crd-webhook 01/29/23 04:29:31.422
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:29:31.446
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:29:31.452
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 01/29/23 04:29:31.457
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/29/23 04:29:32.456
    STEP: Deploying the custom resource conversion webhook pod 01/29/23 04:29:32.47
    STEP: Wait for the deployment to be ready 01/29/23 04:29:32.493
    Jan 29 04:29:32.508: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/29/23 04:29:34.547
    STEP: Verifying the service has paired with the endpoint 01/29/23 04:29:34.598
    Jan 29 04:29:35.598: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Jan 29 04:29:35.607: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Creating a v1 custom resource 01/29/23 04:29:38.222
    STEP: Create a v2 custom resource 01/29/23 04:29:38.245
    STEP: List CRs in v1 01/29/23 04:29:38.308
    STEP: List CRs in v2 01/29/23 04:29:38.317
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 29 04:29:38.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-8834" for this suite. 01/29/23 04:29:38.854
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:29:39.001
Jan 29 04:29:39.002: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename projected 01/29/23 04:29:39.003
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:29:39.133
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:29:39.139
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
STEP: Creating configMap with name projected-configmap-test-volume-53e7b43a-1477-4b9d-92d0-b16c69d71dcf 01/29/23 04:29:39.151
STEP: Creating a pod to test consume configMaps 01/29/23 04:29:39.158
Jan 29 04:29:39.181: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e4c93eaf-db00-4c16-aa86-e31b0c5281a7" in namespace "projected-1980" to be "Succeeded or Failed"
Jan 29 04:29:39.188: INFO: Pod "pod-projected-configmaps-e4c93eaf-db00-4c16-aa86-e31b0c5281a7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.954089ms
Jan 29 04:29:41.196: INFO: Pod "pod-projected-configmaps-e4c93eaf-db00-4c16-aa86-e31b0c5281a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014958043s
Jan 29 04:29:43.197: INFO: Pod "pod-projected-configmaps-e4c93eaf-db00-4c16-aa86-e31b0c5281a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016019388s
STEP: Saw pod success 01/29/23 04:29:43.197
Jan 29 04:29:43.197: INFO: Pod "pod-projected-configmaps-e4c93eaf-db00-4c16-aa86-e31b0c5281a7" satisfied condition "Succeeded or Failed"
Jan 29 04:29:43.203: INFO: Trying to get logs from node slave2 pod pod-projected-configmaps-e4c93eaf-db00-4c16-aa86-e31b0c5281a7 container projected-configmap-volume-test: <nil>
STEP: delete the pod 01/29/23 04:29:43.219
Jan 29 04:29:43.315: INFO: Waiting for pod pod-projected-configmaps-e4c93eaf-db00-4c16-aa86-e31b0c5281a7 to disappear
Jan 29 04:29:43.321: INFO: Pod pod-projected-configmaps-e4c93eaf-db00-4c16-aa86-e31b0c5281a7 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 29 04:29:43.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1980" for this suite. 01/29/23 04:29:43.331
{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":322,"skipped":5963,"failed":0}
------------------------------
• [4.339 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:29:39.001
    Jan 29 04:29:39.002: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename projected 01/29/23 04:29:39.003
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:29:39.133
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:29:39.139
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:374
    STEP: Creating configMap with name projected-configmap-test-volume-53e7b43a-1477-4b9d-92d0-b16c69d71dcf 01/29/23 04:29:39.151
    STEP: Creating a pod to test consume configMaps 01/29/23 04:29:39.158
    Jan 29 04:29:39.181: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e4c93eaf-db00-4c16-aa86-e31b0c5281a7" in namespace "projected-1980" to be "Succeeded or Failed"
    Jan 29 04:29:39.188: INFO: Pod "pod-projected-configmaps-e4c93eaf-db00-4c16-aa86-e31b0c5281a7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.954089ms
    Jan 29 04:29:41.196: INFO: Pod "pod-projected-configmaps-e4c93eaf-db00-4c16-aa86-e31b0c5281a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014958043s
    Jan 29 04:29:43.197: INFO: Pod "pod-projected-configmaps-e4c93eaf-db00-4c16-aa86-e31b0c5281a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016019388s
    STEP: Saw pod success 01/29/23 04:29:43.197
    Jan 29 04:29:43.197: INFO: Pod "pod-projected-configmaps-e4c93eaf-db00-4c16-aa86-e31b0c5281a7" satisfied condition "Succeeded or Failed"
    Jan 29 04:29:43.203: INFO: Trying to get logs from node slave2 pod pod-projected-configmaps-e4c93eaf-db00-4c16-aa86-e31b0c5281a7 container projected-configmap-volume-test: <nil>
    STEP: delete the pod 01/29/23 04:29:43.219
    Jan 29 04:29:43.315: INFO: Waiting for pod pod-projected-configmaps-e4c93eaf-db00-4c16-aa86-e31b0c5281a7 to disappear
    Jan 29 04:29:43.321: INFO: Pod pod-projected-configmaps-e4c93eaf-db00-4c16-aa86-e31b0c5281a7 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 29 04:29:43.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1980" for this suite. 01/29/23 04:29:43.331
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:29:43.341
Jan 29 04:29:43.341: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename security-context 01/29/23 04:29:43.342
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:29:43.364
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:29:43.369
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/29/23 04:29:43.375
Jan 29 04:29:43.397: INFO: Waiting up to 5m0s for pod "security-context-3fff9056-d51c-4c39-b4c9-d65e7a141036" in namespace "security-context-1886" to be "Succeeded or Failed"
Jan 29 04:29:43.406: INFO: Pod "security-context-3fff9056-d51c-4c39-b4c9-d65e7a141036": Phase="Pending", Reason="", readiness=false. Elapsed: 9.185984ms
Jan 29 04:29:45.415: INFO: Pod "security-context-3fff9056-d51c-4c39-b4c9-d65e7a141036": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01750528s
Jan 29 04:29:47.414: INFO: Pod "security-context-3fff9056-d51c-4c39-b4c9-d65e7a141036": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017174536s
STEP: Saw pod success 01/29/23 04:29:47.414
Jan 29 04:29:47.414: INFO: Pod "security-context-3fff9056-d51c-4c39-b4c9-d65e7a141036" satisfied condition "Succeeded or Failed"
Jan 29 04:29:47.421: INFO: Trying to get logs from node slave2 pod security-context-3fff9056-d51c-4c39-b4c9-d65e7a141036 container test-container: <nil>
STEP: delete the pod 01/29/23 04:29:47.434
Jan 29 04:29:47.526: INFO: Waiting for pod security-context-3fff9056-d51c-4c39-b4c9-d65e7a141036 to disappear
Jan 29 04:29:47.532: INFO: Pod security-context-3fff9056-d51c-4c39-b4c9-d65e7a141036 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jan 29 04:29:47.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-1886" for this suite. 01/29/23 04:29:47.542
{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":323,"skipped":5966,"failed":0}
------------------------------
• [4.212 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:29:43.341
    Jan 29 04:29:43.341: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename security-context 01/29/23 04:29:43.342
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:29:43.364
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:29:43.369
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:97
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/29/23 04:29:43.375
    Jan 29 04:29:43.397: INFO: Waiting up to 5m0s for pod "security-context-3fff9056-d51c-4c39-b4c9-d65e7a141036" in namespace "security-context-1886" to be "Succeeded or Failed"
    Jan 29 04:29:43.406: INFO: Pod "security-context-3fff9056-d51c-4c39-b4c9-d65e7a141036": Phase="Pending", Reason="", readiness=false. Elapsed: 9.185984ms
    Jan 29 04:29:45.415: INFO: Pod "security-context-3fff9056-d51c-4c39-b4c9-d65e7a141036": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01750528s
    Jan 29 04:29:47.414: INFO: Pod "security-context-3fff9056-d51c-4c39-b4c9-d65e7a141036": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017174536s
    STEP: Saw pod success 01/29/23 04:29:47.414
    Jan 29 04:29:47.414: INFO: Pod "security-context-3fff9056-d51c-4c39-b4c9-d65e7a141036" satisfied condition "Succeeded or Failed"
    Jan 29 04:29:47.421: INFO: Trying to get logs from node slave2 pod security-context-3fff9056-d51c-4c39-b4c9-d65e7a141036 container test-container: <nil>
    STEP: delete the pod 01/29/23 04:29:47.434
    Jan 29 04:29:47.526: INFO: Waiting for pod security-context-3fff9056-d51c-4c39-b4c9-d65e7a141036 to disappear
    Jan 29 04:29:47.532: INFO: Pod security-context-3fff9056-d51c-4c39-b4c9-d65e7a141036 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jan 29 04:29:47.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-1886" for this suite. 01/29/23 04:29:47.542
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:29:47.553
Jan 29 04:29:47.553: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename pod-network-test 01/29/23 04:29:47.555
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:29:47.579
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:29:47.584
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-4535 01/29/23 04:29:47.589
STEP: creating a selector 01/29/23 04:29:47.59
STEP: Creating the service pods in kubernetes 01/29/23 04:29:47.59
Jan 29 04:29:47.590: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 29 04:29:47.712: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-4535" to be "running and ready"
Jan 29 04:29:47.724: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 11.47978ms
Jan 29 04:29:47.724: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 29 04:29:49.732: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.019393054s
Jan 29 04:29:49.732: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 29 04:29:51.733: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.020547101s
Jan 29 04:29:51.733: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 29 04:29:53.732: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.019688275s
Jan 29 04:29:53.732: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 29 04:29:55.732: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.019296172s
Jan 29 04:29:55.732: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 29 04:29:57.732: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.019402152s
Jan 29 04:29:57.732: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 29 04:29:59.732: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.019899535s
Jan 29 04:29:59.732: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 29 04:30:01.742: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.030015565s
Jan 29 04:30:01.742: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 29 04:30:03.733: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.021097822s
Jan 29 04:30:03.733: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 29 04:30:05.731: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.019093147s
Jan 29 04:30:05.731: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 29 04:30:07.732: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.019657811s
Jan 29 04:30:07.732: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 29 04:30:09.732: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.019477629s
Jan 29 04:30:09.732: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan 29 04:30:09.732: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan 29 04:30:09.739: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-4535" to be "running and ready"
Jan 29 04:30:09.745: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 5.571799ms
Jan 29 04:30:09.745: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan 29 04:30:09.745: INFO: Pod "netserver-1" satisfied condition "running and ready"
Jan 29 04:30:09.750: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-4535" to be "running and ready"
Jan 29 04:30:09.756: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 5.832901ms
Jan 29 04:30:09.756: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Jan 29 04:30:09.756: INFO: Pod "netserver-2" satisfied condition "running and ready"
Jan 29 04:30:09.762: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-4535" to be "running and ready"
Jan 29 04:30:09.768: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 6.080443ms
Jan 29 04:30:09.768: INFO: The phase of Pod netserver-3 is Running (Ready = true)
Jan 29 04:30:09.768: INFO: Pod "netserver-3" satisfied condition "running and ready"
Jan 29 04:30:09.774: INFO: Waiting up to 5m0s for pod "netserver-4" in namespace "pod-network-test-4535" to be "running and ready"
Jan 29 04:30:09.781: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=true. Elapsed: 6.861448ms
Jan 29 04:30:09.781: INFO: The phase of Pod netserver-4 is Running (Ready = true)
Jan 29 04:30:09.781: INFO: Pod "netserver-4" satisfied condition "running and ready"
STEP: Creating test pods 01/29/23 04:30:09.788
Jan 29 04:30:09.815: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-4535" to be "running"
Jan 29 04:30:09.822: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.835788ms
Jan 29 04:30:11.834: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018852331s
Jan 29 04:30:13.831: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.015716429s
Jan 29 04:30:13.831: INFO: Pod "test-container-pod" satisfied condition "running"
Jan 29 04:30:13.837: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-4535" to be "running"
Jan 29 04:30:13.847: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 10.184491ms
Jan 29 04:30:13.848: INFO: Pod "host-test-container-pod" satisfied condition "running"
Jan 29 04:30:13.854: INFO: Setting MaxTries for pod polling to 55 for networking test based on endpoint count 5
Jan 29 04:30:13.854: INFO: Going to poll 100.101.161.247 on port 8083 at least 0 times, with a maximum of 55 tries before failing
Jan 29 04:30:13.860: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.101.161.247:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4535 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 29 04:30:13.860: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
Jan 29 04:30:13.861: INFO: ExecWithOptions: Clientset creation
Jan 29 04:30:13.861: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/pod-network-test-4535/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F100.101.161.247%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 29 04:30:13.999: INFO: Found all 1 expected endpoints: [netserver-0]
Jan 29 04:30:13.999: INFO: Going to poll 100.101.208.10 on port 8083 at least 0 times, with a maximum of 55 tries before failing
Jan 29 04:30:14.005: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.101.208.10:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4535 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 29 04:30:14.005: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
Jan 29 04:30:14.006: INFO: ExecWithOptions: Clientset creation
Jan 29 04:30:14.006: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/pod-network-test-4535/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F100.101.208.10%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 29 04:30:14.136: INFO: Found all 1 expected endpoints: [netserver-1]
Jan 29 04:30:14.136: INFO: Going to poll 100.101.32.153 on port 8083 at least 0 times, with a maximum of 55 tries before failing
Jan 29 04:30:14.144: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.101.32.153:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4535 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 29 04:30:14.144: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
Jan 29 04:30:14.145: INFO: ExecWithOptions: Clientset creation
Jan 29 04:30:14.145: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/pod-network-test-4535/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F100.101.32.153%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 29 04:30:14.275: INFO: Found all 1 expected endpoints: [netserver-2]
Jan 29 04:30:14.275: INFO: Going to poll 100.101.51.123 on port 8083 at least 0 times, with a maximum of 55 tries before failing
Jan 29 04:30:14.282: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.101.51.123:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4535 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 29 04:30:14.282: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
Jan 29 04:30:14.283: INFO: ExecWithOptions: Clientset creation
Jan 29 04:30:14.283: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/pod-network-test-4535/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F100.101.51.123%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 29 04:30:14.418: INFO: Found all 1 expected endpoints: [netserver-3]
Jan 29 04:30:14.418: INFO: Going to poll 100.101.49.206 on port 8083 at least 0 times, with a maximum of 55 tries before failing
Jan 29 04:30:14.425: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.101.49.206:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4535 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 29 04:30:14.425: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
Jan 29 04:30:14.426: INFO: ExecWithOptions: Clientset creation
Jan 29 04:30:14.426: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/pod-network-test-4535/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F100.101.49.206%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 29 04:30:14.559: INFO: Found all 1 expected endpoints: [netserver-4]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Jan 29 04:30:14.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4535" for this suite. 01/29/23 04:30:14.569
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","completed":324,"skipped":5968,"failed":0}
------------------------------
• [SLOW TEST] [27.026 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:29:47.553
    Jan 29 04:29:47.553: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename pod-network-test 01/29/23 04:29:47.555
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:29:47.579
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:29:47.584
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-4535 01/29/23 04:29:47.589
    STEP: creating a selector 01/29/23 04:29:47.59
    STEP: Creating the service pods in kubernetes 01/29/23 04:29:47.59
    Jan 29 04:29:47.590: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan 29 04:29:47.712: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-4535" to be "running and ready"
    Jan 29 04:29:47.724: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 11.47978ms
    Jan 29 04:29:47.724: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 29 04:29:49.732: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.019393054s
    Jan 29 04:29:49.732: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 29 04:29:51.733: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.020547101s
    Jan 29 04:29:51.733: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 29 04:29:53.732: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.019688275s
    Jan 29 04:29:53.732: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 29 04:29:55.732: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.019296172s
    Jan 29 04:29:55.732: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 29 04:29:57.732: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.019402152s
    Jan 29 04:29:57.732: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 29 04:29:59.732: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.019899535s
    Jan 29 04:29:59.732: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 29 04:30:01.742: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.030015565s
    Jan 29 04:30:01.742: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 29 04:30:03.733: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.021097822s
    Jan 29 04:30:03.733: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 29 04:30:05.731: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.019093147s
    Jan 29 04:30:05.731: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 29 04:30:07.732: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.019657811s
    Jan 29 04:30:07.732: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 29 04:30:09.732: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.019477629s
    Jan 29 04:30:09.732: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan 29 04:30:09.732: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan 29 04:30:09.739: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-4535" to be "running and ready"
    Jan 29 04:30:09.745: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 5.571799ms
    Jan 29 04:30:09.745: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan 29 04:30:09.745: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Jan 29 04:30:09.750: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-4535" to be "running and ready"
    Jan 29 04:30:09.756: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 5.832901ms
    Jan 29 04:30:09.756: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Jan 29 04:30:09.756: INFO: Pod "netserver-2" satisfied condition "running and ready"
    Jan 29 04:30:09.762: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-4535" to be "running and ready"
    Jan 29 04:30:09.768: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 6.080443ms
    Jan 29 04:30:09.768: INFO: The phase of Pod netserver-3 is Running (Ready = true)
    Jan 29 04:30:09.768: INFO: Pod "netserver-3" satisfied condition "running and ready"
    Jan 29 04:30:09.774: INFO: Waiting up to 5m0s for pod "netserver-4" in namespace "pod-network-test-4535" to be "running and ready"
    Jan 29 04:30:09.781: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=true. Elapsed: 6.861448ms
    Jan 29 04:30:09.781: INFO: The phase of Pod netserver-4 is Running (Ready = true)
    Jan 29 04:30:09.781: INFO: Pod "netserver-4" satisfied condition "running and ready"
    STEP: Creating test pods 01/29/23 04:30:09.788
    Jan 29 04:30:09.815: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-4535" to be "running"
    Jan 29 04:30:09.822: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.835788ms
    Jan 29 04:30:11.834: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018852331s
    Jan 29 04:30:13.831: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.015716429s
    Jan 29 04:30:13.831: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan 29 04:30:13.837: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-4535" to be "running"
    Jan 29 04:30:13.847: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 10.184491ms
    Jan 29 04:30:13.848: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Jan 29 04:30:13.854: INFO: Setting MaxTries for pod polling to 55 for networking test based on endpoint count 5
    Jan 29 04:30:13.854: INFO: Going to poll 100.101.161.247 on port 8083 at least 0 times, with a maximum of 55 tries before failing
    Jan 29 04:30:13.860: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.101.161.247:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4535 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 29 04:30:13.860: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    Jan 29 04:30:13.861: INFO: ExecWithOptions: Clientset creation
    Jan 29 04:30:13.861: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/pod-network-test-4535/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F100.101.161.247%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 29 04:30:13.999: INFO: Found all 1 expected endpoints: [netserver-0]
    Jan 29 04:30:13.999: INFO: Going to poll 100.101.208.10 on port 8083 at least 0 times, with a maximum of 55 tries before failing
    Jan 29 04:30:14.005: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.101.208.10:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4535 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 29 04:30:14.005: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    Jan 29 04:30:14.006: INFO: ExecWithOptions: Clientset creation
    Jan 29 04:30:14.006: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/pod-network-test-4535/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F100.101.208.10%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 29 04:30:14.136: INFO: Found all 1 expected endpoints: [netserver-1]
    Jan 29 04:30:14.136: INFO: Going to poll 100.101.32.153 on port 8083 at least 0 times, with a maximum of 55 tries before failing
    Jan 29 04:30:14.144: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.101.32.153:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4535 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 29 04:30:14.144: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    Jan 29 04:30:14.145: INFO: ExecWithOptions: Clientset creation
    Jan 29 04:30:14.145: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/pod-network-test-4535/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F100.101.32.153%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 29 04:30:14.275: INFO: Found all 1 expected endpoints: [netserver-2]
    Jan 29 04:30:14.275: INFO: Going to poll 100.101.51.123 on port 8083 at least 0 times, with a maximum of 55 tries before failing
    Jan 29 04:30:14.282: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.101.51.123:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4535 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 29 04:30:14.282: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    Jan 29 04:30:14.283: INFO: ExecWithOptions: Clientset creation
    Jan 29 04:30:14.283: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/pod-network-test-4535/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F100.101.51.123%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 29 04:30:14.418: INFO: Found all 1 expected endpoints: [netserver-3]
    Jan 29 04:30:14.418: INFO: Going to poll 100.101.49.206 on port 8083 at least 0 times, with a maximum of 55 tries before failing
    Jan 29 04:30:14.425: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.101.49.206:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4535 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 29 04:30:14.425: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    Jan 29 04:30:14.426: INFO: ExecWithOptions: Clientset creation
    Jan 29 04:30:14.426: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/pod-network-test-4535/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F100.101.49.206%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 29 04:30:14.559: INFO: Found all 1 expected endpoints: [netserver-4]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Jan 29 04:30:14.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-4535" for this suite. 01/29/23 04:30:14.569
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:30:14.58
Jan 29 04:30:14.581: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename secrets 01/29/23 04:30:14.582
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:30:14.604
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:30:14.61
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
STEP: Creating secret with name secret-test-map-91534d30-3dee-4315-9030-3fb94f00749c 01/29/23 04:30:14.615
STEP: Creating a pod to test consume secrets 01/29/23 04:30:14.622
Jan 29 04:30:14.640: INFO: Waiting up to 5m0s for pod "pod-secrets-aa8da724-22ce-4d64-9d15-e9c458a98e49" in namespace "secrets-3535" to be "Succeeded or Failed"
Jan 29 04:30:14.649: INFO: Pod "pod-secrets-aa8da724-22ce-4d64-9d15-e9c458a98e49": Phase="Pending", Reason="", readiness=false. Elapsed: 8.6049ms
Jan 29 04:30:16.657: INFO: Pod "pod-secrets-aa8da724-22ce-4d64-9d15-e9c458a98e49": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016629876s
Jan 29 04:30:18.657: INFO: Pod "pod-secrets-aa8da724-22ce-4d64-9d15-e9c458a98e49": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017145039s
Jan 29 04:30:20.679: INFO: Pod "pod-secrets-aa8da724-22ce-4d64-9d15-e9c458a98e49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039091872s
STEP: Saw pod success 01/29/23 04:30:20.679
Jan 29 04:30:20.680: INFO: Pod "pod-secrets-aa8da724-22ce-4d64-9d15-e9c458a98e49" satisfied condition "Succeeded or Failed"
Jan 29 04:30:20.687: INFO: Trying to get logs from node slave2 pod pod-secrets-aa8da724-22ce-4d64-9d15-e9c458a98e49 container secret-volume-test: <nil>
STEP: delete the pod 01/29/23 04:30:20.707
Jan 29 04:30:20.840: INFO: Waiting for pod pod-secrets-aa8da724-22ce-4d64-9d15-e9c458a98e49 to disappear
Jan 29 04:30:20.863: INFO: Pod pod-secrets-aa8da724-22ce-4d64-9d15-e9c458a98e49 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 29 04:30:20.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3535" for this suite. 01/29/23 04:30:20.883
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":325,"skipped":5989,"failed":0}
------------------------------
• [SLOW TEST] [6.315 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:30:14.58
    Jan 29 04:30:14.581: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename secrets 01/29/23 04:30:14.582
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:30:14.604
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:30:14.61
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:78
    STEP: Creating secret with name secret-test-map-91534d30-3dee-4315-9030-3fb94f00749c 01/29/23 04:30:14.615
    STEP: Creating a pod to test consume secrets 01/29/23 04:30:14.622
    Jan 29 04:30:14.640: INFO: Waiting up to 5m0s for pod "pod-secrets-aa8da724-22ce-4d64-9d15-e9c458a98e49" in namespace "secrets-3535" to be "Succeeded or Failed"
    Jan 29 04:30:14.649: INFO: Pod "pod-secrets-aa8da724-22ce-4d64-9d15-e9c458a98e49": Phase="Pending", Reason="", readiness=false. Elapsed: 8.6049ms
    Jan 29 04:30:16.657: INFO: Pod "pod-secrets-aa8da724-22ce-4d64-9d15-e9c458a98e49": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016629876s
    Jan 29 04:30:18.657: INFO: Pod "pod-secrets-aa8da724-22ce-4d64-9d15-e9c458a98e49": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017145039s
    Jan 29 04:30:20.679: INFO: Pod "pod-secrets-aa8da724-22ce-4d64-9d15-e9c458a98e49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039091872s
    STEP: Saw pod success 01/29/23 04:30:20.679
    Jan 29 04:30:20.680: INFO: Pod "pod-secrets-aa8da724-22ce-4d64-9d15-e9c458a98e49" satisfied condition "Succeeded or Failed"
    Jan 29 04:30:20.687: INFO: Trying to get logs from node slave2 pod pod-secrets-aa8da724-22ce-4d64-9d15-e9c458a98e49 container secret-volume-test: <nil>
    STEP: delete the pod 01/29/23 04:30:20.707
    Jan 29 04:30:20.840: INFO: Waiting for pod pod-secrets-aa8da724-22ce-4d64-9d15-e9c458a98e49 to disappear
    Jan 29 04:30:20.863: INFO: Pod pod-secrets-aa8da724-22ce-4d64-9d15-e9c458a98e49 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 29 04:30:20.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-3535" for this suite. 01/29/23 04:30:20.883
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:30:20.899
Jan 29 04:30:20.899: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename webhook 01/29/23 04:30:20.9
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:30:20.933
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:30:20.947
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/29/23 04:30:20.993
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/29/23 04:30:22.373
STEP: Deploying the webhook pod 01/29/23 04:30:22.387
STEP: Wait for the deployment to be ready 01/29/23 04:30:22.409
Jan 29 04:30:22.428: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/29/23 04:30:24.449
STEP: Verifying the service has paired with the endpoint 01/29/23 04:30:24.464
Jan 29 04:30:25.464: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
STEP: Listing all of the created validation webhooks 01/29/23 04:30:25.535
STEP: Creating a configMap that should be mutated 01/29/23 04:30:25.558
STEP: Deleting the collection of validation webhooks 01/29/23 04:30:25.609
STEP: Creating a configMap that should not be mutated 01/29/23 04:30:25.672
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 29 04:30:25.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3459" for this suite. 01/29/23 04:30:25.697
STEP: Destroying namespace "webhook-3459-markers" for this suite. 01/29/23 04:30:25.705
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","completed":326,"skipped":6028,"failed":0}
------------------------------
• [4.908 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:30:20.899
    Jan 29 04:30:20.899: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename webhook 01/29/23 04:30:20.9
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:30:20.933
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:30:20.947
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/29/23 04:30:20.993
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/29/23 04:30:22.373
    STEP: Deploying the webhook pod 01/29/23 04:30:22.387
    STEP: Wait for the deployment to be ready 01/29/23 04:30:22.409
    Jan 29 04:30:22.428: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/29/23 04:30:24.449
    STEP: Verifying the service has paired with the endpoint 01/29/23 04:30:24.464
    Jan 29 04:30:25.464: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:655
    STEP: Listing all of the created validation webhooks 01/29/23 04:30:25.535
    STEP: Creating a configMap that should be mutated 01/29/23 04:30:25.558
    STEP: Deleting the collection of validation webhooks 01/29/23 04:30:25.609
    STEP: Creating a configMap that should not be mutated 01/29/23 04:30:25.672
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 29 04:30:25.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-3459" for this suite. 01/29/23 04:30:25.697
    STEP: Destroying namespace "webhook-3459-markers" for this suite. 01/29/23 04:30:25.705
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:30:25.807
Jan 29 04:30:25.808: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename secrets 01/29/23 04:30:25.809
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:30:25.834
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:30:25.84
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
STEP: Creating secret with name secret-test-755997ee-b3d5-4dde-9b9f-d08a1e950319 01/29/23 04:30:25.875
STEP: Creating a pod to test consume secrets 01/29/23 04:30:25.883
Jan 29 04:30:25.906: INFO: Waiting up to 5m0s for pod "pod-secrets-b5648f59-b021-4056-ab68-7f67b927e180" in namespace "secrets-7103" to be "Succeeded or Failed"
Jan 29 04:30:25.918: INFO: Pod "pod-secrets-b5648f59-b021-4056-ab68-7f67b927e180": Phase="Pending", Reason="", readiness=false. Elapsed: 11.813103ms
Jan 29 04:30:27.925: INFO: Pod "pod-secrets-b5648f59-b021-4056-ab68-7f67b927e180": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019334355s
Jan 29 04:30:29.927: INFO: Pod "pod-secrets-b5648f59-b021-4056-ab68-7f67b927e180": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020687684s
STEP: Saw pod success 01/29/23 04:30:29.927
Jan 29 04:30:29.927: INFO: Pod "pod-secrets-b5648f59-b021-4056-ab68-7f67b927e180" satisfied condition "Succeeded or Failed"
Jan 29 04:30:29.933: INFO: Trying to get logs from node slave2 pod pod-secrets-b5648f59-b021-4056-ab68-7f67b927e180 container secret-volume-test: <nil>
STEP: delete the pod 01/29/23 04:30:29.948
Jan 29 04:30:30.041: INFO: Waiting for pod pod-secrets-b5648f59-b021-4056-ab68-7f67b927e180 to disappear
Jan 29 04:30:30.049: INFO: Pod pod-secrets-b5648f59-b021-4056-ab68-7f67b927e180 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 29 04:30:30.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7103" for this suite. 01/29/23 04:30:30.059
STEP: Destroying namespace "secret-namespace-8752" for this suite. 01/29/23 04:30:30.068
{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","completed":327,"skipped":6032,"failed":0}
------------------------------
• [4.271 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:30:25.807
    Jan 29 04:30:25.808: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename secrets 01/29/23 04:30:25.809
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:30:25.834
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:30:25.84
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:98
    STEP: Creating secret with name secret-test-755997ee-b3d5-4dde-9b9f-d08a1e950319 01/29/23 04:30:25.875
    STEP: Creating a pod to test consume secrets 01/29/23 04:30:25.883
    Jan 29 04:30:25.906: INFO: Waiting up to 5m0s for pod "pod-secrets-b5648f59-b021-4056-ab68-7f67b927e180" in namespace "secrets-7103" to be "Succeeded or Failed"
    Jan 29 04:30:25.918: INFO: Pod "pod-secrets-b5648f59-b021-4056-ab68-7f67b927e180": Phase="Pending", Reason="", readiness=false. Elapsed: 11.813103ms
    Jan 29 04:30:27.925: INFO: Pod "pod-secrets-b5648f59-b021-4056-ab68-7f67b927e180": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019334355s
    Jan 29 04:30:29.927: INFO: Pod "pod-secrets-b5648f59-b021-4056-ab68-7f67b927e180": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020687684s
    STEP: Saw pod success 01/29/23 04:30:29.927
    Jan 29 04:30:29.927: INFO: Pod "pod-secrets-b5648f59-b021-4056-ab68-7f67b927e180" satisfied condition "Succeeded or Failed"
    Jan 29 04:30:29.933: INFO: Trying to get logs from node slave2 pod pod-secrets-b5648f59-b021-4056-ab68-7f67b927e180 container secret-volume-test: <nil>
    STEP: delete the pod 01/29/23 04:30:29.948
    Jan 29 04:30:30.041: INFO: Waiting for pod pod-secrets-b5648f59-b021-4056-ab68-7f67b927e180 to disappear
    Jan 29 04:30:30.049: INFO: Pod pod-secrets-b5648f59-b021-4056-ab68-7f67b927e180 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 29 04:30:30.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-7103" for this suite. 01/29/23 04:30:30.059
    STEP: Destroying namespace "secret-namespace-8752" for this suite. 01/29/23 04:30:30.068
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:30:30.08
Jan 29 04:30:30.080: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename var-expansion 01/29/23 04:30:30.082
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:30:30.107
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:30:30.114
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
Jan 29 04:30:30.140: INFO: Waiting up to 2m0s for pod "var-expansion-0216d262-8734-41e0-b214-9d52a29c42ab" in namespace "var-expansion-5753" to be "container 0 failed with reason CreateContainerConfigError"
Jan 29 04:30:30.147: INFO: Pod "var-expansion-0216d262-8734-41e0-b214-9d52a29c42ab": Phase="Pending", Reason="", readiness=false. Elapsed: 6.291804ms
Jan 29 04:30:32.155: INFO: Pod "var-expansion-0216d262-8734-41e0-b214-9d52a29c42ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014141018s
Jan 29 04:30:32.155: INFO: Pod "var-expansion-0216d262-8734-41e0-b214-9d52a29c42ab" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Jan 29 04:30:32.155: INFO: Deleting pod "var-expansion-0216d262-8734-41e0-b214-9d52a29c42ab" in namespace "var-expansion-5753"
Jan 29 04:30:32.191: INFO: Wait up to 5m0s for pod "var-expansion-0216d262-8734-41e0-b214-9d52a29c42ab" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan 29 04:30:36.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5753" for this suite. 01/29/23 04:30:36.22
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","completed":328,"skipped":6056,"failed":0}
------------------------------
• [SLOW TEST] [6.151 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:30:30.08
    Jan 29 04:30:30.080: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename var-expansion 01/29/23 04:30:30.082
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:30:30.107
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:30:30.114
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:185
    Jan 29 04:30:30.140: INFO: Waiting up to 2m0s for pod "var-expansion-0216d262-8734-41e0-b214-9d52a29c42ab" in namespace "var-expansion-5753" to be "container 0 failed with reason CreateContainerConfigError"
    Jan 29 04:30:30.147: INFO: Pod "var-expansion-0216d262-8734-41e0-b214-9d52a29c42ab": Phase="Pending", Reason="", readiness=false. Elapsed: 6.291804ms
    Jan 29 04:30:32.155: INFO: Pod "var-expansion-0216d262-8734-41e0-b214-9d52a29c42ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014141018s
    Jan 29 04:30:32.155: INFO: Pod "var-expansion-0216d262-8734-41e0-b214-9d52a29c42ab" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Jan 29 04:30:32.155: INFO: Deleting pod "var-expansion-0216d262-8734-41e0-b214-9d52a29c42ab" in namespace "var-expansion-5753"
    Jan 29 04:30:32.191: INFO: Wait up to 5m0s for pod "var-expansion-0216d262-8734-41e0-b214-9d52a29c42ab" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan 29 04:30:36.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-5753" for this suite. 01/29/23 04:30:36.22
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:30:36.233
Jan 29 04:30:36.233: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename disruption 01/29/23 04:30:36.234
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:30:36.265
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:30:36.27
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:30:36.275
Jan 29 04:30:36.276: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename disruption-2 01/29/23 04:30:36.277
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:30:36.3
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:30:36.305
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
STEP: Waiting for the pdb to be processed 01/29/23 04:30:36.321
STEP: Waiting for the pdb to be processed 01/29/23 04:30:38.34
STEP: Waiting for the pdb to be processed 01/29/23 04:30:38.356
STEP: listing a collection of PDBs across all namespaces 01/29/23 04:30:40.368
STEP: listing a collection of PDBs in namespace disruption-9814 01/29/23 04:30:40.375
STEP: deleting a collection of PDBs 01/29/23 04:30:40.381
STEP: Waiting for the PDB collection to be deleted 01/29/23 04:30:40.402
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:187
Jan 29 04:30:40.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-1795" for this suite. 01/29/23 04:30:40.416
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jan 29 04:30:40.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-9814" for this suite. 01/29/23 04:30:40.447
{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","completed":329,"skipped":6068,"failed":0}
------------------------------
• [4.226 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:77
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:30:36.233
    Jan 29 04:30:36.233: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename disruption 01/29/23 04:30:36.234
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:30:36.265
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:30:36.27
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:30:36.275
    Jan 29 04:30:36.276: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename disruption-2 01/29/23 04:30:36.277
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:30:36.3
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:30:36.305
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:86
    STEP: Waiting for the pdb to be processed 01/29/23 04:30:36.321
    STEP: Waiting for the pdb to be processed 01/29/23 04:30:38.34
    STEP: Waiting for the pdb to be processed 01/29/23 04:30:38.356
    STEP: listing a collection of PDBs across all namespaces 01/29/23 04:30:40.368
    STEP: listing a collection of PDBs in namespace disruption-9814 01/29/23 04:30:40.375
    STEP: deleting a collection of PDBs 01/29/23 04:30:40.381
    STEP: Waiting for the PDB collection to be deleted 01/29/23 04:30:40.402
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:187
    Jan 29 04:30:40.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-2-1795" for this suite. 01/29/23 04:30:40.416
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jan 29 04:30:40.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-9814" for this suite. 01/29/23 04:30:40.447
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:30:40.463
Jan 29 04:30:40.463: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename projected 01/29/23 04:30:40.464
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:30:40.488
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:30:40.494
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
STEP: Creating the pod 01/29/23 04:30:40.499
Jan 29 04:30:40.521: INFO: Waiting up to 5m0s for pod "labelsupdate3c9a86b1-7413-41dc-ba09-cd6206fd0645" in namespace "projected-1099" to be "running and ready"
Jan 29 04:30:40.532: INFO: Pod "labelsupdate3c9a86b1-7413-41dc-ba09-cd6206fd0645": Phase="Pending", Reason="", readiness=false. Elapsed: 10.443434ms
Jan 29 04:30:40.532: INFO: The phase of Pod labelsupdate3c9a86b1-7413-41dc-ba09-cd6206fd0645 is Pending, waiting for it to be Running (with Ready = true)
Jan 29 04:30:42.539: INFO: Pod "labelsupdate3c9a86b1-7413-41dc-ba09-cd6206fd0645": Phase="Running", Reason="", readiness=true. Elapsed: 2.017538383s
Jan 29 04:30:42.539: INFO: The phase of Pod labelsupdate3c9a86b1-7413-41dc-ba09-cd6206fd0645 is Running (Ready = true)
Jan 29 04:30:42.539: INFO: Pod "labelsupdate3c9a86b1-7413-41dc-ba09-cd6206fd0645" satisfied condition "running and ready"
Jan 29 04:30:43.086: INFO: Successfully updated pod "labelsupdate3c9a86b1-7413-41dc-ba09-cd6206fd0645"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 29 04:30:45.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1099" for this suite. 01/29/23 04:30:45.164
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","completed":330,"skipped":6122,"failed":0}
------------------------------
• [4.711 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:30:40.463
    Jan 29 04:30:40.463: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename projected 01/29/23 04:30:40.464
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:30:40.488
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:30:40.494
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:129
    STEP: Creating the pod 01/29/23 04:30:40.499
    Jan 29 04:30:40.521: INFO: Waiting up to 5m0s for pod "labelsupdate3c9a86b1-7413-41dc-ba09-cd6206fd0645" in namespace "projected-1099" to be "running and ready"
    Jan 29 04:30:40.532: INFO: Pod "labelsupdate3c9a86b1-7413-41dc-ba09-cd6206fd0645": Phase="Pending", Reason="", readiness=false. Elapsed: 10.443434ms
    Jan 29 04:30:40.532: INFO: The phase of Pod labelsupdate3c9a86b1-7413-41dc-ba09-cd6206fd0645 is Pending, waiting for it to be Running (with Ready = true)
    Jan 29 04:30:42.539: INFO: Pod "labelsupdate3c9a86b1-7413-41dc-ba09-cd6206fd0645": Phase="Running", Reason="", readiness=true. Elapsed: 2.017538383s
    Jan 29 04:30:42.539: INFO: The phase of Pod labelsupdate3c9a86b1-7413-41dc-ba09-cd6206fd0645 is Running (Ready = true)
    Jan 29 04:30:42.539: INFO: Pod "labelsupdate3c9a86b1-7413-41dc-ba09-cd6206fd0645" satisfied condition "running and ready"
    Jan 29 04:30:43.086: INFO: Successfully updated pod "labelsupdate3c9a86b1-7413-41dc-ba09-cd6206fd0645"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 29 04:30:45.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1099" for this suite. 01/29/23 04:30:45.164
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:30:45.175
Jan 29 04:30:45.175: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename var-expansion 01/29/23 04:30:45.176
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:30:45.202
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:30:45.207
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
STEP: creating the pod with failed condition 01/29/23 04:30:45.211
Jan 29 04:30:45.228: INFO: Waiting up to 2m0s for pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513" in namespace "var-expansion-1488" to be "running"
Jan 29 04:30:45.235: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 6.405525ms
Jan 29 04:30:47.243: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014412521s
Jan 29 04:30:49.242: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013392353s
Jan 29 04:30:51.243: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 6.014336959s
Jan 29 04:30:53.243: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 8.014408679s
Jan 29 04:30:55.244: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 10.015566346s
Jan 29 04:30:57.243: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 12.0147183s
Jan 29 04:30:59.245: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 14.016635073s
Jan 29 04:31:01.243: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 16.014515397s
Jan 29 04:31:03.242: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 18.01347767s
Jan 29 04:31:05.244: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 20.015843106s
Jan 29 04:31:07.243: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 22.014627516s
Jan 29 04:31:09.265: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 24.036285807s
Jan 29 04:31:11.250: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 26.021475963s
Jan 29 04:31:13.243: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 28.014698275s
Jan 29 04:31:15.243: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 30.014308912s
Jan 29 04:31:17.242: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 32.013748107s
Jan 29 04:31:19.244: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 34.015876022s
Jan 29 04:31:21.243: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 36.014452471s
Jan 29 04:31:23.243: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 38.014772013s
Jan 29 04:31:25.242: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 40.013501383s
Jan 29 04:31:27.242: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 42.014024086s
Jan 29 04:31:29.242: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 44.013652043s
Jan 29 04:31:31.243: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 46.015108653s
Jan 29 04:31:33.243: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 48.014680529s
Jan 29 04:31:35.242: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 50.013656022s
Jan 29 04:31:37.242: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 52.01355596s
Jan 29 04:31:39.243: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 54.014566887s
Jan 29 04:31:41.244: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 56.015860415s
Jan 29 04:31:43.242: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 58.013429158s
Jan 29 04:31:45.242: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.013535778s
Jan 29 04:31:47.243: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.014273282s
Jan 29 04:31:49.243: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.015013647s
Jan 29 04:31:51.244: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.01558425s
Jan 29 04:31:53.243: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.014490682s
Jan 29 04:31:55.243: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.014380781s
Jan 29 04:31:57.243: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.014281159s
Jan 29 04:31:59.248: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.019811365s
Jan 29 04:32:01.242: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.013691392s
Jan 29 04:32:03.244: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.01621442s
Jan 29 04:32:05.243: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.014886521s
Jan 29 04:32:07.243: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.015137613s
Jan 29 04:32:09.243: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.014956442s
Jan 29 04:32:11.243: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.014572549s
Jan 29 04:32:13.246: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.017822802s
Jan 29 04:32:15.242: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.013908425s
Jan 29 04:32:17.243: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.014976503s
Jan 29 04:32:19.251: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.023167931s
Jan 29 04:32:21.242: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.014233678s
Jan 29 04:32:23.244: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.016104582s
Jan 29 04:32:25.245: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.016247193s
Jan 29 04:32:27.242: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.013887467s
Jan 29 04:32:29.241: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.013166112s
Jan 29 04:32:31.243: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.014723833s
Jan 29 04:32:33.243: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.014975985s
Jan 29 04:32:35.242: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.013701086s
Jan 29 04:32:37.243: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.014366081s
Jan 29 04:32:39.244: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.016170904s
Jan 29 04:32:41.243: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.014797564s
Jan 29 04:32:43.242: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.01411267s
Jan 29 04:32:45.242: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.013707117s
Jan 29 04:32:45.248: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.01982284s
STEP: updating the pod 01/29/23 04:32:45.248
Jan 29 04:32:45.774: INFO: Successfully updated pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513"
STEP: waiting for pod running 01/29/23 04:32:45.774
Jan 29 04:32:45.774: INFO: Waiting up to 2m0s for pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513" in namespace "var-expansion-1488" to be "running"
Jan 29 04:32:45.783: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 8.67092ms
Jan 29 04:32:47.794: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Running", Reason="", readiness=true. Elapsed: 2.019869009s
Jan 29 04:32:47.794: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513" satisfied condition "running"
STEP: deleting the pod gracefully 01/29/23 04:32:47.795
Jan 29 04:32:47.795: INFO: Deleting pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513" in namespace "var-expansion-1488"
Jan 29 04:32:47.840: INFO: Wait up to 5m0s for pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan 29 04:33:19.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1488" for this suite. 01/29/23 04:33:19.865
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","completed":331,"skipped":6140,"failed":0}
------------------------------
• [SLOW TEST] [154.699 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:30:45.175
    Jan 29 04:30:45.175: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename var-expansion 01/29/23 04:30:45.176
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:30:45.202
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:30:45.207
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:224
    STEP: creating the pod with failed condition 01/29/23 04:30:45.211
    Jan 29 04:30:45.228: INFO: Waiting up to 2m0s for pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513" in namespace "var-expansion-1488" to be "running"
    Jan 29 04:30:45.235: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 6.405525ms
    Jan 29 04:30:47.243: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014412521s
    Jan 29 04:30:49.242: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013392353s
    Jan 29 04:30:51.243: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 6.014336959s
    Jan 29 04:30:53.243: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 8.014408679s
    Jan 29 04:30:55.244: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 10.015566346s
    Jan 29 04:30:57.243: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 12.0147183s
    Jan 29 04:30:59.245: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 14.016635073s
    Jan 29 04:31:01.243: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 16.014515397s
    Jan 29 04:31:03.242: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 18.01347767s
    Jan 29 04:31:05.244: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 20.015843106s
    Jan 29 04:31:07.243: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 22.014627516s
    Jan 29 04:31:09.265: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 24.036285807s
    Jan 29 04:31:11.250: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 26.021475963s
    Jan 29 04:31:13.243: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 28.014698275s
    Jan 29 04:31:15.243: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 30.014308912s
    Jan 29 04:31:17.242: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 32.013748107s
    Jan 29 04:31:19.244: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 34.015876022s
    Jan 29 04:31:21.243: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 36.014452471s
    Jan 29 04:31:23.243: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 38.014772013s
    Jan 29 04:31:25.242: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 40.013501383s
    Jan 29 04:31:27.242: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 42.014024086s
    Jan 29 04:31:29.242: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 44.013652043s
    Jan 29 04:31:31.243: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 46.015108653s
    Jan 29 04:31:33.243: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 48.014680529s
    Jan 29 04:31:35.242: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 50.013656022s
    Jan 29 04:31:37.242: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 52.01355596s
    Jan 29 04:31:39.243: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 54.014566887s
    Jan 29 04:31:41.244: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 56.015860415s
    Jan 29 04:31:43.242: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 58.013429158s
    Jan 29 04:31:45.242: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.013535778s
    Jan 29 04:31:47.243: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.014273282s
    Jan 29 04:31:49.243: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.015013647s
    Jan 29 04:31:51.244: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.01558425s
    Jan 29 04:31:53.243: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.014490682s
    Jan 29 04:31:55.243: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.014380781s
    Jan 29 04:31:57.243: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.014281159s
    Jan 29 04:31:59.248: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.019811365s
    Jan 29 04:32:01.242: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.013691392s
    Jan 29 04:32:03.244: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.01621442s
    Jan 29 04:32:05.243: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.014886521s
    Jan 29 04:32:07.243: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.015137613s
    Jan 29 04:32:09.243: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.014956442s
    Jan 29 04:32:11.243: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.014572549s
    Jan 29 04:32:13.246: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.017822802s
    Jan 29 04:32:15.242: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.013908425s
    Jan 29 04:32:17.243: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.014976503s
    Jan 29 04:32:19.251: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.023167931s
    Jan 29 04:32:21.242: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.014233678s
    Jan 29 04:32:23.244: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.016104582s
    Jan 29 04:32:25.245: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.016247193s
    Jan 29 04:32:27.242: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.013887467s
    Jan 29 04:32:29.241: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.013166112s
    Jan 29 04:32:31.243: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.014723833s
    Jan 29 04:32:33.243: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.014975985s
    Jan 29 04:32:35.242: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.013701086s
    Jan 29 04:32:37.243: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.014366081s
    Jan 29 04:32:39.244: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.016170904s
    Jan 29 04:32:41.243: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.014797564s
    Jan 29 04:32:43.242: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.01411267s
    Jan 29 04:32:45.242: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.013707117s
    Jan 29 04:32:45.248: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.01982284s
    STEP: updating the pod 01/29/23 04:32:45.248
    Jan 29 04:32:45.774: INFO: Successfully updated pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513"
    STEP: waiting for pod running 01/29/23 04:32:45.774
    Jan 29 04:32:45.774: INFO: Waiting up to 2m0s for pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513" in namespace "var-expansion-1488" to be "running"
    Jan 29 04:32:45.783: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Pending", Reason="", readiness=false. Elapsed: 8.67092ms
    Jan 29 04:32:47.794: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513": Phase="Running", Reason="", readiness=true. Elapsed: 2.019869009s
    Jan 29 04:32:47.794: INFO: Pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513" satisfied condition "running"
    STEP: deleting the pod gracefully 01/29/23 04:32:47.795
    Jan 29 04:32:47.795: INFO: Deleting pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513" in namespace "var-expansion-1488"
    Jan 29 04:32:47.840: INFO: Wait up to 5m0s for pod "var-expansion-021fea47-aa45-43c7-830c-0e528b8cb513" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan 29 04:33:19.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-1488" for this suite. 01/29/23 04:33:19.865
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:33:19.875
Jan 29 04:33:19.875: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename deployment 01/29/23 04:33:19.877
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:33:19.912
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:33:19.918
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Jan 29 04:33:19.923: INFO: Creating deployment "test-recreate-deployment"
Jan 29 04:33:19.936: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jan 29 04:33:19.948: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
Jan 29 04:33:21.969: INFO: Waiting deployment "test-recreate-deployment" to complete
Jan 29 04:33:21.984: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jan 29 04:33:22.002: INFO: Updating deployment test-recreate-deployment
Jan 29 04:33:22.002: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 29 04:33:22.178: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-6776  54f9bf66-da77-4e35-a052-46ed730b30e4 5988312 2 2023-01-29 04:33:19 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/revision:2] [] [] []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0x40044029b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-29 04:33:22 +0000 UTC,LastTransitionTime:2023-01-29 04:33:22 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-9d58999df" is progressing.,LastUpdateTime:2023-01-29 04:33:22 +0000 UTC,LastTransitionTime:2023-01-29 04:33:19 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Jan 29 04:33:22.184: INFO: New ReplicaSet "test-recreate-deployment-9d58999df" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-9d58999df  deployment-6776  1474cce3-155e-4607-bfc4-8ed53f44e0e3 5988309 1 2023-01-29 04:33:22 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 54f9bf66-da77-4e35-a052-46ed730b30e4 0x4004402e9e 0x4004402e9f}] [] []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 9d58999df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0x4004402f58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 29 04:33:22.185: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jan 29 04:33:22.185: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d8b6f647f  deployment-6776  8b035ffd-4868-4f86-a54a-a90b3c213c30 5988301 2 2023-01-29 04:33:19 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 54f9bf66-da77-4e35-a052-46ed730b30e4 0x4004402d8e 0x4004402d8f}] [] []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d8b6f647f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0x4004402e38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 29 04:33:22.192: INFO: Pod "test-recreate-deployment-9d58999df-8mstr" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-9d58999df-8mstr test-recreate-deployment-9d58999df- deployment-6776  6f33c5cd-6f40-43bf-9096-1462e7b82c48 5988313 0 2023-01-29 04:33:22 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [{apps/v1 ReplicaSet test-recreate-deployment-9d58999df 1474cce3-155e-4607-bfc4-8ed53f44e0e3 0x4004403410 0x4004403411}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vbskc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vbskc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 04:33:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 04:33:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 04:33:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 04:33:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.245,PodIP:,StartTime:2023-01-29 04:33:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan 29 04:33:22.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6776" for this suite. 01/29/23 04:33:22.201
{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","completed":332,"skipped":6142,"failed":0}
------------------------------
• [2.335 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:33:19.875
    Jan 29 04:33:19.875: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename deployment 01/29/23 04:33:19.877
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:33:19.912
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:33:19.918
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Jan 29 04:33:19.923: INFO: Creating deployment "test-recreate-deployment"
    Jan 29 04:33:19.936: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Jan 29 04:33:19.948: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
    Jan 29 04:33:21.969: INFO: Waiting deployment "test-recreate-deployment" to complete
    Jan 29 04:33:21.984: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Jan 29 04:33:22.002: INFO: Updating deployment test-recreate-deployment
    Jan 29 04:33:22.002: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 29 04:33:22.178: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-6776  54f9bf66-da77-4e35-a052-46ed730b30e4 5988312 2 2023-01-29 04:33:19 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/revision:2] [] [] []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0x40044029b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-29 04:33:22 +0000 UTC,LastTransitionTime:2023-01-29 04:33:22 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-9d58999df" is progressing.,LastUpdateTime:2023-01-29 04:33:22 +0000 UTC,LastTransitionTime:2023-01-29 04:33:19 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Jan 29 04:33:22.184: INFO: New ReplicaSet "test-recreate-deployment-9d58999df" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-9d58999df  deployment-6776  1474cce3-155e-4607-bfc4-8ed53f44e0e3 5988309 1 2023-01-29 04:33:22 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 54f9bf66-da77-4e35-a052-46ed730b30e4 0x4004402e9e 0x4004402e9f}] [] []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 9d58999df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0x4004402f58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 29 04:33:22.185: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Jan 29 04:33:22.185: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d8b6f647f  deployment-6776  8b035ffd-4868-4f86-a54a-a90b3c213c30 5988301 2 2023-01-29 04:33:19 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[cke-admission.inspur.com/protect:true cke-admission.inspur.com/status:mutated deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 54f9bf66-da77-4e35-a052-46ed730b30e4 0x4004402d8e 0x4004402d8f}] [] []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d8b6f647f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0x4004402e38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 29 04:33:22.192: INFO: Pod "test-recreate-deployment-9d58999df-8mstr" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-9d58999df-8mstr test-recreate-deployment-9d58999df- deployment-6776  6f33c5cd-6f40-43bf-9096-1462e7b82c48 5988313 0 2023-01-29 04:33:22 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [{apps/v1 ReplicaSet test-recreate-deployment-9d58999df 1474cce3-155e-4607-bfc4-8ed53f44e0e3 0x4004403410 0x4004403411}] [] []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vbskc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vbskc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 04:33:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 04:33:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 04:33:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-29 04:33:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.245,PodIP:,StartTime:2023-01-29 04:33:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan 29 04:33:22.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-6776" for this suite. 01/29/23 04:33:22.201
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:33:22.211
Jan 29 04:33:22.211: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename projected 01/29/23 04:33:22.213
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:33:22.235
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:33:22.241
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
STEP: Creating configMap with name configmap-projected-all-test-volume-5367aecf-a25d-4175-873d-334fd46e790e 01/29/23 04:33:22.246
STEP: Creating secret with name secret-projected-all-test-volume-1d631093-5288-45c1-b383-41cdeb424c26 01/29/23 04:33:22.253
STEP: Creating a pod to test Check all projections for projected volume plugin 01/29/23 04:33:22.26
Jan 29 04:33:22.278: INFO: Waiting up to 5m0s for pod "projected-volume-b01a40cf-38eb-4556-a2f5-3757b6ccbf7d" in namespace "projected-8768" to be "Succeeded or Failed"
Jan 29 04:33:22.284: INFO: Pod "projected-volume-b01a40cf-38eb-4556-a2f5-3757b6ccbf7d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.830521ms
Jan 29 04:33:24.291: INFO: Pod "projected-volume-b01a40cf-38eb-4556-a2f5-3757b6ccbf7d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013198343s
Jan 29 04:33:26.291: INFO: Pod "projected-volume-b01a40cf-38eb-4556-a2f5-3757b6ccbf7d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012818971s
Jan 29 04:33:28.291: INFO: Pod "projected-volume-b01a40cf-38eb-4556-a2f5-3757b6ccbf7d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012811821s
STEP: Saw pod success 01/29/23 04:33:28.291
Jan 29 04:33:28.291: INFO: Pod "projected-volume-b01a40cf-38eb-4556-a2f5-3757b6ccbf7d" satisfied condition "Succeeded or Failed"
Jan 29 04:33:28.297: INFO: Trying to get logs from node slave2 pod projected-volume-b01a40cf-38eb-4556-a2f5-3757b6ccbf7d container projected-all-volume-test: <nil>
STEP: delete the pod 01/29/23 04:33:28.326
Jan 29 04:33:28.392: INFO: Waiting for pod projected-volume-b01a40cf-38eb-4556-a2f5-3757b6ccbf7d to disappear
Jan 29 04:33:28.397: INFO: Pod projected-volume-b01a40cf-38eb-4556-a2f5-3757b6ccbf7d no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:187
Jan 29 04:33:28.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8768" for this suite. 01/29/23 04:33:28.406
{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","completed":333,"skipped":6143,"failed":0}
------------------------------
• [SLOW TEST] [6.203 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:33:22.211
    Jan 29 04:33:22.211: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename projected 01/29/23 04:33:22.213
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:33:22.235
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:33:22.241
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:43
    STEP: Creating configMap with name configmap-projected-all-test-volume-5367aecf-a25d-4175-873d-334fd46e790e 01/29/23 04:33:22.246
    STEP: Creating secret with name secret-projected-all-test-volume-1d631093-5288-45c1-b383-41cdeb424c26 01/29/23 04:33:22.253
    STEP: Creating a pod to test Check all projections for projected volume plugin 01/29/23 04:33:22.26
    Jan 29 04:33:22.278: INFO: Waiting up to 5m0s for pod "projected-volume-b01a40cf-38eb-4556-a2f5-3757b6ccbf7d" in namespace "projected-8768" to be "Succeeded or Failed"
    Jan 29 04:33:22.284: INFO: Pod "projected-volume-b01a40cf-38eb-4556-a2f5-3757b6ccbf7d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.830521ms
    Jan 29 04:33:24.291: INFO: Pod "projected-volume-b01a40cf-38eb-4556-a2f5-3757b6ccbf7d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013198343s
    Jan 29 04:33:26.291: INFO: Pod "projected-volume-b01a40cf-38eb-4556-a2f5-3757b6ccbf7d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012818971s
    Jan 29 04:33:28.291: INFO: Pod "projected-volume-b01a40cf-38eb-4556-a2f5-3757b6ccbf7d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012811821s
    STEP: Saw pod success 01/29/23 04:33:28.291
    Jan 29 04:33:28.291: INFO: Pod "projected-volume-b01a40cf-38eb-4556-a2f5-3757b6ccbf7d" satisfied condition "Succeeded or Failed"
    Jan 29 04:33:28.297: INFO: Trying to get logs from node slave2 pod projected-volume-b01a40cf-38eb-4556-a2f5-3757b6ccbf7d container projected-all-volume-test: <nil>
    STEP: delete the pod 01/29/23 04:33:28.326
    Jan 29 04:33:28.392: INFO: Waiting for pod projected-volume-b01a40cf-38eb-4556-a2f5-3757b6ccbf7d to disappear
    Jan 29 04:33:28.397: INFO: Pod projected-volume-b01a40cf-38eb-4556-a2f5-3757b6ccbf7d no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:187
    Jan 29 04:33:28.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8768" for this suite. 01/29/23 04:33:28.406
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:33:28.415
Jan 29 04:33:28.415: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename projected 01/29/23 04:33:28.416
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:33:28.437
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:33:28.442
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
STEP: Creating a pod to test downward API volume plugin 01/29/23 04:33:28.449
Jan 29 04:33:28.468: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3dee88d6-4e92-48fb-a5b1-23a3990a081a" in namespace "projected-5322" to be "Succeeded or Failed"
Jan 29 04:33:28.476: INFO: Pod "downwardapi-volume-3dee88d6-4e92-48fb-a5b1-23a3990a081a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.758394ms
Jan 29 04:33:30.484: INFO: Pod "downwardapi-volume-3dee88d6-4e92-48fb-a5b1-23a3990a081a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01563908s
Jan 29 04:33:32.485: INFO: Pod "downwardapi-volume-3dee88d6-4e92-48fb-a5b1-23a3990a081a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016985819s
STEP: Saw pod success 01/29/23 04:33:32.485
Jan 29 04:33:32.485: INFO: Pod "downwardapi-volume-3dee88d6-4e92-48fb-a5b1-23a3990a081a" satisfied condition "Succeeded or Failed"
Jan 29 04:33:32.492: INFO: Trying to get logs from node slave2 pod downwardapi-volume-3dee88d6-4e92-48fb-a5b1-23a3990a081a container client-container: <nil>
STEP: delete the pod 01/29/23 04:33:32.507
Jan 29 04:33:32.578: INFO: Waiting for pod downwardapi-volume-3dee88d6-4e92-48fb-a5b1-23a3990a081a to disappear
Jan 29 04:33:32.583: INFO: Pod downwardapi-volume-3dee88d6-4e92-48fb-a5b1-23a3990a081a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 29 04:33:32.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5322" for this suite. 01/29/23 04:33:32.593
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","completed":334,"skipped":6144,"failed":0}
------------------------------
• [4.188 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:33:28.415
    Jan 29 04:33:28.415: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename projected 01/29/23 04:33:28.416
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:33:28.437
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:33:28.442
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:206
    STEP: Creating a pod to test downward API volume plugin 01/29/23 04:33:28.449
    Jan 29 04:33:28.468: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3dee88d6-4e92-48fb-a5b1-23a3990a081a" in namespace "projected-5322" to be "Succeeded or Failed"
    Jan 29 04:33:28.476: INFO: Pod "downwardapi-volume-3dee88d6-4e92-48fb-a5b1-23a3990a081a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.758394ms
    Jan 29 04:33:30.484: INFO: Pod "downwardapi-volume-3dee88d6-4e92-48fb-a5b1-23a3990a081a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01563908s
    Jan 29 04:33:32.485: INFO: Pod "downwardapi-volume-3dee88d6-4e92-48fb-a5b1-23a3990a081a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016985819s
    STEP: Saw pod success 01/29/23 04:33:32.485
    Jan 29 04:33:32.485: INFO: Pod "downwardapi-volume-3dee88d6-4e92-48fb-a5b1-23a3990a081a" satisfied condition "Succeeded or Failed"
    Jan 29 04:33:32.492: INFO: Trying to get logs from node slave2 pod downwardapi-volume-3dee88d6-4e92-48fb-a5b1-23a3990a081a container client-container: <nil>
    STEP: delete the pod 01/29/23 04:33:32.507
    Jan 29 04:33:32.578: INFO: Waiting for pod downwardapi-volume-3dee88d6-4e92-48fb-a5b1-23a3990a081a to disappear
    Jan 29 04:33:32.583: INFO: Pod downwardapi-volume-3dee88d6-4e92-48fb-a5b1-23a3990a081a no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 29 04:33:32.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5322" for this suite. 01/29/23 04:33:32.593
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:33:32.605
Jan 29 04:33:32.605: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename crd-publish-openapi 01/29/23 04:33:32.606
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:33:32.631
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:33:32.637
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 01/29/23 04:33:32.643
Jan 29 04:33:32.644: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
Jan 29 04:33:37.778: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 29 04:34:01.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9808" for this suite. 01/29/23 04:34:01.734
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","completed":335,"skipped":6175,"failed":0}
------------------------------
• [SLOW TEST] [29.140 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:33:32.605
    Jan 29 04:33:32.605: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename crd-publish-openapi 01/29/23 04:33:32.606
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:33:32.631
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:33:32.637
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:356
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 01/29/23 04:33:32.643
    Jan 29 04:33:32.644: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    Jan 29 04:33:37.778: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 29 04:34:01.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-9808" for this suite. 01/29/23 04:34:01.734
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:34:01.746
Jan 29 04:34:01.747: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename lease-test 01/29/23 04:34:01.748
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:34:01.771
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:34:01.777
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/framework.go:187
Jan 29 04:34:01.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-3404" for this suite. 01/29/23 04:34:01.928
{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","completed":336,"skipped":6194,"failed":0}
------------------------------
• [0.193 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:34:01.746
    Jan 29 04:34:01.747: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename lease-test 01/29/23 04:34:01.748
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:34:01.771
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:34:01.777
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/framework.go:187
    Jan 29 04:34:01.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "lease-test-3404" for this suite. 01/29/23 04:34:01.928
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:34:01.942
Jan 29 04:34:01.942: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename services 01/29/23 04:34:01.943
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:34:01.975
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:34:01.981
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237
STEP: creating service in namespace services-6440 01/29/23 04:34:01.986
STEP: creating service affinity-nodeport-transition in namespace services-6440 01/29/23 04:34:01.986
STEP: creating replication controller affinity-nodeport-transition in namespace services-6440 01/29/23 04:34:02.026
I0129 04:34:02.038976      22 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-6440, replica count: 3
I0129 04:34:05.090794      22 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 29 04:34:05.113: INFO: Creating new exec pod
Jan 29 04:34:05.126: INFO: Waiting up to 5m0s for pod "execpod-affinity6l8zl" in namespace "services-6440" to be "running"
Jan 29 04:34:05.133: INFO: Pod "execpod-affinity6l8zl": Phase="Pending", Reason="", readiness=false. Elapsed: 6.710627ms
Jan 29 04:34:07.140: INFO: Pod "execpod-affinity6l8zl": Phase="Running", Reason="", readiness=true. Elapsed: 2.014128989s
Jan 29 04:34:07.140: INFO: Pod "execpod-affinity6l8zl" satisfied condition "running"
Jan 29 04:34:08.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-6440 exec execpod-affinity6l8zl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Jan 29 04:34:08.390: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Jan 29 04:34:08.390: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 29 04:34:08.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-6440 exec execpod-affinity6l8zl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.105.138.162 80'
Jan 29 04:34:08.647: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.105.138.162 80\nConnection to 100.105.138.162 80 port [tcp/http] succeeded!\n"
Jan 29 04:34:08.647: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 29 04:34:08.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-6440 exec execpod-affinity6l8zl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.122.245 31733'
Jan 29 04:34:08.882: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.122.245 31733\nConnection to 192.168.122.245 31733 port [tcp/*] succeeded!\n"
Jan 29 04:34:08.882: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 29 04:34:08.882: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-6440 exec execpod-affinity6l8zl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.122.244 31733'
Jan 29 04:34:09.131: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.122.244 31733\nConnection to 192.168.122.244 31733 port [tcp/*] succeeded!\n"
Jan 29 04:34:09.132: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 29 04:34:09.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-6440 exec execpod-affinity6l8zl -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.122.241:31733/ ; done'
Jan 29 04:34:09.502: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:31733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:31733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:31733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:31733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:31733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:31733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:31733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:31733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:31733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:31733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:31733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:31733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:31733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:31733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:31733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:31733/\n"
Jan 29 04:34:09.502: INFO: stdout: "\naffinity-nodeport-transition-9z4d8\naffinity-nodeport-transition-9z4d8\naffinity-nodeport-transition-9z4d8\naffinity-nodeport-transition-vxw95\naffinity-nodeport-transition-9z4d8\naffinity-nodeport-transition-9z4d8\naffinity-nodeport-transition-dqdrp\naffinity-nodeport-transition-dqdrp\naffinity-nodeport-transition-9z4d8\naffinity-nodeport-transition-9z4d8\naffinity-nodeport-transition-vxw95\naffinity-nodeport-transition-vxw95\naffinity-nodeport-transition-vxw95\naffinity-nodeport-transition-dqdrp\naffinity-nodeport-transition-vxw95\naffinity-nodeport-transition-9z4d8"
Jan 29 04:34:09.502: INFO: Received response from host: affinity-nodeport-transition-9z4d8
Jan 29 04:34:09.502: INFO: Received response from host: affinity-nodeport-transition-9z4d8
Jan 29 04:34:09.502: INFO: Received response from host: affinity-nodeport-transition-9z4d8
Jan 29 04:34:09.502: INFO: Received response from host: affinity-nodeport-transition-vxw95
Jan 29 04:34:09.502: INFO: Received response from host: affinity-nodeport-transition-9z4d8
Jan 29 04:34:09.502: INFO: Received response from host: affinity-nodeport-transition-9z4d8
Jan 29 04:34:09.502: INFO: Received response from host: affinity-nodeport-transition-dqdrp
Jan 29 04:34:09.502: INFO: Received response from host: affinity-nodeport-transition-dqdrp
Jan 29 04:34:09.502: INFO: Received response from host: affinity-nodeport-transition-9z4d8
Jan 29 04:34:09.502: INFO: Received response from host: affinity-nodeport-transition-9z4d8
Jan 29 04:34:09.502: INFO: Received response from host: affinity-nodeport-transition-vxw95
Jan 29 04:34:09.502: INFO: Received response from host: affinity-nodeport-transition-vxw95
Jan 29 04:34:09.502: INFO: Received response from host: affinity-nodeport-transition-vxw95
Jan 29 04:34:09.502: INFO: Received response from host: affinity-nodeport-transition-dqdrp
Jan 29 04:34:09.502: INFO: Received response from host: affinity-nodeport-transition-vxw95
Jan 29 04:34:09.502: INFO: Received response from host: affinity-nodeport-transition-9z4d8
Jan 29 04:34:09.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-6440 exec execpod-affinity6l8zl -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.122.241:31733/ ; done'
Jan 29 04:34:09.873: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:31733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:31733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:31733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:31733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:31733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:31733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:31733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:31733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:31733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:31733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:31733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:31733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:31733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:31733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:31733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:31733/\n"
Jan 29 04:34:09.873: INFO: stdout: "\naffinity-nodeport-transition-dqdrp\naffinity-nodeport-transition-dqdrp\naffinity-nodeport-transition-dqdrp\naffinity-nodeport-transition-dqdrp\naffinity-nodeport-transition-dqdrp\naffinity-nodeport-transition-dqdrp\naffinity-nodeport-transition-dqdrp\naffinity-nodeport-transition-dqdrp\naffinity-nodeport-transition-dqdrp\naffinity-nodeport-transition-dqdrp\naffinity-nodeport-transition-dqdrp\naffinity-nodeport-transition-dqdrp\naffinity-nodeport-transition-dqdrp\naffinity-nodeport-transition-dqdrp\naffinity-nodeport-transition-dqdrp\naffinity-nodeport-transition-dqdrp"
Jan 29 04:34:09.873: INFO: Received response from host: affinity-nodeport-transition-dqdrp
Jan 29 04:34:09.873: INFO: Received response from host: affinity-nodeport-transition-dqdrp
Jan 29 04:34:09.873: INFO: Received response from host: affinity-nodeport-transition-dqdrp
Jan 29 04:34:09.873: INFO: Received response from host: affinity-nodeport-transition-dqdrp
Jan 29 04:34:09.873: INFO: Received response from host: affinity-nodeport-transition-dqdrp
Jan 29 04:34:09.873: INFO: Received response from host: affinity-nodeport-transition-dqdrp
Jan 29 04:34:09.873: INFO: Received response from host: affinity-nodeport-transition-dqdrp
Jan 29 04:34:09.873: INFO: Received response from host: affinity-nodeport-transition-dqdrp
Jan 29 04:34:09.873: INFO: Received response from host: affinity-nodeport-transition-dqdrp
Jan 29 04:34:09.873: INFO: Received response from host: affinity-nodeport-transition-dqdrp
Jan 29 04:34:09.873: INFO: Received response from host: affinity-nodeport-transition-dqdrp
Jan 29 04:34:09.873: INFO: Received response from host: affinity-nodeport-transition-dqdrp
Jan 29 04:34:09.873: INFO: Received response from host: affinity-nodeport-transition-dqdrp
Jan 29 04:34:09.873: INFO: Received response from host: affinity-nodeport-transition-dqdrp
Jan 29 04:34:09.873: INFO: Received response from host: affinity-nodeport-transition-dqdrp
Jan 29 04:34:09.873: INFO: Received response from host: affinity-nodeport-transition-dqdrp
Jan 29 04:34:09.873: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-6440, will wait for the garbage collector to delete the pods 01/29/23 04:34:09.972
Jan 29 04:34:10.041: INFO: Deleting ReplicationController affinity-nodeport-transition took: 12.087284ms
Jan 29 04:34:10.241: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 200.231703ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 29 04:34:13.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6440" for this suite. 01/29/23 04:34:13.095
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","completed":337,"skipped":6219,"failed":0}
------------------------------
• [SLOW TEST] [11.165 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:34:01.942
    Jan 29 04:34:01.942: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename services 01/29/23 04:34:01.943
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:34:01.975
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:34:01.981
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2237
    STEP: creating service in namespace services-6440 01/29/23 04:34:01.986
    STEP: creating service affinity-nodeport-transition in namespace services-6440 01/29/23 04:34:01.986
    STEP: creating replication controller affinity-nodeport-transition in namespace services-6440 01/29/23 04:34:02.026
    I0129 04:34:02.038976      22 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-6440, replica count: 3
    I0129 04:34:05.090794      22 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 29 04:34:05.113: INFO: Creating new exec pod
    Jan 29 04:34:05.126: INFO: Waiting up to 5m0s for pod "execpod-affinity6l8zl" in namespace "services-6440" to be "running"
    Jan 29 04:34:05.133: INFO: Pod "execpod-affinity6l8zl": Phase="Pending", Reason="", readiness=false. Elapsed: 6.710627ms
    Jan 29 04:34:07.140: INFO: Pod "execpod-affinity6l8zl": Phase="Running", Reason="", readiness=true. Elapsed: 2.014128989s
    Jan 29 04:34:07.140: INFO: Pod "execpod-affinity6l8zl" satisfied condition "running"
    Jan 29 04:34:08.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-6440 exec execpod-affinity6l8zl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
    Jan 29 04:34:08.390: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Jan 29 04:34:08.390: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 29 04:34:08.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-6440 exec execpod-affinity6l8zl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.105.138.162 80'
    Jan 29 04:34:08.647: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.105.138.162 80\nConnection to 100.105.138.162 80 port [tcp/http] succeeded!\n"
    Jan 29 04:34:08.647: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 29 04:34:08.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-6440 exec execpod-affinity6l8zl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.122.245 31733'
    Jan 29 04:34:08.882: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.122.245 31733\nConnection to 192.168.122.245 31733 port [tcp/*] succeeded!\n"
    Jan 29 04:34:08.882: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 29 04:34:08.882: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-6440 exec execpod-affinity6l8zl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.122.244 31733'
    Jan 29 04:34:09.131: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.122.244 31733\nConnection to 192.168.122.244 31733 port [tcp/*] succeeded!\n"
    Jan 29 04:34:09.132: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 29 04:34:09.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-6440 exec execpod-affinity6l8zl -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.122.241:31733/ ; done'
    Jan 29 04:34:09.502: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:31733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:31733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:31733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:31733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:31733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:31733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:31733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:31733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:31733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:31733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:31733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:31733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:31733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:31733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:31733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:31733/\n"
    Jan 29 04:34:09.502: INFO: stdout: "\naffinity-nodeport-transition-9z4d8\naffinity-nodeport-transition-9z4d8\naffinity-nodeport-transition-9z4d8\naffinity-nodeport-transition-vxw95\naffinity-nodeport-transition-9z4d8\naffinity-nodeport-transition-9z4d8\naffinity-nodeport-transition-dqdrp\naffinity-nodeport-transition-dqdrp\naffinity-nodeport-transition-9z4d8\naffinity-nodeport-transition-9z4d8\naffinity-nodeport-transition-vxw95\naffinity-nodeport-transition-vxw95\naffinity-nodeport-transition-vxw95\naffinity-nodeport-transition-dqdrp\naffinity-nodeport-transition-vxw95\naffinity-nodeport-transition-9z4d8"
    Jan 29 04:34:09.502: INFO: Received response from host: affinity-nodeport-transition-9z4d8
    Jan 29 04:34:09.502: INFO: Received response from host: affinity-nodeport-transition-9z4d8
    Jan 29 04:34:09.502: INFO: Received response from host: affinity-nodeport-transition-9z4d8
    Jan 29 04:34:09.502: INFO: Received response from host: affinity-nodeport-transition-vxw95
    Jan 29 04:34:09.502: INFO: Received response from host: affinity-nodeport-transition-9z4d8
    Jan 29 04:34:09.502: INFO: Received response from host: affinity-nodeport-transition-9z4d8
    Jan 29 04:34:09.502: INFO: Received response from host: affinity-nodeport-transition-dqdrp
    Jan 29 04:34:09.502: INFO: Received response from host: affinity-nodeport-transition-dqdrp
    Jan 29 04:34:09.502: INFO: Received response from host: affinity-nodeport-transition-9z4d8
    Jan 29 04:34:09.502: INFO: Received response from host: affinity-nodeport-transition-9z4d8
    Jan 29 04:34:09.502: INFO: Received response from host: affinity-nodeport-transition-vxw95
    Jan 29 04:34:09.502: INFO: Received response from host: affinity-nodeport-transition-vxw95
    Jan 29 04:34:09.502: INFO: Received response from host: affinity-nodeport-transition-vxw95
    Jan 29 04:34:09.502: INFO: Received response from host: affinity-nodeport-transition-dqdrp
    Jan 29 04:34:09.502: INFO: Received response from host: affinity-nodeport-transition-vxw95
    Jan 29 04:34:09.502: INFO: Received response from host: affinity-nodeport-transition-9z4d8
    Jan 29 04:34:09.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=services-6440 exec execpod-affinity6l8zl -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.122.241:31733/ ; done'
    Jan 29 04:34:09.873: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:31733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:31733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:31733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:31733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:31733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:31733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:31733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:31733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:31733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:31733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:31733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:31733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:31733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:31733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:31733/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.241:31733/\n"
    Jan 29 04:34:09.873: INFO: stdout: "\naffinity-nodeport-transition-dqdrp\naffinity-nodeport-transition-dqdrp\naffinity-nodeport-transition-dqdrp\naffinity-nodeport-transition-dqdrp\naffinity-nodeport-transition-dqdrp\naffinity-nodeport-transition-dqdrp\naffinity-nodeport-transition-dqdrp\naffinity-nodeport-transition-dqdrp\naffinity-nodeport-transition-dqdrp\naffinity-nodeport-transition-dqdrp\naffinity-nodeport-transition-dqdrp\naffinity-nodeport-transition-dqdrp\naffinity-nodeport-transition-dqdrp\naffinity-nodeport-transition-dqdrp\naffinity-nodeport-transition-dqdrp\naffinity-nodeport-transition-dqdrp"
    Jan 29 04:34:09.873: INFO: Received response from host: affinity-nodeport-transition-dqdrp
    Jan 29 04:34:09.873: INFO: Received response from host: affinity-nodeport-transition-dqdrp
    Jan 29 04:34:09.873: INFO: Received response from host: affinity-nodeport-transition-dqdrp
    Jan 29 04:34:09.873: INFO: Received response from host: affinity-nodeport-transition-dqdrp
    Jan 29 04:34:09.873: INFO: Received response from host: affinity-nodeport-transition-dqdrp
    Jan 29 04:34:09.873: INFO: Received response from host: affinity-nodeport-transition-dqdrp
    Jan 29 04:34:09.873: INFO: Received response from host: affinity-nodeport-transition-dqdrp
    Jan 29 04:34:09.873: INFO: Received response from host: affinity-nodeport-transition-dqdrp
    Jan 29 04:34:09.873: INFO: Received response from host: affinity-nodeport-transition-dqdrp
    Jan 29 04:34:09.873: INFO: Received response from host: affinity-nodeport-transition-dqdrp
    Jan 29 04:34:09.873: INFO: Received response from host: affinity-nodeport-transition-dqdrp
    Jan 29 04:34:09.873: INFO: Received response from host: affinity-nodeport-transition-dqdrp
    Jan 29 04:34:09.873: INFO: Received response from host: affinity-nodeport-transition-dqdrp
    Jan 29 04:34:09.873: INFO: Received response from host: affinity-nodeport-transition-dqdrp
    Jan 29 04:34:09.873: INFO: Received response from host: affinity-nodeport-transition-dqdrp
    Jan 29 04:34:09.873: INFO: Received response from host: affinity-nodeport-transition-dqdrp
    Jan 29 04:34:09.873: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-6440, will wait for the garbage collector to delete the pods 01/29/23 04:34:09.972
    Jan 29 04:34:10.041: INFO: Deleting ReplicationController affinity-nodeport-transition took: 12.087284ms
    Jan 29 04:34:10.241: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 200.231703ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 29 04:34:13.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-6440" for this suite. 01/29/23 04:34:13.095
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:34:13.107
Jan 29 04:34:13.107: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename services 01/29/23 04:34:13.109
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:34:13.13
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:34:13.136
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415
STEP: creating a Service 01/29/23 04:34:13.15
STEP: watching for the Service to be added 01/29/23 04:34:13.166
Jan 29 04:34:13.169: INFO: Found Service test-service-wtdxx in namespace services-2419 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Jan 29 04:34:13.169: INFO: Service test-service-wtdxx created
STEP: Getting /status 01/29/23 04:34:13.169
Jan 29 04:34:13.177: INFO: Service test-service-wtdxx has LoadBalancer: {[]}
STEP: patching the ServiceStatus 01/29/23 04:34:13.178
STEP: watching for the Service to be patched 01/29/23 04:34:13.19
Jan 29 04:34:13.194: INFO: observed Service test-service-wtdxx in namespace services-2419 with annotations: map[] & LoadBalancer: {[]}
Jan 29 04:34:13.194: INFO: Found Service test-service-wtdxx in namespace services-2419 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Jan 29 04:34:13.194: INFO: Service test-service-wtdxx has service status patched
STEP: updating the ServiceStatus 01/29/23 04:34:13.194
Jan 29 04:34:13.212: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 01/29/23 04:34:13.212
Jan 29 04:34:13.216: INFO: Observed Service test-service-wtdxx in namespace services-2419 with annotations: map[] & Conditions: {[]}
Jan 29 04:34:13.216: INFO: Observed event: &Service{ObjectMeta:{test-service-wtdxx  services-2419  085012fc-a211-41eb-a059-e3a8edf23fb6 5988790 0 2023-01-29 04:34:13 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] []},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:100.105.201.6,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[100.105.201.6],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Jan 29 04:34:13.216: INFO: Found Service test-service-wtdxx in namespace services-2419 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 29 04:34:13.216: INFO: Service test-service-wtdxx has service status updated
STEP: patching the service 01/29/23 04:34:13.216
STEP: watching for the Service to be patched 01/29/23 04:34:13.226
Jan 29 04:34:13.229: INFO: observed Service test-service-wtdxx in namespace services-2419 with labels: map[test-service-static:true]
Jan 29 04:34:13.229: INFO: observed Service test-service-wtdxx in namespace services-2419 with labels: map[test-service-static:true]
Jan 29 04:34:13.230: INFO: observed Service test-service-wtdxx in namespace services-2419 with labels: map[test-service-static:true]
Jan 29 04:34:13.230: INFO: Found Service test-service-wtdxx in namespace services-2419 with labels: map[test-service:patched test-service-static:true]
Jan 29 04:34:13.230: INFO: Service test-service-wtdxx patched
STEP: deleting the service 01/29/23 04:34:13.23
STEP: watching for the Service to be deleted 01/29/23 04:34:13.256
Jan 29 04:34:13.259: INFO: Observed event: ADDED
Jan 29 04:34:13.259: INFO: Observed event: MODIFIED
Jan 29 04:34:13.259: INFO: Observed event: MODIFIED
Jan 29 04:34:13.259: INFO: Observed event: MODIFIED
Jan 29 04:34:13.260: INFO: Found Service test-service-wtdxx in namespace services-2419 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Jan 29 04:34:13.260: INFO: Service test-service-wtdxx deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 29 04:34:13.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2419" for this suite. 01/29/23 04:34:13.272
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","completed":338,"skipped":6221,"failed":0}
------------------------------
• [0.175 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:34:13.107
    Jan 29 04:34:13.107: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename services 01/29/23 04:34:13.109
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:34:13.13
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:34:13.136
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3415
    STEP: creating a Service 01/29/23 04:34:13.15
    STEP: watching for the Service to be added 01/29/23 04:34:13.166
    Jan 29 04:34:13.169: INFO: Found Service test-service-wtdxx in namespace services-2419 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Jan 29 04:34:13.169: INFO: Service test-service-wtdxx created
    STEP: Getting /status 01/29/23 04:34:13.169
    Jan 29 04:34:13.177: INFO: Service test-service-wtdxx has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 01/29/23 04:34:13.178
    STEP: watching for the Service to be patched 01/29/23 04:34:13.19
    Jan 29 04:34:13.194: INFO: observed Service test-service-wtdxx in namespace services-2419 with annotations: map[] & LoadBalancer: {[]}
    Jan 29 04:34:13.194: INFO: Found Service test-service-wtdxx in namespace services-2419 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Jan 29 04:34:13.194: INFO: Service test-service-wtdxx has service status patched
    STEP: updating the ServiceStatus 01/29/23 04:34:13.194
    Jan 29 04:34:13.212: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 01/29/23 04:34:13.212
    Jan 29 04:34:13.216: INFO: Observed Service test-service-wtdxx in namespace services-2419 with annotations: map[] & Conditions: {[]}
    Jan 29 04:34:13.216: INFO: Observed event: &Service{ObjectMeta:{test-service-wtdxx  services-2419  085012fc-a211-41eb-a059-e3a8edf23fb6 5988790 0 2023-01-29 04:34:13 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] []},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:100.105.201.6,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[100.105.201.6],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Jan 29 04:34:13.216: INFO: Found Service test-service-wtdxx in namespace services-2419 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan 29 04:34:13.216: INFO: Service test-service-wtdxx has service status updated
    STEP: patching the service 01/29/23 04:34:13.216
    STEP: watching for the Service to be patched 01/29/23 04:34:13.226
    Jan 29 04:34:13.229: INFO: observed Service test-service-wtdxx in namespace services-2419 with labels: map[test-service-static:true]
    Jan 29 04:34:13.229: INFO: observed Service test-service-wtdxx in namespace services-2419 with labels: map[test-service-static:true]
    Jan 29 04:34:13.230: INFO: observed Service test-service-wtdxx in namespace services-2419 with labels: map[test-service-static:true]
    Jan 29 04:34:13.230: INFO: Found Service test-service-wtdxx in namespace services-2419 with labels: map[test-service:patched test-service-static:true]
    Jan 29 04:34:13.230: INFO: Service test-service-wtdxx patched
    STEP: deleting the service 01/29/23 04:34:13.23
    STEP: watching for the Service to be deleted 01/29/23 04:34:13.256
    Jan 29 04:34:13.259: INFO: Observed event: ADDED
    Jan 29 04:34:13.259: INFO: Observed event: MODIFIED
    Jan 29 04:34:13.259: INFO: Observed event: MODIFIED
    Jan 29 04:34:13.259: INFO: Observed event: MODIFIED
    Jan 29 04:34:13.260: INFO: Found Service test-service-wtdxx in namespace services-2419 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Jan 29 04:34:13.260: INFO: Service test-service-wtdxx deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 29 04:34:13.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-2419" for this suite. 01/29/23 04:34:13.272
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:34:13.287
Jan 29 04:34:13.287: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename emptydir-wrapper 01/29/23 04:34:13.288
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:34:13.31
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:34:13.315
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 01/29/23 04:34:13.321
STEP: Creating RC which spawns configmap-volume pods 01/29/23 04:34:13.742
Jan 29 04:34:13.760: INFO: Pod name wrapped-volume-race-2d10102d-d001-4cca-95e1-86a29e1a1f99: Found 0 pods out of 5
Jan 29 04:34:18.771: INFO: Pod name wrapped-volume-race-2d10102d-d001-4cca-95e1-86a29e1a1f99: Found 5 pods out of 5
STEP: Ensuring each pod is running 01/29/23 04:34:18.771
Jan 29 04:34:18.771: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2d10102d-d001-4cca-95e1-86a29e1a1f99-2l4r4" in namespace "emptydir-wrapper-2316" to be "running"
Jan 29 04:34:18.783: INFO: Pod "wrapped-volume-race-2d10102d-d001-4cca-95e1-86a29e1a1f99-2l4r4": Phase="Pending", Reason="", readiness=false. Elapsed: 11.948344ms
Jan 29 04:34:20.791: INFO: Pod "wrapped-volume-race-2d10102d-d001-4cca-95e1-86a29e1a1f99-2l4r4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019823908s
Jan 29 04:34:22.793: INFO: Pod "wrapped-volume-race-2d10102d-d001-4cca-95e1-86a29e1a1f99-2l4r4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02163859s
Jan 29 04:34:24.794: INFO: Pod "wrapped-volume-race-2d10102d-d001-4cca-95e1-86a29e1a1f99-2l4r4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.023042829s
Jan 29 04:34:26.792: INFO: Pod "wrapped-volume-race-2d10102d-d001-4cca-95e1-86a29e1a1f99-2l4r4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.02033084s
Jan 29 04:34:28.793: INFO: Pod "wrapped-volume-race-2d10102d-d001-4cca-95e1-86a29e1a1f99-2l4r4": Phase="Running", Reason="", readiness=true. Elapsed: 10.022017541s
Jan 29 04:34:28.793: INFO: Pod "wrapped-volume-race-2d10102d-d001-4cca-95e1-86a29e1a1f99-2l4r4" satisfied condition "running"
Jan 29 04:34:28.793: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2d10102d-d001-4cca-95e1-86a29e1a1f99-hcmc6" in namespace "emptydir-wrapper-2316" to be "running"
Jan 29 04:34:28.804: INFO: Pod "wrapped-volume-race-2d10102d-d001-4cca-95e1-86a29e1a1f99-hcmc6": Phase="Running", Reason="", readiness=true. Elapsed: 10.137891ms
Jan 29 04:34:28.804: INFO: Pod "wrapped-volume-race-2d10102d-d001-4cca-95e1-86a29e1a1f99-hcmc6" satisfied condition "running"
Jan 29 04:34:28.804: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2d10102d-d001-4cca-95e1-86a29e1a1f99-wbbph" in namespace "emptydir-wrapper-2316" to be "running"
Jan 29 04:34:28.812: INFO: Pod "wrapped-volume-race-2d10102d-d001-4cca-95e1-86a29e1a1f99-wbbph": Phase="Running", Reason="", readiness=true. Elapsed: 8.62028ms
Jan 29 04:34:28.812: INFO: Pod "wrapped-volume-race-2d10102d-d001-4cca-95e1-86a29e1a1f99-wbbph" satisfied condition "running"
Jan 29 04:34:28.812: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2d10102d-d001-4cca-95e1-86a29e1a1f99-wngtg" in namespace "emptydir-wrapper-2316" to be "running"
Jan 29 04:34:28.819: INFO: Pod "wrapped-volume-race-2d10102d-d001-4cca-95e1-86a29e1a1f99-wngtg": Phase="Running", Reason="", readiness=true. Elapsed: 6.979069ms
Jan 29 04:34:28.819: INFO: Pod "wrapped-volume-race-2d10102d-d001-4cca-95e1-86a29e1a1f99-wngtg" satisfied condition "running"
Jan 29 04:34:28.820: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2d10102d-d001-4cca-95e1-86a29e1a1f99-xwhhz" in namespace "emptydir-wrapper-2316" to be "running"
Jan 29 04:34:28.826: INFO: Pod "wrapped-volume-race-2d10102d-d001-4cca-95e1-86a29e1a1f99-xwhhz": Phase="Running", Reason="", readiness=true. Elapsed: 6.836908ms
Jan 29 04:34:28.826: INFO: Pod "wrapped-volume-race-2d10102d-d001-4cca-95e1-86a29e1a1f99-xwhhz" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-2d10102d-d001-4cca-95e1-86a29e1a1f99 in namespace emptydir-wrapper-2316, will wait for the garbage collector to delete the pods 01/29/23 04:34:28.826
Jan 29 04:34:28.897: INFO: Deleting ReplicationController wrapped-volume-race-2d10102d-d001-4cca-95e1-86a29e1a1f99 took: 12.017105ms
Jan 29 04:34:29.098: INFO: Terminating ReplicationController wrapped-volume-race-2d10102d-d001-4cca-95e1-86a29e1a1f99 pods took: 201.187589ms
STEP: Creating RC which spawns configmap-volume pods 01/29/23 04:34:34.209
Jan 29 04:34:34.229: INFO: Pod name wrapped-volume-race-bb30c6fe-51c0-4270-8876-e11e4c7ffed7: Found 0 pods out of 5
Jan 29 04:34:39.242: INFO: Pod name wrapped-volume-race-bb30c6fe-51c0-4270-8876-e11e4c7ffed7: Found 5 pods out of 5
STEP: Ensuring each pod is running 01/29/23 04:34:39.242
Jan 29 04:34:39.242: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-bb30c6fe-51c0-4270-8876-e11e4c7ffed7-6j78z" in namespace "emptydir-wrapper-2316" to be "running"
Jan 29 04:34:39.249: INFO: Pod "wrapped-volume-race-bb30c6fe-51c0-4270-8876-e11e4c7ffed7-6j78z": Phase="Pending", Reason="", readiness=false. Elapsed: 7.15181ms
Jan 29 04:34:41.257: INFO: Pod "wrapped-volume-race-bb30c6fe-51c0-4270-8876-e11e4c7ffed7-6j78z": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014699632s
Jan 29 04:34:43.258: INFO: Pod "wrapped-volume-race-bb30c6fe-51c0-4270-8876-e11e4c7ffed7-6j78z": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015589568s
Jan 29 04:34:45.256: INFO: Pod "wrapped-volume-race-bb30c6fe-51c0-4270-8876-e11e4c7ffed7-6j78z": Phase="Pending", Reason="", readiness=false. Elapsed: 6.014277328s
Jan 29 04:34:47.258: INFO: Pod "wrapped-volume-race-bb30c6fe-51c0-4270-8876-e11e4c7ffed7-6j78z": Phase="Pending", Reason="", readiness=false. Elapsed: 8.01612905s
Jan 29 04:34:49.272: INFO: Pod "wrapped-volume-race-bb30c6fe-51c0-4270-8876-e11e4c7ffed7-6j78z": Phase="Running", Reason="", readiness=true. Elapsed: 10.029729575s
Jan 29 04:34:49.272: INFO: Pod "wrapped-volume-race-bb30c6fe-51c0-4270-8876-e11e4c7ffed7-6j78z" satisfied condition "running"
Jan 29 04:34:49.272: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-bb30c6fe-51c0-4270-8876-e11e4c7ffed7-9xtjk" in namespace "emptydir-wrapper-2316" to be "running"
Jan 29 04:34:49.292: INFO: Pod "wrapped-volume-race-bb30c6fe-51c0-4270-8876-e11e4c7ffed7-9xtjk": Phase="Running", Reason="", readiness=true. Elapsed: 20.239122ms
Jan 29 04:34:49.292: INFO: Pod "wrapped-volume-race-bb30c6fe-51c0-4270-8876-e11e4c7ffed7-9xtjk" satisfied condition "running"
Jan 29 04:34:49.292: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-bb30c6fe-51c0-4270-8876-e11e4c7ffed7-dfww8" in namespace "emptydir-wrapper-2316" to be "running"
Jan 29 04:34:49.306: INFO: Pod "wrapped-volume-race-bb30c6fe-51c0-4270-8876-e11e4c7ffed7-dfww8": Phase="Running", Reason="", readiness=true. Elapsed: 13.182732ms
Jan 29 04:34:49.306: INFO: Pod "wrapped-volume-race-bb30c6fe-51c0-4270-8876-e11e4c7ffed7-dfww8" satisfied condition "running"
Jan 29 04:34:49.306: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-bb30c6fe-51c0-4270-8876-e11e4c7ffed7-l8dwn" in namespace "emptydir-wrapper-2316" to be "running"
Jan 29 04:34:49.322: INFO: Pod "wrapped-volume-race-bb30c6fe-51c0-4270-8876-e11e4c7ffed7-l8dwn": Phase="Running", Reason="", readiness=true. Elapsed: 16.357895ms
Jan 29 04:34:49.322: INFO: Pod "wrapped-volume-race-bb30c6fe-51c0-4270-8876-e11e4c7ffed7-l8dwn" satisfied condition "running"
Jan 29 04:34:49.322: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-bb30c6fe-51c0-4270-8876-e11e4c7ffed7-mh548" in namespace "emptydir-wrapper-2316" to be "running"
Jan 29 04:34:49.335: INFO: Pod "wrapped-volume-race-bb30c6fe-51c0-4270-8876-e11e4c7ffed7-mh548": Phase="Running", Reason="", readiness=true. Elapsed: 13.153392ms
Jan 29 04:34:49.335: INFO: Pod "wrapped-volume-race-bb30c6fe-51c0-4270-8876-e11e4c7ffed7-mh548" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-bb30c6fe-51c0-4270-8876-e11e4c7ffed7 in namespace emptydir-wrapper-2316, will wait for the garbage collector to delete the pods 01/29/23 04:34:49.335
Jan 29 04:34:49.424: INFO: Deleting ReplicationController wrapped-volume-race-bb30c6fe-51c0-4270-8876-e11e4c7ffed7 took: 21.37893ms
Jan 29 04:34:49.625: INFO: Terminating ReplicationController wrapped-volume-race-bb30c6fe-51c0-4270-8876-e11e4c7ffed7 pods took: 200.834186ms
STEP: Creating RC which spawns configmap-volume pods 01/29/23 04:34:54.241
Jan 29 04:34:54.281: INFO: Pod name wrapped-volume-race-fc08c5b6-4d7d-467a-abf1-902eefaf3029: Found 0 pods out of 5
Jan 29 04:34:59.295: INFO: Pod name wrapped-volume-race-fc08c5b6-4d7d-467a-abf1-902eefaf3029: Found 5 pods out of 5
STEP: Ensuring each pod is running 01/29/23 04:34:59.295
Jan 29 04:34:59.295: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-fc08c5b6-4d7d-467a-abf1-902eefaf3029-9tjpp" in namespace "emptydir-wrapper-2316" to be "running"
Jan 29 04:34:59.302: INFO: Pod "wrapped-volume-race-fc08c5b6-4d7d-467a-abf1-902eefaf3029-9tjpp": Phase="Pending", Reason="", readiness=false. Elapsed: 6.603566ms
Jan 29 04:35:01.311: INFO: Pod "wrapped-volume-race-fc08c5b6-4d7d-467a-abf1-902eefaf3029-9tjpp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015289357s
Jan 29 04:35:03.312: INFO: Pod "wrapped-volume-race-fc08c5b6-4d7d-467a-abf1-902eefaf3029-9tjpp": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016648656s
Jan 29 04:35:05.312: INFO: Pod "wrapped-volume-race-fc08c5b6-4d7d-467a-abf1-902eefaf3029-9tjpp": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016505244s
Jan 29 04:35:07.327: INFO: Pod "wrapped-volume-race-fc08c5b6-4d7d-467a-abf1-902eefaf3029-9tjpp": Phase="Pending", Reason="", readiness=false. Elapsed: 8.031153096s
Jan 29 04:35:09.310: INFO: Pod "wrapped-volume-race-fc08c5b6-4d7d-467a-abf1-902eefaf3029-9tjpp": Phase="Running", Reason="", readiness=true. Elapsed: 10.015062292s
Jan 29 04:35:09.311: INFO: Pod "wrapped-volume-race-fc08c5b6-4d7d-467a-abf1-902eefaf3029-9tjpp" satisfied condition "running"
Jan 29 04:35:09.311: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-fc08c5b6-4d7d-467a-abf1-902eefaf3029-m2ds9" in namespace "emptydir-wrapper-2316" to be "running"
Jan 29 04:35:09.318: INFO: Pod "wrapped-volume-race-fc08c5b6-4d7d-467a-abf1-902eefaf3029-m2ds9": Phase="Running", Reason="", readiness=true. Elapsed: 7.245871ms
Jan 29 04:35:09.318: INFO: Pod "wrapped-volume-race-fc08c5b6-4d7d-467a-abf1-902eefaf3029-m2ds9" satisfied condition "running"
Jan 29 04:35:09.318: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-fc08c5b6-4d7d-467a-abf1-902eefaf3029-nv6v9" in namespace "emptydir-wrapper-2316" to be "running"
Jan 29 04:35:09.325: INFO: Pod "wrapped-volume-race-fc08c5b6-4d7d-467a-abf1-902eefaf3029-nv6v9": Phase="Pending", Reason="", readiness=false. Elapsed: 7.177771ms
Jan 29 04:35:11.334: INFO: Pod "wrapped-volume-race-fc08c5b6-4d7d-467a-abf1-902eefaf3029-nv6v9": Phase="Running", Reason="", readiness=true. Elapsed: 2.016463945s
Jan 29 04:35:11.334: INFO: Pod "wrapped-volume-race-fc08c5b6-4d7d-467a-abf1-902eefaf3029-nv6v9" satisfied condition "running"
Jan 29 04:35:11.338: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-fc08c5b6-4d7d-467a-abf1-902eefaf3029-nxq4h" in namespace "emptydir-wrapper-2316" to be "running"
Jan 29 04:35:11.350: INFO: Pod "wrapped-volume-race-fc08c5b6-4d7d-467a-abf1-902eefaf3029-nxq4h": Phase="Running", Reason="", readiness=true. Elapsed: 12.260246ms
Jan 29 04:35:11.350: INFO: Pod "wrapped-volume-race-fc08c5b6-4d7d-467a-abf1-902eefaf3029-nxq4h" satisfied condition "running"
Jan 29 04:35:11.350: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-fc08c5b6-4d7d-467a-abf1-902eefaf3029-zx2f5" in namespace "emptydir-wrapper-2316" to be "running"
Jan 29 04:35:11.358: INFO: Pod "wrapped-volume-race-fc08c5b6-4d7d-467a-abf1-902eefaf3029-zx2f5": Phase="Running", Reason="", readiness=true. Elapsed: 7.612853ms
Jan 29 04:35:11.358: INFO: Pod "wrapped-volume-race-fc08c5b6-4d7d-467a-abf1-902eefaf3029-zx2f5" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-fc08c5b6-4d7d-467a-abf1-902eefaf3029 in namespace emptydir-wrapper-2316, will wait for the garbage collector to delete the pods 01/29/23 04:35:11.358
Jan 29 04:35:11.431: INFO: Deleting ReplicationController wrapped-volume-race-fc08c5b6-4d7d-467a-abf1-902eefaf3029 took: 14.336461ms
Jan 29 04:35:11.632: INFO: Terminating ReplicationController wrapped-volume-race-fc08c5b6-4d7d-467a-abf1-902eefaf3029 pods took: 201.181969ms
STEP: Cleaning up the configMaps 01/29/23 04:35:14.933
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
Jan 29 04:35:15.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-2316" for this suite. 01/29/23 04:35:15.553
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","completed":339,"skipped":6284,"failed":0}
------------------------------
• [SLOW TEST] [62.277 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:34:13.287
    Jan 29 04:34:13.287: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename emptydir-wrapper 01/29/23 04:34:13.288
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:34:13.31
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:34:13.315
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 01/29/23 04:34:13.321
    STEP: Creating RC which spawns configmap-volume pods 01/29/23 04:34:13.742
    Jan 29 04:34:13.760: INFO: Pod name wrapped-volume-race-2d10102d-d001-4cca-95e1-86a29e1a1f99: Found 0 pods out of 5
    Jan 29 04:34:18.771: INFO: Pod name wrapped-volume-race-2d10102d-d001-4cca-95e1-86a29e1a1f99: Found 5 pods out of 5
    STEP: Ensuring each pod is running 01/29/23 04:34:18.771
    Jan 29 04:34:18.771: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2d10102d-d001-4cca-95e1-86a29e1a1f99-2l4r4" in namespace "emptydir-wrapper-2316" to be "running"
    Jan 29 04:34:18.783: INFO: Pod "wrapped-volume-race-2d10102d-d001-4cca-95e1-86a29e1a1f99-2l4r4": Phase="Pending", Reason="", readiness=false. Elapsed: 11.948344ms
    Jan 29 04:34:20.791: INFO: Pod "wrapped-volume-race-2d10102d-d001-4cca-95e1-86a29e1a1f99-2l4r4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019823908s
    Jan 29 04:34:22.793: INFO: Pod "wrapped-volume-race-2d10102d-d001-4cca-95e1-86a29e1a1f99-2l4r4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02163859s
    Jan 29 04:34:24.794: INFO: Pod "wrapped-volume-race-2d10102d-d001-4cca-95e1-86a29e1a1f99-2l4r4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.023042829s
    Jan 29 04:34:26.792: INFO: Pod "wrapped-volume-race-2d10102d-d001-4cca-95e1-86a29e1a1f99-2l4r4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.02033084s
    Jan 29 04:34:28.793: INFO: Pod "wrapped-volume-race-2d10102d-d001-4cca-95e1-86a29e1a1f99-2l4r4": Phase="Running", Reason="", readiness=true. Elapsed: 10.022017541s
    Jan 29 04:34:28.793: INFO: Pod "wrapped-volume-race-2d10102d-d001-4cca-95e1-86a29e1a1f99-2l4r4" satisfied condition "running"
    Jan 29 04:34:28.793: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2d10102d-d001-4cca-95e1-86a29e1a1f99-hcmc6" in namespace "emptydir-wrapper-2316" to be "running"
    Jan 29 04:34:28.804: INFO: Pod "wrapped-volume-race-2d10102d-d001-4cca-95e1-86a29e1a1f99-hcmc6": Phase="Running", Reason="", readiness=true. Elapsed: 10.137891ms
    Jan 29 04:34:28.804: INFO: Pod "wrapped-volume-race-2d10102d-d001-4cca-95e1-86a29e1a1f99-hcmc6" satisfied condition "running"
    Jan 29 04:34:28.804: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2d10102d-d001-4cca-95e1-86a29e1a1f99-wbbph" in namespace "emptydir-wrapper-2316" to be "running"
    Jan 29 04:34:28.812: INFO: Pod "wrapped-volume-race-2d10102d-d001-4cca-95e1-86a29e1a1f99-wbbph": Phase="Running", Reason="", readiness=true. Elapsed: 8.62028ms
    Jan 29 04:34:28.812: INFO: Pod "wrapped-volume-race-2d10102d-d001-4cca-95e1-86a29e1a1f99-wbbph" satisfied condition "running"
    Jan 29 04:34:28.812: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2d10102d-d001-4cca-95e1-86a29e1a1f99-wngtg" in namespace "emptydir-wrapper-2316" to be "running"
    Jan 29 04:34:28.819: INFO: Pod "wrapped-volume-race-2d10102d-d001-4cca-95e1-86a29e1a1f99-wngtg": Phase="Running", Reason="", readiness=true. Elapsed: 6.979069ms
    Jan 29 04:34:28.819: INFO: Pod "wrapped-volume-race-2d10102d-d001-4cca-95e1-86a29e1a1f99-wngtg" satisfied condition "running"
    Jan 29 04:34:28.820: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2d10102d-d001-4cca-95e1-86a29e1a1f99-xwhhz" in namespace "emptydir-wrapper-2316" to be "running"
    Jan 29 04:34:28.826: INFO: Pod "wrapped-volume-race-2d10102d-d001-4cca-95e1-86a29e1a1f99-xwhhz": Phase="Running", Reason="", readiness=true. Elapsed: 6.836908ms
    Jan 29 04:34:28.826: INFO: Pod "wrapped-volume-race-2d10102d-d001-4cca-95e1-86a29e1a1f99-xwhhz" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-2d10102d-d001-4cca-95e1-86a29e1a1f99 in namespace emptydir-wrapper-2316, will wait for the garbage collector to delete the pods 01/29/23 04:34:28.826
    Jan 29 04:34:28.897: INFO: Deleting ReplicationController wrapped-volume-race-2d10102d-d001-4cca-95e1-86a29e1a1f99 took: 12.017105ms
    Jan 29 04:34:29.098: INFO: Terminating ReplicationController wrapped-volume-race-2d10102d-d001-4cca-95e1-86a29e1a1f99 pods took: 201.187589ms
    STEP: Creating RC which spawns configmap-volume pods 01/29/23 04:34:34.209
    Jan 29 04:34:34.229: INFO: Pod name wrapped-volume-race-bb30c6fe-51c0-4270-8876-e11e4c7ffed7: Found 0 pods out of 5
    Jan 29 04:34:39.242: INFO: Pod name wrapped-volume-race-bb30c6fe-51c0-4270-8876-e11e4c7ffed7: Found 5 pods out of 5
    STEP: Ensuring each pod is running 01/29/23 04:34:39.242
    Jan 29 04:34:39.242: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-bb30c6fe-51c0-4270-8876-e11e4c7ffed7-6j78z" in namespace "emptydir-wrapper-2316" to be "running"
    Jan 29 04:34:39.249: INFO: Pod "wrapped-volume-race-bb30c6fe-51c0-4270-8876-e11e4c7ffed7-6j78z": Phase="Pending", Reason="", readiness=false. Elapsed: 7.15181ms
    Jan 29 04:34:41.257: INFO: Pod "wrapped-volume-race-bb30c6fe-51c0-4270-8876-e11e4c7ffed7-6j78z": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014699632s
    Jan 29 04:34:43.258: INFO: Pod "wrapped-volume-race-bb30c6fe-51c0-4270-8876-e11e4c7ffed7-6j78z": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015589568s
    Jan 29 04:34:45.256: INFO: Pod "wrapped-volume-race-bb30c6fe-51c0-4270-8876-e11e4c7ffed7-6j78z": Phase="Pending", Reason="", readiness=false. Elapsed: 6.014277328s
    Jan 29 04:34:47.258: INFO: Pod "wrapped-volume-race-bb30c6fe-51c0-4270-8876-e11e4c7ffed7-6j78z": Phase="Pending", Reason="", readiness=false. Elapsed: 8.01612905s
    Jan 29 04:34:49.272: INFO: Pod "wrapped-volume-race-bb30c6fe-51c0-4270-8876-e11e4c7ffed7-6j78z": Phase="Running", Reason="", readiness=true. Elapsed: 10.029729575s
    Jan 29 04:34:49.272: INFO: Pod "wrapped-volume-race-bb30c6fe-51c0-4270-8876-e11e4c7ffed7-6j78z" satisfied condition "running"
    Jan 29 04:34:49.272: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-bb30c6fe-51c0-4270-8876-e11e4c7ffed7-9xtjk" in namespace "emptydir-wrapper-2316" to be "running"
    Jan 29 04:34:49.292: INFO: Pod "wrapped-volume-race-bb30c6fe-51c0-4270-8876-e11e4c7ffed7-9xtjk": Phase="Running", Reason="", readiness=true. Elapsed: 20.239122ms
    Jan 29 04:34:49.292: INFO: Pod "wrapped-volume-race-bb30c6fe-51c0-4270-8876-e11e4c7ffed7-9xtjk" satisfied condition "running"
    Jan 29 04:34:49.292: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-bb30c6fe-51c0-4270-8876-e11e4c7ffed7-dfww8" in namespace "emptydir-wrapper-2316" to be "running"
    Jan 29 04:34:49.306: INFO: Pod "wrapped-volume-race-bb30c6fe-51c0-4270-8876-e11e4c7ffed7-dfww8": Phase="Running", Reason="", readiness=true. Elapsed: 13.182732ms
    Jan 29 04:34:49.306: INFO: Pod "wrapped-volume-race-bb30c6fe-51c0-4270-8876-e11e4c7ffed7-dfww8" satisfied condition "running"
    Jan 29 04:34:49.306: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-bb30c6fe-51c0-4270-8876-e11e4c7ffed7-l8dwn" in namespace "emptydir-wrapper-2316" to be "running"
    Jan 29 04:34:49.322: INFO: Pod "wrapped-volume-race-bb30c6fe-51c0-4270-8876-e11e4c7ffed7-l8dwn": Phase="Running", Reason="", readiness=true. Elapsed: 16.357895ms
    Jan 29 04:34:49.322: INFO: Pod "wrapped-volume-race-bb30c6fe-51c0-4270-8876-e11e4c7ffed7-l8dwn" satisfied condition "running"
    Jan 29 04:34:49.322: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-bb30c6fe-51c0-4270-8876-e11e4c7ffed7-mh548" in namespace "emptydir-wrapper-2316" to be "running"
    Jan 29 04:34:49.335: INFO: Pod "wrapped-volume-race-bb30c6fe-51c0-4270-8876-e11e4c7ffed7-mh548": Phase="Running", Reason="", readiness=true. Elapsed: 13.153392ms
    Jan 29 04:34:49.335: INFO: Pod "wrapped-volume-race-bb30c6fe-51c0-4270-8876-e11e4c7ffed7-mh548" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-bb30c6fe-51c0-4270-8876-e11e4c7ffed7 in namespace emptydir-wrapper-2316, will wait for the garbage collector to delete the pods 01/29/23 04:34:49.335
    Jan 29 04:34:49.424: INFO: Deleting ReplicationController wrapped-volume-race-bb30c6fe-51c0-4270-8876-e11e4c7ffed7 took: 21.37893ms
    Jan 29 04:34:49.625: INFO: Terminating ReplicationController wrapped-volume-race-bb30c6fe-51c0-4270-8876-e11e4c7ffed7 pods took: 200.834186ms
    STEP: Creating RC which spawns configmap-volume pods 01/29/23 04:34:54.241
    Jan 29 04:34:54.281: INFO: Pod name wrapped-volume-race-fc08c5b6-4d7d-467a-abf1-902eefaf3029: Found 0 pods out of 5
    Jan 29 04:34:59.295: INFO: Pod name wrapped-volume-race-fc08c5b6-4d7d-467a-abf1-902eefaf3029: Found 5 pods out of 5
    STEP: Ensuring each pod is running 01/29/23 04:34:59.295
    Jan 29 04:34:59.295: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-fc08c5b6-4d7d-467a-abf1-902eefaf3029-9tjpp" in namespace "emptydir-wrapper-2316" to be "running"
    Jan 29 04:34:59.302: INFO: Pod "wrapped-volume-race-fc08c5b6-4d7d-467a-abf1-902eefaf3029-9tjpp": Phase="Pending", Reason="", readiness=false. Elapsed: 6.603566ms
    Jan 29 04:35:01.311: INFO: Pod "wrapped-volume-race-fc08c5b6-4d7d-467a-abf1-902eefaf3029-9tjpp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015289357s
    Jan 29 04:35:03.312: INFO: Pod "wrapped-volume-race-fc08c5b6-4d7d-467a-abf1-902eefaf3029-9tjpp": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016648656s
    Jan 29 04:35:05.312: INFO: Pod "wrapped-volume-race-fc08c5b6-4d7d-467a-abf1-902eefaf3029-9tjpp": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016505244s
    Jan 29 04:35:07.327: INFO: Pod "wrapped-volume-race-fc08c5b6-4d7d-467a-abf1-902eefaf3029-9tjpp": Phase="Pending", Reason="", readiness=false. Elapsed: 8.031153096s
    Jan 29 04:35:09.310: INFO: Pod "wrapped-volume-race-fc08c5b6-4d7d-467a-abf1-902eefaf3029-9tjpp": Phase="Running", Reason="", readiness=true. Elapsed: 10.015062292s
    Jan 29 04:35:09.311: INFO: Pod "wrapped-volume-race-fc08c5b6-4d7d-467a-abf1-902eefaf3029-9tjpp" satisfied condition "running"
    Jan 29 04:35:09.311: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-fc08c5b6-4d7d-467a-abf1-902eefaf3029-m2ds9" in namespace "emptydir-wrapper-2316" to be "running"
    Jan 29 04:35:09.318: INFO: Pod "wrapped-volume-race-fc08c5b6-4d7d-467a-abf1-902eefaf3029-m2ds9": Phase="Running", Reason="", readiness=true. Elapsed: 7.245871ms
    Jan 29 04:35:09.318: INFO: Pod "wrapped-volume-race-fc08c5b6-4d7d-467a-abf1-902eefaf3029-m2ds9" satisfied condition "running"
    Jan 29 04:35:09.318: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-fc08c5b6-4d7d-467a-abf1-902eefaf3029-nv6v9" in namespace "emptydir-wrapper-2316" to be "running"
    Jan 29 04:35:09.325: INFO: Pod "wrapped-volume-race-fc08c5b6-4d7d-467a-abf1-902eefaf3029-nv6v9": Phase="Pending", Reason="", readiness=false. Elapsed: 7.177771ms
    Jan 29 04:35:11.334: INFO: Pod "wrapped-volume-race-fc08c5b6-4d7d-467a-abf1-902eefaf3029-nv6v9": Phase="Running", Reason="", readiness=true. Elapsed: 2.016463945s
    Jan 29 04:35:11.334: INFO: Pod "wrapped-volume-race-fc08c5b6-4d7d-467a-abf1-902eefaf3029-nv6v9" satisfied condition "running"
    Jan 29 04:35:11.338: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-fc08c5b6-4d7d-467a-abf1-902eefaf3029-nxq4h" in namespace "emptydir-wrapper-2316" to be "running"
    Jan 29 04:35:11.350: INFO: Pod "wrapped-volume-race-fc08c5b6-4d7d-467a-abf1-902eefaf3029-nxq4h": Phase="Running", Reason="", readiness=true. Elapsed: 12.260246ms
    Jan 29 04:35:11.350: INFO: Pod "wrapped-volume-race-fc08c5b6-4d7d-467a-abf1-902eefaf3029-nxq4h" satisfied condition "running"
    Jan 29 04:35:11.350: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-fc08c5b6-4d7d-467a-abf1-902eefaf3029-zx2f5" in namespace "emptydir-wrapper-2316" to be "running"
    Jan 29 04:35:11.358: INFO: Pod "wrapped-volume-race-fc08c5b6-4d7d-467a-abf1-902eefaf3029-zx2f5": Phase="Running", Reason="", readiness=true. Elapsed: 7.612853ms
    Jan 29 04:35:11.358: INFO: Pod "wrapped-volume-race-fc08c5b6-4d7d-467a-abf1-902eefaf3029-zx2f5" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-fc08c5b6-4d7d-467a-abf1-902eefaf3029 in namespace emptydir-wrapper-2316, will wait for the garbage collector to delete the pods 01/29/23 04:35:11.358
    Jan 29 04:35:11.431: INFO: Deleting ReplicationController wrapped-volume-race-fc08c5b6-4d7d-467a-abf1-902eefaf3029 took: 14.336461ms
    Jan 29 04:35:11.632: INFO: Terminating ReplicationController wrapped-volume-race-fc08c5b6-4d7d-467a-abf1-902eefaf3029 pods took: 201.181969ms
    STEP: Cleaning up the configMaps 01/29/23 04:35:14.933
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    Jan 29 04:35:15.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-2316" for this suite. 01/29/23 04:35:15.553
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:35:15.564
Jan 29 04:35:15.564: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename subpath 01/29/23 04:35:15.566
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:35:15.59
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:35:15.595
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/29/23 04:35:15.6
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-96fn 01/29/23 04:35:15.615
STEP: Creating a pod to test atomic-volume-subpath 01/29/23 04:35:15.615
Jan 29 04:35:15.635: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-96fn" in namespace "subpath-3545" to be "Succeeded or Failed"
Jan 29 04:35:15.642: INFO: Pod "pod-subpath-test-configmap-96fn": Phase="Pending", Reason="", readiness=false. Elapsed: 7.22951ms
Jan 29 04:35:17.651: INFO: Pod "pod-subpath-test-configmap-96fn": Phase="Running", Reason="", readiness=true. Elapsed: 2.01589096s
Jan 29 04:35:19.655: INFO: Pod "pod-subpath-test-configmap-96fn": Phase="Running", Reason="", readiness=true. Elapsed: 4.020037159s
Jan 29 04:35:21.651: INFO: Pod "pod-subpath-test-configmap-96fn": Phase="Running", Reason="", readiness=true. Elapsed: 6.016268601s
Jan 29 04:35:23.650: INFO: Pod "pod-subpath-test-configmap-96fn": Phase="Running", Reason="", readiness=true. Elapsed: 8.015401245s
Jan 29 04:35:25.650: INFO: Pod "pod-subpath-test-configmap-96fn": Phase="Running", Reason="", readiness=true. Elapsed: 10.015186813s
Jan 29 04:35:27.649: INFO: Pod "pod-subpath-test-configmap-96fn": Phase="Running", Reason="", readiness=true. Elapsed: 12.014280736s
Jan 29 04:35:29.652: INFO: Pod "pod-subpath-test-configmap-96fn": Phase="Running", Reason="", readiness=true. Elapsed: 14.017626748s
Jan 29 04:35:31.652: INFO: Pod "pod-subpath-test-configmap-96fn": Phase="Running", Reason="", readiness=true. Elapsed: 16.017635438s
Jan 29 04:35:33.651: INFO: Pod "pod-subpath-test-configmap-96fn": Phase="Running", Reason="", readiness=true. Elapsed: 18.015905795s
Jan 29 04:35:35.651: INFO: Pod "pod-subpath-test-configmap-96fn": Phase="Running", Reason="", readiness=true. Elapsed: 20.016181546s
Jan 29 04:35:37.650: INFO: Pod "pod-subpath-test-configmap-96fn": Phase="Running", Reason="", readiness=true. Elapsed: 22.01542821s
Jan 29 04:35:39.651: INFO: Pod "pod-subpath-test-configmap-96fn": Phase="Running", Reason="", readiness=false. Elapsed: 24.016128445s
Jan 29 04:35:41.655: INFO: Pod "pod-subpath-test-configmap-96fn": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.020178622s
STEP: Saw pod success 01/29/23 04:35:41.655
Jan 29 04:35:41.655: INFO: Pod "pod-subpath-test-configmap-96fn" satisfied condition "Succeeded or Failed"
Jan 29 04:35:41.662: INFO: Trying to get logs from node slave2 pod pod-subpath-test-configmap-96fn container test-container-subpath-configmap-96fn: <nil>
STEP: delete the pod 01/29/23 04:35:41.695
Jan 29 04:35:41.795: INFO: Waiting for pod pod-subpath-test-configmap-96fn to disappear
Jan 29 04:35:41.801: INFO: Pod pod-subpath-test-configmap-96fn no longer exists
STEP: Deleting pod pod-subpath-test-configmap-96fn 01/29/23 04:35:41.801
Jan 29 04:35:41.802: INFO: Deleting pod "pod-subpath-test-configmap-96fn" in namespace "subpath-3545"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jan 29 04:35:41.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3545" for this suite. 01/29/23 04:35:41.819
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]","completed":340,"skipped":6286,"failed":0}
------------------------------
• [SLOW TEST] [26.264 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:35:15.564
    Jan 29 04:35:15.564: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename subpath 01/29/23 04:35:15.566
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:35:15.59
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:35:15.595
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/29/23 04:35:15.6
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-96fn 01/29/23 04:35:15.615
    STEP: Creating a pod to test atomic-volume-subpath 01/29/23 04:35:15.615
    Jan 29 04:35:15.635: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-96fn" in namespace "subpath-3545" to be "Succeeded or Failed"
    Jan 29 04:35:15.642: INFO: Pod "pod-subpath-test-configmap-96fn": Phase="Pending", Reason="", readiness=false. Elapsed: 7.22951ms
    Jan 29 04:35:17.651: INFO: Pod "pod-subpath-test-configmap-96fn": Phase="Running", Reason="", readiness=true. Elapsed: 2.01589096s
    Jan 29 04:35:19.655: INFO: Pod "pod-subpath-test-configmap-96fn": Phase="Running", Reason="", readiness=true. Elapsed: 4.020037159s
    Jan 29 04:35:21.651: INFO: Pod "pod-subpath-test-configmap-96fn": Phase="Running", Reason="", readiness=true. Elapsed: 6.016268601s
    Jan 29 04:35:23.650: INFO: Pod "pod-subpath-test-configmap-96fn": Phase="Running", Reason="", readiness=true. Elapsed: 8.015401245s
    Jan 29 04:35:25.650: INFO: Pod "pod-subpath-test-configmap-96fn": Phase="Running", Reason="", readiness=true. Elapsed: 10.015186813s
    Jan 29 04:35:27.649: INFO: Pod "pod-subpath-test-configmap-96fn": Phase="Running", Reason="", readiness=true. Elapsed: 12.014280736s
    Jan 29 04:35:29.652: INFO: Pod "pod-subpath-test-configmap-96fn": Phase="Running", Reason="", readiness=true. Elapsed: 14.017626748s
    Jan 29 04:35:31.652: INFO: Pod "pod-subpath-test-configmap-96fn": Phase="Running", Reason="", readiness=true. Elapsed: 16.017635438s
    Jan 29 04:35:33.651: INFO: Pod "pod-subpath-test-configmap-96fn": Phase="Running", Reason="", readiness=true. Elapsed: 18.015905795s
    Jan 29 04:35:35.651: INFO: Pod "pod-subpath-test-configmap-96fn": Phase="Running", Reason="", readiness=true. Elapsed: 20.016181546s
    Jan 29 04:35:37.650: INFO: Pod "pod-subpath-test-configmap-96fn": Phase="Running", Reason="", readiness=true. Elapsed: 22.01542821s
    Jan 29 04:35:39.651: INFO: Pod "pod-subpath-test-configmap-96fn": Phase="Running", Reason="", readiness=false. Elapsed: 24.016128445s
    Jan 29 04:35:41.655: INFO: Pod "pod-subpath-test-configmap-96fn": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.020178622s
    STEP: Saw pod success 01/29/23 04:35:41.655
    Jan 29 04:35:41.655: INFO: Pod "pod-subpath-test-configmap-96fn" satisfied condition "Succeeded or Failed"
    Jan 29 04:35:41.662: INFO: Trying to get logs from node slave2 pod pod-subpath-test-configmap-96fn container test-container-subpath-configmap-96fn: <nil>
    STEP: delete the pod 01/29/23 04:35:41.695
    Jan 29 04:35:41.795: INFO: Waiting for pod pod-subpath-test-configmap-96fn to disappear
    Jan 29 04:35:41.801: INFO: Pod pod-subpath-test-configmap-96fn no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-96fn 01/29/23 04:35:41.801
    Jan 29 04:35:41.802: INFO: Deleting pod "pod-subpath-test-configmap-96fn" in namespace "subpath-3545"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jan 29 04:35:41.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-3545" for this suite. 01/29/23 04:35:41.819
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:35:41.83
Jan 29 04:35:41.830: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename downward-api 01/29/23 04:35:41.832
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:35:41.858
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:35:41.864
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
STEP: Creating a pod to test downward API volume plugin 01/29/23 04:35:41.869
Jan 29 04:35:41.898: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3c8e6b48-f674-4c46-bc17-e3d496cc37f0" in namespace "downward-api-5764" to be "Succeeded or Failed"
Jan 29 04:35:41.906: INFO: Pod "downwardapi-volume-3c8e6b48-f674-4c46-bc17-e3d496cc37f0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.591353ms
Jan 29 04:35:43.913: INFO: Pod "downwardapi-volume-3c8e6b48-f674-4c46-bc17-e3d496cc37f0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014742832s
Jan 29 04:35:45.915: INFO: Pod "downwardapi-volume-3c8e6b48-f674-4c46-bc17-e3d496cc37f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017258479s
STEP: Saw pod success 01/29/23 04:35:45.916
Jan 29 04:35:45.916: INFO: Pod "downwardapi-volume-3c8e6b48-f674-4c46-bc17-e3d496cc37f0" satisfied condition "Succeeded or Failed"
Jan 29 04:35:45.923: INFO: Trying to get logs from node slave2 pod downwardapi-volume-3c8e6b48-f674-4c46-bc17-e3d496cc37f0 container client-container: <nil>
STEP: delete the pod 01/29/23 04:35:45.938
Jan 29 04:35:46.039: INFO: Waiting for pod downwardapi-volume-3c8e6b48-f674-4c46-bc17-e3d496cc37f0 to disappear
Jan 29 04:35:46.047: INFO: Pod downwardapi-volume-3c8e6b48-f674-4c46-bc17-e3d496cc37f0 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 29 04:35:46.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5764" for this suite. 01/29/23 04:35:46.056
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","completed":341,"skipped":6311,"failed":0}
------------------------------
• [4.237 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:35:41.83
    Jan 29 04:35:41.830: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename downward-api 01/29/23 04:35:41.832
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:35:41.858
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:35:41.864
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:234
    STEP: Creating a pod to test downward API volume plugin 01/29/23 04:35:41.869
    Jan 29 04:35:41.898: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3c8e6b48-f674-4c46-bc17-e3d496cc37f0" in namespace "downward-api-5764" to be "Succeeded or Failed"
    Jan 29 04:35:41.906: INFO: Pod "downwardapi-volume-3c8e6b48-f674-4c46-bc17-e3d496cc37f0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.591353ms
    Jan 29 04:35:43.913: INFO: Pod "downwardapi-volume-3c8e6b48-f674-4c46-bc17-e3d496cc37f0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014742832s
    Jan 29 04:35:45.915: INFO: Pod "downwardapi-volume-3c8e6b48-f674-4c46-bc17-e3d496cc37f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017258479s
    STEP: Saw pod success 01/29/23 04:35:45.916
    Jan 29 04:35:45.916: INFO: Pod "downwardapi-volume-3c8e6b48-f674-4c46-bc17-e3d496cc37f0" satisfied condition "Succeeded or Failed"
    Jan 29 04:35:45.923: INFO: Trying to get logs from node slave2 pod downwardapi-volume-3c8e6b48-f674-4c46-bc17-e3d496cc37f0 container client-container: <nil>
    STEP: delete the pod 01/29/23 04:35:45.938
    Jan 29 04:35:46.039: INFO: Waiting for pod downwardapi-volume-3c8e6b48-f674-4c46-bc17-e3d496cc37f0 to disappear
    Jan 29 04:35:46.047: INFO: Pod downwardapi-volume-3c8e6b48-f674-4c46-bc17-e3d496cc37f0 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 29 04:35:46.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-5764" for this suite. 01/29/23 04:35:46.056
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:35:46.068
Jan 29 04:35:46.068: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename downward-api 01/29/23 04:35:46.069
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:35:46.094
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:35:46.1
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
STEP: Creating a pod to test downward api env vars 01/29/23 04:35:46.106
Jan 29 04:35:46.125: INFO: Waiting up to 5m0s for pod "downward-api-af08d7b1-a35b-44c4-9e7b-2fdec4fb7e14" in namespace "downward-api-7305" to be "Succeeded or Failed"
Jan 29 04:35:46.133: INFO: Pod "downward-api-af08d7b1-a35b-44c4-9e7b-2fdec4fb7e14": Phase="Pending", Reason="", readiness=false. Elapsed: 7.877016ms
Jan 29 04:35:48.142: INFO: Pod "downward-api-af08d7b1-a35b-44c4-9e7b-2fdec4fb7e14": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016775507s
Jan 29 04:35:50.141: INFO: Pod "downward-api-af08d7b1-a35b-44c4-9e7b-2fdec4fb7e14": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015940691s
STEP: Saw pod success 01/29/23 04:35:50.141
Jan 29 04:35:50.141: INFO: Pod "downward-api-af08d7b1-a35b-44c4-9e7b-2fdec4fb7e14" satisfied condition "Succeeded or Failed"
Jan 29 04:35:50.147: INFO: Trying to get logs from node slave2 pod downward-api-af08d7b1-a35b-44c4-9e7b-2fdec4fb7e14 container dapi-container: <nil>
STEP: delete the pod 01/29/23 04:35:50.165
Jan 29 04:35:50.236: INFO: Waiting for pod downward-api-af08d7b1-a35b-44c4-9e7b-2fdec4fb7e14 to disappear
Jan 29 04:35:50.243: INFO: Pod downward-api-af08d7b1-a35b-44c4-9e7b-2fdec4fb7e14 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jan 29 04:35:50.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7305" for this suite. 01/29/23 04:35:50.253
{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","completed":342,"skipped":6311,"failed":0}
------------------------------
• [4.195 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:35:46.068
    Jan 29 04:35:46.068: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename downward-api 01/29/23 04:35:46.069
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:35:46.094
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:35:46.1
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:266
    STEP: Creating a pod to test downward api env vars 01/29/23 04:35:46.106
    Jan 29 04:35:46.125: INFO: Waiting up to 5m0s for pod "downward-api-af08d7b1-a35b-44c4-9e7b-2fdec4fb7e14" in namespace "downward-api-7305" to be "Succeeded or Failed"
    Jan 29 04:35:46.133: INFO: Pod "downward-api-af08d7b1-a35b-44c4-9e7b-2fdec4fb7e14": Phase="Pending", Reason="", readiness=false. Elapsed: 7.877016ms
    Jan 29 04:35:48.142: INFO: Pod "downward-api-af08d7b1-a35b-44c4-9e7b-2fdec4fb7e14": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016775507s
    Jan 29 04:35:50.141: INFO: Pod "downward-api-af08d7b1-a35b-44c4-9e7b-2fdec4fb7e14": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015940691s
    STEP: Saw pod success 01/29/23 04:35:50.141
    Jan 29 04:35:50.141: INFO: Pod "downward-api-af08d7b1-a35b-44c4-9e7b-2fdec4fb7e14" satisfied condition "Succeeded or Failed"
    Jan 29 04:35:50.147: INFO: Trying to get logs from node slave2 pod downward-api-af08d7b1-a35b-44c4-9e7b-2fdec4fb7e14 container dapi-container: <nil>
    STEP: delete the pod 01/29/23 04:35:50.165
    Jan 29 04:35:50.236: INFO: Waiting for pod downward-api-af08d7b1-a35b-44c4-9e7b-2fdec4fb7e14 to disappear
    Jan 29 04:35:50.243: INFO: Pod downward-api-af08d7b1-a35b-44c4-9e7b-2fdec4fb7e14 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jan 29 04:35:50.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-7305" for this suite. 01/29/23 04:35:50.253
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:35:50.266
Jan 29 04:35:50.266: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename projected 01/29/23 04:35:50.267
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:35:50.292
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:35:50.298
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
STEP: Creating a pod to test downward API volume plugin 01/29/23 04:35:50.304
Jan 29 04:35:50.324: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a43519ab-cc32-4c7d-a3b0-136bc3505fbd" in namespace "projected-3661" to be "Succeeded or Failed"
Jan 29 04:35:50.332: INFO: Pod "downwardapi-volume-a43519ab-cc32-4c7d-a3b0-136bc3505fbd": Phase="Pending", Reason="", readiness=false. Elapsed: 7.943015ms
Jan 29 04:35:52.341: INFO: Pod "downwardapi-volume-a43519ab-cc32-4c7d-a3b0-136bc3505fbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016828047s
Jan 29 04:35:54.341: INFO: Pod "downwardapi-volume-a43519ab-cc32-4c7d-a3b0-136bc3505fbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016668375s
STEP: Saw pod success 01/29/23 04:35:54.341
Jan 29 04:35:54.341: INFO: Pod "downwardapi-volume-a43519ab-cc32-4c7d-a3b0-136bc3505fbd" satisfied condition "Succeeded or Failed"
Jan 29 04:35:54.347: INFO: Trying to get logs from node slave2 pod downwardapi-volume-a43519ab-cc32-4c7d-a3b0-136bc3505fbd container client-container: <nil>
STEP: delete the pod 01/29/23 04:35:54.362
Jan 29 04:35:54.426: INFO: Waiting for pod downwardapi-volume-a43519ab-cc32-4c7d-a3b0-136bc3505fbd to disappear
Jan 29 04:35:54.432: INFO: Pod downwardapi-volume-a43519ab-cc32-4c7d-a3b0-136bc3505fbd no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 29 04:35:54.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3661" for this suite. 01/29/23 04:35:54.442
{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":343,"skipped":6345,"failed":0}
------------------------------
• [4.186 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:35:50.266
    Jan 29 04:35:50.266: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename projected 01/29/23 04:35:50.267
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:35:50.292
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:35:50.298
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:83
    STEP: Creating a pod to test downward API volume plugin 01/29/23 04:35:50.304
    Jan 29 04:35:50.324: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a43519ab-cc32-4c7d-a3b0-136bc3505fbd" in namespace "projected-3661" to be "Succeeded or Failed"
    Jan 29 04:35:50.332: INFO: Pod "downwardapi-volume-a43519ab-cc32-4c7d-a3b0-136bc3505fbd": Phase="Pending", Reason="", readiness=false. Elapsed: 7.943015ms
    Jan 29 04:35:52.341: INFO: Pod "downwardapi-volume-a43519ab-cc32-4c7d-a3b0-136bc3505fbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016828047s
    Jan 29 04:35:54.341: INFO: Pod "downwardapi-volume-a43519ab-cc32-4c7d-a3b0-136bc3505fbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016668375s
    STEP: Saw pod success 01/29/23 04:35:54.341
    Jan 29 04:35:54.341: INFO: Pod "downwardapi-volume-a43519ab-cc32-4c7d-a3b0-136bc3505fbd" satisfied condition "Succeeded or Failed"
    Jan 29 04:35:54.347: INFO: Trying to get logs from node slave2 pod downwardapi-volume-a43519ab-cc32-4c7d-a3b0-136bc3505fbd container client-container: <nil>
    STEP: delete the pod 01/29/23 04:35:54.362
    Jan 29 04:35:54.426: INFO: Waiting for pod downwardapi-volume-a43519ab-cc32-4c7d-a3b0-136bc3505fbd to disappear
    Jan 29 04:35:54.432: INFO: Pod downwardapi-volume-a43519ab-cc32-4c7d-a3b0-136bc3505fbd no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 29 04:35:54.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3661" for this suite. 01/29/23 04:35:54.442
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:35:54.453
Jan 29 04:35:54.453: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename certificates 01/29/23 04:35:54.454
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:35:54.48
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:35:54.486
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 01/29/23 04:35:56.098
STEP: getting /apis/certificates.k8s.io 01/29/23 04:35:56.103
STEP: getting /apis/certificates.k8s.io/v1 01/29/23 04:35:56.106
STEP: creating 01/29/23 04:35:56.108
STEP: getting 01/29/23 04:35:56.132
STEP: listing 01/29/23 04:35:56.138
STEP: watching 01/29/23 04:35:56.144
Jan 29 04:35:56.144: INFO: starting watch
STEP: patching 01/29/23 04:35:56.146
STEP: updating 01/29/23 04:35:56.154
Jan 29 04:35:56.163: INFO: waiting for watch events with expected annotations
Jan 29 04:35:56.163: INFO: saw patched and updated annotations
STEP: getting /approval 01/29/23 04:35:56.163
STEP: patching /approval 01/29/23 04:35:56.17
STEP: updating /approval 01/29/23 04:35:56.178
STEP: getting /status 01/29/23 04:35:56.193
STEP: patching /status 01/29/23 04:35:56.2
STEP: updating /status 01/29/23 04:35:56.212
STEP: deleting 01/29/23 04:35:56.221
STEP: deleting a collection 01/29/23 04:35:56.243
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 29 04:35:56.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-5304" for this suite. 01/29/23 04:35:56.278
{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","completed":344,"skipped":6353,"failed":0}
------------------------------
• [1.835 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:35:54.453
    Jan 29 04:35:54.453: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename certificates 01/29/23 04:35:54.454
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:35:54.48
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:35:54.486
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 01/29/23 04:35:56.098
    STEP: getting /apis/certificates.k8s.io 01/29/23 04:35:56.103
    STEP: getting /apis/certificates.k8s.io/v1 01/29/23 04:35:56.106
    STEP: creating 01/29/23 04:35:56.108
    STEP: getting 01/29/23 04:35:56.132
    STEP: listing 01/29/23 04:35:56.138
    STEP: watching 01/29/23 04:35:56.144
    Jan 29 04:35:56.144: INFO: starting watch
    STEP: patching 01/29/23 04:35:56.146
    STEP: updating 01/29/23 04:35:56.154
    Jan 29 04:35:56.163: INFO: waiting for watch events with expected annotations
    Jan 29 04:35:56.163: INFO: saw patched and updated annotations
    STEP: getting /approval 01/29/23 04:35:56.163
    STEP: patching /approval 01/29/23 04:35:56.17
    STEP: updating /approval 01/29/23 04:35:56.178
    STEP: getting /status 01/29/23 04:35:56.193
    STEP: patching /status 01/29/23 04:35:56.2
    STEP: updating /status 01/29/23 04:35:56.212
    STEP: deleting 01/29/23 04:35:56.221
    STEP: deleting a collection 01/29/23 04:35:56.243
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 29 04:35:56.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "certificates-5304" for this suite. 01/29/23 04:35:56.278
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:35:56.288
Jan 29 04:35:56.288: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename gc 01/29/23 04:35:56.29
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:35:56.311
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:35:56.319
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 01/29/23 04:35:56.324
STEP: Wait for the Deployment to create new ReplicaSet 01/29/23 04:35:56.339
STEP: delete the deployment 01/29/23 04:35:56.869
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 01/29/23 04:35:56.882
STEP: Gathering metrics 01/29/23 04:35:57.432
Jan 29 04:35:57.494: INFO: Waiting up to 5m0s for pod "kube-controller-manager-master3" in namespace "kube-system" to be "running and ready"
Jan 29 04:35:57.501: INFO: Pod "kube-controller-manager-master3": Phase="Running", Reason="", readiness=true. Elapsed: 6.717687ms
Jan 29 04:35:57.501: INFO: The phase of Pod kube-controller-manager-master3 is Running (Ready = true)
Jan 29 04:35:57.501: INFO: Pod "kube-controller-manager-master3" satisfied condition "running and ready"
E0129 04:35:57.571433      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:35:57.571433      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:35:58.650119      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:35:58.650119      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:35:59.730439      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:35:59.730439      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:00.805059      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:00.805059      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:01.874141      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:01.874141      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:02.941437      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:02.941437      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:04.026334      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:04.026334      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:05.092986      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:05.092986      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:06.162614      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:06.162614      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:07.228872      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:07.228872      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:08.314868      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:08.314868      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:09.387743      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:09.387743      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:10.459665      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:10.459665      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:11.528417      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:11.528417      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:12.595510      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:12.595510      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:13.682739      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:13.682739      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:14.754265      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:14.754265      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:15.834819      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:15.834819      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:16.919133      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:16.919133      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:17.990048      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:17.990048      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:19.057716      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:19.057716      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:20.126020      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:20.126020      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:20.386192      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:20.386192      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:21.474577      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:21.474577      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:22.549687      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:22.549687      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:23.619556      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:23.619556      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:24.703913      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:24.703913      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:25.772576      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:25.772576      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:26.839183      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:26.839183      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:27.912469      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:27.912469      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:28.996669      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:28.996669      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:30.080018      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:30.080018      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:31.148673      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:31.148673      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:33.526116      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:33.526116      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:34.616066      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:34.616066      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:35.688564      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:35.688564      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:36.764504      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:36.764504      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:37.855997      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:37.855997      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:38.940607      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:38.940607      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:40.016523      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:40.016523      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:41.097530      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:41.097530      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:42.187743      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:42.187743      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:42.421237      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:42.421237      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:43.493852      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:43.493852      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:44.569207      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:44.569207      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:45.651818      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:45.651818      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:46.717934      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:46.717934      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:47.795281      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:47.795281      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:48.873324      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:48.873324      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:49.959138      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:49.959138      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:51.026073      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:51.026073      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:52.095470      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:52.095470      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:53.167813      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:53.167813      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:53.385608      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:53.385608      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:54.466406      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:54.466406      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:55.534683      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:55.534683      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:56.601628      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:56.601628      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:57.674628      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:57.674628      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:58.759940      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:58.759940      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:59.842088      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:36:59.842088      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:37:00.913459      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:37:00.913459      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:37:01.989257      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:37:01.989257      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:37:03.057673      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:37:03.057673      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:37:04.130547      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:37:04.130547      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:37:04.552096      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:37:04.552096      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:37:05.627231      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:37:05.627231      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:37:06.700867      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:37:06.700867      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:37:07.790422      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:37:07.790422      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:37:08.864546      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:37:08.864546      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:37:09.937447      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:37:09.937447      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:37:11.005418      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:37:11.005418      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:37:12.077279      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:37:12.077279      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:37:13.158892      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:37:13.158892      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:37:14.228918      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:37:14.228918      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:37:15.305537      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:37:15.305537      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:37:15.373943      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:37:15.373943      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:37:16.442840      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:37:16.442840      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:37:17.510788      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:37:17.510788      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:37:18.579820      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:37:18.579820      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:37:19.654093      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:37:19.654093      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:37:21.808073      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:37:21.808073      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:37:22.891003      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:37:22.891003      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:37:24.198585      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:37:24.198585      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:37:25.279299      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:37:25.279299      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
Jan 29 04:37:26.372: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan 29 04:37:26.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7256" for this suite. 01/29/23 04:37:26.382
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","completed":345,"skipped":6362,"failed":0}
------------------------------
• [SLOW TEST] [90.103 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:35:56.288
    Jan 29 04:35:56.288: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename gc 01/29/23 04:35:56.29
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:35:56.311
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:35:56.319
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 01/29/23 04:35:56.324
    STEP: Wait for the Deployment to create new ReplicaSet 01/29/23 04:35:56.339
    STEP: delete the deployment 01/29/23 04:35:56.869
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 01/29/23 04:35:56.882
    STEP: Gathering metrics 01/29/23 04:35:57.432
    Jan 29 04:35:57.494: INFO: Waiting up to 5m0s for pod "kube-controller-manager-master3" in namespace "kube-system" to be "running and ready"
    Jan 29 04:35:57.501: INFO: Pod "kube-controller-manager-master3": Phase="Running", Reason="", readiness=true. Elapsed: 6.717687ms
    Jan 29 04:35:57.501: INFO: The phase of Pod kube-controller-manager-master3 is Running (Ready = true)
    Jan 29 04:35:57.501: INFO: Pod "kube-controller-manager-master3" satisfied condition "running and ready"
    E0129 04:35:57.571433      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:35:58.650119      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:35:59.730439      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:36:00.805059      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:36:01.874141      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:36:02.941437      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:36:04.026334      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:36:05.092986      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:36:06.162614      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:36:07.228872      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:36:08.314868      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:36:09.387743      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:36:10.459665      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:36:11.528417      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:36:12.595510      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:36:13.682739      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:36:14.754265      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:36:15.834819      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:36:16.919133      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:36:17.990048      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:36:19.057716      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:36:20.126020      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:36:20.386192      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:36:21.474577      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:36:22.549687      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:36:23.619556      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:36:24.703913      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:36:25.772576      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:36:26.839183      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:36:27.912469      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:36:28.996669      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:36:30.080018      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:36:31.148673      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:36:33.526116      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:36:34.616066      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:36:35.688564      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:36:36.764504      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:36:37.855997      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:36:38.940607      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:36:40.016523      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:36:41.097530      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:36:42.187743      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:36:42.421237      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:36:43.493852      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:36:44.569207      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:36:45.651818      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:36:46.717934      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:36:47.795281      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:36:48.873324      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:36:49.959138      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:36:51.026073      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:36:52.095470      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:36:53.167813      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:36:53.385608      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:36:54.466406      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:36:55.534683      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:36:56.601628      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:36:57.674628      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:36:58.759940      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:36:59.842088      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:37:00.913459      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:37:01.989257      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:37:03.057673      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:37:04.130547      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:37:04.552096      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:37:05.627231      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:37:06.700867      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:37:07.790422      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:37:08.864546      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:37:09.937447      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:37:11.005418      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:37:12.077279      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:37:13.158892      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:37:14.228918      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:37:15.305537      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:37:15.373943      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:37:16.442840      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:37:17.510788      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:37:18.579820      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:37:19.654093      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:37:21.808073      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:37:22.891003      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:37:24.198585      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:37:25.279299      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    Jan 29 04:37:26.372: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan 29 04:37:26.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-7256" for this suite. 01/29/23 04:37:26.382
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:37:26.394
Jan 29 04:37:26.394: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename webhook 01/29/23 04:37:26.395
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:37:26.428
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:37:26.434
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/29/23 04:37:26.46
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/29/23 04:37:27.466
STEP: Deploying the webhook pod 01/29/23 04:37:27.485
STEP: Wait for the deployment to be ready 01/29/23 04:37:27.52
Jan 29 04:37:27.534: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jan 29 04:37:29.557: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 29, 4, 37, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 4, 37, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 29, 4, 37, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 4, 37, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/29/23 04:37:31.569
STEP: Verifying the service has paired with the endpoint 01/29/23 04:37:31.598
Jan 29 04:37:32.599: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
Jan 29 04:37:32.606: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9784-crds.webhook.example.com via the AdmissionRegistration API 01/29/23 04:37:33.12
STEP: Creating a custom resource that should be mutated by the webhook 01/29/23 04:37:34.744
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 29 04:37:35.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-931" for this suite. 01/29/23 04:37:35.329
STEP: Destroying namespace "webhook-931-markers" for this suite. 01/29/23 04:37:35.339
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","completed":346,"skipped":6383,"failed":0}
------------------------------
• [SLOW TEST] [9.035 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:37:26.394
    Jan 29 04:37:26.394: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename webhook 01/29/23 04:37:26.395
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:37:26.428
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:37:26.434
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/29/23 04:37:26.46
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/29/23 04:37:27.466
    STEP: Deploying the webhook pod 01/29/23 04:37:27.485
    STEP: Wait for the deployment to be ready 01/29/23 04:37:27.52
    Jan 29 04:37:27.534: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    Jan 29 04:37:29.557: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 29, 4, 37, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 4, 37, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 29, 4, 37, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 4, 37, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/29/23 04:37:31.569
    STEP: Verifying the service has paired with the endpoint 01/29/23 04:37:31.598
    Jan 29 04:37:32.599: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:290
    Jan 29 04:37:32.606: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9784-crds.webhook.example.com via the AdmissionRegistration API 01/29/23 04:37:33.12
    STEP: Creating a custom resource that should be mutated by the webhook 01/29/23 04:37:34.744
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 29 04:37:35.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-931" for this suite. 01/29/23 04:37:35.329
    STEP: Destroying namespace "webhook-931-markers" for this suite. 01/29/23 04:37:35.339
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:37:35.435
Jan 29 04:37:35.435: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename kubectl 01/29/23 04:37:35.437
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:37:35.472
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:37:35.481
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
STEP: starting the proxy server 01/29/23 04:37:35.488
Jan 29 04:37:35.488: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-5117 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 01/29/23 04:37:35.6
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 29 04:37:35.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5117" for this suite. 01/29/23 04:37:35.624
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","completed":347,"skipped":6411,"failed":0}
------------------------------
• [0.199 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1785

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:37:35.435
    Jan 29 04:37:35.435: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename kubectl 01/29/23 04:37:35.437
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:37:35.472
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:37:35.481
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1785
    STEP: starting the proxy server 01/29/23 04:37:35.488
    Jan 29 04:37:35.488: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=kubectl-5117 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 01/29/23 04:37:35.6
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 29 04:37:35.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-5117" for this suite. 01/29/23 04:37:35.624
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:37:35.635
Jan 29 04:37:35.636: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename gc 01/29/23 04:37:35.637
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:37:35.661
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:37:35.667
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 01/29/23 04:37:35.683
STEP: delete the rc 01/29/23 04:37:40.954
STEP: wait for the rc to be deleted 01/29/23 04:37:41.131
Jan 29 04:37:42.862: INFO: 100 pods remaining
Jan 29 04:37:42.862: INFO: 81 pods has nil DeletionTimestamp
Jan 29 04:37:42.862: INFO: 
Jan 29 04:37:43.484: INFO: 81 pods remaining
Jan 29 04:37:43.484: INFO: 80 pods has nil DeletionTimestamp
Jan 29 04:37:43.484: INFO: 
Jan 29 04:37:44.485: INFO: 79 pods remaining
Jan 29 04:37:44.485: INFO: 66 pods has nil DeletionTimestamp
Jan 29 04:37:44.485: INFO: 
Jan 29 04:37:45.519: INFO: 66 pods remaining
Jan 29 04:37:45.519: INFO: 59 pods has nil DeletionTimestamp
Jan 29 04:37:45.519: INFO: 
Jan 29 04:37:46.459: INFO: 56 pods remaining
Jan 29 04:37:46.459: INFO: 43 pods has nil DeletionTimestamp
Jan 29 04:37:46.459: INFO: 
Jan 29 04:37:47.449: INFO: 39 pods remaining
Jan 29 04:37:47.449: INFO: 31 pods has nil DeletionTimestamp
Jan 29 04:37:47.449: INFO: 
Jan 29 04:37:48.317: INFO: 24 pods remaining
Jan 29 04:37:48.318: INFO: 20 pods has nil DeletionTimestamp
Jan 29 04:37:48.318: INFO: 
Jan 29 04:37:49.462: INFO: 7 pods remaining
Jan 29 04:37:49.462: INFO: 1 pods has nil DeletionTimestamp
Jan 29 04:37:49.462: INFO: 
STEP: Gathering metrics 01/29/23 04:37:50.194
Jan 29 04:37:50.377: INFO: Waiting up to 5m0s for pod "kube-controller-manager-master3" in namespace "kube-system" to be "running and ready"
Jan 29 04:37:50.404: INFO: Pod "kube-controller-manager-master3": Phase="Running", Reason="", readiness=true. Elapsed: 26.563526ms
Jan 29 04:37:50.404: INFO: The phase of Pod kube-controller-manager-master3 is Running (Ready = true)
Jan 29 04:37:50.404: INFO: Pod "kube-controller-manager-master3" satisfied condition "running and ready"
E0129 04:37:50.667097      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:37:50.667097      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:37:54.422284      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:37:54.422284      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:37:55.552268      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:37:55.552268      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:37:56.647787      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:37:56.647787      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:37:57.722137      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:37:57.722137      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:37:58.801181      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:37:58.801181      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:37:59.875378      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:37:59.875378      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:00.974815      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:00.974815      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:02.050004      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:02.050004      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:03.119651      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:03.119651      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:04.202744      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:04.202744      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:05.291537      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:05.291537      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:06.358952      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:06.358952      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:09.592510      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:09.592510      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:10.668175      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:10.668175      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:11.738154      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:11.738154      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:12.811349      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:12.811349      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:14.119772      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:14.119772      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:15.199713      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:15.199713      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:16.270449      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:16.270449      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:17.339858      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:17.339858      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:18.419706      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:18.419706      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:19.509344      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:19.509344      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:20.589995      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:20.589995      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:21.654861      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:21.654861      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:22.732306      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:22.732306      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:23.838501      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:23.838501      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:24.911814      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:24.911814      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:25.117207      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:25.117207      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:26.189121      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:26.189121      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:27.262072      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:27.262072      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:28.335323      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:28.335323      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:29.411942      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:29.411942      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:30.487605      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:30.487605      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:31.573625      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:31.573625      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:32.655443      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:32.655443      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:33.739511      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:33.739511      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:34.806955      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:34.806955      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:36.123157      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:36.123157      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:37.192698      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:37.192698      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:38.259466      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:38.259466      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:39.331441      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:39.331441      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:40.409791      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:40.409791      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:41.474338      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:41.474338      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:42.541846      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:42.541846      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:43.616545      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:43.616545      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:44.694022      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:44.694022      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:45.762488      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:45.762488      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:46.831231      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:46.831231      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:47.130750      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:47.130750      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:48.201412      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:48.201412      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:49.272036      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:49.272036      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:50.341852      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:50.341852      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:52.485867      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:52.485867      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:53.553862      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:53.553862      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:54.685283      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:54.685283      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:55.758979      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:55.758979      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:56.839699      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:56.839699      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:57.909633      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:57.909633      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:58.120929      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:58.120929      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:59.189299      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:38:59.189299      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:39:00.255731      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:39:00.255731      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:39:01.328738      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:39:01.328738      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:39:02.393851      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:39:02.393851      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:39:03.468439      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:39:03.468439      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:39:04.559597      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:39:04.559597      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:39:05.631109      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:39:05.631109      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:39:06.706543      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:39:06.706543      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:39:07.781558      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:39:07.781558      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:39:08.857350      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:39:08.857350      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:39:08.928082      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:39:08.928082      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:39:09.999780      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:39:09.999780      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:39:11.066358      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:39:11.066358      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:39:13.206227      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:39:13.206227      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:39:14.277954      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:39:14.277954      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:39:15.354199      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:39:15.354199      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:39:16.422051      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:39:16.422051      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:39:17.493419      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:39:17.493419      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:39:18.567345      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:39:18.567345      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:39:19.634099      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:39:19.634099      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
Jan 29 04:39:19.634: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan 29 04:39:19.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2828" for this suite. 01/29/23 04:39:19.643
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","completed":348,"skipped":6424,"failed":0}
------------------------------
• [SLOW TEST] [104.017 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:37:35.635
    Jan 29 04:37:35.636: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename gc 01/29/23 04:37:35.637
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:37:35.661
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:37:35.667
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 01/29/23 04:37:35.683
    STEP: delete the rc 01/29/23 04:37:40.954
    STEP: wait for the rc to be deleted 01/29/23 04:37:41.131
    Jan 29 04:37:42.862: INFO: 100 pods remaining
    Jan 29 04:37:42.862: INFO: 81 pods has nil DeletionTimestamp
    Jan 29 04:37:42.862: INFO: 
    Jan 29 04:37:43.484: INFO: 81 pods remaining
    Jan 29 04:37:43.484: INFO: 80 pods has nil DeletionTimestamp
    Jan 29 04:37:43.484: INFO: 
    Jan 29 04:37:44.485: INFO: 79 pods remaining
    Jan 29 04:37:44.485: INFO: 66 pods has nil DeletionTimestamp
    Jan 29 04:37:44.485: INFO: 
    Jan 29 04:37:45.519: INFO: 66 pods remaining
    Jan 29 04:37:45.519: INFO: 59 pods has nil DeletionTimestamp
    Jan 29 04:37:45.519: INFO: 
    Jan 29 04:37:46.459: INFO: 56 pods remaining
    Jan 29 04:37:46.459: INFO: 43 pods has nil DeletionTimestamp
    Jan 29 04:37:46.459: INFO: 
    Jan 29 04:37:47.449: INFO: 39 pods remaining
    Jan 29 04:37:47.449: INFO: 31 pods has nil DeletionTimestamp
    Jan 29 04:37:47.449: INFO: 
    Jan 29 04:37:48.317: INFO: 24 pods remaining
    Jan 29 04:37:48.318: INFO: 20 pods has nil DeletionTimestamp
    Jan 29 04:37:48.318: INFO: 
    Jan 29 04:37:49.462: INFO: 7 pods remaining
    Jan 29 04:37:49.462: INFO: 1 pods has nil DeletionTimestamp
    Jan 29 04:37:49.462: INFO: 
    STEP: Gathering metrics 01/29/23 04:37:50.194
    Jan 29 04:37:50.377: INFO: Waiting up to 5m0s for pod "kube-controller-manager-master3" in namespace "kube-system" to be "running and ready"
    Jan 29 04:37:50.404: INFO: Pod "kube-controller-manager-master3": Phase="Running", Reason="", readiness=true. Elapsed: 26.563526ms
    Jan 29 04:37:50.404: INFO: The phase of Pod kube-controller-manager-master3 is Running (Ready = true)
    Jan 29 04:37:50.404: INFO: Pod "kube-controller-manager-master3" satisfied condition "running and ready"
    E0129 04:37:50.667097      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:37:54.422284      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:37:55.552268      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:37:56.647787      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:37:57.722137      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:37:58.801181      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:37:59.875378      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:38:00.974815      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:38:02.050004      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:38:03.119651      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:38:04.202744      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:38:05.291537      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:38:06.358952      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:38:09.592510      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:38:10.668175      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:38:11.738154      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:38:12.811349      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:38:14.119772      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:38:15.199713      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:38:16.270449      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:38:17.339858      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:38:18.419706      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:38:19.509344      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:38:20.589995      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:38:21.654861      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:38:22.732306      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:38:23.838501      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:38:24.911814      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:38:25.117207      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:38:26.189121      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:38:27.262072      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:38:28.335323      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:38:29.411942      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:38:30.487605      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:38:31.573625      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:38:32.655443      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:38:33.739511      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:38:34.806955      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:38:36.123157      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:38:37.192698      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:38:38.259466      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:38:39.331441      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:38:40.409791      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:38:41.474338      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:38:42.541846      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:38:43.616545      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:38:44.694022      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:38:45.762488      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:38:46.831231      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:38:47.130750      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:38:48.201412      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:38:49.272036      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:38:50.341852      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:38:52.485867      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:38:53.553862      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:38:54.685283      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:38:55.758979      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:38:56.839699      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:38:57.909633      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:38:58.120929      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:38:59.189299      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:39:00.255731      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:39:01.328738      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:39:02.393851      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:39:03.468439      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:39:04.559597      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:39:05.631109      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:39:06.706543      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:39:07.781558      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:39:08.857350      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:39:08.928082      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:39:09.999780      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:39:11.066358      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:39:13.206227      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:39:14.277954      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:39:15.354199      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:39:16.422051      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:39:17.493419      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:39:18.567345      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:39:19.634099      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    Jan 29 04:39:19.634: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan 29 04:39:19.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-2828" for this suite. 01/29/23 04:39:19.643
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:39:19.659
Jan 29 04:39:19.660: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename projected 01/29/23 04:39:19.661
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:39:19.696
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:39:19.702
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
STEP: Creating configMap with name cm-test-opt-del-10943aa5-8e7a-475d-8f8d-d4f054b3cb04 01/29/23 04:39:19.724
STEP: Creating configMap with name cm-test-opt-upd-a1068fc0-185c-495e-9c35-cf932a77af28 01/29/23 04:39:19.732
STEP: Creating the pod 01/29/23 04:39:19.739
Jan 29 04:39:19.776: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4209a109-3ea1-4472-a618-76529a36f837" in namespace "projected-6783" to be "running and ready"
Jan 29 04:39:19.785: INFO: Pod "pod-projected-configmaps-4209a109-3ea1-4472-a618-76529a36f837": Phase="Pending", Reason="", readiness=false. Elapsed: 8.866522ms
Jan 29 04:39:19.785: INFO: The phase of Pod pod-projected-configmaps-4209a109-3ea1-4472-a618-76529a36f837 is Pending, waiting for it to be Running (with Ready = true)
Jan 29 04:39:21.794: INFO: Pod "pod-projected-configmaps-4209a109-3ea1-4472-a618-76529a36f837": Phase="Running", Reason="", readiness=true. Elapsed: 2.017601556s
Jan 29 04:39:21.794: INFO: The phase of Pod pod-projected-configmaps-4209a109-3ea1-4472-a618-76529a36f837 is Running (Ready = true)
Jan 29 04:39:21.794: INFO: Pod "pod-projected-configmaps-4209a109-3ea1-4472-a618-76529a36f837" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-10943aa5-8e7a-475d-8f8d-d4f054b3cb04 01/29/23 04:39:21.862
STEP: Updating configmap cm-test-opt-upd-a1068fc0-185c-495e-9c35-cf932a77af28 01/29/23 04:39:21.872
STEP: Creating configMap with name cm-test-opt-create-ea828237-3b06-4bdb-a342-315d250a363f 01/29/23 04:39:21.88
STEP: waiting to observe update in volume 01/29/23 04:39:21.888
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 29 04:39:23.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6783" for this suite. 01/29/23 04:39:23.963
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":349,"skipped":6531,"failed":0}
------------------------------
• [4.313 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:39:19.659
    Jan 29 04:39:19.660: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename projected 01/29/23 04:39:19.661
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:39:19.696
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:39:19.702
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:173
    STEP: Creating configMap with name cm-test-opt-del-10943aa5-8e7a-475d-8f8d-d4f054b3cb04 01/29/23 04:39:19.724
    STEP: Creating configMap with name cm-test-opt-upd-a1068fc0-185c-495e-9c35-cf932a77af28 01/29/23 04:39:19.732
    STEP: Creating the pod 01/29/23 04:39:19.739
    Jan 29 04:39:19.776: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4209a109-3ea1-4472-a618-76529a36f837" in namespace "projected-6783" to be "running and ready"
    Jan 29 04:39:19.785: INFO: Pod "pod-projected-configmaps-4209a109-3ea1-4472-a618-76529a36f837": Phase="Pending", Reason="", readiness=false. Elapsed: 8.866522ms
    Jan 29 04:39:19.785: INFO: The phase of Pod pod-projected-configmaps-4209a109-3ea1-4472-a618-76529a36f837 is Pending, waiting for it to be Running (with Ready = true)
    Jan 29 04:39:21.794: INFO: Pod "pod-projected-configmaps-4209a109-3ea1-4472-a618-76529a36f837": Phase="Running", Reason="", readiness=true. Elapsed: 2.017601556s
    Jan 29 04:39:21.794: INFO: The phase of Pod pod-projected-configmaps-4209a109-3ea1-4472-a618-76529a36f837 is Running (Ready = true)
    Jan 29 04:39:21.794: INFO: Pod "pod-projected-configmaps-4209a109-3ea1-4472-a618-76529a36f837" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-10943aa5-8e7a-475d-8f8d-d4f054b3cb04 01/29/23 04:39:21.862
    STEP: Updating configmap cm-test-opt-upd-a1068fc0-185c-495e-9c35-cf932a77af28 01/29/23 04:39:21.872
    STEP: Creating configMap with name cm-test-opt-create-ea828237-3b06-4bdb-a342-315d250a363f 01/29/23 04:39:21.88
    STEP: waiting to observe update in volume 01/29/23 04:39:21.888
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 29 04:39:23.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6783" for this suite. 01/29/23 04:39:23.963
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:39:23.973
Jan 29 04:39:23.974: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename daemonsets 01/29/23 04:39:23.975
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:39:24
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:39:24.006
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
STEP: Creating a simple DaemonSet "daemon-set" 01/29/23 04:39:24.057
STEP: Check that daemon pods launch on every node of the cluster. 01/29/23 04:39:24.072
Jan 29 04:39:24.086: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 29 04:39:24.087: INFO: Node master1 is running 0 daemon pod, expected 1
Jan 29 04:39:25.109: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 29 04:39:25.109: INFO: Node master1 is running 0 daemon pod, expected 1
Jan 29 04:39:26.104: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan 29 04:39:26.104: INFO: Node master1 is running 0 daemon pod, expected 1
Jan 29 04:39:27.106: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Jan 29 04:39:27.108: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 01/29/23 04:39:27.115
Jan 29 04:39:27.166: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Jan 29 04:39:27.166: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 01/29/23 04:39:27.166
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 01/29/23 04:39:28.186
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2543, will wait for the garbage collector to delete the pods 01/29/23 04:39:28.187
Jan 29 04:39:28.255: INFO: Deleting DaemonSet.extensions daemon-set took: 11.043498ms
Jan 29 04:39:28.456: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.908847ms
Jan 29 04:39:31.363: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 29 04:39:31.363: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 29 04:39:31.370: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"5993607"},"items":null}

Jan 29 04:39:31.376: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"5993607"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan 29 04:39:31.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2543" for this suite. 01/29/23 04:39:31.427
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","completed":350,"skipped":6538,"failed":0}
------------------------------
• [SLOW TEST] [7.466 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:39:23.973
    Jan 29 04:39:23.974: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename daemonsets 01/29/23 04:39:23.975
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:39:24
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:39:24.006
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:293
    STEP: Creating a simple DaemonSet "daemon-set" 01/29/23 04:39:24.057
    STEP: Check that daemon pods launch on every node of the cluster. 01/29/23 04:39:24.072
    Jan 29 04:39:24.086: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 29 04:39:24.087: INFO: Node master1 is running 0 daemon pod, expected 1
    Jan 29 04:39:25.109: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 29 04:39:25.109: INFO: Node master1 is running 0 daemon pod, expected 1
    Jan 29 04:39:26.104: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan 29 04:39:26.104: INFO: Node master1 is running 0 daemon pod, expected 1
    Jan 29 04:39:27.106: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
    Jan 29 04:39:27.108: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 01/29/23 04:39:27.115
    Jan 29 04:39:27.166: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
    Jan 29 04:39:27.166: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 01/29/23 04:39:27.166
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 01/29/23 04:39:28.186
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2543, will wait for the garbage collector to delete the pods 01/29/23 04:39:28.187
    Jan 29 04:39:28.255: INFO: Deleting DaemonSet.extensions daemon-set took: 11.043498ms
    Jan 29 04:39:28.456: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.908847ms
    Jan 29 04:39:31.363: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 29 04:39:31.363: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 29 04:39:31.370: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"5993607"},"items":null}

    Jan 29 04:39:31.376: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"5993607"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan 29 04:39:31.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-2543" for this suite. 01/29/23 04:39:31.427
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:39:31.442
Jan 29 04:39:31.443: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename namespaces 01/29/23 04:39:31.444
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:39:31.483
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:39:31.49
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
STEP: Creating a test namespace 01/29/23 04:39:31.496
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:39:31.531
STEP: Creating a service in the namespace 01/29/23 04:39:31.538
STEP: Deleting the namespace 01/29/23 04:39:31.563
STEP: Waiting for the namespace to be removed. 01/29/23 04:39:31.581
STEP: Recreating the namespace 01/29/23 04:39:37.589
STEP: Verifying there is no service in the namespace 01/29/23 04:39:37.617
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Jan 29 04:39:37.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3299" for this suite. 01/29/23 04:39:37.637
STEP: Destroying namespace "nsdeletetest-8936" for this suite. 01/29/23 04:39:37.648
Jan 29 04:39:37.654: INFO: Namespace nsdeletetest-8936 was already deleted
STEP: Destroying namespace "nsdeletetest-1628" for this suite. 01/29/23 04:39:37.654
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","completed":351,"skipped":6576,"failed":0}
------------------------------
• [SLOW TEST] [6.223 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:39:31.442
    Jan 29 04:39:31.443: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename namespaces 01/29/23 04:39:31.444
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:39:31.483
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:39:31.49
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:250
    STEP: Creating a test namespace 01/29/23 04:39:31.496
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:39:31.531
    STEP: Creating a service in the namespace 01/29/23 04:39:31.538
    STEP: Deleting the namespace 01/29/23 04:39:31.563
    STEP: Waiting for the namespace to be removed. 01/29/23 04:39:31.581
    STEP: Recreating the namespace 01/29/23 04:39:37.589
    STEP: Verifying there is no service in the namespace 01/29/23 04:39:37.617
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Jan 29 04:39:37.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-3299" for this suite. 01/29/23 04:39:37.637
    STEP: Destroying namespace "nsdeletetest-8936" for this suite. 01/29/23 04:39:37.648
    Jan 29 04:39:37.654: INFO: Namespace nsdeletetest-8936 was already deleted
    STEP: Destroying namespace "nsdeletetest-1628" for this suite. 01/29/23 04:39:37.654
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:39:37.666
Jan 29 04:39:37.666: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename pod-network-test 01/29/23 04:39:37.667
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:39:37.693
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:39:37.699
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-1267 01/29/23 04:39:37.705
STEP: creating a selector 01/29/23 04:39:37.706
STEP: Creating the service pods in kubernetes 01/29/23 04:39:37.706
Jan 29 04:39:37.706: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 29 04:39:37.824: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-1267" to be "running and ready"
Jan 29 04:39:37.835: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.704035ms
Jan 29 04:39:37.835: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 29 04:39:39.846: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021559404s
Jan 29 04:39:39.846: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 29 04:39:41.844: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.019634683s
Jan 29 04:39:41.844: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 29 04:39:43.842: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.018201086s
Jan 29 04:39:43.842: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 29 04:39:45.842: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.017999497s
Jan 29 04:39:45.842: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 29 04:39:47.847: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.023081326s
Jan 29 04:39:47.847: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 29 04:39:49.843: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.019120891s
Jan 29 04:39:49.843: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 29 04:39:51.843: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.018889562s
Jan 29 04:39:51.843: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 29 04:39:53.845: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.021391973s
Jan 29 04:39:53.845: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 29 04:39:55.845: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.020655881s
Jan 29 04:39:55.845: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 29 04:39:57.843: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.019468125s
Jan 29 04:39:57.843: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 29 04:39:59.843: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.018869194s
Jan 29 04:39:59.843: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan 29 04:39:59.843: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan 29 04:39:59.849: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-1267" to be "running and ready"
Jan 29 04:39:59.855: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 6.015823ms
Jan 29 04:39:59.855: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan 29 04:39:59.855: INFO: Pod "netserver-1" satisfied condition "running and ready"
Jan 29 04:39:59.861: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-1267" to be "running and ready"
Jan 29 04:39:59.867: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 5.978462ms
Jan 29 04:39:59.867: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Jan 29 04:39:59.867: INFO: Pod "netserver-2" satisfied condition "running and ready"
Jan 29 04:39:59.873: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-1267" to be "running and ready"
Jan 29 04:39:59.879: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 5.906741ms
Jan 29 04:39:59.879: INFO: The phase of Pod netserver-3 is Running (Ready = true)
Jan 29 04:39:59.879: INFO: Pod "netserver-3" satisfied condition "running and ready"
Jan 29 04:39:59.884: INFO: Waiting up to 5m0s for pod "netserver-4" in namespace "pod-network-test-1267" to be "running and ready"
Jan 29 04:39:59.890: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=true. Elapsed: 5.80282ms
Jan 29 04:39:59.890: INFO: The phase of Pod netserver-4 is Running (Ready = true)
Jan 29 04:39:59.890: INFO: Pod "netserver-4" satisfied condition "running and ready"
STEP: Creating test pods 01/29/23 04:39:59.896
Jan 29 04:39:59.923: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-1267" to be "running"
Jan 29 04:39:59.932: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.278365ms
Jan 29 04:40:01.948: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.025477771s
Jan 29 04:40:01.948: INFO: Pod "test-container-pod" satisfied condition "running"
Jan 29 04:40:01.956: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-1267" to be "running"
Jan 29 04:40:01.966: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 9.576907ms
Jan 29 04:40:01.966: INFO: Pod "host-test-container-pod" satisfied condition "running"
Jan 29 04:40:01.986: INFO: Setting MaxTries for pod polling to 55 for networking test based on endpoint count 5
Jan 29 04:40:01.986: INFO: Going to poll 100.101.161.10 on port 8081 at least 0 times, with a maximum of 55 tries before failing
Jan 29 04:40:02.005: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.101.161.10 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1267 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 29 04:40:02.005: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
Jan 29 04:40:02.007: INFO: ExecWithOptions: Clientset creation
Jan 29 04:40:02.007: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/pod-network-test-1267/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+100.101.161.10+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 29 04:40:03.167: INFO: Found all 1 expected endpoints: [netserver-0]
Jan 29 04:40:03.167: INFO: Going to poll 100.101.208.26 on port 8081 at least 0 times, with a maximum of 55 tries before failing
Jan 29 04:40:03.181: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.101.208.26 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1267 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 29 04:40:03.181: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
Jan 29 04:40:03.182: INFO: ExecWithOptions: Clientset creation
Jan 29 04:40:03.182: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/pod-network-test-1267/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+100.101.208.26+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 29 04:40:04.326: INFO: Found all 1 expected endpoints: [netserver-1]
Jan 29 04:40:04.326: INFO: Going to poll 100.101.32.173 on port 8081 at least 0 times, with a maximum of 55 tries before failing
Jan 29 04:40:04.356: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.101.32.173 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1267 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 29 04:40:04.356: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
Jan 29 04:40:04.358: INFO: ExecWithOptions: Clientset creation
Jan 29 04:40:04.358: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/pod-network-test-1267/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+100.101.32.173+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 29 04:40:05.531: INFO: Found all 1 expected endpoints: [netserver-2]
Jan 29 04:40:05.531: INFO: Going to poll 100.101.51.145 on port 8081 at least 0 times, with a maximum of 55 tries before failing
Jan 29 04:40:05.538: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.101.51.145 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1267 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 29 04:40:05.538: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
Jan 29 04:40:05.539: INFO: ExecWithOptions: Clientset creation
Jan 29 04:40:05.539: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/pod-network-test-1267/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+100.101.51.145+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 29 04:40:06.667: INFO: Found all 1 expected endpoints: [netserver-3]
Jan 29 04:40:06.667: INFO: Going to poll 100.101.49.3 on port 8081 at least 0 times, with a maximum of 55 tries before failing
Jan 29 04:40:06.674: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.101.49.3 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1267 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 29 04:40:06.674: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
Jan 29 04:40:06.675: INFO: ExecWithOptions: Clientset creation
Jan 29 04:40:06.675: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/pod-network-test-1267/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+100.101.49.3+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 29 04:40:07.808: INFO: Found all 1 expected endpoints: [netserver-4]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Jan 29 04:40:07.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1267" for this suite. 01/29/23 04:40:07.818
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","completed":352,"skipped":6576,"failed":0}
------------------------------
• [SLOW TEST] [30.163 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:39:37.666
    Jan 29 04:39:37.666: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename pod-network-test 01/29/23 04:39:37.667
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:39:37.693
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:39:37.699
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-1267 01/29/23 04:39:37.705
    STEP: creating a selector 01/29/23 04:39:37.706
    STEP: Creating the service pods in kubernetes 01/29/23 04:39:37.706
    Jan 29 04:39:37.706: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan 29 04:39:37.824: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-1267" to be "running and ready"
    Jan 29 04:39:37.835: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.704035ms
    Jan 29 04:39:37.835: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 29 04:39:39.846: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021559404s
    Jan 29 04:39:39.846: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 29 04:39:41.844: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.019634683s
    Jan 29 04:39:41.844: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 29 04:39:43.842: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.018201086s
    Jan 29 04:39:43.842: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 29 04:39:45.842: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.017999497s
    Jan 29 04:39:45.842: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 29 04:39:47.847: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.023081326s
    Jan 29 04:39:47.847: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 29 04:39:49.843: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.019120891s
    Jan 29 04:39:49.843: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 29 04:39:51.843: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.018889562s
    Jan 29 04:39:51.843: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 29 04:39:53.845: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.021391973s
    Jan 29 04:39:53.845: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 29 04:39:55.845: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.020655881s
    Jan 29 04:39:55.845: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 29 04:39:57.843: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.019468125s
    Jan 29 04:39:57.843: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 29 04:39:59.843: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.018869194s
    Jan 29 04:39:59.843: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan 29 04:39:59.843: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan 29 04:39:59.849: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-1267" to be "running and ready"
    Jan 29 04:39:59.855: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 6.015823ms
    Jan 29 04:39:59.855: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan 29 04:39:59.855: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Jan 29 04:39:59.861: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-1267" to be "running and ready"
    Jan 29 04:39:59.867: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 5.978462ms
    Jan 29 04:39:59.867: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Jan 29 04:39:59.867: INFO: Pod "netserver-2" satisfied condition "running and ready"
    Jan 29 04:39:59.873: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-1267" to be "running and ready"
    Jan 29 04:39:59.879: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 5.906741ms
    Jan 29 04:39:59.879: INFO: The phase of Pod netserver-3 is Running (Ready = true)
    Jan 29 04:39:59.879: INFO: Pod "netserver-3" satisfied condition "running and ready"
    Jan 29 04:39:59.884: INFO: Waiting up to 5m0s for pod "netserver-4" in namespace "pod-network-test-1267" to be "running and ready"
    Jan 29 04:39:59.890: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=true. Elapsed: 5.80282ms
    Jan 29 04:39:59.890: INFO: The phase of Pod netserver-4 is Running (Ready = true)
    Jan 29 04:39:59.890: INFO: Pod "netserver-4" satisfied condition "running and ready"
    STEP: Creating test pods 01/29/23 04:39:59.896
    Jan 29 04:39:59.923: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-1267" to be "running"
    Jan 29 04:39:59.932: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.278365ms
    Jan 29 04:40:01.948: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.025477771s
    Jan 29 04:40:01.948: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan 29 04:40:01.956: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-1267" to be "running"
    Jan 29 04:40:01.966: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 9.576907ms
    Jan 29 04:40:01.966: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Jan 29 04:40:01.986: INFO: Setting MaxTries for pod polling to 55 for networking test based on endpoint count 5
    Jan 29 04:40:01.986: INFO: Going to poll 100.101.161.10 on port 8081 at least 0 times, with a maximum of 55 tries before failing
    Jan 29 04:40:02.005: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.101.161.10 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1267 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 29 04:40:02.005: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    Jan 29 04:40:02.007: INFO: ExecWithOptions: Clientset creation
    Jan 29 04:40:02.007: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/pod-network-test-1267/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+100.101.161.10+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 29 04:40:03.167: INFO: Found all 1 expected endpoints: [netserver-0]
    Jan 29 04:40:03.167: INFO: Going to poll 100.101.208.26 on port 8081 at least 0 times, with a maximum of 55 tries before failing
    Jan 29 04:40:03.181: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.101.208.26 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1267 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 29 04:40:03.181: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    Jan 29 04:40:03.182: INFO: ExecWithOptions: Clientset creation
    Jan 29 04:40:03.182: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/pod-network-test-1267/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+100.101.208.26+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 29 04:40:04.326: INFO: Found all 1 expected endpoints: [netserver-1]
    Jan 29 04:40:04.326: INFO: Going to poll 100.101.32.173 on port 8081 at least 0 times, with a maximum of 55 tries before failing
    Jan 29 04:40:04.356: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.101.32.173 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1267 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 29 04:40:04.356: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    Jan 29 04:40:04.358: INFO: ExecWithOptions: Clientset creation
    Jan 29 04:40:04.358: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/pod-network-test-1267/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+100.101.32.173+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 29 04:40:05.531: INFO: Found all 1 expected endpoints: [netserver-2]
    Jan 29 04:40:05.531: INFO: Going to poll 100.101.51.145 on port 8081 at least 0 times, with a maximum of 55 tries before failing
    Jan 29 04:40:05.538: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.101.51.145 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1267 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 29 04:40:05.538: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    Jan 29 04:40:05.539: INFO: ExecWithOptions: Clientset creation
    Jan 29 04:40:05.539: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/pod-network-test-1267/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+100.101.51.145+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 29 04:40:06.667: INFO: Found all 1 expected endpoints: [netserver-3]
    Jan 29 04:40:06.667: INFO: Going to poll 100.101.49.3 on port 8081 at least 0 times, with a maximum of 55 tries before failing
    Jan 29 04:40:06.674: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.101.49.3 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1267 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 29 04:40:06.674: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    Jan 29 04:40:06.675: INFO: ExecWithOptions: Clientset creation
    Jan 29 04:40:06.675: INFO: ExecWithOptions: execute(POST https://100.105.0.1:443/api/v1/namespaces/pod-network-test-1267/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+100.101.49.3+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 29 04:40:07.808: INFO: Found all 1 expected endpoints: [netserver-4]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Jan 29 04:40:07.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-1267" for this suite. 01/29/23 04:40:07.818
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:40:07.83
Jan 29 04:40:07.831: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename taint-single-pod 01/29/23 04:40:07.835
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:40:07.861
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:40:07.867
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:166
Jan 29 04:40:07.872: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 29 04:41:07.925: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
Jan 29 04:41:07.932: INFO: Starting informer...
STEP: Starting pod... 01/29/23 04:41:07.932
Jan 29 04:41:08.162: INFO: Pod is running on slave2. Tainting Node
STEP: Trying to apply a taint on the Node 01/29/23 04:41:08.162
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/29/23 04:41:08.186
STEP: Waiting short time to make sure Pod is queued for deletion 01/29/23 04:41:08.201
Jan 29 04:41:08.201: INFO: Pod wasn't evicted. Proceeding
Jan 29 04:41:08.201: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/29/23 04:41:08.224
STEP: Waiting some time to make sure that toleration time passed. 01/29/23 04:41:08.231
Jan 29 04:42:23.233: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:187
Jan 29 04:42:23.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-203" for this suite. 01/29/23 04:42:23.244
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","completed":353,"skipped":6585,"failed":0}
------------------------------
• [SLOW TEST] [135.428 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:40:07.83
    Jan 29 04:40:07.831: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename taint-single-pod 01/29/23 04:40:07.835
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:40:07.861
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:40:07.867
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:166
    Jan 29 04:40:07.872: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 29 04:41:07.925: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:289
    Jan 29 04:41:07.932: INFO: Starting informer...
    STEP: Starting pod... 01/29/23 04:41:07.932
    Jan 29 04:41:08.162: INFO: Pod is running on slave2. Tainting Node
    STEP: Trying to apply a taint on the Node 01/29/23 04:41:08.162
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/29/23 04:41:08.186
    STEP: Waiting short time to make sure Pod is queued for deletion 01/29/23 04:41:08.201
    Jan 29 04:41:08.201: INFO: Pod wasn't evicted. Proceeding
    Jan 29 04:41:08.201: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/29/23 04:41:08.224
    STEP: Waiting some time to make sure that toleration time passed. 01/29/23 04:41:08.231
    Jan 29 04:42:23.233: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:187
    Jan 29 04:42:23.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "taint-single-pod-203" for this suite. 01/29/23 04:42:23.244
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:42:23.26
Jan 29 04:42:23.260: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename webhook 01/29/23 04:42:23.262
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:42:23.287
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:42:23.294
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/29/23 04:42:23.324
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/29/23 04:42:24.034
STEP: Deploying the webhook pod 01/29/23 04:42:24.054
STEP: Wait for the deployment to be ready 01/29/23 04:42:24.08
Jan 29 04:42:24.096: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 29 04:42:26.118: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 29, 4, 42, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 4, 42, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 29, 4, 42, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 4, 42, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/29/23 04:42:28.128
STEP: Verifying the service has paired with the endpoint 01/29/23 04:42:28.146
Jan 29 04:42:29.149: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
STEP: Listing all of the created validation webhooks 01/29/23 04:42:29.249
STEP: Creating a configMap that does not comply to the validation webhook rules 01/29/23 04:42:29.299
STEP: Deleting the collection of validation webhooks 01/29/23 04:42:29.342
STEP: Creating a configMap that does not comply to the validation webhook rules 01/29/23 04:42:29.43
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 29 04:42:29.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7846" for this suite. 01/29/23 04:42:29.461
STEP: Destroying namespace "webhook-7846-markers" for this suite. 01/29/23 04:42:29.471
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","completed":354,"skipped":6599,"failed":0}
------------------------------
• [SLOW TEST] [6.314 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:42:23.26
    Jan 29 04:42:23.260: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename webhook 01/29/23 04:42:23.262
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:42:23.287
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:42:23.294
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/29/23 04:42:23.324
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/29/23 04:42:24.034
    STEP: Deploying the webhook pod 01/29/23 04:42:24.054
    STEP: Wait for the deployment to be ready 01/29/23 04:42:24.08
    Jan 29 04:42:24.096: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 29 04:42:26.118: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 29, 4, 42, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 4, 42, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 29, 4, 42, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 29, 4, 42, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/29/23 04:42:28.128
    STEP: Verifying the service has paired with the endpoint 01/29/23 04:42:28.146
    Jan 29 04:42:29.149: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:581
    STEP: Listing all of the created validation webhooks 01/29/23 04:42:29.249
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/29/23 04:42:29.299
    STEP: Deleting the collection of validation webhooks 01/29/23 04:42:29.342
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/29/23 04:42:29.43
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 29 04:42:29.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-7846" for this suite. 01/29/23 04:42:29.461
    STEP: Destroying namespace "webhook-7846-markers" for this suite. 01/29/23 04:42:29.471
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:42:29.575
Jan 29 04:42:29.576: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename crd-publish-openapi 01/29/23 04:42:29.578
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:42:29.609
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:42:29.615
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
Jan 29 04:42:29.622: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/29/23 04:42:37.062
Jan 29 04:42:37.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=crd-publish-openapi-1037 --namespace=crd-publish-openapi-1037 create -f -'
Jan 29 04:42:38.336: INFO: stderr: ""
Jan 29 04:42:38.336: INFO: stdout: "e2e-test-crd-publish-openapi-8785-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jan 29 04:42:38.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=crd-publish-openapi-1037 --namespace=crd-publish-openapi-1037 delete e2e-test-crd-publish-openapi-8785-crds test-cr'
Jan 29 04:42:38.482: INFO: stderr: ""
Jan 29 04:42:38.482: INFO: stdout: "e2e-test-crd-publish-openapi-8785-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Jan 29 04:42:38.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=crd-publish-openapi-1037 --namespace=crd-publish-openapi-1037 apply -f -'
Jan 29 04:42:38.870: INFO: stderr: ""
Jan 29 04:42:38.870: INFO: stdout: "e2e-test-crd-publish-openapi-8785-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jan 29 04:42:38.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=crd-publish-openapi-1037 --namespace=crd-publish-openapi-1037 delete e2e-test-crd-publish-openapi-8785-crds test-cr'
Jan 29 04:42:39.003: INFO: stderr: ""
Jan 29 04:42:39.003: INFO: stdout: "e2e-test-crd-publish-openapi-8785-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 01/29/23 04:42:39.003
Jan 29 04:42:39.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=crd-publish-openapi-1037 explain e2e-test-crd-publish-openapi-8785-crds'
Jan 29 04:42:39.367: INFO: stderr: ""
Jan 29 04:42:39.367: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8785-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 29 04:42:44.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1037" for this suite. 01/29/23 04:42:44.045
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","completed":355,"skipped":6603,"failed":0}
------------------------------
• [SLOW TEST] [14.480 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:42:29.575
    Jan 29 04:42:29.576: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename crd-publish-openapi 01/29/23 04:42:29.578
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:42:29.609
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:42:29.615
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:152
    Jan 29 04:42:29.622: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/29/23 04:42:37.062
    Jan 29 04:42:37.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=crd-publish-openapi-1037 --namespace=crd-publish-openapi-1037 create -f -'
    Jan 29 04:42:38.336: INFO: stderr: ""
    Jan 29 04:42:38.336: INFO: stdout: "e2e-test-crd-publish-openapi-8785-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Jan 29 04:42:38.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=crd-publish-openapi-1037 --namespace=crd-publish-openapi-1037 delete e2e-test-crd-publish-openapi-8785-crds test-cr'
    Jan 29 04:42:38.482: INFO: stderr: ""
    Jan 29 04:42:38.482: INFO: stdout: "e2e-test-crd-publish-openapi-8785-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Jan 29 04:42:38.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=crd-publish-openapi-1037 --namespace=crd-publish-openapi-1037 apply -f -'
    Jan 29 04:42:38.870: INFO: stderr: ""
    Jan 29 04:42:38.870: INFO: stdout: "e2e-test-crd-publish-openapi-8785-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Jan 29 04:42:38.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=crd-publish-openapi-1037 --namespace=crd-publish-openapi-1037 delete e2e-test-crd-publish-openapi-8785-crds test-cr'
    Jan 29 04:42:39.003: INFO: stderr: ""
    Jan 29 04:42:39.003: INFO: stdout: "e2e-test-crd-publish-openapi-8785-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 01/29/23 04:42:39.003
    Jan 29 04:42:39.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-184919500 --namespace=crd-publish-openapi-1037 explain e2e-test-crd-publish-openapi-8785-crds'
    Jan 29 04:42:39.367: INFO: stderr: ""
    Jan 29 04:42:39.367: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8785-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 29 04:42:44.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-1037" for this suite. 01/29/23 04:42:44.045
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:42:44.058
Jan 29 04:42:44.058: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename webhook 01/29/23 04:42:44.059
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:42:44.101
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:42:44.107
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/29/23 04:42:44.144
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/29/23 04:42:45.529
STEP: Deploying the webhook pod 01/29/23 04:42:45.549
STEP: Wait for the deployment to be ready 01/29/23 04:42:45.594
Jan 29 04:42:45.619: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/29/23 04:42:47.637
STEP: Verifying the service has paired with the endpoint 01/29/23 04:42:47.655
Jan 29 04:42:48.655: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
STEP: Registering the crd webhook via the AdmissionRegistration API 01/29/23 04:42:48.662
STEP: Creating a custom resource definition that should be denied by the webhook 01/29/23 04:42:48.687
Jan 29 04:42:48.687: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 29 04:42:48.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6282" for this suite. 01/29/23 04:42:48.725
STEP: Destroying namespace "webhook-6282-markers" for this suite. 01/29/23 04:42:48.736
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","completed":356,"skipped":6631,"failed":0}
------------------------------
• [4.784 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:42:44.058
    Jan 29 04:42:44.058: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename webhook 01/29/23 04:42:44.059
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:42:44.101
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:42:44.107
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/29/23 04:42:44.144
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/29/23 04:42:45.529
    STEP: Deploying the webhook pod 01/29/23 04:42:45.549
    STEP: Wait for the deployment to be ready 01/29/23 04:42:45.594
    Jan 29 04:42:45.619: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/29/23 04:42:47.637
    STEP: Verifying the service has paired with the endpoint 01/29/23 04:42:47.655
    Jan 29 04:42:48.655: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:307
    STEP: Registering the crd webhook via the AdmissionRegistration API 01/29/23 04:42:48.662
    STEP: Creating a custom resource definition that should be denied by the webhook 01/29/23 04:42:48.687
    Jan 29 04:42:48.687: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 29 04:42:48.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-6282" for this suite. 01/29/23 04:42:48.725
    STEP: Destroying namespace "webhook-6282-markers" for this suite. 01/29/23 04:42:48.736
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:42:48.842
Jan 29 04:42:48.843: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename configmap 01/29/23 04:42:48.844
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:42:48.88
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:42:48.887
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
STEP: Creating configMap with name cm-test-opt-del-71058fbb-0c1c-40b7-b0c2-a19b7b200dbe 01/29/23 04:42:48.902
STEP: Creating configMap with name cm-test-opt-upd-807d5220-ac11-4b14-8d62-4ab8c8d8c77e 01/29/23 04:42:48.91
STEP: Creating the pod 01/29/23 04:42:48.924
Jan 29 04:42:48.944: INFO: Waiting up to 5m0s for pod "pod-configmaps-dd079068-cb41-467b-9e6a-f3c651174913" in namespace "configmap-522" to be "running and ready"
Jan 29 04:42:48.950: INFO: Pod "pod-configmaps-dd079068-cb41-467b-9e6a-f3c651174913": Phase="Pending", Reason="", readiness=false. Elapsed: 5.919282ms
Jan 29 04:42:48.950: INFO: The phase of Pod pod-configmaps-dd079068-cb41-467b-9e6a-f3c651174913 is Pending, waiting for it to be Running (with Ready = true)
Jan 29 04:42:50.957: INFO: Pod "pod-configmaps-dd079068-cb41-467b-9e6a-f3c651174913": Phase="Running", Reason="", readiness=true. Elapsed: 2.0127652s
Jan 29 04:42:50.957: INFO: The phase of Pod pod-configmaps-dd079068-cb41-467b-9e6a-f3c651174913 is Running (Ready = true)
Jan 29 04:42:50.957: INFO: Pod "pod-configmaps-dd079068-cb41-467b-9e6a-f3c651174913" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-71058fbb-0c1c-40b7-b0c2-a19b7b200dbe 01/29/23 04:42:51.024
STEP: Updating configmap cm-test-opt-upd-807d5220-ac11-4b14-8d62-4ab8c8d8c77e 01/29/23 04:42:51.036
STEP: Creating configMap with name cm-test-opt-create-91cf2e8b-77e1-4729-b871-9021b26728ad 01/29/23 04:42:51.043
STEP: waiting to observe update in volume 01/29/23 04:42:51.051
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 29 04:42:53.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-522" for this suite. 01/29/23 04:42:53.125
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":357,"skipped":6632,"failed":0}
------------------------------
• [4.294 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:42:48.842
    Jan 29 04:42:48.843: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename configmap 01/29/23 04:42:48.844
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:42:48.88
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:42:48.887
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:239
    STEP: Creating configMap with name cm-test-opt-del-71058fbb-0c1c-40b7-b0c2-a19b7b200dbe 01/29/23 04:42:48.902
    STEP: Creating configMap with name cm-test-opt-upd-807d5220-ac11-4b14-8d62-4ab8c8d8c77e 01/29/23 04:42:48.91
    STEP: Creating the pod 01/29/23 04:42:48.924
    Jan 29 04:42:48.944: INFO: Waiting up to 5m0s for pod "pod-configmaps-dd079068-cb41-467b-9e6a-f3c651174913" in namespace "configmap-522" to be "running and ready"
    Jan 29 04:42:48.950: INFO: Pod "pod-configmaps-dd079068-cb41-467b-9e6a-f3c651174913": Phase="Pending", Reason="", readiness=false. Elapsed: 5.919282ms
    Jan 29 04:42:48.950: INFO: The phase of Pod pod-configmaps-dd079068-cb41-467b-9e6a-f3c651174913 is Pending, waiting for it to be Running (with Ready = true)
    Jan 29 04:42:50.957: INFO: Pod "pod-configmaps-dd079068-cb41-467b-9e6a-f3c651174913": Phase="Running", Reason="", readiness=true. Elapsed: 2.0127652s
    Jan 29 04:42:50.957: INFO: The phase of Pod pod-configmaps-dd079068-cb41-467b-9e6a-f3c651174913 is Running (Ready = true)
    Jan 29 04:42:50.957: INFO: Pod "pod-configmaps-dd079068-cb41-467b-9e6a-f3c651174913" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-71058fbb-0c1c-40b7-b0c2-a19b7b200dbe 01/29/23 04:42:51.024
    STEP: Updating configmap cm-test-opt-upd-807d5220-ac11-4b14-8d62-4ab8c8d8c77e 01/29/23 04:42:51.036
    STEP: Creating configMap with name cm-test-opt-create-91cf2e8b-77e1-4729-b871-9021b26728ad 01/29/23 04:42:51.043
    STEP: waiting to observe update in volume 01/29/23 04:42:51.051
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 29 04:42:53.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-522" for this suite. 01/29/23 04:42:53.125
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:42:53.138
Jan 29 04:42:53.138: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename pods 01/29/23 04:42:53.139
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:42:53.166
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:42:53.173
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
STEP: creating pod 01/29/23 04:42:53.179
Jan 29 04:42:53.196: INFO: Waiting up to 5m0s for pod "pod-hostip-61c80c89-d358-447b-b8e4-786146554417" in namespace "pods-6330" to be "running and ready"
Jan 29 04:42:53.202: INFO: Pod "pod-hostip-61c80c89-d358-447b-b8e4-786146554417": Phase="Pending", Reason="", readiness=false. Elapsed: 6.346265ms
Jan 29 04:42:53.202: INFO: The phase of Pod pod-hostip-61c80c89-d358-447b-b8e4-786146554417 is Pending, waiting for it to be Running (with Ready = true)
Jan 29 04:42:55.211: INFO: Pod "pod-hostip-61c80c89-d358-447b-b8e4-786146554417": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014773794s
Jan 29 04:42:55.211: INFO: The phase of Pod pod-hostip-61c80c89-d358-447b-b8e4-786146554417 is Pending, waiting for it to be Running (with Ready = true)
Jan 29 04:42:57.210: INFO: Pod "pod-hostip-61c80c89-d358-447b-b8e4-786146554417": Phase="Running", Reason="", readiness=true. Elapsed: 4.014064599s
Jan 29 04:42:57.210: INFO: The phase of Pod pod-hostip-61c80c89-d358-447b-b8e4-786146554417 is Running (Ready = true)
Jan 29 04:42:57.210: INFO: Pod "pod-hostip-61c80c89-d358-447b-b8e4-786146554417" satisfied condition "running and ready"
Jan 29 04:42:57.221: INFO: Pod pod-hostip-61c80c89-d358-447b-b8e4-786146554417 has hostIP: 192.168.122.244
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 29 04:42:57.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6330" for this suite. 01/29/23 04:42:57.23
{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","completed":358,"skipped":6637,"failed":0}
------------------------------
• [4.103 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:42:53.138
    Jan 29 04:42:53.138: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename pods 01/29/23 04:42:53.139
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:42:53.166
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:42:53.173
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:203
    STEP: creating pod 01/29/23 04:42:53.179
    Jan 29 04:42:53.196: INFO: Waiting up to 5m0s for pod "pod-hostip-61c80c89-d358-447b-b8e4-786146554417" in namespace "pods-6330" to be "running and ready"
    Jan 29 04:42:53.202: INFO: Pod "pod-hostip-61c80c89-d358-447b-b8e4-786146554417": Phase="Pending", Reason="", readiness=false. Elapsed: 6.346265ms
    Jan 29 04:42:53.202: INFO: The phase of Pod pod-hostip-61c80c89-d358-447b-b8e4-786146554417 is Pending, waiting for it to be Running (with Ready = true)
    Jan 29 04:42:55.211: INFO: Pod "pod-hostip-61c80c89-d358-447b-b8e4-786146554417": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014773794s
    Jan 29 04:42:55.211: INFO: The phase of Pod pod-hostip-61c80c89-d358-447b-b8e4-786146554417 is Pending, waiting for it to be Running (with Ready = true)
    Jan 29 04:42:57.210: INFO: Pod "pod-hostip-61c80c89-d358-447b-b8e4-786146554417": Phase="Running", Reason="", readiness=true. Elapsed: 4.014064599s
    Jan 29 04:42:57.210: INFO: The phase of Pod pod-hostip-61c80c89-d358-447b-b8e4-786146554417 is Running (Ready = true)
    Jan 29 04:42:57.210: INFO: Pod "pod-hostip-61c80c89-d358-447b-b8e4-786146554417" satisfied condition "running and ready"
    Jan 29 04:42:57.221: INFO: Pod pod-hostip-61c80c89-d358-447b-b8e4-786146554417 has hostIP: 192.168.122.244
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 29 04:42:57.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-6330" for this suite. 01/29/23 04:42:57.23
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:42:57.241
Jan 29 04:42:57.241: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename subpath 01/29/23 04:42:57.243
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:42:57.273
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:42:57.278
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/29/23 04:42:57.283
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-wx66 01/29/23 04:42:57.3
STEP: Creating a pod to test atomic-volume-subpath 01/29/23 04:42:57.3
Jan 29 04:42:57.317: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-wx66" in namespace "subpath-1939" to be "Succeeded or Failed"
Jan 29 04:42:57.324: INFO: Pod "pod-subpath-test-configmap-wx66": Phase="Pending", Reason="", readiness=false. Elapsed: 6.401665ms
Jan 29 04:42:59.331: INFO: Pod "pod-subpath-test-configmap-wx66": Phase="Running", Reason="", readiness=true. Elapsed: 2.013195142s
Jan 29 04:43:01.331: INFO: Pod "pod-subpath-test-configmap-wx66": Phase="Running", Reason="", readiness=true. Elapsed: 4.013166432s
Jan 29 04:43:03.332: INFO: Pod "pod-subpath-test-configmap-wx66": Phase="Running", Reason="", readiness=true. Elapsed: 6.014371311s
Jan 29 04:43:05.332: INFO: Pod "pod-subpath-test-configmap-wx66": Phase="Running", Reason="", readiness=true. Elapsed: 8.014159179s
Jan 29 04:43:07.332: INFO: Pod "pod-subpath-test-configmap-wx66": Phase="Running", Reason="", readiness=true. Elapsed: 10.014599373s
Jan 29 04:43:09.331: INFO: Pod "pod-subpath-test-configmap-wx66": Phase="Running", Reason="", readiness=true. Elapsed: 12.013511195s
Jan 29 04:43:11.332: INFO: Pod "pod-subpath-test-configmap-wx66": Phase="Running", Reason="", readiness=true. Elapsed: 14.014548292s
Jan 29 04:43:13.331: INFO: Pod "pod-subpath-test-configmap-wx66": Phase="Running", Reason="", readiness=true. Elapsed: 16.013519955s
Jan 29 04:43:15.331: INFO: Pod "pod-subpath-test-configmap-wx66": Phase="Running", Reason="", readiness=true. Elapsed: 18.013189303s
Jan 29 04:43:17.332: INFO: Pod "pod-subpath-test-configmap-wx66": Phase="Running", Reason="", readiness=true. Elapsed: 20.014786544s
Jan 29 04:43:19.332: INFO: Pod "pod-subpath-test-configmap-wx66": Phase="Running", Reason="", readiness=false. Elapsed: 22.014801974s
Jan 29 04:43:21.331: INFO: Pod "pod-subpath-test-configmap-wx66": Phase="Running", Reason="", readiness=false. Elapsed: 24.013862058s
Jan 29 04:43:23.334: INFO: Pod "pod-subpath-test-configmap-wx66": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.016233525s
STEP: Saw pod success 01/29/23 04:43:23.334
Jan 29 04:43:23.334: INFO: Pod "pod-subpath-test-configmap-wx66" satisfied condition "Succeeded or Failed"
Jan 29 04:43:23.341: INFO: Trying to get logs from node slave2 pod pod-subpath-test-configmap-wx66 container test-container-subpath-configmap-wx66: <nil>
STEP: delete the pod 01/29/23 04:43:23.361
Jan 29 04:43:23.465: INFO: Waiting for pod pod-subpath-test-configmap-wx66 to disappear
Jan 29 04:43:23.471: INFO: Pod pod-subpath-test-configmap-wx66 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-wx66 01/29/23 04:43:23.471
Jan 29 04:43:23.471: INFO: Deleting pod "pod-subpath-test-configmap-wx66" in namespace "subpath-1939"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jan 29 04:43:23.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1939" for this suite. 01/29/23 04:43:23.488
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]","completed":359,"skipped":6643,"failed":0}
------------------------------
• [SLOW TEST] [26.259 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:42:57.241
    Jan 29 04:42:57.241: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename subpath 01/29/23 04:42:57.243
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:42:57.273
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:42:57.278
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/29/23 04:42:57.283
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-wx66 01/29/23 04:42:57.3
    STEP: Creating a pod to test atomic-volume-subpath 01/29/23 04:42:57.3
    Jan 29 04:42:57.317: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-wx66" in namespace "subpath-1939" to be "Succeeded or Failed"
    Jan 29 04:42:57.324: INFO: Pod "pod-subpath-test-configmap-wx66": Phase="Pending", Reason="", readiness=false. Elapsed: 6.401665ms
    Jan 29 04:42:59.331: INFO: Pod "pod-subpath-test-configmap-wx66": Phase="Running", Reason="", readiness=true. Elapsed: 2.013195142s
    Jan 29 04:43:01.331: INFO: Pod "pod-subpath-test-configmap-wx66": Phase="Running", Reason="", readiness=true. Elapsed: 4.013166432s
    Jan 29 04:43:03.332: INFO: Pod "pod-subpath-test-configmap-wx66": Phase="Running", Reason="", readiness=true. Elapsed: 6.014371311s
    Jan 29 04:43:05.332: INFO: Pod "pod-subpath-test-configmap-wx66": Phase="Running", Reason="", readiness=true. Elapsed: 8.014159179s
    Jan 29 04:43:07.332: INFO: Pod "pod-subpath-test-configmap-wx66": Phase="Running", Reason="", readiness=true. Elapsed: 10.014599373s
    Jan 29 04:43:09.331: INFO: Pod "pod-subpath-test-configmap-wx66": Phase="Running", Reason="", readiness=true. Elapsed: 12.013511195s
    Jan 29 04:43:11.332: INFO: Pod "pod-subpath-test-configmap-wx66": Phase="Running", Reason="", readiness=true. Elapsed: 14.014548292s
    Jan 29 04:43:13.331: INFO: Pod "pod-subpath-test-configmap-wx66": Phase="Running", Reason="", readiness=true. Elapsed: 16.013519955s
    Jan 29 04:43:15.331: INFO: Pod "pod-subpath-test-configmap-wx66": Phase="Running", Reason="", readiness=true. Elapsed: 18.013189303s
    Jan 29 04:43:17.332: INFO: Pod "pod-subpath-test-configmap-wx66": Phase="Running", Reason="", readiness=true. Elapsed: 20.014786544s
    Jan 29 04:43:19.332: INFO: Pod "pod-subpath-test-configmap-wx66": Phase="Running", Reason="", readiness=false. Elapsed: 22.014801974s
    Jan 29 04:43:21.331: INFO: Pod "pod-subpath-test-configmap-wx66": Phase="Running", Reason="", readiness=false. Elapsed: 24.013862058s
    Jan 29 04:43:23.334: INFO: Pod "pod-subpath-test-configmap-wx66": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.016233525s
    STEP: Saw pod success 01/29/23 04:43:23.334
    Jan 29 04:43:23.334: INFO: Pod "pod-subpath-test-configmap-wx66" satisfied condition "Succeeded or Failed"
    Jan 29 04:43:23.341: INFO: Trying to get logs from node slave2 pod pod-subpath-test-configmap-wx66 container test-container-subpath-configmap-wx66: <nil>
    STEP: delete the pod 01/29/23 04:43:23.361
    Jan 29 04:43:23.465: INFO: Waiting for pod pod-subpath-test-configmap-wx66 to disappear
    Jan 29 04:43:23.471: INFO: Pod pod-subpath-test-configmap-wx66 no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-wx66 01/29/23 04:43:23.471
    Jan 29 04:43:23.471: INFO: Deleting pod "pod-subpath-test-configmap-wx66" in namespace "subpath-1939"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jan 29 04:43:23.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-1939" for this suite. 01/29/23 04:43:23.488
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:43:23.501
Jan 29 04:43:23.502: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename runtimeclass 01/29/23 04:43:23.503
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:43:23.533
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:43:23.539
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Jan 29 04:43:23.573: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-4733 to be scheduled
Jan 29 04:43:23.579: INFO: 1 pods are not scheduled: [runtimeclass-4733/test-runtimeclass-runtimeclass-4733-preconfigured-handler-z97tv(06926d58-ac40-4c34-b55a-0867fb5db0c9)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jan 29 04:43:25.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-4733" for this suite. 01/29/23 04:43:25.63
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]","completed":360,"skipped":6660,"failed":0}
------------------------------
• [2.142 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:43:23.501
    Jan 29 04:43:23.502: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename runtimeclass 01/29/23 04:43:23.503
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:43:23.533
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:43:23.539
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Jan 29 04:43:23.573: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-4733 to be scheduled
    Jan 29 04:43:23.579: INFO: 1 pods are not scheduled: [runtimeclass-4733/test-runtimeclass-runtimeclass-4733-preconfigured-handler-z97tv(06926d58-ac40-4c34-b55a-0867fb5db0c9)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jan 29 04:43:25.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-4733" for this suite. 01/29/23 04:43:25.63
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:43:25.647
Jan 29 04:43:25.647: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename gc 01/29/23 04:43:25.649
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:43:25.696
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:43:25.702
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 01/29/23 04:43:25.708
STEP: Wait for the Deployment to create new ReplicaSet 01/29/23 04:43:25.731
STEP: delete the deployment 01/29/23 04:43:26.248
STEP: wait for all rs to be garbage collected 01/29/23 04:43:26.259
STEP: expected 0 rs, got 1 rs 01/29/23 04:43:26.281
STEP: expected 0 pods, got 2 pods 01/29/23 04:43:26.287
STEP: Gathering metrics 01/29/23 04:43:26.81
Jan 29 04:43:26.865: INFO: Waiting up to 5m0s for pod "kube-controller-manager-master3" in namespace "kube-system" to be "running and ready"
Jan 29 04:43:26.872: INFO: Pod "kube-controller-manager-master3": Phase="Running", Reason="", readiness=true. Elapsed: 6.514426ms
Jan 29 04:43:26.872: INFO: The phase of Pod kube-controller-manager-master3 is Running (Ready = true)
Jan 29 04:43:26.872: INFO: Pod "kube-controller-manager-master3" satisfied condition "running and ready"
E0129 04:43:28.034795      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:43:28.034795      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:43:29.113066      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:43:29.113066      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:43:30.184932      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:43:30.184932      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:43:31.256699      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:43:31.256699      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:43:32.329997      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:43:32.329997      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:43:33.396365      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:43:33.396365      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:43:34.593182      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:43:34.593182      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:43:35.665520      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:43:35.665520      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:43:36.759781      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:43:36.759781      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:43:37.831507      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:43:37.831507      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:43:38.909615      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:43:38.909615      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:43:39.978905      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:43:39.978905      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:43:41.114410      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:43:41.114410      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:43:42.185260      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:43:42.185260      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:43:43.258908      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:43:43.258908      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:43:44.331334      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:43:44.331334      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:43:45.398552      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:43:45.398552      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:43:46.496507      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:43:46.496507      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:43:47.578615      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:43:47.578615      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:43:48.649388      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:43:48.649388      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:43:49.716276      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:43:49.716276      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:43:49.904724      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:43:49.904724      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:43:50.979406      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:43:50.979406      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:43:52.051092      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:43:52.051092      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:43:53.201350      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:43:53.201350      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:43:54.296420      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:43:54.296420      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:43:55.365383      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:43:55.365383      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:43:56.444228      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:43:56.444228      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:43:57.512187      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:43:57.512187      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:43:58.588838      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:43:58.588838      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:43:59.656887      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:43:59.656887      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:00.735769      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:00.735769      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:00.918925      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:00.918925      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:02.055490      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:02.055490      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:03.126574      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:03.126574      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:04.191952      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:04.191952      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:05.260982      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:05.260982      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:06.333730      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:06.333730      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:07.400331      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:07.400331      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:08.466692      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:08.466692      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:09.539449      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:09.539449      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:10.607881      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:10.607881      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:11.682808      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:11.682808      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:11.900995      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:11.900995      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:12.967202      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:12.967202      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:14.042325      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:14.042325      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:15.112076      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:15.112076      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:16.182729      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:16.182729      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:17.251826      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:17.251826      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:18.332112      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:18.332112      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:19.402057      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:19.402057      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:20.488260      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:20.488260      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:21.605046      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:21.605046      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:22.676367      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:22.676367      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:22.911324      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:22.911324      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:23.978958      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:23.978958      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:25.064496      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:25.064496      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:26.137109      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:26.137109      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:27.205711      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:27.205711      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:29.374131      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:29.374131      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:30.444255      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:30.444255      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:31.519204      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:31.519204      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:32.589202      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:32.589202      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:33.667412      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:33.667412      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:33.901941      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:33.901941      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:34.977279      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:34.977279      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:36.048068      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:36.048068      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:37.129714      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:37.129714      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:38.198035      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:38.198035      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:39.265249      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:39.265249      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:40.336900      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:40.336900      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:41.404566      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:41.404566      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:42.482738      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:42.482738      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:43.561181      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:43.561181      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:44.636431      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:44.636431      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:44.705570      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:44.705570      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:45.801686      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:45.801686      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:46.872949      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:46.872949      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:47.948292      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:47.948292      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:49.020557      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:49.020557      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:50.102087      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:50.102087      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:51.170975      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:51.170975      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:52.247051      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:52.247051      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:53.323487      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:53.323487      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:54.399868      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:54.399868      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:55.527575      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
E0129 04:44:55.527575      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
Jan 29 04:44:55.527: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan 29 04:44:55.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-673" for this suite. 01/29/23 04:44:55.54
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","completed":361,"skipped":6685,"failed":0}
------------------------------
• [SLOW TEST] [89.912 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:43:25.647
    Jan 29 04:43:25.647: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename gc 01/29/23 04:43:25.649
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:43:25.696
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:43:25.702
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 01/29/23 04:43:25.708
    STEP: Wait for the Deployment to create new ReplicaSet 01/29/23 04:43:25.731
    STEP: delete the deployment 01/29/23 04:43:26.248
    STEP: wait for all rs to be garbage collected 01/29/23 04:43:26.259
    STEP: expected 0 rs, got 1 rs 01/29/23 04:43:26.281
    STEP: expected 0 pods, got 2 pods 01/29/23 04:43:26.287
    STEP: Gathering metrics 01/29/23 04:43:26.81
    Jan 29 04:43:26.865: INFO: Waiting up to 5m0s for pod "kube-controller-manager-master3" in namespace "kube-system" to be "running and ready"
    Jan 29 04:43:26.872: INFO: Pod "kube-controller-manager-master3": Phase="Running", Reason="", readiness=true. Elapsed: 6.514426ms
    Jan 29 04:43:26.872: INFO: The phase of Pod kube-controller-manager-master3 is Running (Ready = true)
    Jan 29 04:43:26.872: INFO: Pod "kube-controller-manager-master3" satisfied condition "running and ready"
    E0129 04:43:28.034795      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:43:29.113066      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:43:30.184932      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:43:31.256699      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:43:32.329997      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:43:33.396365      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:43:34.593182      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:43:35.665520      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:43:36.759781      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:43:37.831507      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:43:38.909615      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:43:39.978905      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:43:41.114410      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:43:42.185260      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:43:43.258908      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:43:44.331334      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:43:45.398552      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:43:46.496507      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:43:47.578615      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:43:48.649388      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:43:49.716276      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:43:49.904724      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:43:50.979406      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:43:52.051092      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:43:53.201350      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:43:54.296420      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:43:55.365383      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:43:56.444228      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:43:57.512187      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:43:58.588838      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:43:59.656887      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:44:00.735769      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:44:00.918925      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:44:02.055490      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:44:03.126574      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:44:04.191952      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:44:05.260982      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:44:06.333730      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:44:07.400331      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:44:08.466692      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:44:09.539449      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:44:10.607881      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:44:11.682808      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:44:11.900995      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:44:12.967202      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:44:14.042325      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:44:15.112076      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:44:16.182729      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:44:17.251826      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:44:18.332112      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:44:19.402057      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:44:20.488260      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:44:21.605046      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:44:22.676367      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:44:22.911324      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:44:23.978958      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:44:25.064496      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:44:26.137109      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:44:27.205711      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:44:29.374131      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:44:30.444255      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:44:31.519204      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:44:32.589202      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:44:33.667412      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:44:33.901941      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:44:34.977279      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:44:36.048068      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:44:37.129714      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:44:38.198035      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:44:39.265249      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:44:40.336900      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:44:41.404566      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:44:42.482738      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:44:43.561181      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:44:44.636431      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:44:44.705570      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:44:45.801686      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:44:46.872949      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:44:47.948292      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:44:49.020557      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:44:50.102087      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:44:51.170975      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:44:52.247051      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:44:53.323487      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:44:54.399868      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    E0129 04:44:55.527575      22 dial.go:124] "an error occurred connecting to the remote port" err="error forwarding port 10257 to pod d6bed493953fda8e06a83c527f5705a8a0dd40bca5689782d1a6524b108a98a1, uid : unable to do port forwarding: socat not found"
    Jan 29 04:44:55.527: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan 29 04:44:55.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-673" for this suite. 01/29/23 04:44:55.54
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/29/23 04:44:55.561
Jan 29 04:44:55.561: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Building a namespace api object, basename crd-watch 01/29/23 04:44:55.562
STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:44:55.598
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:44:55.604
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Jan 29 04:44:55.611: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
STEP: Creating first CR  01/29/23 04:44:58.181
Jan 29 04:44:58.189: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-29T04:44:58Z generation:1 name:name1 resourceVersion:5995573 uid:de7bdbb1-5017-4ed3-a54f-89ab34c4dd05] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 01/29/23 04:45:08.189
Jan 29 04:45:08.201: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-29T04:45:08Z generation:1 name:name2 resourceVersion:5995623 uid:5ac66c97-519c-44dd-b538-afae060ca650] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 01/29/23 04:45:18.202
Jan 29 04:45:18.213: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-29T04:44:58Z generation:2 name:name1 resourceVersion:5995661 uid:de7bdbb1-5017-4ed3-a54f-89ab34c4dd05] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 01/29/23 04:45:28.214
Jan 29 04:45:28.223: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-29T04:45:08Z generation:2 name:name2 resourceVersion:5995700 uid:5ac66c97-519c-44dd-b538-afae060ca650] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 01/29/23 04:45:38.224
Jan 29 04:45:38.236: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-29T04:44:58Z generation:2 name:name1 resourceVersion:5995738 uid:de7bdbb1-5017-4ed3-a54f-89ab34c4dd05] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 01/29/23 04:45:48.237
Jan 29 04:45:48.249: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-29T04:45:08Z generation:2 name:name2 resourceVersion:5995776 uid:5ac66c97-519c-44dd-b538-afae060ca650] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 29 04:45:58.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-9068" for this suite. 01/29/23 04:45:58.779
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","completed":362,"skipped":6704,"failed":0}
------------------------------
• [SLOW TEST] [63.229 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/29/23 04:44:55.561
    Jan 29 04:44:55.561: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Building a namespace api object, basename crd-watch 01/29/23 04:44:55.562
    STEP: Waiting for a default service account to be provisioned in namespace 01/29/23 04:44:55.598
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/29/23 04:44:55.604
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Jan 29 04:44:55.611: INFO: >>> kubeConfig: /tmp/kubeconfig-184919500
    STEP: Creating first CR  01/29/23 04:44:58.181
    Jan 29 04:44:58.189: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-29T04:44:58Z generation:1 name:name1 resourceVersion:5995573 uid:de7bdbb1-5017-4ed3-a54f-89ab34c4dd05] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 01/29/23 04:45:08.189
    Jan 29 04:45:08.201: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-29T04:45:08Z generation:1 name:name2 resourceVersion:5995623 uid:5ac66c97-519c-44dd-b538-afae060ca650] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 01/29/23 04:45:18.202
    Jan 29 04:45:18.213: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-29T04:44:58Z generation:2 name:name1 resourceVersion:5995661 uid:de7bdbb1-5017-4ed3-a54f-89ab34c4dd05] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 01/29/23 04:45:28.214
    Jan 29 04:45:28.223: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-29T04:45:08Z generation:2 name:name2 resourceVersion:5995700 uid:5ac66c97-519c-44dd-b538-afae060ca650] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 01/29/23 04:45:38.224
    Jan 29 04:45:38.236: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-29T04:44:58Z generation:2 name:name1 resourceVersion:5995738 uid:de7bdbb1-5017-4ed3-a54f-89ab34c4dd05] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 01/29/23 04:45:48.237
    Jan 29 04:45:48.249: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-29T04:45:08Z generation:2 name:name2 resourceVersion:5995776 uid:5ac66c97-519c-44dd-b538-afae060ca650] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 29 04:45:58.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-watch-9068" for this suite. 01/29/23 04:45:58.779
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
{"msg":"Test Suite completed","completed":362,"skipped":6705,"failed":0}
Jan 29 04:45:58.793: INFO: Running AfterSuite actions on all nodes
Jan 29 04:45:58.793: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
Jan 29 04:45:58.793: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
Jan 29 04:45:58.793: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
Jan 29 04:45:58.793: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
Jan 29 04:45:58.793: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
Jan 29 04:45:58.793: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
Jan 29 04:45:58.793: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
Jan 29 04:45:58.793: INFO: Running AfterSuite actions on node 1
Jan 29 04:45:58.793: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.000 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    Jan 29 04:45:58.793: INFO: Running AfterSuite actions on all nodes
    Jan 29 04:45:58.793: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
    Jan 29 04:45:58.793: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
    Jan 29 04:45:58.793: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
    Jan 29 04:45:58.793: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
    Jan 29 04:45:58.793: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
    Jan 29 04:45:58.793: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
    Jan 29 04:45:58.793: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    Jan 29 04:45:58.793: INFO: Running AfterSuite actions on node 1
    Jan 29 04:45:58.793: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:146
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:146
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:559
------------------------------
[ReportAfterSuite] PASSED [0.128 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:559
  << End Captured GinkgoWriter Output
------------------------------

Ran 362 of 7067 Specs in 6561.773 seconds
SUCCESS! -- 362 Passed | 0 Failed | 0 Pending | 6705 Skipped
PASS

Ginkgo ran 1 suite in 1h49m22.292580395s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.1.6[0m

