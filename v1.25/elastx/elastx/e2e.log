I0301 11:42:22.166753      19 e2e.go:116] Starting e2e run "8dccadbc-6480-4735-8e50-691d6b653465" on Ginkgo node 1
Mar  1 11:42:22.180: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1677670942 - will randomize all specs

Will run 362 of 7066 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
{"msg":"Test Suite starting","completed":0,"skipped":0,"failed":0}
Mar  1 11:42:22.289: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
Mar  1 11:42:22.290: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
E0301 11:42:22.294453      19 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
E0301 11:42:22.294453      19 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
Mar  1 11:42:22.314: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Mar  1 11:42:22.375: INFO: 46 / 46 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Mar  1 11:42:22.375: INFO: expected 7 pod replicas in namespace 'kube-system', 7 are Running and Ready.
Mar  1 11:42:22.375: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Mar  1 11:42:22.387: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Mar  1 11:42:22.387: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'csi-cinder-nodeplugin' (0 seconds elapsed)
Mar  1 11:42:22.387: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Mar  1 11:42:22.387: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'nodelocaldns' (0 seconds elapsed)
Mar  1 11:42:22.387: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'openstack-cloud-controller-manager' (0 seconds elapsed)
Mar  1 11:42:22.387: INFO: e2e test version: v1.25.6
Mar  1 11:42:22.388: INFO: kube-apiserver version: v1.25.6
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
Mar  1 11:42:22.388: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
Mar  1 11:42:22.394: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.107 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    Mar  1 11:42:22.289: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    Mar  1 11:42:22.290: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    E0301 11:42:22.294453      19 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
    Mar  1 11:42:22.314: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Mar  1 11:42:22.375: INFO: 46 / 46 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Mar  1 11:42:22.375: INFO: expected 7 pod replicas in namespace 'kube-system', 7 are Running and Ready.
    Mar  1 11:42:22.375: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Mar  1 11:42:22.387: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
    Mar  1 11:42:22.387: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'csi-cinder-nodeplugin' (0 seconds elapsed)
    Mar  1 11:42:22.387: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
    Mar  1 11:42:22.387: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'nodelocaldns' (0 seconds elapsed)
    Mar  1 11:42:22.387: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'openstack-cloud-controller-manager' (0 seconds elapsed)
    Mar  1 11:42:22.387: INFO: e2e test version: v1.25.6
    Mar  1 11:42:22.388: INFO: kube-apiserver version: v1.25.6
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    Mar  1 11:42:22.388: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    Mar  1 11:42:22.394: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:42:22.421
Mar  1 11:42:22.421: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename pods 03/01/23 11:42:22.422
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:42:22.444
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:42:22.448
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
STEP: creating a Pod with a static label 03/01/23 11:42:22.482
STEP: watching for Pod to be ready 03/01/23 11:42:22.496
Mar  1 11:42:22.498: INFO: observed Pod pod-test in namespace pods-2630 in phase Pending with labels: map[test-pod-static:true] & conditions []
Mar  1 11:42:22.502: INFO: observed Pod pod-test in namespace pods-2630 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-01 11:42:22 +0000 UTC  }]
Mar  1 11:42:22.519: INFO: observed Pod pod-test in namespace pods-2630 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-01 11:42:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-01 11:42:22 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-01 11:42:22 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-01 11:42:22 +0000 UTC  }]
Mar  1 11:42:23.069: INFO: observed Pod pod-test in namespace pods-2630 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-01 11:42:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-01 11:42:22 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-01 11:42:22 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-01 11:42:22 +0000 UTC  }]
Mar  1 11:42:26.242: INFO: Found Pod pod-test in namespace pods-2630 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-01 11:42:22 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-03-01 11:42:26 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-03-01 11:42:26 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-01 11:42:22 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 03/01/23 11:42:26.25
STEP: getting the Pod and ensuring that it's patched 03/01/23 11:42:26.264
STEP: replacing the Pod's status Ready condition to False 03/01/23 11:42:26.269
STEP: check the Pod again to ensure its Ready conditions are False 03/01/23 11:42:26.283
STEP: deleting the Pod via a Collection with a LabelSelector 03/01/23 11:42:26.284
STEP: watching for the Pod to be deleted 03/01/23 11:42:26.297
Mar  1 11:42:26.299: INFO: observed event type MODIFIED
Mar  1 11:42:28.117: INFO: observed event type MODIFIED
Mar  1 11:42:28.567: INFO: observed event type MODIFIED
Mar  1 11:42:29.121: INFO: observed event type MODIFIED
Mar  1 11:42:29.139: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Mar  1 11:42:29.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2630" for this suite. 03/01/23 11:42:29.165
{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","completed":1,"skipped":26,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.754 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:42:22.421
    Mar  1 11:42:22.421: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename pods 03/01/23 11:42:22.422
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:42:22.444
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:42:22.448
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:895
    STEP: creating a Pod with a static label 03/01/23 11:42:22.482
    STEP: watching for Pod to be ready 03/01/23 11:42:22.496
    Mar  1 11:42:22.498: INFO: observed Pod pod-test in namespace pods-2630 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Mar  1 11:42:22.502: INFO: observed Pod pod-test in namespace pods-2630 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-01 11:42:22 +0000 UTC  }]
    Mar  1 11:42:22.519: INFO: observed Pod pod-test in namespace pods-2630 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-01 11:42:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-01 11:42:22 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-01 11:42:22 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-01 11:42:22 +0000 UTC  }]
    Mar  1 11:42:23.069: INFO: observed Pod pod-test in namespace pods-2630 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-01 11:42:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-01 11:42:22 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-01 11:42:22 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-01 11:42:22 +0000 UTC  }]
    Mar  1 11:42:26.242: INFO: Found Pod pod-test in namespace pods-2630 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-01 11:42:22 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-03-01 11:42:26 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-03-01 11:42:26 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-01 11:42:22 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 03/01/23 11:42:26.25
    STEP: getting the Pod and ensuring that it's patched 03/01/23 11:42:26.264
    STEP: replacing the Pod's status Ready condition to False 03/01/23 11:42:26.269
    STEP: check the Pod again to ensure its Ready conditions are False 03/01/23 11:42:26.283
    STEP: deleting the Pod via a Collection with a LabelSelector 03/01/23 11:42:26.284
    STEP: watching for the Pod to be deleted 03/01/23 11:42:26.297
    Mar  1 11:42:26.299: INFO: observed event type MODIFIED
    Mar  1 11:42:28.117: INFO: observed event type MODIFIED
    Mar  1 11:42:28.567: INFO: observed event type MODIFIED
    Mar  1 11:42:29.121: INFO: observed event type MODIFIED
    Mar  1 11:42:29.139: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Mar  1 11:42:29.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-2630" for this suite. 03/01/23 11:42:29.165
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:42:29.176
Mar  1 11:42:29.176: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename configmap 03/01/23 11:42:29.18
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:42:29.203
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:42:29.207
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
STEP: Creating configMap with name configmap-test-volume-b48b76e3-ffb6-4e34-ae33-4a36efedcb98 03/01/23 11:42:29.21
STEP: Creating a pod to test consume configMaps 03/01/23 11:42:29.217
Mar  1 11:42:29.227: INFO: Waiting up to 5m0s for pod "pod-configmaps-9aa9cdca-63dc-433d-aaad-b1c634c3e878" in namespace "configmap-5373" to be "Succeeded or Failed"
Mar  1 11:42:29.233: INFO: Pod "pod-configmaps-9aa9cdca-63dc-433d-aaad-b1c634c3e878": Phase="Pending", Reason="", readiness=false. Elapsed: 5.867289ms
Mar  1 11:42:31.243: INFO: Pod "pod-configmaps-9aa9cdca-63dc-433d-aaad-b1c634c3e878": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015808093s
Mar  1 11:42:33.247: INFO: Pod "pod-configmaps-9aa9cdca-63dc-433d-aaad-b1c634c3e878": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019825312s
STEP: Saw pod success 03/01/23 11:42:33.247
Mar  1 11:42:33.248: INFO: Pod "pod-configmaps-9aa9cdca-63dc-433d-aaad-b1c634c3e878" satisfied condition "Succeeded or Failed"
Mar  1 11:42:33.263: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-configmaps-9aa9cdca-63dc-433d-aaad-b1c634c3e878 container agnhost-container: <nil>
STEP: delete the pod 03/01/23 11:42:33.318
Mar  1 11:42:33.367: INFO: Waiting for pod pod-configmaps-9aa9cdca-63dc-433d-aaad-b1c634c3e878 to disappear
Mar  1 11:42:33.387: INFO: Pod pod-configmaps-9aa9cdca-63dc-433d-aaad-b1c634c3e878 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Mar  1 11:42:33.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5373" for this suite. 03/01/23 11:42:33.418
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":2,"skipped":44,"failed":0}
------------------------------
â€¢ [4.272 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:42:29.176
    Mar  1 11:42:29.176: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename configmap 03/01/23 11:42:29.18
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:42:29.203
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:42:29.207
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:56
    STEP: Creating configMap with name configmap-test-volume-b48b76e3-ffb6-4e34-ae33-4a36efedcb98 03/01/23 11:42:29.21
    STEP: Creating a pod to test consume configMaps 03/01/23 11:42:29.217
    Mar  1 11:42:29.227: INFO: Waiting up to 5m0s for pod "pod-configmaps-9aa9cdca-63dc-433d-aaad-b1c634c3e878" in namespace "configmap-5373" to be "Succeeded or Failed"
    Mar  1 11:42:29.233: INFO: Pod "pod-configmaps-9aa9cdca-63dc-433d-aaad-b1c634c3e878": Phase="Pending", Reason="", readiness=false. Elapsed: 5.867289ms
    Mar  1 11:42:31.243: INFO: Pod "pod-configmaps-9aa9cdca-63dc-433d-aaad-b1c634c3e878": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015808093s
    Mar  1 11:42:33.247: INFO: Pod "pod-configmaps-9aa9cdca-63dc-433d-aaad-b1c634c3e878": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019825312s
    STEP: Saw pod success 03/01/23 11:42:33.247
    Mar  1 11:42:33.248: INFO: Pod "pod-configmaps-9aa9cdca-63dc-433d-aaad-b1c634c3e878" satisfied condition "Succeeded or Failed"
    Mar  1 11:42:33.263: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-configmaps-9aa9cdca-63dc-433d-aaad-b1c634c3e878 container agnhost-container: <nil>
    STEP: delete the pod 03/01/23 11:42:33.318
    Mar  1 11:42:33.367: INFO: Waiting for pod pod-configmaps-9aa9cdca-63dc-433d-aaad-b1c634c3e878 to disappear
    Mar  1 11:42:33.387: INFO: Pod pod-configmaps-9aa9cdca-63dc-433d-aaad-b1c634c3e878 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Mar  1 11:42:33.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-5373" for this suite. 03/01/23 11:42:33.418
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] Services
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:42:33.451
Mar  1 11:42:33.451: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename services 03/01/23 11:42:33.452
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:42:33.513
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:42:33.518
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221
STEP: creating service in namespace services-5285 03/01/23 11:42:33.523
Mar  1 11:42:33.570: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-5285" to be "running and ready"
Mar  1 11:42:33.595: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 25.096805ms
Mar  1 11:42:33.595: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Mar  1 11:42:35.601: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.03110627s
Mar  1 11:42:35.601: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
Mar  1 11:42:35.601: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
Mar  1 11:42:35.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-5285 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Mar  1 11:42:35.785: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Mar  1 11:42:35.785: INFO: stdout: "iptables"
Mar  1 11:42:35.785: INFO: proxyMode: iptables
Mar  1 11:42:35.801: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Mar  1 11:42:35.805: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-5285 03/01/23 11:42:35.805
STEP: creating replication controller affinity-nodeport-timeout in namespace services-5285 03/01/23 11:42:35.825
I0301 11:42:35.843043      19 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-5285, replica count: 3
I0301 11:42:38.894873      19 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0301 11:42:41.895100      19 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  1 11:42:41.912: INFO: Creating new exec pod
Mar  1 11:42:41.921: INFO: Waiting up to 5m0s for pod "execpod-affinityxlm2h" in namespace "services-5285" to be "running"
Mar  1 11:42:41.925: INFO: Pod "execpod-affinityxlm2h": Phase="Pending", Reason="", readiness=false. Elapsed: 3.995521ms
Mar  1 11:42:43.930: INFO: Pod "execpod-affinityxlm2h": Phase="Running", Reason="", readiness=true. Elapsed: 2.009509147s
Mar  1 11:42:43.930: INFO: Pod "execpod-affinityxlm2h" satisfied condition "running"
Mar  1 11:42:44.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-5285 exec execpod-affinityxlm2h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
Mar  1 11:42:45.082: INFO: stderr: "+ nc -v -t -w 2 affinity-nodeport-timeout 80\n+ echo hostName\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Mar  1 11:42:45.082: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  1 11:42:45.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-5285 exec execpod-affinityxlm2h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.31.232 80'
Mar  1 11:42:45.252: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.31.232 80\nConnection to 10.233.31.232 80 port [tcp/http] succeeded!\n"
Mar  1 11:42:45.252: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  1 11:42:45.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-5285 exec execpod-affinityxlm2h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.128.0.76 31449'
Mar  1 11:42:45.404: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.128.0.76 31449\nConnection to 10.128.0.76 31449 port [tcp/*] succeeded!\n"
Mar  1 11:42:45.404: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  1 11:42:45.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-5285 exec execpod-affinityxlm2h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.128.2.241 31449'
Mar  1 11:42:45.524: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.128.2.241 31449\nConnection to 10.128.2.241 31449 port [tcp/*] succeeded!\n"
Mar  1 11:42:45.524: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  1 11:42:45.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-5285 exec execpod-affinityxlm2h -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.128.0.178:31449/ ; done'
Mar  1 11:42:45.762: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31449/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31449/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31449/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31449/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31449/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31449/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31449/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31449/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31449/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31449/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31449/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31449/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31449/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31449/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31449/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31449/\n"
Mar  1 11:42:45.762: INFO: stdout: "\naffinity-nodeport-timeout-j2jhl\naffinity-nodeport-timeout-j2jhl\naffinity-nodeport-timeout-j2jhl\naffinity-nodeport-timeout-j2jhl\naffinity-nodeport-timeout-j2jhl\naffinity-nodeport-timeout-j2jhl\naffinity-nodeport-timeout-j2jhl\naffinity-nodeport-timeout-j2jhl\naffinity-nodeport-timeout-j2jhl\naffinity-nodeport-timeout-j2jhl\naffinity-nodeport-timeout-j2jhl\naffinity-nodeport-timeout-j2jhl\naffinity-nodeport-timeout-j2jhl\naffinity-nodeport-timeout-j2jhl\naffinity-nodeport-timeout-j2jhl\naffinity-nodeport-timeout-j2jhl"
Mar  1 11:42:45.762: INFO: Received response from host: affinity-nodeport-timeout-j2jhl
Mar  1 11:42:45.762: INFO: Received response from host: affinity-nodeport-timeout-j2jhl
Mar  1 11:42:45.762: INFO: Received response from host: affinity-nodeport-timeout-j2jhl
Mar  1 11:42:45.762: INFO: Received response from host: affinity-nodeport-timeout-j2jhl
Mar  1 11:42:45.762: INFO: Received response from host: affinity-nodeport-timeout-j2jhl
Mar  1 11:42:45.762: INFO: Received response from host: affinity-nodeport-timeout-j2jhl
Mar  1 11:42:45.762: INFO: Received response from host: affinity-nodeport-timeout-j2jhl
Mar  1 11:42:45.762: INFO: Received response from host: affinity-nodeport-timeout-j2jhl
Mar  1 11:42:45.762: INFO: Received response from host: affinity-nodeport-timeout-j2jhl
Mar  1 11:42:45.762: INFO: Received response from host: affinity-nodeport-timeout-j2jhl
Mar  1 11:42:45.762: INFO: Received response from host: affinity-nodeport-timeout-j2jhl
Mar  1 11:42:45.762: INFO: Received response from host: affinity-nodeport-timeout-j2jhl
Mar  1 11:42:45.762: INFO: Received response from host: affinity-nodeport-timeout-j2jhl
Mar  1 11:42:45.762: INFO: Received response from host: affinity-nodeport-timeout-j2jhl
Mar  1 11:42:45.762: INFO: Received response from host: affinity-nodeport-timeout-j2jhl
Mar  1 11:42:45.762: INFO: Received response from host: affinity-nodeport-timeout-j2jhl
Mar  1 11:42:45.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-5285 exec execpod-affinityxlm2h -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.128.0.178:31449/'
Mar  1 11:42:45.894: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.128.0.178:31449/\n"
Mar  1 11:42:45.894: INFO: stdout: "affinity-nodeport-timeout-j2jhl"
Mar  1 11:43:05.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-5285 exec execpod-affinityxlm2h -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.128.0.178:31449/'
Mar  1 11:43:06.827: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.128.0.178:31449/\n"
Mar  1 11:43:06.827: INFO: stdout: "affinity-nodeport-timeout-lcrjf"
Mar  1 11:43:06.827: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-5285, will wait for the garbage collector to delete the pods 03/01/23 11:43:06.858
Mar  1 11:43:06.927: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 9.223995ms
Mar  1 11:43:07.028: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 100.537008ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar  1 11:43:09.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5285" for this suite. 03/01/23 11:43:09.372
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","completed":3,"skipped":51,"failed":0}
------------------------------
â€¢ [SLOW TEST] [35.932 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:42:33.451
    Mar  1 11:42:33.451: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename services 03/01/23 11:42:33.452
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:42:33.513
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:42:33.518
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2221
    STEP: creating service in namespace services-5285 03/01/23 11:42:33.523
    Mar  1 11:42:33.570: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-5285" to be "running and ready"
    Mar  1 11:42:33.595: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 25.096805ms
    Mar  1 11:42:33.595: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
    Mar  1 11:42:35.601: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.03110627s
    Mar  1 11:42:35.601: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
    Mar  1 11:42:35.601: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
    Mar  1 11:42:35.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-5285 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
    Mar  1 11:42:35.785: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
    Mar  1 11:42:35.785: INFO: stdout: "iptables"
    Mar  1 11:42:35.785: INFO: proxyMode: iptables
    Mar  1 11:42:35.801: INFO: Waiting for pod kube-proxy-mode-detector to disappear
    Mar  1 11:42:35.805: INFO: Pod kube-proxy-mode-detector no longer exists
    STEP: creating service affinity-nodeport-timeout in namespace services-5285 03/01/23 11:42:35.805
    STEP: creating replication controller affinity-nodeport-timeout in namespace services-5285 03/01/23 11:42:35.825
    I0301 11:42:35.843043      19 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-5285, replica count: 3
    I0301 11:42:38.894873      19 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0301 11:42:41.895100      19 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar  1 11:42:41.912: INFO: Creating new exec pod
    Mar  1 11:42:41.921: INFO: Waiting up to 5m0s for pod "execpod-affinityxlm2h" in namespace "services-5285" to be "running"
    Mar  1 11:42:41.925: INFO: Pod "execpod-affinityxlm2h": Phase="Pending", Reason="", readiness=false. Elapsed: 3.995521ms
    Mar  1 11:42:43.930: INFO: Pod "execpod-affinityxlm2h": Phase="Running", Reason="", readiness=true. Elapsed: 2.009509147s
    Mar  1 11:42:43.930: INFO: Pod "execpod-affinityxlm2h" satisfied condition "running"
    Mar  1 11:42:44.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-5285 exec execpod-affinityxlm2h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
    Mar  1 11:42:45.082: INFO: stderr: "+ nc -v -t -w 2 affinity-nodeport-timeout 80\n+ echo hostName\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
    Mar  1 11:42:45.082: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  1 11:42:45.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-5285 exec execpod-affinityxlm2h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.31.232 80'
    Mar  1 11:42:45.252: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.31.232 80\nConnection to 10.233.31.232 80 port [tcp/http] succeeded!\n"
    Mar  1 11:42:45.252: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  1 11:42:45.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-5285 exec execpod-affinityxlm2h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.128.0.76 31449'
    Mar  1 11:42:45.404: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.128.0.76 31449\nConnection to 10.128.0.76 31449 port [tcp/*] succeeded!\n"
    Mar  1 11:42:45.404: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  1 11:42:45.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-5285 exec execpod-affinityxlm2h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.128.2.241 31449'
    Mar  1 11:42:45.524: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.128.2.241 31449\nConnection to 10.128.2.241 31449 port [tcp/*] succeeded!\n"
    Mar  1 11:42:45.524: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  1 11:42:45.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-5285 exec execpod-affinityxlm2h -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.128.0.178:31449/ ; done'
    Mar  1 11:42:45.762: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31449/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31449/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31449/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31449/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31449/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31449/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31449/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31449/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31449/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31449/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31449/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31449/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31449/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31449/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31449/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31449/\n"
    Mar  1 11:42:45.762: INFO: stdout: "\naffinity-nodeport-timeout-j2jhl\naffinity-nodeport-timeout-j2jhl\naffinity-nodeport-timeout-j2jhl\naffinity-nodeport-timeout-j2jhl\naffinity-nodeport-timeout-j2jhl\naffinity-nodeport-timeout-j2jhl\naffinity-nodeport-timeout-j2jhl\naffinity-nodeport-timeout-j2jhl\naffinity-nodeport-timeout-j2jhl\naffinity-nodeport-timeout-j2jhl\naffinity-nodeport-timeout-j2jhl\naffinity-nodeport-timeout-j2jhl\naffinity-nodeport-timeout-j2jhl\naffinity-nodeport-timeout-j2jhl\naffinity-nodeport-timeout-j2jhl\naffinity-nodeport-timeout-j2jhl"
    Mar  1 11:42:45.762: INFO: Received response from host: affinity-nodeport-timeout-j2jhl
    Mar  1 11:42:45.762: INFO: Received response from host: affinity-nodeport-timeout-j2jhl
    Mar  1 11:42:45.762: INFO: Received response from host: affinity-nodeport-timeout-j2jhl
    Mar  1 11:42:45.762: INFO: Received response from host: affinity-nodeport-timeout-j2jhl
    Mar  1 11:42:45.762: INFO: Received response from host: affinity-nodeport-timeout-j2jhl
    Mar  1 11:42:45.762: INFO: Received response from host: affinity-nodeport-timeout-j2jhl
    Mar  1 11:42:45.762: INFO: Received response from host: affinity-nodeport-timeout-j2jhl
    Mar  1 11:42:45.762: INFO: Received response from host: affinity-nodeport-timeout-j2jhl
    Mar  1 11:42:45.762: INFO: Received response from host: affinity-nodeport-timeout-j2jhl
    Mar  1 11:42:45.762: INFO: Received response from host: affinity-nodeport-timeout-j2jhl
    Mar  1 11:42:45.762: INFO: Received response from host: affinity-nodeport-timeout-j2jhl
    Mar  1 11:42:45.762: INFO: Received response from host: affinity-nodeport-timeout-j2jhl
    Mar  1 11:42:45.762: INFO: Received response from host: affinity-nodeport-timeout-j2jhl
    Mar  1 11:42:45.762: INFO: Received response from host: affinity-nodeport-timeout-j2jhl
    Mar  1 11:42:45.762: INFO: Received response from host: affinity-nodeport-timeout-j2jhl
    Mar  1 11:42:45.762: INFO: Received response from host: affinity-nodeport-timeout-j2jhl
    Mar  1 11:42:45.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-5285 exec execpod-affinityxlm2h -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.128.0.178:31449/'
    Mar  1 11:42:45.894: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.128.0.178:31449/\n"
    Mar  1 11:42:45.894: INFO: stdout: "affinity-nodeport-timeout-j2jhl"
    Mar  1 11:43:05.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-5285 exec execpod-affinityxlm2h -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.128.0.178:31449/'
    Mar  1 11:43:06.827: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.128.0.178:31449/\n"
    Mar  1 11:43:06.827: INFO: stdout: "affinity-nodeport-timeout-lcrjf"
    Mar  1 11:43:06.827: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-5285, will wait for the garbage collector to delete the pods 03/01/23 11:43:06.858
    Mar  1 11:43:06.927: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 9.223995ms
    Mar  1 11:43:07.028: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 100.537008ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar  1 11:43:09.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-5285" for this suite. 03/01/23 11:43:09.372
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:43:09.383
Mar  1 11:43:09.383: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename replicaset 03/01/23 11:43:09.384
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:43:09.404
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:43:09.408
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Mar  1 11:43:09.426: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar  1 11:43:14.433: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 03/01/23 11:43:14.433
STEP: Scaling up "test-rs" replicaset  03/01/23 11:43:14.434
Mar  1 11:43:14.446: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 03/01/23 11:43:14.446
W0301 11:43:14.457641      19 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Mar  1 11:43:14.463: INFO: observed ReplicaSet test-rs in namespace replicaset-4989 with ReadyReplicas 1, AvailableReplicas 1
Mar  1 11:43:14.481: INFO: observed ReplicaSet test-rs in namespace replicaset-4989 with ReadyReplicas 1, AvailableReplicas 1
Mar  1 11:43:14.499: INFO: observed ReplicaSet test-rs in namespace replicaset-4989 with ReadyReplicas 1, AvailableReplicas 1
Mar  1 11:43:14.507: INFO: observed ReplicaSet test-rs in namespace replicaset-4989 with ReadyReplicas 1, AvailableReplicas 1
Mar  1 11:43:18.540: INFO: observed ReplicaSet test-rs in namespace replicaset-4989 with ReadyReplicas 2, AvailableReplicas 2
Mar  1 11:43:18.616: INFO: observed Replicaset test-rs in namespace replicaset-4989 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Mar  1 11:43:18.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4989" for this suite. 03/01/23 11:43:18.625
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","completed":4,"skipped":51,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.251 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:43:09.383
    Mar  1 11:43:09.383: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename replicaset 03/01/23 11:43:09.384
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:43:09.404
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:43:09.408
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Mar  1 11:43:09.426: INFO: Pod name sample-pod: Found 0 pods out of 1
    Mar  1 11:43:14.433: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 03/01/23 11:43:14.433
    STEP: Scaling up "test-rs" replicaset  03/01/23 11:43:14.434
    Mar  1 11:43:14.446: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 03/01/23 11:43:14.446
    W0301 11:43:14.457641      19 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Mar  1 11:43:14.463: INFO: observed ReplicaSet test-rs in namespace replicaset-4989 with ReadyReplicas 1, AvailableReplicas 1
    Mar  1 11:43:14.481: INFO: observed ReplicaSet test-rs in namespace replicaset-4989 with ReadyReplicas 1, AvailableReplicas 1
    Mar  1 11:43:14.499: INFO: observed ReplicaSet test-rs in namespace replicaset-4989 with ReadyReplicas 1, AvailableReplicas 1
    Mar  1 11:43:14.507: INFO: observed ReplicaSet test-rs in namespace replicaset-4989 with ReadyReplicas 1, AvailableReplicas 1
    Mar  1 11:43:18.540: INFO: observed ReplicaSet test-rs in namespace replicaset-4989 with ReadyReplicas 2, AvailableReplicas 2
    Mar  1 11:43:18.616: INFO: observed Replicaset test-rs in namespace replicaset-4989 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Mar  1 11:43:18.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-4989" for this suite. 03/01/23 11:43:18.625
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:43:18.634
Mar  1 11:43:18.634: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename watch 03/01/23 11:43:18.635
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:43:18.657
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:43:18.661
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 03/01/23 11:43:18.673
STEP: modifying the configmap once 03/01/23 11:43:18.682
STEP: modifying the configmap a second time 03/01/23 11:43:18.694
STEP: deleting the configmap 03/01/23 11:43:18.705
STEP: creating a watch on configmaps from the resource version returned by the first update 03/01/23 11:43:18.713
STEP: Expecting to observe notifications for all changes to the configmap after the first update 03/01/23 11:43:18.715
Mar  1 11:43:18.715: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-3070  ded99c87-5c07-43d4-8066-320c779958e1 4129 0 2023-03-01 11:43:18 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-03-01 11:43:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  1 11:43:18.716: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-3070  ded99c87-5c07-43d4-8066-320c779958e1 4130 0 2023-03-01 11:43:18 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-03-01 11:43:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Mar  1 11:43:18.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3070" for this suite. 03/01/23 11:43:18.723
{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","completed":5,"skipped":61,"failed":0}
------------------------------
â€¢ [0.097 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:43:18.634
    Mar  1 11:43:18.634: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename watch 03/01/23 11:43:18.635
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:43:18.657
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:43:18.661
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 03/01/23 11:43:18.673
    STEP: modifying the configmap once 03/01/23 11:43:18.682
    STEP: modifying the configmap a second time 03/01/23 11:43:18.694
    STEP: deleting the configmap 03/01/23 11:43:18.705
    STEP: creating a watch on configmaps from the resource version returned by the first update 03/01/23 11:43:18.713
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 03/01/23 11:43:18.715
    Mar  1 11:43:18.715: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-3070  ded99c87-5c07-43d4-8066-320c779958e1 4129 0 2023-03-01 11:43:18 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-03-01 11:43:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar  1 11:43:18.716: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-3070  ded99c87-5c07-43d4-8066-320c779958e1 4130 0 2023-03-01 11:43:18 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-03-01 11:43:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Mar  1 11:43:18.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-3070" for this suite. 03/01/23 11:43:18.723
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:43:18.736
Mar  1 11:43:18.736: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename dns 03/01/23 11:43:18.736
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:43:18.765
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:43:18.769
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 03/01/23 11:43:18.772
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-4971.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-4971.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 03/01/23 11:43:18.78
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-4971.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-4971.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 03/01/23 11:43:18.781
STEP: creating a pod to probe DNS 03/01/23 11:43:18.781
STEP: submitting the pod to kubernetes 03/01/23 11:43:18.781
Mar  1 11:43:18.793: INFO: Waiting up to 15m0s for pod "dns-test-021c37de-2995-4aa7-80a9-6c564e9b3dd2" in namespace "dns-4971" to be "running"
Mar  1 11:43:18.797: INFO: Pod "dns-test-021c37de-2995-4aa7-80a9-6c564e9b3dd2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.097777ms
Mar  1 11:43:20.803: INFO: Pod "dns-test-021c37de-2995-4aa7-80a9-6c564e9b3dd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010239564s
Mar  1 11:43:22.803: INFO: Pod "dns-test-021c37de-2995-4aa7-80a9-6c564e9b3dd2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010007636s
Mar  1 11:43:24.804: INFO: Pod "dns-test-021c37de-2995-4aa7-80a9-6c564e9b3dd2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.010737737s
Mar  1 11:43:26.804: INFO: Pod "dns-test-021c37de-2995-4aa7-80a9-6c564e9b3dd2": Phase="Running", Reason="", readiness=true. Elapsed: 8.010761409s
Mar  1 11:43:26.804: INFO: Pod "dns-test-021c37de-2995-4aa7-80a9-6c564e9b3dd2" satisfied condition "running"
STEP: retrieving the pod 03/01/23 11:43:26.804
STEP: looking for the results for each expected name from probers 03/01/23 11:43:26.81
Mar  1 11:43:26.856: INFO: DNS probes using dns-4971/dns-test-021c37de-2995-4aa7-80a9-6c564e9b3dd2 succeeded

STEP: deleting the pod 03/01/23 11:43:26.856
STEP: deleting the test headless service 03/01/23 11:43:26.884
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Mar  1 11:43:26.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4971" for this suite. 03/01/23 11:43:26.921
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [Conformance]","completed":6,"skipped":132,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.195 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:43:18.736
    Mar  1 11:43:18.736: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename dns 03/01/23 11:43:18.736
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:43:18.765
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:43:18.769
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 03/01/23 11:43:18.772
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-4971.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-4971.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     03/01/23 11:43:18.78
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-4971.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-4971.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     03/01/23 11:43:18.781
    STEP: creating a pod to probe DNS 03/01/23 11:43:18.781
    STEP: submitting the pod to kubernetes 03/01/23 11:43:18.781
    Mar  1 11:43:18.793: INFO: Waiting up to 15m0s for pod "dns-test-021c37de-2995-4aa7-80a9-6c564e9b3dd2" in namespace "dns-4971" to be "running"
    Mar  1 11:43:18.797: INFO: Pod "dns-test-021c37de-2995-4aa7-80a9-6c564e9b3dd2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.097777ms
    Mar  1 11:43:20.803: INFO: Pod "dns-test-021c37de-2995-4aa7-80a9-6c564e9b3dd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010239564s
    Mar  1 11:43:22.803: INFO: Pod "dns-test-021c37de-2995-4aa7-80a9-6c564e9b3dd2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010007636s
    Mar  1 11:43:24.804: INFO: Pod "dns-test-021c37de-2995-4aa7-80a9-6c564e9b3dd2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.010737737s
    Mar  1 11:43:26.804: INFO: Pod "dns-test-021c37de-2995-4aa7-80a9-6c564e9b3dd2": Phase="Running", Reason="", readiness=true. Elapsed: 8.010761409s
    Mar  1 11:43:26.804: INFO: Pod "dns-test-021c37de-2995-4aa7-80a9-6c564e9b3dd2" satisfied condition "running"
    STEP: retrieving the pod 03/01/23 11:43:26.804
    STEP: looking for the results for each expected name from probers 03/01/23 11:43:26.81
    Mar  1 11:43:26.856: INFO: DNS probes using dns-4971/dns-test-021c37de-2995-4aa7-80a9-6c564e9b3dd2 succeeded

    STEP: deleting the pod 03/01/23 11:43:26.856
    STEP: deleting the test headless service 03/01/23 11:43:26.884
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Mar  1 11:43:26.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-4971" for this suite. 03/01/23 11:43:26.921
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:43:26.934
Mar  1 11:43:26.934: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename prestop 03/01/23 11:43:26.935
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:43:26.959
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:43:26.964
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-4127 03/01/23 11:43:26.969
STEP: Waiting for pods to come up. 03/01/23 11:43:26.98
Mar  1 11:43:26.980: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-4127" to be "running"
Mar  1 11:43:26.994: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 13.408106ms
Mar  1 11:43:29.001: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.020464298s
Mar  1 11:43:29.001: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-4127 03/01/23 11:43:29.005
Mar  1 11:43:29.013: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-4127" to be "running"
Mar  1 11:43:29.020: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 6.315256ms
Mar  1 11:43:35.759: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 6.745875803s
Mar  1 11:43:35.759: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 03/01/23 11:43:35.759
Mar  1 11:43:40.783: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 03/01/23 11:43:40.783
[AfterEach] [sig-node] PreStop
  test/e2e/framework/framework.go:187
Mar  1 11:43:40.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-4127" for this suite. 03/01/23 11:43:40.809
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","completed":7,"skipped":161,"failed":0}
------------------------------
â€¢ [SLOW TEST] [13.884 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:43:26.934
    Mar  1 11:43:26.934: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename prestop 03/01/23 11:43:26.935
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:43:26.959
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:43:26.964
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-4127 03/01/23 11:43:26.969
    STEP: Waiting for pods to come up. 03/01/23 11:43:26.98
    Mar  1 11:43:26.980: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-4127" to be "running"
    Mar  1 11:43:26.994: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 13.408106ms
    Mar  1 11:43:29.001: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.020464298s
    Mar  1 11:43:29.001: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-4127 03/01/23 11:43:29.005
    Mar  1 11:43:29.013: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-4127" to be "running"
    Mar  1 11:43:29.020: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 6.315256ms
    Mar  1 11:43:35.759: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 6.745875803s
    Mar  1 11:43:35.759: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 03/01/23 11:43:35.759
    Mar  1 11:43:40.783: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 03/01/23 11:43:40.783
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/framework.go:187
    Mar  1 11:43:40.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "prestop-4127" for this suite. 03/01/23 11:43:40.809
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:43:40.819
Mar  1 11:43:40.819: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename crd-webhook 03/01/23 11:43:40.82
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:43:40.844
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:43:40.848
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 03/01/23 11:43:40.851
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 03/01/23 11:43:41.317
STEP: Deploying the custom resource conversion webhook pod 03/01/23 11:43:41.329
STEP: Wait for the deployment to be ready 03/01/23 11:43:41.349
Mar  1 11:43:41.366: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/01/23 11:43:43.387
STEP: Verifying the service has paired with the endpoint 03/01/23 11:43:43.402
Mar  1 11:43:44.403: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Mar  1 11:43:44.408: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Creating a v1 custom resource 03/01/23 11:43:51.998
STEP: v2 custom resource should be converted 03/01/23 11:43:52.004
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  1 11:43:52.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-8662" for this suite. 03/01/23 11:43:52.535
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","completed":8,"skipped":171,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.782 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:43:40.819
    Mar  1 11:43:40.819: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename crd-webhook 03/01/23 11:43:40.82
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:43:40.844
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:43:40.848
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 03/01/23 11:43:40.851
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 03/01/23 11:43:41.317
    STEP: Deploying the custom resource conversion webhook pod 03/01/23 11:43:41.329
    STEP: Wait for the deployment to be ready 03/01/23 11:43:41.349
    Mar  1 11:43:41.366: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/01/23 11:43:43.387
    STEP: Verifying the service has paired with the endpoint 03/01/23 11:43:43.402
    Mar  1 11:43:44.403: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Mar  1 11:43:44.408: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Creating a v1 custom resource 03/01/23 11:43:51.998
    STEP: v2 custom resource should be converted 03/01/23 11:43:52.004
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  1 11:43:52.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-8662" for this suite. 03/01/23 11:43:52.535
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:43:52.606
Mar  1 11:43:52.606: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename sched-preemption 03/01/23 11:43:52.606
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:43:52.639
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:43:52.642
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Mar  1 11:43:52.667: INFO: Waiting up to 1m0s for all nodes to be ready
Mar  1 11:44:52.713: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:44:52.719
Mar  1 11:44:52.719: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename sched-preemption-path 03/01/23 11:44:52.719
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:44:52.742
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:44:52.745
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:690
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
Mar  1 11:44:52.766: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Mar  1 11:44:52.773: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/framework.go:187
Mar  1 11:44:52.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-1274" for this suite. 03/01/23 11:44:52.802
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:706
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Mar  1 11:44:52.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-6396" for this suite. 03/01/23 11:44:52.838
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","completed":9,"skipped":186,"failed":0}
------------------------------
â€¢ [SLOW TEST] [60.300 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:683
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:733

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:43:52.606
    Mar  1 11:43:52.606: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename sched-preemption 03/01/23 11:43:52.606
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:43:52.639
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:43:52.642
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Mar  1 11:43:52.667: INFO: Waiting up to 1m0s for all nodes to be ready
    Mar  1 11:44:52.713: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:44:52.719
    Mar  1 11:44:52.719: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename sched-preemption-path 03/01/23 11:44:52.719
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:44:52.742
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:44:52.745
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:690
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:733
    Mar  1 11:44:52.766: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Mar  1 11:44:52.773: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/framework.go:187
    Mar  1 11:44:52.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-1274" for this suite. 03/01/23 11:44:52.802
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:706
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Mar  1 11:44:52.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-6396" for this suite. 03/01/23 11:44:52.838
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:44:52.911
Mar  1 11:44:52.911: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename projected 03/01/23 11:44:52.911
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:44:52.935
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:44:52.938
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
STEP: Creating configMap with name projected-configmap-test-volume-d689ebd9-92e2-43fc-93ff-a8efa3f982e1 03/01/23 11:44:52.941
STEP: Creating a pod to test consume configMaps 03/01/23 11:44:52.946
Mar  1 11:44:52.957: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ee9dff3a-82a3-4bb9-9699-c56b87b37aae" in namespace "projected-4228" to be "Succeeded or Failed"
Mar  1 11:44:52.963: INFO: Pod "pod-projected-configmaps-ee9dff3a-82a3-4bb9-9699-c56b87b37aae": Phase="Pending", Reason="", readiness=false. Elapsed: 6.086226ms
Mar  1 11:44:54.970: INFO: Pod "pod-projected-configmaps-ee9dff3a-82a3-4bb9-9699-c56b87b37aae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013448s
Mar  1 11:44:56.968: INFO: Pod "pod-projected-configmaps-ee9dff3a-82a3-4bb9-9699-c56b87b37aae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011487185s
STEP: Saw pod success 03/01/23 11:44:56.968
Mar  1 11:44:56.969: INFO: Pod "pod-projected-configmaps-ee9dff3a-82a3-4bb9-9699-c56b87b37aae" satisfied condition "Succeeded or Failed"
Mar  1 11:44:56.973: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-projected-configmaps-ee9dff3a-82a3-4bb9-9699-c56b87b37aae container agnhost-container: <nil>
STEP: delete the pod 03/01/23 11:44:56.99
Mar  1 11:44:57.007: INFO: Waiting for pod pod-projected-configmaps-ee9dff3a-82a3-4bb9-9699-c56b87b37aae to disappear
Mar  1 11:44:57.012: INFO: Pod pod-projected-configmaps-ee9dff3a-82a3-4bb9-9699-c56b87b37aae no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Mar  1 11:44:57.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4228" for this suite. 03/01/23 11:44:57.021
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":10,"skipped":236,"failed":0}
------------------------------
â€¢ [4.120 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:44:52.911
    Mar  1 11:44:52.911: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename projected 03/01/23 11:44:52.911
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:44:52.935
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:44:52.938
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:73
    STEP: Creating configMap with name projected-configmap-test-volume-d689ebd9-92e2-43fc-93ff-a8efa3f982e1 03/01/23 11:44:52.941
    STEP: Creating a pod to test consume configMaps 03/01/23 11:44:52.946
    Mar  1 11:44:52.957: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ee9dff3a-82a3-4bb9-9699-c56b87b37aae" in namespace "projected-4228" to be "Succeeded or Failed"
    Mar  1 11:44:52.963: INFO: Pod "pod-projected-configmaps-ee9dff3a-82a3-4bb9-9699-c56b87b37aae": Phase="Pending", Reason="", readiness=false. Elapsed: 6.086226ms
    Mar  1 11:44:54.970: INFO: Pod "pod-projected-configmaps-ee9dff3a-82a3-4bb9-9699-c56b87b37aae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013448s
    Mar  1 11:44:56.968: INFO: Pod "pod-projected-configmaps-ee9dff3a-82a3-4bb9-9699-c56b87b37aae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011487185s
    STEP: Saw pod success 03/01/23 11:44:56.968
    Mar  1 11:44:56.969: INFO: Pod "pod-projected-configmaps-ee9dff3a-82a3-4bb9-9699-c56b87b37aae" satisfied condition "Succeeded or Failed"
    Mar  1 11:44:56.973: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-projected-configmaps-ee9dff3a-82a3-4bb9-9699-c56b87b37aae container agnhost-container: <nil>
    STEP: delete the pod 03/01/23 11:44:56.99
    Mar  1 11:44:57.007: INFO: Waiting for pod pod-projected-configmaps-ee9dff3a-82a3-4bb9-9699-c56b87b37aae to disappear
    Mar  1 11:44:57.012: INFO: Pod pod-projected-configmaps-ee9dff3a-82a3-4bb9-9699-c56b87b37aae no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Mar  1 11:44:57.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4228" for this suite. 03/01/23 11:44:57.021
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:44:57.032
Mar  1 11:44:57.032: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename runtimeclass 03/01/23 11:44:57.033
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:44:57.056
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:44:57.06
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Mar  1 11:44:57.081: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-9005 to be scheduled
Mar  1 11:44:57.085: INFO: 1 pods are not scheduled: [runtimeclass-9005/test-runtimeclass-runtimeclass-9005-preconfigured-handler-wvsg7(b144327b-0879-4745-9ca3-b1955a13cd50)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Mar  1 11:44:59.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-9005" for this suite. 03/01/23 11:44:59.108
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]","completed":11,"skipped":271,"failed":0}
------------------------------
â€¢ [2.086 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:44:57.032
    Mar  1 11:44:57.032: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename runtimeclass 03/01/23 11:44:57.033
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:44:57.056
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:44:57.06
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Mar  1 11:44:57.081: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-9005 to be scheduled
    Mar  1 11:44:57.085: INFO: 1 pods are not scheduled: [runtimeclass-9005/test-runtimeclass-runtimeclass-9005-preconfigured-handler-wvsg7(b144327b-0879-4745-9ca3-b1955a13cd50)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Mar  1 11:44:59.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-9005" for this suite. 03/01/23 11:44:59.108
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:44:59.118
Mar  1 11:44:59.118: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename webhook 03/01/23 11:44:59.119
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:44:59.145
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:44:59.148
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/01/23 11:44:59.17
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/01/23 11:44:59.476
STEP: Deploying the webhook pod 03/01/23 11:44:59.488
STEP: Wait for the deployment to be ready 03/01/23 11:44:59.502
Mar  1 11:44:59.512: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/01/23 11:45:01.527
STEP: Verifying the service has paired with the endpoint 03/01/23 11:45:01.541
Mar  1 11:45:02.542: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
STEP: Registering the crd webhook via the AdmissionRegistration API 03/01/23 11:45:02.547
STEP: Creating a custom resource definition that should be denied by the webhook 03/01/23 11:45:02.566
Mar  1 11:45:02.567: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  1 11:45:02.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4196" for this suite. 03/01/23 11:45:02.594
STEP: Destroying namespace "webhook-4196-markers" for this suite. 03/01/23 11:45:02.603
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","completed":12,"skipped":281,"failed":0}
------------------------------
â€¢ [3.554 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:44:59.118
    Mar  1 11:44:59.118: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename webhook 03/01/23 11:44:59.119
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:44:59.145
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:44:59.148
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/01/23 11:44:59.17
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/01/23 11:44:59.476
    STEP: Deploying the webhook pod 03/01/23 11:44:59.488
    STEP: Wait for the deployment to be ready 03/01/23 11:44:59.502
    Mar  1 11:44:59.512: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/01/23 11:45:01.527
    STEP: Verifying the service has paired with the endpoint 03/01/23 11:45:01.541
    Mar  1 11:45:02.542: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:307
    STEP: Registering the crd webhook via the AdmissionRegistration API 03/01/23 11:45:02.547
    STEP: Creating a custom resource definition that should be denied by the webhook 03/01/23 11:45:02.566
    Mar  1 11:45:02.567: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  1 11:45:02.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-4196" for this suite. 03/01/23 11:45:02.594
    STEP: Destroying namespace "webhook-4196-markers" for this suite. 03/01/23 11:45:02.603
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:45:02.675
Mar  1 11:45:02.675: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename configmap 03/01/23 11:45:02.676
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:45:02.704
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:45:02.708
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
STEP: Creating configMap configmap-7703/configmap-test-c24a7d65-cbe3-4f64-85fc-578880ad7a7c 03/01/23 11:45:02.711
STEP: Creating a pod to test consume configMaps 03/01/23 11:45:02.718
Mar  1 11:45:02.728: INFO: Waiting up to 5m0s for pod "pod-configmaps-3a044e40-3ce1-481a-9492-a44077b36e9f" in namespace "configmap-7703" to be "Succeeded or Failed"
Mar  1 11:45:02.735: INFO: Pod "pod-configmaps-3a044e40-3ce1-481a-9492-a44077b36e9f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.841783ms
Mar  1 11:45:04.741: INFO: Pod "pod-configmaps-3a044e40-3ce1-481a-9492-a44077b36e9f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012640078s
Mar  1 11:45:06.743: INFO: Pod "pod-configmaps-3a044e40-3ce1-481a-9492-a44077b36e9f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015120165s
STEP: Saw pod success 03/01/23 11:45:06.743
Mar  1 11:45:06.744: INFO: Pod "pod-configmaps-3a044e40-3ce1-481a-9492-a44077b36e9f" satisfied condition "Succeeded or Failed"
Mar  1 11:45:06.749: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-configmaps-3a044e40-3ce1-481a-9492-a44077b36e9f container env-test: <nil>
STEP: delete the pod 03/01/23 11:45:06.758
Mar  1 11:45:06.772: INFO: Waiting for pod pod-configmaps-3a044e40-3ce1-481a-9492-a44077b36e9f to disappear
Mar  1 11:45:06.777: INFO: Pod pod-configmaps-3a044e40-3ce1-481a-9492-a44077b36e9f no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Mar  1 11:45:06.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7703" for this suite. 03/01/23 11:45:06.783
{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","completed":13,"skipped":293,"failed":0}
------------------------------
â€¢ [4.120 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:45:02.675
    Mar  1 11:45:02.675: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename configmap 03/01/23 11:45:02.676
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:45:02.704
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:45:02.708
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:92
    STEP: Creating configMap configmap-7703/configmap-test-c24a7d65-cbe3-4f64-85fc-578880ad7a7c 03/01/23 11:45:02.711
    STEP: Creating a pod to test consume configMaps 03/01/23 11:45:02.718
    Mar  1 11:45:02.728: INFO: Waiting up to 5m0s for pod "pod-configmaps-3a044e40-3ce1-481a-9492-a44077b36e9f" in namespace "configmap-7703" to be "Succeeded or Failed"
    Mar  1 11:45:02.735: INFO: Pod "pod-configmaps-3a044e40-3ce1-481a-9492-a44077b36e9f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.841783ms
    Mar  1 11:45:04.741: INFO: Pod "pod-configmaps-3a044e40-3ce1-481a-9492-a44077b36e9f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012640078s
    Mar  1 11:45:06.743: INFO: Pod "pod-configmaps-3a044e40-3ce1-481a-9492-a44077b36e9f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015120165s
    STEP: Saw pod success 03/01/23 11:45:06.743
    Mar  1 11:45:06.744: INFO: Pod "pod-configmaps-3a044e40-3ce1-481a-9492-a44077b36e9f" satisfied condition "Succeeded or Failed"
    Mar  1 11:45:06.749: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-configmaps-3a044e40-3ce1-481a-9492-a44077b36e9f container env-test: <nil>
    STEP: delete the pod 03/01/23 11:45:06.758
    Mar  1 11:45:06.772: INFO: Waiting for pod pod-configmaps-3a044e40-3ce1-481a-9492-a44077b36e9f to disappear
    Mar  1 11:45:06.777: INFO: Pod pod-configmaps-3a044e40-3ce1-481a-9492-a44077b36e9f no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Mar  1 11:45:06.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-7703" for this suite. 03/01/23 11:45:06.783
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:45:06.799
Mar  1 11:45:06.799: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename container-runtime 03/01/23 11:45:06.799
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:45:06.825
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:45:06.829
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 03/01/23 11:45:06.845
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 03/01/23 11:45:22.952
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 03/01/23 11:45:22.958
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 03/01/23 11:45:22.967
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 03/01/23 11:45:22.968
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 03/01/23 11:45:23
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 03/01/23 11:45:26.03
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 03/01/23 11:45:28.047
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 03/01/23 11:45:28.056
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 03/01/23 11:45:28.057
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 03/01/23 11:45:28.083
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 03/01/23 11:45:29.095
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 03/01/23 11:45:32.116
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 03/01/23 11:45:32.126
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 03/01/23 11:45:32.126
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Mar  1 11:45:32.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1616" for this suite. 03/01/23 11:45:32.169
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","completed":14,"skipped":323,"failed":0}
------------------------------
â€¢ [SLOW TEST] [25.378 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    when starting a container that exits
    test/e2e/common/node/runtime.go:44
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:45:06.799
    Mar  1 11:45:06.799: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename container-runtime 03/01/23 11:45:06.799
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:45:06.825
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:45:06.829
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 03/01/23 11:45:06.845
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 03/01/23 11:45:22.952
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 03/01/23 11:45:22.958
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 03/01/23 11:45:22.967
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 03/01/23 11:45:22.968
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 03/01/23 11:45:23
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 03/01/23 11:45:26.03
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 03/01/23 11:45:28.047
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 03/01/23 11:45:28.056
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 03/01/23 11:45:28.057
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 03/01/23 11:45:28.083
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 03/01/23 11:45:29.095
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 03/01/23 11:45:32.116
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 03/01/23 11:45:32.126
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 03/01/23 11:45:32.126
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Mar  1 11:45:32.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-1616" for this suite. 03/01/23 11:45:32.169
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:45:32.179
Mar  1 11:45:32.179: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename webhook 03/01/23 11:45:32.18
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:45:32.202
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:45:32.208
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/01/23 11:45:32.226
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/01/23 11:45:32.539
STEP: Deploying the webhook pod 03/01/23 11:45:32.551
STEP: Wait for the deployment to be ready 03/01/23 11:45:32.566
Mar  1 11:45:32.578: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/01/23 11:45:34.593
STEP: Verifying the service has paired with the endpoint 03/01/23 11:45:34.608
Mar  1 11:45:35.609: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
Mar  1 11:45:35.614: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-71-crds.webhook.example.com via the AdmissionRegistration API 03/01/23 11:45:41.13
STEP: Creating a custom resource while v1 is storage version 03/01/23 11:45:41.148
STEP: Patching Custom Resource Definition to set v2 as storage 03/01/23 11:45:43.241
STEP: Patching the custom resource while v2 is storage version 03/01/23 11:45:43.258
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  1 11:45:43.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7474" for this suite. 03/01/23 11:45:43.848
STEP: Destroying namespace "webhook-7474-markers" for this suite. 03/01/23 11:45:43.865
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","completed":15,"skipped":329,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.762 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:45:32.179
    Mar  1 11:45:32.179: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename webhook 03/01/23 11:45:32.18
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:45:32.202
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:45:32.208
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/01/23 11:45:32.226
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/01/23 11:45:32.539
    STEP: Deploying the webhook pod 03/01/23 11:45:32.551
    STEP: Wait for the deployment to be ready 03/01/23 11:45:32.566
    Mar  1 11:45:32.578: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/01/23 11:45:34.593
    STEP: Verifying the service has paired with the endpoint 03/01/23 11:45:34.608
    Mar  1 11:45:35.609: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:322
    Mar  1 11:45:35.614: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-71-crds.webhook.example.com via the AdmissionRegistration API 03/01/23 11:45:41.13
    STEP: Creating a custom resource while v1 is storage version 03/01/23 11:45:41.148
    STEP: Patching Custom Resource Definition to set v2 as storage 03/01/23 11:45:43.241
    STEP: Patching the custom resource while v2 is storage version 03/01/23 11:45:43.258
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  1 11:45:43.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-7474" for this suite. 03/01/23 11:45:43.848
    STEP: Destroying namespace "webhook-7474-markers" for this suite. 03/01/23 11:45:43.865
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:45:43.944
Mar  1 11:45:43.944: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename kubelet-test 03/01/23 11:45:43.944
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:45:43.974
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:45:43.979
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 03/01/23 11:45:43.993
Mar  1 11:45:43.993: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases2f1c0a57-fd79-46a6-af01-94a00f811233" in namespace "kubelet-test-5666" to be "completed"
Mar  1 11:45:44.000: INFO: Pod "agnhost-host-aliases2f1c0a57-fd79-46a6-af01-94a00f811233": Phase="Pending", Reason="", readiness=false. Elapsed: 6.716413ms
Mar  1 11:45:46.006: INFO: Pod "agnhost-host-aliases2f1c0a57-fd79-46a6-af01-94a00f811233": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012603892s
Mar  1 11:45:48.005: INFO: Pod "agnhost-host-aliases2f1c0a57-fd79-46a6-af01-94a00f811233": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012337482s
Mar  1 11:45:48.005: INFO: Pod "agnhost-host-aliases2f1c0a57-fd79-46a6-af01-94a00f811233" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Mar  1 11:45:48.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5666" for this suite. 03/01/23 11:45:48.025
{"msg":"PASSED [sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]","completed":16,"skipped":341,"failed":0}
------------------------------
â€¢ [4.090 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:45:43.944
    Mar  1 11:45:43.944: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename kubelet-test 03/01/23 11:45:43.944
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:45:43.974
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:45:43.979
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 03/01/23 11:45:43.993
    Mar  1 11:45:43.993: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases2f1c0a57-fd79-46a6-af01-94a00f811233" in namespace "kubelet-test-5666" to be "completed"
    Mar  1 11:45:44.000: INFO: Pod "agnhost-host-aliases2f1c0a57-fd79-46a6-af01-94a00f811233": Phase="Pending", Reason="", readiness=false. Elapsed: 6.716413ms
    Mar  1 11:45:46.006: INFO: Pod "agnhost-host-aliases2f1c0a57-fd79-46a6-af01-94a00f811233": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012603892s
    Mar  1 11:45:48.005: INFO: Pod "agnhost-host-aliases2f1c0a57-fd79-46a6-af01-94a00f811233": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012337482s
    Mar  1 11:45:48.005: INFO: Pod "agnhost-host-aliases2f1c0a57-fd79-46a6-af01-94a00f811233" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Mar  1 11:45:48.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-5666" for this suite. 03/01/23 11:45:48.025
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:45:48.035
Mar  1 11:45:48.035: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename kubectl 03/01/23 11:45:48.036
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:45:48.06
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:45:48.063
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1698
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 03/01/23 11:45:48.066
Mar  1 11:45:48.067: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-7739 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2'
Mar  1 11:45:48.133: INFO: stderr: ""
Mar  1 11:45:48.133: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 03/01/23 11:45:48.133
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1702
Mar  1 11:45:48.138: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-7739 delete pods e2e-test-httpd-pod'
Mar  1 11:45:50.625: INFO: stderr: ""
Mar  1 11:45:50.625: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar  1 11:45:50.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7739" for this suite. 03/01/23 11:45:50.636
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","completed":17,"skipped":356,"failed":0}
------------------------------
â€¢ [2.612 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1695
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1711

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:45:48.035
    Mar  1 11:45:48.035: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename kubectl 03/01/23 11:45:48.036
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:45:48.06
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:45:48.063
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1698
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1711
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 03/01/23 11:45:48.066
    Mar  1 11:45:48.067: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-7739 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2'
    Mar  1 11:45:48.133: INFO: stderr: ""
    Mar  1 11:45:48.133: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 03/01/23 11:45:48.133
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1702
    Mar  1 11:45:48.138: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-7739 delete pods e2e-test-httpd-pod'
    Mar  1 11:45:50.625: INFO: stderr: ""
    Mar  1 11:45:50.625: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar  1 11:45:50.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-7739" for this suite. 03/01/23 11:45:50.636
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:45:50.647
Mar  1 11:45:50.647: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename runtimeclass 03/01/23 11:45:50.648
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:45:50.684
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:45:50.687
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-4633-delete-me 03/01/23 11:45:50.699
STEP: Waiting for the RuntimeClass to disappear 03/01/23 11:45:50.708
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Mar  1 11:45:50.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-4633" for this suite. 03/01/23 11:45:50.728
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]","completed":18,"skipped":362,"failed":0}
------------------------------
â€¢ [0.090 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:45:50.647
    Mar  1 11:45:50.647: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename runtimeclass 03/01/23 11:45:50.648
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:45:50.684
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:45:50.687
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-4633-delete-me 03/01/23 11:45:50.699
    STEP: Waiting for the RuntimeClass to disappear 03/01/23 11:45:50.708
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Mar  1 11:45:50.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-4633" for this suite. 03/01/23 11:45:50.728
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:45:50.74
Mar  1 11:45:50.740: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename var-expansion 03/01/23 11:45:50.74
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:45:50.763
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:45:50.767
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
STEP: Creating a pod to test substitution in volume subpath 03/01/23 11:45:50.77
Mar  1 11:45:50.784: INFO: Waiting up to 5m0s for pod "var-expansion-afa6765a-1859-4820-af41-ddbad3f32d03" in namespace "var-expansion-777" to be "Succeeded or Failed"
Mar  1 11:45:50.789: INFO: Pod "var-expansion-afa6765a-1859-4820-af41-ddbad3f32d03": Phase="Pending", Reason="", readiness=false. Elapsed: 5.044486ms
Mar  1 11:45:52.796: INFO: Pod "var-expansion-afa6765a-1859-4820-af41-ddbad3f32d03": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012285602s
Mar  1 11:45:54.794: INFO: Pod "var-expansion-afa6765a-1859-4820-af41-ddbad3f32d03": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010148728s
STEP: Saw pod success 03/01/23 11:45:54.794
Mar  1 11:45:54.794: INFO: Pod "var-expansion-afa6765a-1859-4820-af41-ddbad3f32d03" satisfied condition "Succeeded or Failed"
Mar  1 11:45:54.798: INFO: Trying to get logs from node lab1-k8s-node-3 pod var-expansion-afa6765a-1859-4820-af41-ddbad3f32d03 container dapi-container: <nil>
STEP: delete the pod 03/01/23 11:45:54.808
Mar  1 11:45:54.823: INFO: Waiting for pod var-expansion-afa6765a-1859-4820-af41-ddbad3f32d03 to disappear
Mar  1 11:45:54.832: INFO: Pod var-expansion-afa6765a-1859-4820-af41-ddbad3f32d03 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Mar  1 11:45:54.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-777" for this suite. 03/01/23 11:45:54.841
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","completed":19,"skipped":397,"failed":0}
------------------------------
â€¢ [4.112 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:45:50.74
    Mar  1 11:45:50.740: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename var-expansion 03/01/23 11:45:50.74
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:45:50.763
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:45:50.767
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:111
    STEP: Creating a pod to test substitution in volume subpath 03/01/23 11:45:50.77
    Mar  1 11:45:50.784: INFO: Waiting up to 5m0s for pod "var-expansion-afa6765a-1859-4820-af41-ddbad3f32d03" in namespace "var-expansion-777" to be "Succeeded or Failed"
    Mar  1 11:45:50.789: INFO: Pod "var-expansion-afa6765a-1859-4820-af41-ddbad3f32d03": Phase="Pending", Reason="", readiness=false. Elapsed: 5.044486ms
    Mar  1 11:45:52.796: INFO: Pod "var-expansion-afa6765a-1859-4820-af41-ddbad3f32d03": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012285602s
    Mar  1 11:45:54.794: INFO: Pod "var-expansion-afa6765a-1859-4820-af41-ddbad3f32d03": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010148728s
    STEP: Saw pod success 03/01/23 11:45:54.794
    Mar  1 11:45:54.794: INFO: Pod "var-expansion-afa6765a-1859-4820-af41-ddbad3f32d03" satisfied condition "Succeeded or Failed"
    Mar  1 11:45:54.798: INFO: Trying to get logs from node lab1-k8s-node-3 pod var-expansion-afa6765a-1859-4820-af41-ddbad3f32d03 container dapi-container: <nil>
    STEP: delete the pod 03/01/23 11:45:54.808
    Mar  1 11:45:54.823: INFO: Waiting for pod var-expansion-afa6765a-1859-4820-af41-ddbad3f32d03 to disappear
    Mar  1 11:45:54.832: INFO: Pod var-expansion-afa6765a-1859-4820-af41-ddbad3f32d03 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Mar  1 11:45:54.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-777" for this suite. 03/01/23 11:45:54.841
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:45:54.854
Mar  1 11:45:54.855: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename var-expansion 03/01/23 11:45:54.855
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:45:54.878
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:45:54.882
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
Mar  1 11:45:54.898: INFO: Waiting up to 2m0s for pod "var-expansion-2680ed8e-9b57-412b-8646-b988acc28d15" in namespace "var-expansion-993" to be "container 0 failed with reason CreateContainerConfigError"
Mar  1 11:45:54.913: INFO: Pod "var-expansion-2680ed8e-9b57-412b-8646-b988acc28d15": Phase="Pending", Reason="", readiness=false. Elapsed: 14.997221ms
Mar  1 11:45:56.922: INFO: Pod "var-expansion-2680ed8e-9b57-412b-8646-b988acc28d15": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023357005s
Mar  1 11:45:56.922: INFO: Pod "var-expansion-2680ed8e-9b57-412b-8646-b988acc28d15" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Mar  1 11:45:56.922: INFO: Deleting pod "var-expansion-2680ed8e-9b57-412b-8646-b988acc28d15" in namespace "var-expansion-993"
Mar  1 11:45:56.932: INFO: Wait up to 5m0s for pod "var-expansion-2680ed8e-9b57-412b-8646-b988acc28d15" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Mar  1 11:45:58.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-993" for this suite. 03/01/23 11:45:58.949
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","completed":20,"skipped":423,"failed":0}
------------------------------
â€¢ [4.103 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:45:54.854
    Mar  1 11:45:54.855: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename var-expansion 03/01/23 11:45:54.855
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:45:54.878
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:45:54.882
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:151
    Mar  1 11:45:54.898: INFO: Waiting up to 2m0s for pod "var-expansion-2680ed8e-9b57-412b-8646-b988acc28d15" in namespace "var-expansion-993" to be "container 0 failed with reason CreateContainerConfigError"
    Mar  1 11:45:54.913: INFO: Pod "var-expansion-2680ed8e-9b57-412b-8646-b988acc28d15": Phase="Pending", Reason="", readiness=false. Elapsed: 14.997221ms
    Mar  1 11:45:56.922: INFO: Pod "var-expansion-2680ed8e-9b57-412b-8646-b988acc28d15": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023357005s
    Mar  1 11:45:56.922: INFO: Pod "var-expansion-2680ed8e-9b57-412b-8646-b988acc28d15" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Mar  1 11:45:56.922: INFO: Deleting pod "var-expansion-2680ed8e-9b57-412b-8646-b988acc28d15" in namespace "var-expansion-993"
    Mar  1 11:45:56.932: INFO: Wait up to 5m0s for pod "var-expansion-2680ed8e-9b57-412b-8646-b988acc28d15" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Mar  1 11:45:58.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-993" for this suite. 03/01/23 11:45:58.949
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:45:58.958
Mar  1 11:45:58.958: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename resourcequota 03/01/23 11:45:58.959
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:45:58.982
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:45:58.986
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
STEP: Counting existing ResourceQuota 03/01/23 11:45:58.989
STEP: Creating a ResourceQuota 03/01/23 11:46:03.994
STEP: Ensuring resource quota status is calculated 03/01/23 11:46:04.001
STEP: Creating a Pod that fits quota 03/01/23 11:46:06.007
STEP: Ensuring ResourceQuota status captures the pod usage 03/01/23 11:46:06.027
STEP: Not allowing a pod to be created that exceeds remaining quota 03/01/23 11:46:08.033
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 03/01/23 11:46:08.036
STEP: Ensuring a pod cannot update its resource requirements 03/01/23 11:46:08.039
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 03/01/23 11:46:08.045
STEP: Deleting the pod 03/01/23 11:46:10.051
STEP: Ensuring resource quota status released the pod usage 03/01/23 11:46:10.066
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Mar  1 11:46:12.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-877" for this suite. 03/01/23 11:46:12.081
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","completed":21,"skipped":438,"failed":0}
------------------------------
â€¢ [SLOW TEST] [13.132 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:45:58.958
    Mar  1 11:45:58.958: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename resourcequota 03/01/23 11:45:58.959
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:45:58.982
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:45:58.986
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:220
    STEP: Counting existing ResourceQuota 03/01/23 11:45:58.989
    STEP: Creating a ResourceQuota 03/01/23 11:46:03.994
    STEP: Ensuring resource quota status is calculated 03/01/23 11:46:04.001
    STEP: Creating a Pod that fits quota 03/01/23 11:46:06.007
    STEP: Ensuring ResourceQuota status captures the pod usage 03/01/23 11:46:06.027
    STEP: Not allowing a pod to be created that exceeds remaining quota 03/01/23 11:46:08.033
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 03/01/23 11:46:08.036
    STEP: Ensuring a pod cannot update its resource requirements 03/01/23 11:46:08.039
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 03/01/23 11:46:08.045
    STEP: Deleting the pod 03/01/23 11:46:10.051
    STEP: Ensuring resource quota status released the pod usage 03/01/23 11:46:10.066
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Mar  1 11:46:12.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-877" for this suite. 03/01/23 11:46:12.081
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:46:12.092
Mar  1 11:46:12.092: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename kubectl 03/01/23 11:46:12.093
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:46:12.122
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:46:12.125
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
STEP: creating Agnhost RC 03/01/23 11:46:12.129
Mar  1 11:46:12.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-300 create -f -'
Mar  1 11:46:12.924: INFO: stderr: ""
Mar  1 11:46:12.924: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 03/01/23 11:46:12.924
Mar  1 11:46:13.929: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  1 11:46:13.929: INFO: Found 0 / 1
Mar  1 11:46:14.930: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  1 11:46:14.930: INFO: Found 1 / 1
Mar  1 11:46:14.930: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 03/01/23 11:46:14.93
Mar  1 11:46:14.936: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  1 11:46:14.936: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar  1 11:46:14.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-300 patch pod agnhost-primary-982s6 -p {"metadata":{"annotations":{"x":"y"}}}'
Mar  1 11:46:15.014: INFO: stderr: ""
Mar  1 11:46:15.014: INFO: stdout: "pod/agnhost-primary-982s6 patched\n"
STEP: checking annotations 03/01/23 11:46:15.014
Mar  1 11:46:15.019: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  1 11:46:15.019: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar  1 11:46:15.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-300" for this suite. 03/01/23 11:46:15.027
{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","completed":22,"skipped":460,"failed":0}
------------------------------
â€¢ [2.944 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1644
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:46:12.092
    Mar  1 11:46:12.092: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename kubectl 03/01/23 11:46:12.093
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:46:12.122
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:46:12.125
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1650
    STEP: creating Agnhost RC 03/01/23 11:46:12.129
    Mar  1 11:46:12.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-300 create -f -'
    Mar  1 11:46:12.924: INFO: stderr: ""
    Mar  1 11:46:12.924: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 03/01/23 11:46:12.924
    Mar  1 11:46:13.929: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar  1 11:46:13.929: INFO: Found 0 / 1
    Mar  1 11:46:14.930: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar  1 11:46:14.930: INFO: Found 1 / 1
    Mar  1 11:46:14.930: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 03/01/23 11:46:14.93
    Mar  1 11:46:14.936: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar  1 11:46:14.936: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Mar  1 11:46:14.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-300 patch pod agnhost-primary-982s6 -p {"metadata":{"annotations":{"x":"y"}}}'
    Mar  1 11:46:15.014: INFO: stderr: ""
    Mar  1 11:46:15.014: INFO: stdout: "pod/agnhost-primary-982s6 patched\n"
    STEP: checking annotations 03/01/23 11:46:15.014
    Mar  1 11:46:15.019: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar  1 11:46:15.019: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar  1 11:46:15.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-300" for this suite. 03/01/23 11:46:15.027
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:46:15.037
Mar  1 11:46:15.037: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename replication-controller 03/01/23 11:46:15.037
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:46:15.059
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:46:15.062
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
Mar  1 11:46:15.066: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 03/01/23 11:46:15.078
STEP: Checking rc "condition-test" has the desired failure condition set 03/01/23 11:46:15.085
STEP: Scaling down rc "condition-test" to satisfy pod quota 03/01/23 11:46:16.095
Mar  1 11:46:16.110: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 03/01/23 11:46:16.111
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Mar  1 11:46:17.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-333" for this suite. 03/01/23 11:46:17.13
{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","completed":23,"skipped":473,"failed":0}
------------------------------
â€¢ [2.103 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:46:15.037
    Mar  1 11:46:15.037: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename replication-controller 03/01/23 11:46:15.037
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:46:15.059
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:46:15.062
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:82
    Mar  1 11:46:15.066: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 03/01/23 11:46:15.078
    STEP: Checking rc "condition-test" has the desired failure condition set 03/01/23 11:46:15.085
    STEP: Scaling down rc "condition-test" to satisfy pod quota 03/01/23 11:46:16.095
    Mar  1 11:46:16.110: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 03/01/23 11:46:16.111
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Mar  1 11:46:17.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-333" for this suite. 03/01/23 11:46:17.13
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:46:17.14
Mar  1 11:46:17.141: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename emptydir 03/01/23 11:46:17.141
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:46:17.162
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:46:17.165
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
STEP: Creating a pod to test emptydir 0644 on tmpfs 03/01/23 11:46:17.169
Mar  1 11:46:17.179: INFO: Waiting up to 5m0s for pod "pod-2de9360d-d323-48e8-9b3c-32505991173a" in namespace "emptydir-7412" to be "Succeeded or Failed"
Mar  1 11:46:17.189: INFO: Pod "pod-2de9360d-d323-48e8-9b3c-32505991173a": Phase="Pending", Reason="", readiness=false. Elapsed: 9.891108ms
Mar  1 11:46:19.201: INFO: Pod "pod-2de9360d-d323-48e8-9b3c-32505991173a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022662971s
Mar  1 11:46:21.195: INFO: Pod "pod-2de9360d-d323-48e8-9b3c-32505991173a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016451031s
STEP: Saw pod success 03/01/23 11:46:21.195
Mar  1 11:46:21.195: INFO: Pod "pod-2de9360d-d323-48e8-9b3c-32505991173a" satisfied condition "Succeeded or Failed"
Mar  1 11:46:21.200: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-2de9360d-d323-48e8-9b3c-32505991173a container test-container: <nil>
STEP: delete the pod 03/01/23 11:46:21.209
Mar  1 11:46:21.230: INFO: Waiting for pod pod-2de9360d-d323-48e8-9b3c-32505991173a to disappear
Mar  1 11:46:21.235: INFO: Pod pod-2de9360d-d323-48e8-9b3c-32505991173a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar  1 11:46:21.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7412" for this suite. 03/01/23 11:46:21.244
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":24,"skipped":481,"failed":0}
------------------------------
â€¢ [4.111 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:46:17.14
    Mar  1 11:46:17.141: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename emptydir 03/01/23 11:46:17.141
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:46:17.162
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:46:17.165
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:126
    STEP: Creating a pod to test emptydir 0644 on tmpfs 03/01/23 11:46:17.169
    Mar  1 11:46:17.179: INFO: Waiting up to 5m0s for pod "pod-2de9360d-d323-48e8-9b3c-32505991173a" in namespace "emptydir-7412" to be "Succeeded or Failed"
    Mar  1 11:46:17.189: INFO: Pod "pod-2de9360d-d323-48e8-9b3c-32505991173a": Phase="Pending", Reason="", readiness=false. Elapsed: 9.891108ms
    Mar  1 11:46:19.201: INFO: Pod "pod-2de9360d-d323-48e8-9b3c-32505991173a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022662971s
    Mar  1 11:46:21.195: INFO: Pod "pod-2de9360d-d323-48e8-9b3c-32505991173a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016451031s
    STEP: Saw pod success 03/01/23 11:46:21.195
    Mar  1 11:46:21.195: INFO: Pod "pod-2de9360d-d323-48e8-9b3c-32505991173a" satisfied condition "Succeeded or Failed"
    Mar  1 11:46:21.200: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-2de9360d-d323-48e8-9b3c-32505991173a container test-container: <nil>
    STEP: delete the pod 03/01/23 11:46:21.209
    Mar  1 11:46:21.230: INFO: Waiting for pod pod-2de9360d-d323-48e8-9b3c-32505991173a to disappear
    Mar  1 11:46:21.235: INFO: Pod pod-2de9360d-d323-48e8-9b3c-32505991173a no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar  1 11:46:21.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-7412" for this suite. 03/01/23 11:46:21.244
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:46:21.254
Mar  1 11:46:21.254: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename discovery 03/01/23 11:46:21.255
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:46:21.278
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:46:21.281
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 03/01/23 11:46:21.286
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Mar  1 11:46:21.478: INFO: Checking APIGroup: apiregistration.k8s.io
Mar  1 11:46:21.480: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Mar  1 11:46:21.480: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Mar  1 11:46:21.480: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Mar  1 11:46:21.480: INFO: Checking APIGroup: apps
Mar  1 11:46:21.481: INFO: PreferredVersion.GroupVersion: apps/v1
Mar  1 11:46:21.481: INFO: Versions found [{apps/v1 v1}]
Mar  1 11:46:21.481: INFO: apps/v1 matches apps/v1
Mar  1 11:46:21.481: INFO: Checking APIGroup: events.k8s.io
Mar  1 11:46:21.482: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Mar  1 11:46:21.483: INFO: Versions found [{events.k8s.io/v1 v1}]
Mar  1 11:46:21.483: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Mar  1 11:46:21.483: INFO: Checking APIGroup: authentication.k8s.io
Mar  1 11:46:21.484: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Mar  1 11:46:21.484: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Mar  1 11:46:21.484: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Mar  1 11:46:21.484: INFO: Checking APIGroup: authorization.k8s.io
Mar  1 11:46:21.485: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Mar  1 11:46:21.485: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Mar  1 11:46:21.485: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Mar  1 11:46:21.485: INFO: Checking APIGroup: autoscaling
Mar  1 11:46:21.486: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Mar  1 11:46:21.487: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
Mar  1 11:46:21.487: INFO: autoscaling/v2 matches autoscaling/v2
Mar  1 11:46:21.487: INFO: Checking APIGroup: batch
Mar  1 11:46:21.488: INFO: PreferredVersion.GroupVersion: batch/v1
Mar  1 11:46:21.488: INFO: Versions found [{batch/v1 v1}]
Mar  1 11:46:21.488: INFO: batch/v1 matches batch/v1
Mar  1 11:46:21.488: INFO: Checking APIGroup: certificates.k8s.io
Mar  1 11:46:21.489: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Mar  1 11:46:21.489: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Mar  1 11:46:21.489: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Mar  1 11:46:21.489: INFO: Checking APIGroup: networking.k8s.io
Mar  1 11:46:21.490: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Mar  1 11:46:21.490: INFO: Versions found [{networking.k8s.io/v1 v1}]
Mar  1 11:46:21.490: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Mar  1 11:46:21.490: INFO: Checking APIGroup: policy
Mar  1 11:46:21.492: INFO: PreferredVersion.GroupVersion: policy/v1
Mar  1 11:46:21.492: INFO: Versions found [{policy/v1 v1}]
Mar  1 11:46:21.492: INFO: policy/v1 matches policy/v1
Mar  1 11:46:21.492: INFO: Checking APIGroup: rbac.authorization.k8s.io
Mar  1 11:46:21.493: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Mar  1 11:46:21.493: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Mar  1 11:46:21.493: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Mar  1 11:46:21.493: INFO: Checking APIGroup: storage.k8s.io
Mar  1 11:46:21.494: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Mar  1 11:46:21.494: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Mar  1 11:46:21.494: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Mar  1 11:46:21.494: INFO: Checking APIGroup: admissionregistration.k8s.io
Mar  1 11:46:21.496: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Mar  1 11:46:21.496: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Mar  1 11:46:21.496: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Mar  1 11:46:21.496: INFO: Checking APIGroup: apiextensions.k8s.io
Mar  1 11:46:21.497: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Mar  1 11:46:21.497: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Mar  1 11:46:21.497: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Mar  1 11:46:21.497: INFO: Checking APIGroup: scheduling.k8s.io
Mar  1 11:46:21.498: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Mar  1 11:46:21.498: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Mar  1 11:46:21.498: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Mar  1 11:46:21.498: INFO: Checking APIGroup: coordination.k8s.io
Mar  1 11:46:21.499: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Mar  1 11:46:21.499: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Mar  1 11:46:21.499: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Mar  1 11:46:21.499: INFO: Checking APIGroup: node.k8s.io
Mar  1 11:46:21.501: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Mar  1 11:46:21.501: INFO: Versions found [{node.k8s.io/v1 v1}]
Mar  1 11:46:21.501: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Mar  1 11:46:21.501: INFO: Checking APIGroup: discovery.k8s.io
Mar  1 11:46:21.503: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Mar  1 11:46:21.503: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Mar  1 11:46:21.503: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Mar  1 11:46:21.503: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Mar  1 11:46:21.504: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
Mar  1 11:46:21.504: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Mar  1 11:46:21.504: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
Mar  1 11:46:21.504: INFO: Checking APIGroup: crd.projectcalico.org
Mar  1 11:46:21.506: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Mar  1 11:46:21.506: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Mar  1 11:46:21.506: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Mar  1 11:46:21.506: INFO: Checking APIGroup: snapshot.storage.k8s.io
Mar  1 11:46:21.507: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
Mar  1 11:46:21.507: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1} {snapshot.storage.k8s.io/v1beta1 v1beta1}]
Mar  1 11:46:21.507: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
Mar  1 11:46:21.507: INFO: Checking APIGroup: metrics.k8s.io
Mar  1 11:46:21.508: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Mar  1 11:46:21.509: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Mar  1 11:46:21.509: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:187
Mar  1 11:46:21.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-4894" for this suite. 03/01/23 11:46:21.515
{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","completed":25,"skipped":492,"failed":0}
------------------------------
â€¢ [0.268 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:46:21.254
    Mar  1 11:46:21.254: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename discovery 03/01/23 11:46:21.255
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:46:21.278
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:46:21.281
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 03/01/23 11:46:21.286
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Mar  1 11:46:21.478: INFO: Checking APIGroup: apiregistration.k8s.io
    Mar  1 11:46:21.480: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Mar  1 11:46:21.480: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Mar  1 11:46:21.480: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Mar  1 11:46:21.480: INFO: Checking APIGroup: apps
    Mar  1 11:46:21.481: INFO: PreferredVersion.GroupVersion: apps/v1
    Mar  1 11:46:21.481: INFO: Versions found [{apps/v1 v1}]
    Mar  1 11:46:21.481: INFO: apps/v1 matches apps/v1
    Mar  1 11:46:21.481: INFO: Checking APIGroup: events.k8s.io
    Mar  1 11:46:21.482: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Mar  1 11:46:21.483: INFO: Versions found [{events.k8s.io/v1 v1}]
    Mar  1 11:46:21.483: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Mar  1 11:46:21.483: INFO: Checking APIGroup: authentication.k8s.io
    Mar  1 11:46:21.484: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Mar  1 11:46:21.484: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Mar  1 11:46:21.484: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Mar  1 11:46:21.484: INFO: Checking APIGroup: authorization.k8s.io
    Mar  1 11:46:21.485: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Mar  1 11:46:21.485: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Mar  1 11:46:21.485: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Mar  1 11:46:21.485: INFO: Checking APIGroup: autoscaling
    Mar  1 11:46:21.486: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Mar  1 11:46:21.487: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
    Mar  1 11:46:21.487: INFO: autoscaling/v2 matches autoscaling/v2
    Mar  1 11:46:21.487: INFO: Checking APIGroup: batch
    Mar  1 11:46:21.488: INFO: PreferredVersion.GroupVersion: batch/v1
    Mar  1 11:46:21.488: INFO: Versions found [{batch/v1 v1}]
    Mar  1 11:46:21.488: INFO: batch/v1 matches batch/v1
    Mar  1 11:46:21.488: INFO: Checking APIGroup: certificates.k8s.io
    Mar  1 11:46:21.489: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Mar  1 11:46:21.489: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Mar  1 11:46:21.489: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Mar  1 11:46:21.489: INFO: Checking APIGroup: networking.k8s.io
    Mar  1 11:46:21.490: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Mar  1 11:46:21.490: INFO: Versions found [{networking.k8s.io/v1 v1}]
    Mar  1 11:46:21.490: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Mar  1 11:46:21.490: INFO: Checking APIGroup: policy
    Mar  1 11:46:21.492: INFO: PreferredVersion.GroupVersion: policy/v1
    Mar  1 11:46:21.492: INFO: Versions found [{policy/v1 v1}]
    Mar  1 11:46:21.492: INFO: policy/v1 matches policy/v1
    Mar  1 11:46:21.492: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Mar  1 11:46:21.493: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Mar  1 11:46:21.493: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Mar  1 11:46:21.493: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Mar  1 11:46:21.493: INFO: Checking APIGroup: storage.k8s.io
    Mar  1 11:46:21.494: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Mar  1 11:46:21.494: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Mar  1 11:46:21.494: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Mar  1 11:46:21.494: INFO: Checking APIGroup: admissionregistration.k8s.io
    Mar  1 11:46:21.496: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Mar  1 11:46:21.496: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Mar  1 11:46:21.496: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Mar  1 11:46:21.496: INFO: Checking APIGroup: apiextensions.k8s.io
    Mar  1 11:46:21.497: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Mar  1 11:46:21.497: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Mar  1 11:46:21.497: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Mar  1 11:46:21.497: INFO: Checking APIGroup: scheduling.k8s.io
    Mar  1 11:46:21.498: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Mar  1 11:46:21.498: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Mar  1 11:46:21.498: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Mar  1 11:46:21.498: INFO: Checking APIGroup: coordination.k8s.io
    Mar  1 11:46:21.499: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Mar  1 11:46:21.499: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Mar  1 11:46:21.499: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Mar  1 11:46:21.499: INFO: Checking APIGroup: node.k8s.io
    Mar  1 11:46:21.501: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Mar  1 11:46:21.501: INFO: Versions found [{node.k8s.io/v1 v1}]
    Mar  1 11:46:21.501: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Mar  1 11:46:21.501: INFO: Checking APIGroup: discovery.k8s.io
    Mar  1 11:46:21.503: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Mar  1 11:46:21.503: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Mar  1 11:46:21.503: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Mar  1 11:46:21.503: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Mar  1 11:46:21.504: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
    Mar  1 11:46:21.504: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
    Mar  1 11:46:21.504: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
    Mar  1 11:46:21.504: INFO: Checking APIGroup: crd.projectcalico.org
    Mar  1 11:46:21.506: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
    Mar  1 11:46:21.506: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
    Mar  1 11:46:21.506: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
    Mar  1 11:46:21.506: INFO: Checking APIGroup: snapshot.storage.k8s.io
    Mar  1 11:46:21.507: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
    Mar  1 11:46:21.507: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1} {snapshot.storage.k8s.io/v1beta1 v1beta1}]
    Mar  1 11:46:21.507: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
    Mar  1 11:46:21.507: INFO: Checking APIGroup: metrics.k8s.io
    Mar  1 11:46:21.508: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
    Mar  1 11:46:21.509: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
    Mar  1 11:46:21.509: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:187
    Mar  1 11:46:21.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "discovery-4894" for this suite. 03/01/23 11:46:21.515
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:46:21.523
Mar  1 11:46:21.523: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename watch 03/01/23 11:46:21.524
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:46:21.546
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:46:21.549
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 03/01/23 11:46:21.553
STEP: creating a new configmap 03/01/23 11:46:21.554
STEP: modifying the configmap once 03/01/23 11:46:21.561
STEP: changing the label value of the configmap 03/01/23 11:46:21.57
STEP: Expecting to observe a delete notification for the watched object 03/01/23 11:46:21.58
Mar  1 11:46:21.580: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7510  0261efa8-163c-417f-b7ad-d4592b7b2348 5541 0 2023-03-01 11:46:21 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-01 11:46:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  1 11:46:21.581: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7510  0261efa8-163c-417f-b7ad-d4592b7b2348 5542 0 2023-03-01 11:46:21 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-01 11:46:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  1 11:46:21.581: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7510  0261efa8-163c-417f-b7ad-d4592b7b2348 5543 0 2023-03-01 11:46:21 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-01 11:46:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 03/01/23 11:46:21.581
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 03/01/23 11:46:21.595
STEP: changing the label value of the configmap back 03/01/23 11:46:31.596
STEP: modifying the configmap a third time 03/01/23 11:46:31.609
STEP: deleting the configmap 03/01/23 11:46:31.619
STEP: Expecting to observe an add notification for the watched object when the label value was restored 03/01/23 11:46:31.627
Mar  1 11:46:31.627: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7510  0261efa8-163c-417f-b7ad-d4592b7b2348 5628 0 2023-03-01 11:46:21 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-01 11:46:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  1 11:46:31.627: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7510  0261efa8-163c-417f-b7ad-d4592b7b2348 5629 0 2023-03-01 11:46:21 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-01 11:46:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  1 11:46:31.628: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7510  0261efa8-163c-417f-b7ad-d4592b7b2348 5630 0 2023-03-01 11:46:21 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-01 11:46:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Mar  1 11:46:31.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7510" for this suite. 03/01/23 11:46:31.636
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","completed":26,"skipped":522,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.123 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:46:21.523
    Mar  1 11:46:21.523: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename watch 03/01/23 11:46:21.524
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:46:21.546
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:46:21.549
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 03/01/23 11:46:21.553
    STEP: creating a new configmap 03/01/23 11:46:21.554
    STEP: modifying the configmap once 03/01/23 11:46:21.561
    STEP: changing the label value of the configmap 03/01/23 11:46:21.57
    STEP: Expecting to observe a delete notification for the watched object 03/01/23 11:46:21.58
    Mar  1 11:46:21.580: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7510  0261efa8-163c-417f-b7ad-d4592b7b2348 5541 0 2023-03-01 11:46:21 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-01 11:46:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar  1 11:46:21.581: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7510  0261efa8-163c-417f-b7ad-d4592b7b2348 5542 0 2023-03-01 11:46:21 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-01 11:46:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar  1 11:46:21.581: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7510  0261efa8-163c-417f-b7ad-d4592b7b2348 5543 0 2023-03-01 11:46:21 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-01 11:46:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 03/01/23 11:46:21.581
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 03/01/23 11:46:21.595
    STEP: changing the label value of the configmap back 03/01/23 11:46:31.596
    STEP: modifying the configmap a third time 03/01/23 11:46:31.609
    STEP: deleting the configmap 03/01/23 11:46:31.619
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 03/01/23 11:46:31.627
    Mar  1 11:46:31.627: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7510  0261efa8-163c-417f-b7ad-d4592b7b2348 5628 0 2023-03-01 11:46:21 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-01 11:46:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar  1 11:46:31.627: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7510  0261efa8-163c-417f-b7ad-d4592b7b2348 5629 0 2023-03-01 11:46:21 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-01 11:46:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar  1 11:46:31.628: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7510  0261efa8-163c-417f-b7ad-d4592b7b2348 5630 0 2023-03-01 11:46:21 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-01 11:46:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Mar  1 11:46:31.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-7510" for this suite. 03/01/23 11:46:31.636
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:46:31.646
Mar  1 11:46:31.646: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename sched-preemption 03/01/23 11:46:31.647
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:46:31.669
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:46:31.673
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Mar  1 11:46:31.692: INFO: Waiting up to 1m0s for all nodes to be ready
Mar  1 11:47:31.742: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:47:31.747
Mar  1 11:47:31.748: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename sched-preemption-path 03/01/23 11:47:31.748
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:47:31.77
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:47:31.773
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:496
STEP: Finding an available node 03/01/23 11:47:31.776
STEP: Trying to launch a pod without a label to get a node which can launch it. 03/01/23 11:47:31.776
Mar  1 11:47:31.787: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-7714" to be "running"
Mar  1 11:47:31.792: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 5.161331ms
Mar  1 11:47:33.799: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.01149779s
Mar  1 11:47:33.799: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 03/01/23 11:47:33.804
Mar  1 11:47:33.822: INFO: found a healthy node: lab1-k8s-node-3
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
Mar  1 11:47:43.924: INFO: pods created so far: [1 1 1]
Mar  1 11:47:43.924: INFO: length of pods created so far: 3
Mar  1 11:47:45.938: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:187
Mar  1 11:47:52.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-7714" for this suite. 03/01/23 11:47:52.947
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:470
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Mar  1 11:47:53.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-6337" for this suite. 03/01/23 11:47:53.007
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","completed":27,"skipped":524,"failed":0}
------------------------------
â€¢ [SLOW TEST] [81.445 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:458
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:543

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:46:31.646
    Mar  1 11:46:31.646: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename sched-preemption 03/01/23 11:46:31.647
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:46:31.669
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:46:31.673
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Mar  1 11:46:31.692: INFO: Waiting up to 1m0s for all nodes to be ready
    Mar  1 11:47:31.742: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:47:31.747
    Mar  1 11:47:31.748: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename sched-preemption-path 03/01/23 11:47:31.748
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:47:31.77
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:47:31.773
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:496
    STEP: Finding an available node 03/01/23 11:47:31.776
    STEP: Trying to launch a pod without a label to get a node which can launch it. 03/01/23 11:47:31.776
    Mar  1 11:47:31.787: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-7714" to be "running"
    Mar  1 11:47:31.792: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 5.161331ms
    Mar  1 11:47:33.799: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.01149779s
    Mar  1 11:47:33.799: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 03/01/23 11:47:33.804
    Mar  1 11:47:33.822: INFO: found a healthy node: lab1-k8s-node-3
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:543
    Mar  1 11:47:43.924: INFO: pods created so far: [1 1 1]
    Mar  1 11:47:43.924: INFO: length of pods created so far: 3
    Mar  1 11:47:45.938: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:187
    Mar  1 11:47:52.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-7714" for this suite. 03/01/23 11:47:52.947
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:470
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Mar  1 11:47:53.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-6337" for this suite. 03/01/23 11:47:53.007
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:47:53.097
Mar  1 11:47:53.097: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename emptydir 03/01/23 11:47:53.098
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:47:53.122
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:47:53.125
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
STEP: Creating a pod to test emptydir 0644 on node default medium 03/01/23 11:47:53.129
Mar  1 11:47:53.139: INFO: Waiting up to 5m0s for pod "pod-2d5bd825-fa01-4c45-accb-2ba040a6c86f" in namespace "emptydir-5248" to be "Succeeded or Failed"
Mar  1 11:47:53.143: INFO: Pod "pod-2d5bd825-fa01-4c45-accb-2ba040a6c86f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.26766ms
Mar  1 11:47:55.152: INFO: Pod "pod-2d5bd825-fa01-4c45-accb-2ba040a6c86f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013131321s
Mar  1 11:47:57.150: INFO: Pod "pod-2d5bd825-fa01-4c45-accb-2ba040a6c86f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011038384s
STEP: Saw pod success 03/01/23 11:47:57.15
Mar  1 11:47:57.150: INFO: Pod "pod-2d5bd825-fa01-4c45-accb-2ba040a6c86f" satisfied condition "Succeeded or Failed"
Mar  1 11:47:57.155: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-2d5bd825-fa01-4c45-accb-2ba040a6c86f container test-container: <nil>
STEP: delete the pod 03/01/23 11:47:57.176
Mar  1 11:47:57.193: INFO: Waiting for pod pod-2d5bd825-fa01-4c45-accb-2ba040a6c86f to disappear
Mar  1 11:47:57.200: INFO: Pod pod-2d5bd825-fa01-4c45-accb-2ba040a6c86f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar  1 11:47:57.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5248" for this suite. 03/01/23 11:47:57.207
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":28,"skipped":554,"failed":0}
------------------------------
â€¢ [4.119 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:47:53.097
    Mar  1 11:47:53.097: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename emptydir 03/01/23 11:47:53.098
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:47:53.122
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:47:53.125
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:196
    STEP: Creating a pod to test emptydir 0644 on node default medium 03/01/23 11:47:53.129
    Mar  1 11:47:53.139: INFO: Waiting up to 5m0s for pod "pod-2d5bd825-fa01-4c45-accb-2ba040a6c86f" in namespace "emptydir-5248" to be "Succeeded or Failed"
    Mar  1 11:47:53.143: INFO: Pod "pod-2d5bd825-fa01-4c45-accb-2ba040a6c86f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.26766ms
    Mar  1 11:47:55.152: INFO: Pod "pod-2d5bd825-fa01-4c45-accb-2ba040a6c86f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013131321s
    Mar  1 11:47:57.150: INFO: Pod "pod-2d5bd825-fa01-4c45-accb-2ba040a6c86f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011038384s
    STEP: Saw pod success 03/01/23 11:47:57.15
    Mar  1 11:47:57.150: INFO: Pod "pod-2d5bd825-fa01-4c45-accb-2ba040a6c86f" satisfied condition "Succeeded or Failed"
    Mar  1 11:47:57.155: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-2d5bd825-fa01-4c45-accb-2ba040a6c86f container test-container: <nil>
    STEP: delete the pod 03/01/23 11:47:57.176
    Mar  1 11:47:57.193: INFO: Waiting for pod pod-2d5bd825-fa01-4c45-accb-2ba040a6c86f to disappear
    Mar  1 11:47:57.200: INFO: Pod pod-2d5bd825-fa01-4c45-accb-2ba040a6c86f no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar  1 11:47:57.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-5248" for this suite. 03/01/23 11:47:57.207
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:47:57.217
Mar  1 11:47:57.217: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename resourcequota 03/01/23 11:47:57.218
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:47:57.244
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:47:57.248
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
STEP: Counting existing ResourceQuota 03/01/23 11:47:57.252
STEP: Creating a ResourceQuota 03/01/23 11:48:02.257
STEP: Ensuring resource quota status is calculated 03/01/23 11:48:02.266
STEP: Creating a Service 03/01/23 11:48:04.271
STEP: Creating a NodePort Service 03/01/23 11:48:04.294
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 03/01/23 11:48:04.323
STEP: Ensuring resource quota status captures service creation 03/01/23 11:48:04.356
STEP: Deleting Services 03/01/23 11:48:06.364
STEP: Ensuring resource quota status released usage 03/01/23 11:48:06.412
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Mar  1 11:48:08.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9684" for this suite. 03/01/23 11:48:08.425
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","completed":29,"skipped":554,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.219 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:47:57.217
    Mar  1 11:47:57.217: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename resourcequota 03/01/23 11:47:57.218
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:47:57.244
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:47:57.248
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:90
    STEP: Counting existing ResourceQuota 03/01/23 11:47:57.252
    STEP: Creating a ResourceQuota 03/01/23 11:48:02.257
    STEP: Ensuring resource quota status is calculated 03/01/23 11:48:02.266
    STEP: Creating a Service 03/01/23 11:48:04.271
    STEP: Creating a NodePort Service 03/01/23 11:48:04.294
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 03/01/23 11:48:04.323
    STEP: Ensuring resource quota status captures service creation 03/01/23 11:48:04.356
    STEP: Deleting Services 03/01/23 11:48:06.364
    STEP: Ensuring resource quota status released usage 03/01/23 11:48:06.412
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Mar  1 11:48:08.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-9684" for this suite. 03/01/23 11:48:08.425
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:48:08.437
Mar  1 11:48:08.437: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename ingressclass 03/01/23 11:48:08.438
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:48:08.46
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:48:08.465
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 03/01/23 11:48:08.468
STEP: getting /apis/networking.k8s.io 03/01/23 11:48:08.471
STEP: getting /apis/networking.k8s.iov1 03/01/23 11:48:08.472
STEP: creating 03/01/23 11:48:08.474
STEP: getting 03/01/23 11:48:08.495
STEP: listing 03/01/23 11:48:08.499
STEP: watching 03/01/23 11:48:08.505
Mar  1 11:48:08.505: INFO: starting watch
STEP: patching 03/01/23 11:48:08.506
STEP: updating 03/01/23 11:48:08.514
Mar  1 11:48:08.521: INFO: waiting for watch events with expected annotations
Mar  1 11:48:08.521: INFO: saw patched and updated annotations
STEP: deleting 03/01/23 11:48:08.521
STEP: deleting a collection 03/01/23 11:48:08.545
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:187
Mar  1 11:48:08.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-737" for this suite. 03/01/23 11:48:08.575
{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","completed":30,"skipped":565,"failed":0}
------------------------------
â€¢ [0.147 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:48:08.437
    Mar  1 11:48:08.437: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename ingressclass 03/01/23 11:48:08.438
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:48:08.46
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:48:08.465
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 03/01/23 11:48:08.468
    STEP: getting /apis/networking.k8s.io 03/01/23 11:48:08.471
    STEP: getting /apis/networking.k8s.iov1 03/01/23 11:48:08.472
    STEP: creating 03/01/23 11:48:08.474
    STEP: getting 03/01/23 11:48:08.495
    STEP: listing 03/01/23 11:48:08.499
    STEP: watching 03/01/23 11:48:08.505
    Mar  1 11:48:08.505: INFO: starting watch
    STEP: patching 03/01/23 11:48:08.506
    STEP: updating 03/01/23 11:48:08.514
    Mar  1 11:48:08.521: INFO: waiting for watch events with expected annotations
    Mar  1 11:48:08.521: INFO: saw patched and updated annotations
    STEP: deleting 03/01/23 11:48:08.521
    STEP: deleting a collection 03/01/23 11:48:08.545
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:187
    Mar  1 11:48:08.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingressclass-737" for this suite. 03/01/23 11:48:08.575
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:48:08.586
Mar  1 11:48:08.586: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename events 03/01/23 11:48:08.586
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:48:08.608
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:48:08.612
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 03/01/23 11:48:08.615
STEP: listing events in all namespaces 03/01/23 11:48:08.623
STEP: listing events in test namespace 03/01/23 11:48:08.646
STEP: listing events with field selection filtering on source 03/01/23 11:48:08.651
STEP: listing events with field selection filtering on reportingController 03/01/23 11:48:08.656
STEP: getting the test event 03/01/23 11:48:08.666
STEP: patching the test event 03/01/23 11:48:08.67
STEP: getting the test event 03/01/23 11:48:08.688
STEP: updating the test event 03/01/23 11:48:08.693
STEP: getting the test event 03/01/23 11:48:08.702
STEP: deleting the test event 03/01/23 11:48:08.706
STEP: listing events in all namespaces 03/01/23 11:48:08.718
STEP: listing events in test namespace 03/01/23 11:48:08.735
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
Mar  1 11:48:08.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6572" for this suite. 03/01/23 11:48:08.747
{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","completed":31,"skipped":566,"failed":0}
------------------------------
â€¢ [0.171 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:48:08.586
    Mar  1 11:48:08.586: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename events 03/01/23 11:48:08.586
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:48:08.608
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:48:08.612
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 03/01/23 11:48:08.615
    STEP: listing events in all namespaces 03/01/23 11:48:08.623
    STEP: listing events in test namespace 03/01/23 11:48:08.646
    STEP: listing events with field selection filtering on source 03/01/23 11:48:08.651
    STEP: listing events with field selection filtering on reportingController 03/01/23 11:48:08.656
    STEP: getting the test event 03/01/23 11:48:08.666
    STEP: patching the test event 03/01/23 11:48:08.67
    STEP: getting the test event 03/01/23 11:48:08.688
    STEP: updating the test event 03/01/23 11:48:08.693
    STEP: getting the test event 03/01/23 11:48:08.702
    STEP: deleting the test event 03/01/23 11:48:08.706
    STEP: listing events in all namespaces 03/01/23 11:48:08.718
    STEP: listing events in test namespace 03/01/23 11:48:08.735
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    Mar  1 11:48:08.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-6572" for this suite. 03/01/23 11:48:08.747
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:48:08.76
Mar  1 11:48:08.760: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename container-probe 03/01/23 11:48:08.76
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:48:08.78
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:48:08.784
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
STEP: Creating pod busybox-713be7d8-ba48-4d55-93e3-5aee54a278a4 in namespace container-probe-5540 03/01/23 11:48:08.787
Mar  1 11:48:08.796: INFO: Waiting up to 5m0s for pod "busybox-713be7d8-ba48-4d55-93e3-5aee54a278a4" in namespace "container-probe-5540" to be "not pending"
Mar  1 11:48:08.801: INFO: Pod "busybox-713be7d8-ba48-4d55-93e3-5aee54a278a4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.825412ms
Mar  1 11:48:10.807: INFO: Pod "busybox-713be7d8-ba48-4d55-93e3-5aee54a278a4": Phase="Running", Reason="", readiness=true. Elapsed: 2.010609425s
Mar  1 11:48:10.807: INFO: Pod "busybox-713be7d8-ba48-4d55-93e3-5aee54a278a4" satisfied condition "not pending"
Mar  1 11:48:10.807: INFO: Started pod busybox-713be7d8-ba48-4d55-93e3-5aee54a278a4 in namespace container-probe-5540
STEP: checking the pod's current state and verifying that restartCount is present 03/01/23 11:48:10.807
Mar  1 11:48:10.812: INFO: Initial restart count of pod busybox-713be7d8-ba48-4d55-93e3-5aee54a278a4 is 0
Mar  1 11:49:00.971: INFO: Restart count of pod container-probe-5540/busybox-713be7d8-ba48-4d55-93e3-5aee54a278a4 is now 1 (50.158471739s elapsed)
STEP: deleting the pod 03/01/23 11:49:00.971
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Mar  1 11:49:00.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5540" for this suite. 03/01/23 11:49:00.995
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":32,"skipped":585,"failed":0}
------------------------------
â€¢ [SLOW TEST] [52.247 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:48:08.76
    Mar  1 11:48:08.760: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename container-probe 03/01/23 11:48:08.76
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:48:08.78
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:48:08.784
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:131
    STEP: Creating pod busybox-713be7d8-ba48-4d55-93e3-5aee54a278a4 in namespace container-probe-5540 03/01/23 11:48:08.787
    Mar  1 11:48:08.796: INFO: Waiting up to 5m0s for pod "busybox-713be7d8-ba48-4d55-93e3-5aee54a278a4" in namespace "container-probe-5540" to be "not pending"
    Mar  1 11:48:08.801: INFO: Pod "busybox-713be7d8-ba48-4d55-93e3-5aee54a278a4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.825412ms
    Mar  1 11:48:10.807: INFO: Pod "busybox-713be7d8-ba48-4d55-93e3-5aee54a278a4": Phase="Running", Reason="", readiness=true. Elapsed: 2.010609425s
    Mar  1 11:48:10.807: INFO: Pod "busybox-713be7d8-ba48-4d55-93e3-5aee54a278a4" satisfied condition "not pending"
    Mar  1 11:48:10.807: INFO: Started pod busybox-713be7d8-ba48-4d55-93e3-5aee54a278a4 in namespace container-probe-5540
    STEP: checking the pod's current state and verifying that restartCount is present 03/01/23 11:48:10.807
    Mar  1 11:48:10.812: INFO: Initial restart count of pod busybox-713be7d8-ba48-4d55-93e3-5aee54a278a4 is 0
    Mar  1 11:49:00.971: INFO: Restart count of pod container-probe-5540/busybox-713be7d8-ba48-4d55-93e3-5aee54a278a4 is now 1 (50.158471739s elapsed)
    STEP: deleting the pod 03/01/23 11:49:00.971
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Mar  1 11:49:00.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-5540" for this suite. 03/01/23 11:49:00.995
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:49:01.01
Mar  1 11:49:01.011: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename statefulset 03/01/23 11:49:01.011
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:49:01.038
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:49:01.042
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-7902 03/01/23 11:49:01.046
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
STEP: Looking for a node to schedule stateful set and pod 03/01/23 11:49:01.054
STEP: Creating pod with conflicting port in namespace statefulset-7902 03/01/23 11:49:01.062
STEP: Waiting until pod test-pod will start running in namespace statefulset-7902 03/01/23 11:49:01.076
Mar  1 11:49:01.076: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-7902" to be "running"
Mar  1 11:49:01.081: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.650787ms
Mar  1 11:49:03.088: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.012113791s
Mar  1 11:49:03.088: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-7902 03/01/23 11:49:03.088
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-7902 03/01/23 11:49:03.096
Mar  1 11:49:03.125: INFO: Observed stateful pod in namespace: statefulset-7902, name: ss-0, uid: ab8595c5-b05a-4bc1-8740-1e3ae58225b1, status phase: Pending. Waiting for statefulset controller to delete.
Mar  1 11:49:03.149: INFO: Observed stateful pod in namespace: statefulset-7902, name: ss-0, uid: ab8595c5-b05a-4bc1-8740-1e3ae58225b1, status phase: Failed. Waiting for statefulset controller to delete.
Mar  1 11:49:03.163: INFO: Observed stateful pod in namespace: statefulset-7902, name: ss-0, uid: ab8595c5-b05a-4bc1-8740-1e3ae58225b1, status phase: Failed. Waiting for statefulset controller to delete.
Mar  1 11:49:03.169: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-7902
STEP: Removing pod with conflicting port in namespace statefulset-7902 03/01/23 11:49:03.169
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-7902 and will be in running state 03/01/23 11:49:03.192
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Mar  1 11:49:05.220: INFO: Deleting all statefulset in ns statefulset-7902
Mar  1 11:49:05.225: INFO: Scaling statefulset ss to 0
Mar  1 11:49:15.256: INFO: Waiting for statefulset status.replicas updated to 0
Mar  1 11:49:15.265: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Mar  1 11:49:15.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7902" for this suite. 03/01/23 11:49:15.305
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","completed":33,"skipped":634,"failed":0}
------------------------------
â€¢ [SLOW TEST] [14.304 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:737

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:49:01.01
    Mar  1 11:49:01.011: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename statefulset 03/01/23 11:49:01.011
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:49:01.038
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:49:01.042
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-7902 03/01/23 11:49:01.046
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:737
    STEP: Looking for a node to schedule stateful set and pod 03/01/23 11:49:01.054
    STEP: Creating pod with conflicting port in namespace statefulset-7902 03/01/23 11:49:01.062
    STEP: Waiting until pod test-pod will start running in namespace statefulset-7902 03/01/23 11:49:01.076
    Mar  1 11:49:01.076: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-7902" to be "running"
    Mar  1 11:49:01.081: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.650787ms
    Mar  1 11:49:03.088: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.012113791s
    Mar  1 11:49:03.088: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-7902 03/01/23 11:49:03.088
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-7902 03/01/23 11:49:03.096
    Mar  1 11:49:03.125: INFO: Observed stateful pod in namespace: statefulset-7902, name: ss-0, uid: ab8595c5-b05a-4bc1-8740-1e3ae58225b1, status phase: Pending. Waiting for statefulset controller to delete.
    Mar  1 11:49:03.149: INFO: Observed stateful pod in namespace: statefulset-7902, name: ss-0, uid: ab8595c5-b05a-4bc1-8740-1e3ae58225b1, status phase: Failed. Waiting for statefulset controller to delete.
    Mar  1 11:49:03.163: INFO: Observed stateful pod in namespace: statefulset-7902, name: ss-0, uid: ab8595c5-b05a-4bc1-8740-1e3ae58225b1, status phase: Failed. Waiting for statefulset controller to delete.
    Mar  1 11:49:03.169: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-7902
    STEP: Removing pod with conflicting port in namespace statefulset-7902 03/01/23 11:49:03.169
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-7902 and will be in running state 03/01/23 11:49:03.192
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Mar  1 11:49:05.220: INFO: Deleting all statefulset in ns statefulset-7902
    Mar  1 11:49:05.225: INFO: Scaling statefulset ss to 0
    Mar  1 11:49:15.256: INFO: Waiting for statefulset status.replicas updated to 0
    Mar  1 11:49:15.265: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Mar  1 11:49:15.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-7902" for this suite. 03/01/23 11:49:15.305
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:49:15.317
Mar  1 11:49:15.318: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename var-expansion 03/01/23 11:49:15.318
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:49:15.341
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:49:15.345
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
STEP: Creating a pod to test substitution in container's command 03/01/23 11:49:15.348
Mar  1 11:49:15.358: INFO: Waiting up to 5m0s for pod "var-expansion-a3b7699e-e02d-4ead-92c7-9861caf63467" in namespace "var-expansion-7915" to be "Succeeded or Failed"
Mar  1 11:49:15.362: INFO: Pod "var-expansion-a3b7699e-e02d-4ead-92c7-9861caf63467": Phase="Pending", Reason="", readiness=false. Elapsed: 4.410208ms
Mar  1 11:49:17.372: INFO: Pod "var-expansion-a3b7699e-e02d-4ead-92c7-9861caf63467": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014236267s
Mar  1 11:49:19.370: INFO: Pod "var-expansion-a3b7699e-e02d-4ead-92c7-9861caf63467": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01236531s
STEP: Saw pod success 03/01/23 11:49:19.37
Mar  1 11:49:19.371: INFO: Pod "var-expansion-a3b7699e-e02d-4ead-92c7-9861caf63467" satisfied condition "Succeeded or Failed"
Mar  1 11:49:19.376: INFO: Trying to get logs from node lab1-k8s-node-3 pod var-expansion-a3b7699e-e02d-4ead-92c7-9861caf63467 container dapi-container: <nil>
STEP: delete the pod 03/01/23 11:49:19.393
Mar  1 11:49:19.409: INFO: Waiting for pod var-expansion-a3b7699e-e02d-4ead-92c7-9861caf63467 to disappear
Mar  1 11:49:19.413: INFO: Pod var-expansion-a3b7699e-e02d-4ead-92c7-9861caf63467 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Mar  1 11:49:19.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7915" for this suite. 03/01/23 11:49:19.42
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","completed":34,"skipped":644,"failed":0}
------------------------------
â€¢ [4.113 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:49:15.317
    Mar  1 11:49:15.318: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename var-expansion 03/01/23 11:49:15.318
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:49:15.341
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:49:15.345
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:72
    STEP: Creating a pod to test substitution in container's command 03/01/23 11:49:15.348
    Mar  1 11:49:15.358: INFO: Waiting up to 5m0s for pod "var-expansion-a3b7699e-e02d-4ead-92c7-9861caf63467" in namespace "var-expansion-7915" to be "Succeeded or Failed"
    Mar  1 11:49:15.362: INFO: Pod "var-expansion-a3b7699e-e02d-4ead-92c7-9861caf63467": Phase="Pending", Reason="", readiness=false. Elapsed: 4.410208ms
    Mar  1 11:49:17.372: INFO: Pod "var-expansion-a3b7699e-e02d-4ead-92c7-9861caf63467": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014236267s
    Mar  1 11:49:19.370: INFO: Pod "var-expansion-a3b7699e-e02d-4ead-92c7-9861caf63467": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01236531s
    STEP: Saw pod success 03/01/23 11:49:19.37
    Mar  1 11:49:19.371: INFO: Pod "var-expansion-a3b7699e-e02d-4ead-92c7-9861caf63467" satisfied condition "Succeeded or Failed"
    Mar  1 11:49:19.376: INFO: Trying to get logs from node lab1-k8s-node-3 pod var-expansion-a3b7699e-e02d-4ead-92c7-9861caf63467 container dapi-container: <nil>
    STEP: delete the pod 03/01/23 11:49:19.393
    Mar  1 11:49:19.409: INFO: Waiting for pod var-expansion-a3b7699e-e02d-4ead-92c7-9861caf63467 to disappear
    Mar  1 11:49:19.413: INFO: Pod var-expansion-a3b7699e-e02d-4ead-92c7-9861caf63467 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Mar  1 11:49:19.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-7915" for this suite. 03/01/23 11:49:19.42
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:49:19.43
Mar  1 11:49:19.431: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename emptydir-wrapper 03/01/23 11:49:19.432
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:49:19.453
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:49:19.456
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Mar  1 11:49:19.486: INFO: Waiting up to 5m0s for pod "pod-secrets-9dd655cb-8c2d-467c-989f-bf6cc7eab458" in namespace "emptydir-wrapper-2293" to be "running and ready"
Mar  1 11:49:19.490: INFO: Pod "pod-secrets-9dd655cb-8c2d-467c-989f-bf6cc7eab458": Phase="Pending", Reason="", readiness=false. Elapsed: 4.36312ms
Mar  1 11:49:19.490: INFO: The phase of Pod pod-secrets-9dd655cb-8c2d-467c-989f-bf6cc7eab458 is Pending, waiting for it to be Running (with Ready = true)
Mar  1 11:49:21.497: INFO: Pod "pod-secrets-9dd655cb-8c2d-467c-989f-bf6cc7eab458": Phase="Running", Reason="", readiness=true. Elapsed: 2.011180554s
Mar  1 11:49:21.497: INFO: The phase of Pod pod-secrets-9dd655cb-8c2d-467c-989f-bf6cc7eab458 is Running (Ready = true)
Mar  1 11:49:21.497: INFO: Pod "pod-secrets-9dd655cb-8c2d-467c-989f-bf6cc7eab458" satisfied condition "running and ready"
STEP: Cleaning up the secret 03/01/23 11:49:21.501
STEP: Cleaning up the configmap 03/01/23 11:49:21.51
STEP: Cleaning up the pod 03/01/23 11:49:21.517
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
Mar  1 11:49:21.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-2293" for this suite. 03/01/23 11:49:21.54
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","completed":35,"skipped":644,"failed":0}
------------------------------
â€¢ [2.119 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:49:19.43
    Mar  1 11:49:19.431: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename emptydir-wrapper 03/01/23 11:49:19.432
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:49:19.453
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:49:19.456
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Mar  1 11:49:19.486: INFO: Waiting up to 5m0s for pod "pod-secrets-9dd655cb-8c2d-467c-989f-bf6cc7eab458" in namespace "emptydir-wrapper-2293" to be "running and ready"
    Mar  1 11:49:19.490: INFO: Pod "pod-secrets-9dd655cb-8c2d-467c-989f-bf6cc7eab458": Phase="Pending", Reason="", readiness=false. Elapsed: 4.36312ms
    Mar  1 11:49:19.490: INFO: The phase of Pod pod-secrets-9dd655cb-8c2d-467c-989f-bf6cc7eab458 is Pending, waiting for it to be Running (with Ready = true)
    Mar  1 11:49:21.497: INFO: Pod "pod-secrets-9dd655cb-8c2d-467c-989f-bf6cc7eab458": Phase="Running", Reason="", readiness=true. Elapsed: 2.011180554s
    Mar  1 11:49:21.497: INFO: The phase of Pod pod-secrets-9dd655cb-8c2d-467c-989f-bf6cc7eab458 is Running (Ready = true)
    Mar  1 11:49:21.497: INFO: Pod "pod-secrets-9dd655cb-8c2d-467c-989f-bf6cc7eab458" satisfied condition "running and ready"
    STEP: Cleaning up the secret 03/01/23 11:49:21.501
    STEP: Cleaning up the configmap 03/01/23 11:49:21.51
    STEP: Cleaning up the pod 03/01/23 11:49:21.517
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    Mar  1 11:49:21.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-2293" for this suite. 03/01/23 11:49:21.54
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:49:21.555
Mar  1 11:49:21.556: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename webhook 03/01/23 11:49:21.556
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:49:21.578
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:49:21.582
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/01/23 11:49:21.604
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/01/23 11:49:22.074
STEP: Deploying the webhook pod 03/01/23 11:49:22.084
STEP: Wait for the deployment to be ready 03/01/23 11:49:22.104
Mar  1 11:49:22.113: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/01/23 11:49:24.13
STEP: Verifying the service has paired with the endpoint 03/01/23 11:49:24.144
Mar  1 11:49:25.145: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
STEP: Listing all of the created validation webhooks 03/01/23 11:49:25.252
STEP: Creating a configMap that should be mutated 03/01/23 11:49:25.272
STEP: Deleting the collection of validation webhooks 03/01/23 11:49:25.303
STEP: Creating a configMap that should not be mutated 03/01/23 11:49:25.374
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  1 11:49:25.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1991" for this suite. 03/01/23 11:49:25.403
STEP: Destroying namespace "webhook-1991-markers" for this suite. 03/01/23 11:49:25.411
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","completed":36,"skipped":720,"failed":0}
------------------------------
â€¢ [3.929 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:49:21.555
    Mar  1 11:49:21.556: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename webhook 03/01/23 11:49:21.556
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:49:21.578
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:49:21.582
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/01/23 11:49:21.604
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/01/23 11:49:22.074
    STEP: Deploying the webhook pod 03/01/23 11:49:22.084
    STEP: Wait for the deployment to be ready 03/01/23 11:49:22.104
    Mar  1 11:49:22.113: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/01/23 11:49:24.13
    STEP: Verifying the service has paired with the endpoint 03/01/23 11:49:24.144
    Mar  1 11:49:25.145: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:655
    STEP: Listing all of the created validation webhooks 03/01/23 11:49:25.252
    STEP: Creating a configMap that should be mutated 03/01/23 11:49:25.272
    STEP: Deleting the collection of validation webhooks 03/01/23 11:49:25.303
    STEP: Creating a configMap that should not be mutated 03/01/23 11:49:25.374
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  1 11:49:25.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-1991" for this suite. 03/01/23 11:49:25.403
    STEP: Destroying namespace "webhook-1991-markers" for this suite. 03/01/23 11:49:25.411
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:49:25.487
Mar  1 11:49:25.487: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename downward-api 03/01/23 11:49:25.488
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:49:25.511
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:49:25.515
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
STEP: Creating a pod to test downward API volume plugin 03/01/23 11:49:25.518
Mar  1 11:49:25.530: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fa3dfc03-0396-49f1-832a-9246063ceb3b" in namespace "downward-api-2668" to be "Succeeded or Failed"
Mar  1 11:49:25.535: INFO: Pod "downwardapi-volume-fa3dfc03-0396-49f1-832a-9246063ceb3b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.086226ms
Mar  1 11:49:27.542: INFO: Pod "downwardapi-volume-fa3dfc03-0396-49f1-832a-9246063ceb3b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011384207s
Mar  1 11:49:29.542: INFO: Pod "downwardapi-volume-fa3dfc03-0396-49f1-832a-9246063ceb3b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011666206s
STEP: Saw pod success 03/01/23 11:49:29.542
Mar  1 11:49:29.542: INFO: Pod "downwardapi-volume-fa3dfc03-0396-49f1-832a-9246063ceb3b" satisfied condition "Succeeded or Failed"
Mar  1 11:49:29.548: INFO: Trying to get logs from node lab1-k8s-node-3 pod downwardapi-volume-fa3dfc03-0396-49f1-832a-9246063ceb3b container client-container: <nil>
STEP: delete the pod 03/01/23 11:49:29.557
Mar  1 11:49:29.577: INFO: Waiting for pod downwardapi-volume-fa3dfc03-0396-49f1-832a-9246063ceb3b to disappear
Mar  1 11:49:29.583: INFO: Pod downwardapi-volume-fa3dfc03-0396-49f1-832a-9246063ceb3b no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Mar  1 11:49:29.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2668" for this suite. 03/01/23 11:49:29.594
{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","completed":37,"skipped":732,"failed":0}
------------------------------
â€¢ [4.115 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:49:25.487
    Mar  1 11:49:25.487: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename downward-api 03/01/23 11:49:25.488
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:49:25.511
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:49:25.515
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:52
    STEP: Creating a pod to test downward API volume plugin 03/01/23 11:49:25.518
    Mar  1 11:49:25.530: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fa3dfc03-0396-49f1-832a-9246063ceb3b" in namespace "downward-api-2668" to be "Succeeded or Failed"
    Mar  1 11:49:25.535: INFO: Pod "downwardapi-volume-fa3dfc03-0396-49f1-832a-9246063ceb3b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.086226ms
    Mar  1 11:49:27.542: INFO: Pod "downwardapi-volume-fa3dfc03-0396-49f1-832a-9246063ceb3b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011384207s
    Mar  1 11:49:29.542: INFO: Pod "downwardapi-volume-fa3dfc03-0396-49f1-832a-9246063ceb3b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011666206s
    STEP: Saw pod success 03/01/23 11:49:29.542
    Mar  1 11:49:29.542: INFO: Pod "downwardapi-volume-fa3dfc03-0396-49f1-832a-9246063ceb3b" satisfied condition "Succeeded or Failed"
    Mar  1 11:49:29.548: INFO: Trying to get logs from node lab1-k8s-node-3 pod downwardapi-volume-fa3dfc03-0396-49f1-832a-9246063ceb3b container client-container: <nil>
    STEP: delete the pod 03/01/23 11:49:29.557
    Mar  1 11:49:29.577: INFO: Waiting for pod downwardapi-volume-fa3dfc03-0396-49f1-832a-9246063ceb3b to disappear
    Mar  1 11:49:29.583: INFO: Pod downwardapi-volume-fa3dfc03-0396-49f1-832a-9246063ceb3b no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Mar  1 11:49:29.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-2668" for this suite. 03/01/23 11:49:29.594
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:49:29.602
Mar  1 11:49:29.602: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename downward-api 03/01/23 11:49:29.603
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:49:29.626
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:49:29.629
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
STEP: Creating a pod to test downward api env vars 03/01/23 11:49:29.633
Mar  1 11:49:29.642: INFO: Waiting up to 5m0s for pod "downward-api-51553dfe-4df9-41ee-8d64-0b2b2453d6e3" in namespace "downward-api-7994" to be "Succeeded or Failed"
Mar  1 11:49:29.647: INFO: Pod "downward-api-51553dfe-4df9-41ee-8d64-0b2b2453d6e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.280574ms
Mar  1 11:49:31.653: INFO: Pod "downward-api-51553dfe-4df9-41ee-8d64-0b2b2453d6e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010147052s
Mar  1 11:49:33.652: INFO: Pod "downward-api-51553dfe-4df9-41ee-8d64-0b2b2453d6e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010105474s
STEP: Saw pod success 03/01/23 11:49:33.653
Mar  1 11:49:33.653: INFO: Pod "downward-api-51553dfe-4df9-41ee-8d64-0b2b2453d6e3" satisfied condition "Succeeded or Failed"
Mar  1 11:49:33.659: INFO: Trying to get logs from node lab1-k8s-node-3 pod downward-api-51553dfe-4df9-41ee-8d64-0b2b2453d6e3 container dapi-container: <nil>
STEP: delete the pod 03/01/23 11:49:33.669
Mar  1 11:49:33.683: INFO: Waiting for pod downward-api-51553dfe-4df9-41ee-8d64-0b2b2453d6e3 to disappear
Mar  1 11:49:33.688: INFO: Pod downward-api-51553dfe-4df9-41ee-8d64-0b2b2453d6e3 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Mar  1 11:49:33.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7994" for this suite. 03/01/23 11:49:33.695
{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","completed":38,"skipped":739,"failed":0}
------------------------------
â€¢ [4.102 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:49:29.602
    Mar  1 11:49:29.602: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename downward-api 03/01/23 11:49:29.603
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:49:29.626
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:49:29.629
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:89
    STEP: Creating a pod to test downward api env vars 03/01/23 11:49:29.633
    Mar  1 11:49:29.642: INFO: Waiting up to 5m0s for pod "downward-api-51553dfe-4df9-41ee-8d64-0b2b2453d6e3" in namespace "downward-api-7994" to be "Succeeded or Failed"
    Mar  1 11:49:29.647: INFO: Pod "downward-api-51553dfe-4df9-41ee-8d64-0b2b2453d6e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.280574ms
    Mar  1 11:49:31.653: INFO: Pod "downward-api-51553dfe-4df9-41ee-8d64-0b2b2453d6e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010147052s
    Mar  1 11:49:33.652: INFO: Pod "downward-api-51553dfe-4df9-41ee-8d64-0b2b2453d6e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010105474s
    STEP: Saw pod success 03/01/23 11:49:33.653
    Mar  1 11:49:33.653: INFO: Pod "downward-api-51553dfe-4df9-41ee-8d64-0b2b2453d6e3" satisfied condition "Succeeded or Failed"
    Mar  1 11:49:33.659: INFO: Trying to get logs from node lab1-k8s-node-3 pod downward-api-51553dfe-4df9-41ee-8d64-0b2b2453d6e3 container dapi-container: <nil>
    STEP: delete the pod 03/01/23 11:49:33.669
    Mar  1 11:49:33.683: INFO: Waiting for pod downward-api-51553dfe-4df9-41ee-8d64-0b2b2453d6e3 to disappear
    Mar  1 11:49:33.688: INFO: Pod downward-api-51553dfe-4df9-41ee-8d64-0b2b2453d6e3 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Mar  1 11:49:33.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-7994" for this suite. 03/01/23 11:49:33.695
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:49:33.709
Mar  1 11:49:33.709: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename endpointslice 03/01/23 11:49:33.71
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:49:33.73
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:49:33.734
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Mar  1 11:49:35.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-305" for this suite. 03/01/23 11:49:35.822
{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","completed":39,"skipped":762,"failed":0}
------------------------------
â€¢ [2.126 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:49:33.709
    Mar  1 11:49:33.709: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename endpointslice 03/01/23 11:49:33.71
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:49:33.73
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:49:33.734
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:101
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Mar  1 11:49:35.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-305" for this suite. 03/01/23 11:49:35.822
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:49:35.836
Mar  1 11:49:35.836: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename configmap 03/01/23 11:49:35.837
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:49:35.857
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:49:35.861
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
STEP: creating a ConfigMap 03/01/23 11:49:35.865
STEP: fetching the ConfigMap 03/01/23 11:49:35.871
STEP: patching the ConfigMap 03/01/23 11:49:35.876
STEP: listing all ConfigMaps in all namespaces with a label selector 03/01/23 11:49:35.889
STEP: deleting the ConfigMap by collection with a label selector 03/01/23 11:49:35.893
STEP: listing all ConfigMaps in test namespace 03/01/23 11:49:35.904
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Mar  1 11:49:35.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5281" for this suite. 03/01/23 11:49:35.917
{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","completed":40,"skipped":779,"failed":0}
------------------------------
â€¢ [0.091 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:49:35.836
    Mar  1 11:49:35.836: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename configmap 03/01/23 11:49:35.837
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:49:35.857
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:49:35.861
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:168
    STEP: creating a ConfigMap 03/01/23 11:49:35.865
    STEP: fetching the ConfigMap 03/01/23 11:49:35.871
    STEP: patching the ConfigMap 03/01/23 11:49:35.876
    STEP: listing all ConfigMaps in all namespaces with a label selector 03/01/23 11:49:35.889
    STEP: deleting the ConfigMap by collection with a label selector 03/01/23 11:49:35.893
    STEP: listing all ConfigMaps in test namespace 03/01/23 11:49:35.904
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Mar  1 11:49:35.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-5281" for this suite. 03/01/23 11:49:35.917
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:49:35.929
Mar  1 11:49:35.929: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename subpath 03/01/23 11:49:35.93
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:49:35.949
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:49:35.953
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 03/01/23 11:49:35.958
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-jhsb 03/01/23 11:49:35.969
STEP: Creating a pod to test atomic-volume-subpath 03/01/23 11:49:35.969
Mar  1 11:49:35.981: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-jhsb" in namespace "subpath-5000" to be "Succeeded or Failed"
Mar  1 11:49:35.986: INFO: Pod "pod-subpath-test-secret-jhsb": Phase="Pending", Reason="", readiness=false. Elapsed: 5.312069ms
Mar  1 11:49:37.993: INFO: Pod "pod-subpath-test-secret-jhsb": Phase="Running", Reason="", readiness=true. Elapsed: 2.012010513s
Mar  1 11:49:39.992: INFO: Pod "pod-subpath-test-secret-jhsb": Phase="Running", Reason="", readiness=true. Elapsed: 4.010713092s
Mar  1 11:49:41.992: INFO: Pod "pod-subpath-test-secret-jhsb": Phase="Running", Reason="", readiness=true. Elapsed: 6.010803146s
Mar  1 11:49:43.993: INFO: Pod "pod-subpath-test-secret-jhsb": Phase="Running", Reason="", readiness=true. Elapsed: 8.011973659s
Mar  1 11:49:45.993: INFO: Pod "pod-subpath-test-secret-jhsb": Phase="Running", Reason="", readiness=true. Elapsed: 10.011648422s
Mar  1 11:49:47.995: INFO: Pod "pod-subpath-test-secret-jhsb": Phase="Running", Reason="", readiness=true. Elapsed: 12.01387398s
Mar  1 11:49:49.993: INFO: Pod "pod-subpath-test-secret-jhsb": Phase="Running", Reason="", readiness=true. Elapsed: 14.011965933s
Mar  1 11:49:51.992: INFO: Pod "pod-subpath-test-secret-jhsb": Phase="Running", Reason="", readiness=true. Elapsed: 16.010632806s
Mar  1 11:49:53.991: INFO: Pod "pod-subpath-test-secret-jhsb": Phase="Running", Reason="", readiness=true. Elapsed: 18.010018317s
Mar  1 11:49:55.992: INFO: Pod "pod-subpath-test-secret-jhsb": Phase="Running", Reason="", readiness=true. Elapsed: 20.0110092s
Mar  1 11:49:57.993: INFO: Pod "pod-subpath-test-secret-jhsb": Phase="Running", Reason="", readiness=false. Elapsed: 22.011705203s
Mar  1 11:49:59.992: INFO: Pod "pod-subpath-test-secret-jhsb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.011076523s
STEP: Saw pod success 03/01/23 11:49:59.992
Mar  1 11:49:59.992: INFO: Pod "pod-subpath-test-secret-jhsb" satisfied condition "Succeeded or Failed"
Mar  1 11:49:59.997: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-subpath-test-secret-jhsb container test-container-subpath-secret-jhsb: <nil>
STEP: delete the pod 03/01/23 11:50:00.008
Mar  1 11:50:00.028: INFO: Waiting for pod pod-subpath-test-secret-jhsb to disappear
Mar  1 11:50:00.033: INFO: Pod pod-subpath-test-secret-jhsb no longer exists
STEP: Deleting pod pod-subpath-test-secret-jhsb 03/01/23 11:50:00.033
Mar  1 11:50:00.033: INFO: Deleting pod "pod-subpath-test-secret-jhsb" in namespace "subpath-5000"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Mar  1 11:50:00.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5000" for this suite. 03/01/23 11:50:00.05
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]","completed":41,"skipped":791,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.131 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:49:35.929
    Mar  1 11:49:35.929: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename subpath 03/01/23 11:49:35.93
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:49:35.949
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:49:35.953
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 03/01/23 11:49:35.958
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-jhsb 03/01/23 11:49:35.969
    STEP: Creating a pod to test atomic-volume-subpath 03/01/23 11:49:35.969
    Mar  1 11:49:35.981: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-jhsb" in namespace "subpath-5000" to be "Succeeded or Failed"
    Mar  1 11:49:35.986: INFO: Pod "pod-subpath-test-secret-jhsb": Phase="Pending", Reason="", readiness=false. Elapsed: 5.312069ms
    Mar  1 11:49:37.993: INFO: Pod "pod-subpath-test-secret-jhsb": Phase="Running", Reason="", readiness=true. Elapsed: 2.012010513s
    Mar  1 11:49:39.992: INFO: Pod "pod-subpath-test-secret-jhsb": Phase="Running", Reason="", readiness=true. Elapsed: 4.010713092s
    Mar  1 11:49:41.992: INFO: Pod "pod-subpath-test-secret-jhsb": Phase="Running", Reason="", readiness=true. Elapsed: 6.010803146s
    Mar  1 11:49:43.993: INFO: Pod "pod-subpath-test-secret-jhsb": Phase="Running", Reason="", readiness=true. Elapsed: 8.011973659s
    Mar  1 11:49:45.993: INFO: Pod "pod-subpath-test-secret-jhsb": Phase="Running", Reason="", readiness=true. Elapsed: 10.011648422s
    Mar  1 11:49:47.995: INFO: Pod "pod-subpath-test-secret-jhsb": Phase="Running", Reason="", readiness=true. Elapsed: 12.01387398s
    Mar  1 11:49:49.993: INFO: Pod "pod-subpath-test-secret-jhsb": Phase="Running", Reason="", readiness=true. Elapsed: 14.011965933s
    Mar  1 11:49:51.992: INFO: Pod "pod-subpath-test-secret-jhsb": Phase="Running", Reason="", readiness=true. Elapsed: 16.010632806s
    Mar  1 11:49:53.991: INFO: Pod "pod-subpath-test-secret-jhsb": Phase="Running", Reason="", readiness=true. Elapsed: 18.010018317s
    Mar  1 11:49:55.992: INFO: Pod "pod-subpath-test-secret-jhsb": Phase="Running", Reason="", readiness=true. Elapsed: 20.0110092s
    Mar  1 11:49:57.993: INFO: Pod "pod-subpath-test-secret-jhsb": Phase="Running", Reason="", readiness=false. Elapsed: 22.011705203s
    Mar  1 11:49:59.992: INFO: Pod "pod-subpath-test-secret-jhsb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.011076523s
    STEP: Saw pod success 03/01/23 11:49:59.992
    Mar  1 11:49:59.992: INFO: Pod "pod-subpath-test-secret-jhsb" satisfied condition "Succeeded or Failed"
    Mar  1 11:49:59.997: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-subpath-test-secret-jhsb container test-container-subpath-secret-jhsb: <nil>
    STEP: delete the pod 03/01/23 11:50:00.008
    Mar  1 11:50:00.028: INFO: Waiting for pod pod-subpath-test-secret-jhsb to disappear
    Mar  1 11:50:00.033: INFO: Pod pod-subpath-test-secret-jhsb no longer exists
    STEP: Deleting pod pod-subpath-test-secret-jhsb 03/01/23 11:50:00.033
    Mar  1 11:50:00.033: INFO: Deleting pod "pod-subpath-test-secret-jhsb" in namespace "subpath-5000"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Mar  1 11:50:00.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-5000" for this suite. 03/01/23 11:50:00.05
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:50:00.063
Mar  1 11:50:00.063: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename downward-api 03/01/23 11:50:00.064
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:50:00.095
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:50:00.099
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
STEP: Creating a pod to test downward API volume plugin 03/01/23 11:50:00.102
Mar  1 11:50:00.120: INFO: Waiting up to 5m0s for pod "downwardapi-volume-faa0f303-31da-4341-ba23-5d55765b4e9d" in namespace "downward-api-465" to be "Succeeded or Failed"
Mar  1 11:50:00.127: INFO: Pod "downwardapi-volume-faa0f303-31da-4341-ba23-5d55765b4e9d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.85552ms
Mar  1 11:50:02.135: INFO: Pod "downwardapi-volume-faa0f303-31da-4341-ba23-5d55765b4e9d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014852704s
Mar  1 11:50:04.134: INFO: Pod "downwardapi-volume-faa0f303-31da-4341-ba23-5d55765b4e9d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013954817s
STEP: Saw pod success 03/01/23 11:50:04.134
Mar  1 11:50:04.134: INFO: Pod "downwardapi-volume-faa0f303-31da-4341-ba23-5d55765b4e9d" satisfied condition "Succeeded or Failed"
Mar  1 11:50:04.139: INFO: Trying to get logs from node lab1-k8s-node-3 pod downwardapi-volume-faa0f303-31da-4341-ba23-5d55765b4e9d container client-container: <nil>
STEP: delete the pod 03/01/23 11:50:04.15
Mar  1 11:50:04.165: INFO: Waiting for pod downwardapi-volume-faa0f303-31da-4341-ba23-5d55765b4e9d to disappear
Mar  1 11:50:04.169: INFO: Pod downwardapi-volume-faa0f303-31da-4341-ba23-5d55765b4e9d no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Mar  1 11:50:04.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-465" for this suite. 03/01/23 11:50:04.177
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","completed":42,"skipped":840,"failed":0}
------------------------------
â€¢ [4.125 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:50:00.063
    Mar  1 11:50:00.063: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename downward-api 03/01/23 11:50:00.064
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:50:00.095
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:50:00.099
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:220
    STEP: Creating a pod to test downward API volume plugin 03/01/23 11:50:00.102
    Mar  1 11:50:00.120: INFO: Waiting up to 5m0s for pod "downwardapi-volume-faa0f303-31da-4341-ba23-5d55765b4e9d" in namespace "downward-api-465" to be "Succeeded or Failed"
    Mar  1 11:50:00.127: INFO: Pod "downwardapi-volume-faa0f303-31da-4341-ba23-5d55765b4e9d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.85552ms
    Mar  1 11:50:02.135: INFO: Pod "downwardapi-volume-faa0f303-31da-4341-ba23-5d55765b4e9d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014852704s
    Mar  1 11:50:04.134: INFO: Pod "downwardapi-volume-faa0f303-31da-4341-ba23-5d55765b4e9d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013954817s
    STEP: Saw pod success 03/01/23 11:50:04.134
    Mar  1 11:50:04.134: INFO: Pod "downwardapi-volume-faa0f303-31da-4341-ba23-5d55765b4e9d" satisfied condition "Succeeded or Failed"
    Mar  1 11:50:04.139: INFO: Trying to get logs from node lab1-k8s-node-3 pod downwardapi-volume-faa0f303-31da-4341-ba23-5d55765b4e9d container client-container: <nil>
    STEP: delete the pod 03/01/23 11:50:04.15
    Mar  1 11:50:04.165: INFO: Waiting for pod downwardapi-volume-faa0f303-31da-4341-ba23-5d55765b4e9d to disappear
    Mar  1 11:50:04.169: INFO: Pod downwardapi-volume-faa0f303-31da-4341-ba23-5d55765b4e9d no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Mar  1 11:50:04.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-465" for this suite. 03/01/23 11:50:04.177
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:50:04.189
Mar  1 11:50:04.189: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename replicaset 03/01/23 11:50:04.19
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:50:04.215
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:50:04.219
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 03/01/23 11:50:04.229
STEP: Verify that the required pods have come up. 03/01/23 11:50:04.242
Mar  1 11:50:04.254: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 03/01/23 11:50:04.254
Mar  1 11:50:04.254: INFO: Waiting up to 5m0s for pod "test-rs-hf9b8" in namespace "replicaset-6669" to be "running"
Mar  1 11:50:04.260: INFO: Pod "test-rs-hf9b8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.342561ms
Mar  1 11:50:06.266: INFO: Pod "test-rs-hf9b8": Phase="Running", Reason="", readiness=true. Elapsed: 2.011501713s
Mar  1 11:50:06.266: INFO: Pod "test-rs-hf9b8" satisfied condition "running"
STEP: Getting /status 03/01/23 11:50:06.266
Mar  1 11:50:06.271: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 03/01/23 11:50:06.271
Mar  1 11:50:06.283: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 03/01/23 11:50:06.283
Mar  1 11:50:06.286: INFO: Observed &ReplicaSet event: ADDED
Mar  1 11:50:06.286: INFO: Observed &ReplicaSet event: MODIFIED
Mar  1 11:50:06.286: INFO: Observed &ReplicaSet event: MODIFIED
Mar  1 11:50:06.286: INFO: Observed &ReplicaSet event: MODIFIED
Mar  1 11:50:06.286: INFO: Found replicaset test-rs in namespace replicaset-6669 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Mar  1 11:50:06.286: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 03/01/23 11:50:06.286
Mar  1 11:50:06.286: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Mar  1 11:50:06.294: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 03/01/23 11:50:06.294
Mar  1 11:50:06.296: INFO: Observed &ReplicaSet event: ADDED
Mar  1 11:50:06.296: INFO: Observed &ReplicaSet event: MODIFIED
Mar  1 11:50:06.296: INFO: Observed &ReplicaSet event: MODIFIED
Mar  1 11:50:06.296: INFO: Observed &ReplicaSet event: MODIFIED
Mar  1 11:50:06.296: INFO: Observed replicaset test-rs in namespace replicaset-6669 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Mar  1 11:50:06.296: INFO: Observed &ReplicaSet event: MODIFIED
Mar  1 11:50:06.296: INFO: Found replicaset test-rs in namespace replicaset-6669 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Mar  1 11:50:06.297: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Mar  1 11:50:06.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6669" for this suite. 03/01/23 11:50:06.305
{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","completed":43,"skipped":842,"failed":0}
------------------------------
â€¢ [2.126 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:50:04.189
    Mar  1 11:50:04.189: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename replicaset 03/01/23 11:50:04.19
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:50:04.215
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:50:04.219
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 03/01/23 11:50:04.229
    STEP: Verify that the required pods have come up. 03/01/23 11:50:04.242
    Mar  1 11:50:04.254: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 03/01/23 11:50:04.254
    Mar  1 11:50:04.254: INFO: Waiting up to 5m0s for pod "test-rs-hf9b8" in namespace "replicaset-6669" to be "running"
    Mar  1 11:50:04.260: INFO: Pod "test-rs-hf9b8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.342561ms
    Mar  1 11:50:06.266: INFO: Pod "test-rs-hf9b8": Phase="Running", Reason="", readiness=true. Elapsed: 2.011501713s
    Mar  1 11:50:06.266: INFO: Pod "test-rs-hf9b8" satisfied condition "running"
    STEP: Getting /status 03/01/23 11:50:06.266
    Mar  1 11:50:06.271: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 03/01/23 11:50:06.271
    Mar  1 11:50:06.283: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 03/01/23 11:50:06.283
    Mar  1 11:50:06.286: INFO: Observed &ReplicaSet event: ADDED
    Mar  1 11:50:06.286: INFO: Observed &ReplicaSet event: MODIFIED
    Mar  1 11:50:06.286: INFO: Observed &ReplicaSet event: MODIFIED
    Mar  1 11:50:06.286: INFO: Observed &ReplicaSet event: MODIFIED
    Mar  1 11:50:06.286: INFO: Found replicaset test-rs in namespace replicaset-6669 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Mar  1 11:50:06.286: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 03/01/23 11:50:06.286
    Mar  1 11:50:06.286: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Mar  1 11:50:06.294: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 03/01/23 11:50:06.294
    Mar  1 11:50:06.296: INFO: Observed &ReplicaSet event: ADDED
    Mar  1 11:50:06.296: INFO: Observed &ReplicaSet event: MODIFIED
    Mar  1 11:50:06.296: INFO: Observed &ReplicaSet event: MODIFIED
    Mar  1 11:50:06.296: INFO: Observed &ReplicaSet event: MODIFIED
    Mar  1 11:50:06.296: INFO: Observed replicaset test-rs in namespace replicaset-6669 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Mar  1 11:50:06.296: INFO: Observed &ReplicaSet event: MODIFIED
    Mar  1 11:50:06.296: INFO: Found replicaset test-rs in namespace replicaset-6669 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Mar  1 11:50:06.297: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Mar  1 11:50:06.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-6669" for this suite. 03/01/23 11:50:06.305
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:50:06.319
Mar  1 11:50:06.319: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename sched-pred 03/01/23 11:50:06.319
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:50:06.355
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:50:06.358
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Mar  1 11:50:06.362: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar  1 11:50:06.373: INFO: Waiting for terminating namespaces to be deleted...
Mar  1 11:50:06.377: INFO: 
Logging pods the apiserver thinks is on node lab1-k8s-node-1 before test
Mar  1 11:50:06.394: INFO: calico-node-kjj57 from kube-system started at 2023-03-01 11:36:06 +0000 UTC (1 container statuses recorded)
Mar  1 11:50:06.394: INFO: 	Container calico-node ready: true, restart count 1
Mar  1 11:50:06.394: INFO: csi-cinder-nodeplugin-fjt6c from kube-system started at 2023-03-01 11:38:24 +0000 UTC (3 container statuses recorded)
Mar  1 11:50:06.394: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Mar  1 11:50:06.394: INFO: 	Container liveness-probe ready: true, restart count 0
Mar  1 11:50:06.394: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar  1 11:50:06.394: INFO: kube-proxy-xmdzj from kube-system started at 2023-03-01 11:35:32 +0000 UTC (1 container statuses recorded)
Mar  1 11:50:06.394: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  1 11:50:06.394: INFO: metrics-server-6bd8d699c5-pwxfp from kube-system started at 2023-03-01 11:38:24 +0000 UTC (1 container statuses recorded)
Mar  1 11:50:06.394: INFO: 	Container metrics-server ready: true, restart count 0
Mar  1 11:50:06.394: INFO: nginx-proxy-lab1-k8s-node-1 from kube-system started at 2023-03-01 11:36:22 +0000 UTC (1 container statuses recorded)
Mar  1 11:50:06.394: INFO: 	Container nginx-proxy ready: true, restart count 0
Mar  1 11:50:06.394: INFO: nodelocaldns-mclb6 from kube-system started at 2023-03-01 11:37:23 +0000 UTC (1 container statuses recorded)
Mar  1 11:50:06.394: INFO: 	Container node-cache ready: true, restart count 0
Mar  1 11:50:06.394: INFO: sonobuoy-e2e-job-5f1d4571b8c24260 from sonobuoy started at 2023-03-01 11:42:09 +0000 UTC (2 container statuses recorded)
Mar  1 11:50:06.394: INFO: 	Container e2e ready: true, restart count 0
Mar  1 11:50:06.394: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  1 11:50:06.394: INFO: sonobuoy-systemd-logs-daemon-set-1321f7b040854c53-9ckr7 from sonobuoy started at 2023-03-01 11:42:09 +0000 UTC (2 container statuses recorded)
Mar  1 11:50:06.394: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  1 11:50:06.394: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  1 11:50:06.394: INFO: 
Logging pods the apiserver thinks is on node lab1-k8s-node-2 before test
Mar  1 11:50:06.407: INFO: calico-kube-controllers-ff45567bb-9k2q7 from kube-system started at 2023-03-01 11:37:06 +0000 UTC (1 container statuses recorded)
Mar  1 11:50:06.407: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Mar  1 11:50:06.407: INFO: calico-node-5vzf7 from kube-system started at 2023-03-01 11:36:06 +0000 UTC (1 container statuses recorded)
Mar  1 11:50:06.407: INFO: 	Container calico-node ready: true, restart count 1
Mar  1 11:50:06.407: INFO: csi-cinder-controllerplugin-6f68fbd578-krvcc from kube-system started at 2023-03-01 11:38:24 +0000 UTC (6 container statuses recorded)
Mar  1 11:50:06.407: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Mar  1 11:50:06.407: INFO: 	Container csi-attacher ready: true, restart count 0
Mar  1 11:50:06.407: INFO: 	Container csi-provisioner ready: true, restart count 0
Mar  1 11:50:06.407: INFO: 	Container csi-resizer ready: true, restart count 0
Mar  1 11:50:06.407: INFO: 	Container csi-snapshotter ready: true, restart count 0
Mar  1 11:50:06.407: INFO: 	Container liveness-probe ready: true, restart count 0
Mar  1 11:50:06.407: INFO: csi-cinder-nodeplugin-zl564 from kube-system started at 2023-03-01 11:38:24 +0000 UTC (3 container statuses recorded)
Mar  1 11:50:06.407: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Mar  1 11:50:06.407: INFO: 	Container liveness-probe ready: true, restart count 0
Mar  1 11:50:06.407: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar  1 11:50:06.407: INFO: kube-proxy-jllc6 from kube-system started at 2023-03-01 11:35:32 +0000 UTC (1 container statuses recorded)
Mar  1 11:50:06.407: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  1 11:50:06.407: INFO: nginx-proxy-lab1-k8s-node-2 from kube-system started at 2023-03-01 11:36:22 +0000 UTC (1 container statuses recorded)
Mar  1 11:50:06.407: INFO: 	Container nginx-proxy ready: true, restart count 0
Mar  1 11:50:06.407: INFO: nodelocaldns-mhm2s from kube-system started at 2023-03-01 11:37:23 +0000 UTC (1 container statuses recorded)
Mar  1 11:50:06.407: INFO: 	Container node-cache ready: true, restart count 0
Mar  1 11:50:06.407: INFO: sonobuoy-systemd-logs-daemon-set-1321f7b040854c53-nklmj from sonobuoy started at 2023-03-01 11:42:09 +0000 UTC (2 container statuses recorded)
Mar  1 11:50:06.407: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  1 11:50:06.407: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  1 11:50:06.407: INFO: 
Logging pods the apiserver thinks is on node lab1-k8s-node-3 before test
Mar  1 11:50:06.419: INFO: calico-node-zjksl from kube-system started at 2023-03-01 11:36:06 +0000 UTC (1 container statuses recorded)
Mar  1 11:50:06.419: INFO: 	Container calico-node ready: true, restart count 1
Mar  1 11:50:06.419: INFO: csi-cinder-nodeplugin-4j6sg from kube-system started at 2023-03-01 11:38:24 +0000 UTC (3 container statuses recorded)
Mar  1 11:50:06.419: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Mar  1 11:50:06.419: INFO: 	Container liveness-probe ready: true, restart count 0
Mar  1 11:50:06.419: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar  1 11:50:06.419: INFO: kube-proxy-2jcfl from kube-system started at 2023-03-01 11:35:29 +0000 UTC (1 container statuses recorded)
Mar  1 11:50:06.419: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  1 11:50:06.419: INFO: nginx-proxy-lab1-k8s-node-3 from kube-system started at 2023-03-01 11:38:59 +0000 UTC (1 container statuses recorded)
Mar  1 11:50:06.419: INFO: 	Container nginx-proxy ready: true, restart count 0
Mar  1 11:50:06.419: INFO: nodelocaldns-5tw4l from kube-system started at 2023-03-01 11:37:23 +0000 UTC (1 container statuses recorded)
Mar  1 11:50:06.419: INFO: 	Container node-cache ready: true, restart count 0
Mar  1 11:50:06.419: INFO: snapshot-controller-7d445c66c9-6pjgl from kube-system started at 2023-03-01 11:38:24 +0000 UTC (1 container statuses recorded)
Mar  1 11:50:06.419: INFO: 	Container snapshot-controller ready: true, restart count 0
Mar  1 11:50:06.419: INFO: test-rs-hf9b8 from replicaset-6669 started at 2023-03-01 11:50:04 +0000 UTC (1 container statuses recorded)
Mar  1 11:50:06.419: INFO: 	Container httpd ready: true, restart count 0
Mar  1 11:50:06.419: INFO: sonobuoy from sonobuoy started at 2023-03-01 11:42:06 +0000 UTC (1 container statuses recorded)
Mar  1 11:50:06.419: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar  1 11:50:06.419: INFO: sonobuoy-systemd-logs-daemon-set-1321f7b040854c53-9b52c from sonobuoy started at 2023-03-01 11:42:09 +0000 UTC (2 container statuses recorded)
Mar  1 11:50:06.419: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  1 11:50:06.419: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
STEP: Trying to schedule Pod with nonempty NodeSelector. 03/01/23 11:50:06.419
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.1748494169cce4a0], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 node(s) didn't match Pod's node affinity/selector, 3 node(s) had untolerated taint {node-role.kubernetes.io/master: }. preemption: 0/6 nodes are available: 6 Preemption is not helpful for scheduling.] 03/01/23 11:50:06.482
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Mar  1 11:50:07.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-511" for this suite. 03/01/23 11:50:07.487
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","completed":44,"skipped":875,"failed":0}
------------------------------
â€¢ [1.177 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:50:06.319
    Mar  1 11:50:06.319: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename sched-pred 03/01/23 11:50:06.319
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:50:06.355
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:50:06.358
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Mar  1 11:50:06.362: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Mar  1 11:50:06.373: INFO: Waiting for terminating namespaces to be deleted...
    Mar  1 11:50:06.377: INFO: 
    Logging pods the apiserver thinks is on node lab1-k8s-node-1 before test
    Mar  1 11:50:06.394: INFO: calico-node-kjj57 from kube-system started at 2023-03-01 11:36:06 +0000 UTC (1 container statuses recorded)
    Mar  1 11:50:06.394: INFO: 	Container calico-node ready: true, restart count 1
    Mar  1 11:50:06.394: INFO: csi-cinder-nodeplugin-fjt6c from kube-system started at 2023-03-01 11:38:24 +0000 UTC (3 container statuses recorded)
    Mar  1 11:50:06.394: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Mar  1 11:50:06.394: INFO: 	Container liveness-probe ready: true, restart count 0
    Mar  1 11:50:06.394: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Mar  1 11:50:06.394: INFO: kube-proxy-xmdzj from kube-system started at 2023-03-01 11:35:32 +0000 UTC (1 container statuses recorded)
    Mar  1 11:50:06.394: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar  1 11:50:06.394: INFO: metrics-server-6bd8d699c5-pwxfp from kube-system started at 2023-03-01 11:38:24 +0000 UTC (1 container statuses recorded)
    Mar  1 11:50:06.394: INFO: 	Container metrics-server ready: true, restart count 0
    Mar  1 11:50:06.394: INFO: nginx-proxy-lab1-k8s-node-1 from kube-system started at 2023-03-01 11:36:22 +0000 UTC (1 container statuses recorded)
    Mar  1 11:50:06.394: INFO: 	Container nginx-proxy ready: true, restart count 0
    Mar  1 11:50:06.394: INFO: nodelocaldns-mclb6 from kube-system started at 2023-03-01 11:37:23 +0000 UTC (1 container statuses recorded)
    Mar  1 11:50:06.394: INFO: 	Container node-cache ready: true, restart count 0
    Mar  1 11:50:06.394: INFO: sonobuoy-e2e-job-5f1d4571b8c24260 from sonobuoy started at 2023-03-01 11:42:09 +0000 UTC (2 container statuses recorded)
    Mar  1 11:50:06.394: INFO: 	Container e2e ready: true, restart count 0
    Mar  1 11:50:06.394: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar  1 11:50:06.394: INFO: sonobuoy-systemd-logs-daemon-set-1321f7b040854c53-9ckr7 from sonobuoy started at 2023-03-01 11:42:09 +0000 UTC (2 container statuses recorded)
    Mar  1 11:50:06.394: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar  1 11:50:06.394: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar  1 11:50:06.394: INFO: 
    Logging pods the apiserver thinks is on node lab1-k8s-node-2 before test
    Mar  1 11:50:06.407: INFO: calico-kube-controllers-ff45567bb-9k2q7 from kube-system started at 2023-03-01 11:37:06 +0000 UTC (1 container statuses recorded)
    Mar  1 11:50:06.407: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Mar  1 11:50:06.407: INFO: calico-node-5vzf7 from kube-system started at 2023-03-01 11:36:06 +0000 UTC (1 container statuses recorded)
    Mar  1 11:50:06.407: INFO: 	Container calico-node ready: true, restart count 1
    Mar  1 11:50:06.407: INFO: csi-cinder-controllerplugin-6f68fbd578-krvcc from kube-system started at 2023-03-01 11:38:24 +0000 UTC (6 container statuses recorded)
    Mar  1 11:50:06.407: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Mar  1 11:50:06.407: INFO: 	Container csi-attacher ready: true, restart count 0
    Mar  1 11:50:06.407: INFO: 	Container csi-provisioner ready: true, restart count 0
    Mar  1 11:50:06.407: INFO: 	Container csi-resizer ready: true, restart count 0
    Mar  1 11:50:06.407: INFO: 	Container csi-snapshotter ready: true, restart count 0
    Mar  1 11:50:06.407: INFO: 	Container liveness-probe ready: true, restart count 0
    Mar  1 11:50:06.407: INFO: csi-cinder-nodeplugin-zl564 from kube-system started at 2023-03-01 11:38:24 +0000 UTC (3 container statuses recorded)
    Mar  1 11:50:06.407: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Mar  1 11:50:06.407: INFO: 	Container liveness-probe ready: true, restart count 0
    Mar  1 11:50:06.407: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Mar  1 11:50:06.407: INFO: kube-proxy-jllc6 from kube-system started at 2023-03-01 11:35:32 +0000 UTC (1 container statuses recorded)
    Mar  1 11:50:06.407: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar  1 11:50:06.407: INFO: nginx-proxy-lab1-k8s-node-2 from kube-system started at 2023-03-01 11:36:22 +0000 UTC (1 container statuses recorded)
    Mar  1 11:50:06.407: INFO: 	Container nginx-proxy ready: true, restart count 0
    Mar  1 11:50:06.407: INFO: nodelocaldns-mhm2s from kube-system started at 2023-03-01 11:37:23 +0000 UTC (1 container statuses recorded)
    Mar  1 11:50:06.407: INFO: 	Container node-cache ready: true, restart count 0
    Mar  1 11:50:06.407: INFO: sonobuoy-systemd-logs-daemon-set-1321f7b040854c53-nklmj from sonobuoy started at 2023-03-01 11:42:09 +0000 UTC (2 container statuses recorded)
    Mar  1 11:50:06.407: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar  1 11:50:06.407: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar  1 11:50:06.407: INFO: 
    Logging pods the apiserver thinks is on node lab1-k8s-node-3 before test
    Mar  1 11:50:06.419: INFO: calico-node-zjksl from kube-system started at 2023-03-01 11:36:06 +0000 UTC (1 container statuses recorded)
    Mar  1 11:50:06.419: INFO: 	Container calico-node ready: true, restart count 1
    Mar  1 11:50:06.419: INFO: csi-cinder-nodeplugin-4j6sg from kube-system started at 2023-03-01 11:38:24 +0000 UTC (3 container statuses recorded)
    Mar  1 11:50:06.419: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Mar  1 11:50:06.419: INFO: 	Container liveness-probe ready: true, restart count 0
    Mar  1 11:50:06.419: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Mar  1 11:50:06.419: INFO: kube-proxy-2jcfl from kube-system started at 2023-03-01 11:35:29 +0000 UTC (1 container statuses recorded)
    Mar  1 11:50:06.419: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar  1 11:50:06.419: INFO: nginx-proxy-lab1-k8s-node-3 from kube-system started at 2023-03-01 11:38:59 +0000 UTC (1 container statuses recorded)
    Mar  1 11:50:06.419: INFO: 	Container nginx-proxy ready: true, restart count 0
    Mar  1 11:50:06.419: INFO: nodelocaldns-5tw4l from kube-system started at 2023-03-01 11:37:23 +0000 UTC (1 container statuses recorded)
    Mar  1 11:50:06.419: INFO: 	Container node-cache ready: true, restart count 0
    Mar  1 11:50:06.419: INFO: snapshot-controller-7d445c66c9-6pjgl from kube-system started at 2023-03-01 11:38:24 +0000 UTC (1 container statuses recorded)
    Mar  1 11:50:06.419: INFO: 	Container snapshot-controller ready: true, restart count 0
    Mar  1 11:50:06.419: INFO: test-rs-hf9b8 from replicaset-6669 started at 2023-03-01 11:50:04 +0000 UTC (1 container statuses recorded)
    Mar  1 11:50:06.419: INFO: 	Container httpd ready: true, restart count 0
    Mar  1 11:50:06.419: INFO: sonobuoy from sonobuoy started at 2023-03-01 11:42:06 +0000 UTC (1 container statuses recorded)
    Mar  1 11:50:06.419: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Mar  1 11:50:06.419: INFO: sonobuoy-systemd-logs-daemon-set-1321f7b040854c53-9b52c from sonobuoy started at 2023-03-01 11:42:09 +0000 UTC (2 container statuses recorded)
    Mar  1 11:50:06.419: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar  1 11:50:06.419: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:438
    STEP: Trying to schedule Pod with nonempty NodeSelector. 03/01/23 11:50:06.419
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.1748494169cce4a0], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 node(s) didn't match Pod's node affinity/selector, 3 node(s) had untolerated taint {node-role.kubernetes.io/master: }. preemption: 0/6 nodes are available: 6 Preemption is not helpful for scheduling.] 03/01/23 11:50:06.482
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Mar  1 11:50:07.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-511" for this suite. 03/01/23 11:50:07.487
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:50:07.496
Mar  1 11:50:07.496: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename projected 03/01/23 11:50:07.497
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:50:07.521
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:50:07.525
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
STEP: Creating configMap with name projected-configmap-test-volume-06dfb04a-720e-409d-8783-daa8d067d49f 03/01/23 11:50:07.528
STEP: Creating a pod to test consume configMaps 03/01/23 11:50:07.535
Mar  1 11:50:07.548: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d1416826-c5b3-4e4b-a8cb-3b5ba50d6110" in namespace "projected-6655" to be "Succeeded or Failed"
Mar  1 11:50:07.556: INFO: Pod "pod-projected-configmaps-d1416826-c5b3-4e4b-a8cb-3b5ba50d6110": Phase="Pending", Reason="", readiness=false. Elapsed: 8.321035ms
Mar  1 11:50:09.563: INFO: Pod "pod-projected-configmaps-d1416826-c5b3-4e4b-a8cb-3b5ba50d6110": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014928679s
Mar  1 11:50:11.561: INFO: Pod "pod-projected-configmaps-d1416826-c5b3-4e4b-a8cb-3b5ba50d6110": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013406519s
STEP: Saw pod success 03/01/23 11:50:11.561
Mar  1 11:50:11.561: INFO: Pod "pod-projected-configmaps-d1416826-c5b3-4e4b-a8cb-3b5ba50d6110" satisfied condition "Succeeded or Failed"
Mar  1 11:50:11.566: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-projected-configmaps-d1416826-c5b3-4e4b-a8cb-3b5ba50d6110 container projected-configmap-volume-test: <nil>
STEP: delete the pod 03/01/23 11:50:11.58
Mar  1 11:50:11.601: INFO: Waiting for pod pod-projected-configmaps-d1416826-c5b3-4e4b-a8cb-3b5ba50d6110 to disappear
Mar  1 11:50:11.606: INFO: Pod pod-projected-configmaps-d1416826-c5b3-4e4b-a8cb-3b5ba50d6110 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Mar  1 11:50:11.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6655" for this suite. 03/01/23 11:50:11.615
{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":45,"skipped":881,"failed":0}
------------------------------
â€¢ [4.127 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:50:07.496
    Mar  1 11:50:07.496: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename projected 03/01/23 11:50:07.497
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:50:07.521
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:50:07.525
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:374
    STEP: Creating configMap with name projected-configmap-test-volume-06dfb04a-720e-409d-8783-daa8d067d49f 03/01/23 11:50:07.528
    STEP: Creating a pod to test consume configMaps 03/01/23 11:50:07.535
    Mar  1 11:50:07.548: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d1416826-c5b3-4e4b-a8cb-3b5ba50d6110" in namespace "projected-6655" to be "Succeeded or Failed"
    Mar  1 11:50:07.556: INFO: Pod "pod-projected-configmaps-d1416826-c5b3-4e4b-a8cb-3b5ba50d6110": Phase="Pending", Reason="", readiness=false. Elapsed: 8.321035ms
    Mar  1 11:50:09.563: INFO: Pod "pod-projected-configmaps-d1416826-c5b3-4e4b-a8cb-3b5ba50d6110": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014928679s
    Mar  1 11:50:11.561: INFO: Pod "pod-projected-configmaps-d1416826-c5b3-4e4b-a8cb-3b5ba50d6110": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013406519s
    STEP: Saw pod success 03/01/23 11:50:11.561
    Mar  1 11:50:11.561: INFO: Pod "pod-projected-configmaps-d1416826-c5b3-4e4b-a8cb-3b5ba50d6110" satisfied condition "Succeeded or Failed"
    Mar  1 11:50:11.566: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-projected-configmaps-d1416826-c5b3-4e4b-a8cb-3b5ba50d6110 container projected-configmap-volume-test: <nil>
    STEP: delete the pod 03/01/23 11:50:11.58
    Mar  1 11:50:11.601: INFO: Waiting for pod pod-projected-configmaps-d1416826-c5b3-4e4b-a8cb-3b5ba50d6110 to disappear
    Mar  1 11:50:11.606: INFO: Pod pod-projected-configmaps-d1416826-c5b3-4e4b-a8cb-3b5ba50d6110 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Mar  1 11:50:11.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6655" for this suite. 03/01/23 11:50:11.615
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:50:11.626
Mar  1 11:50:11.627: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename webhook 03/01/23 11:50:11.627
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:50:11.653
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:50:11.656
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/01/23 11:50:11.675
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/01/23 11:50:12.266
STEP: Deploying the webhook pod 03/01/23 11:50:12.279
STEP: Wait for the deployment to be ready 03/01/23 11:50:12.294
Mar  1 11:50:12.303: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/01/23 11:50:14.319
STEP: Verifying the service has paired with the endpoint 03/01/23 11:50:14.337
Mar  1 11:50:15.338: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
STEP: Listing all of the created validation webhooks 03/01/23 11:50:15.425
STEP: Creating a configMap that does not comply to the validation webhook rules 03/01/23 11:50:15.459
STEP: Deleting the collection of validation webhooks 03/01/23 11:50:15.49
STEP: Creating a configMap that does not comply to the validation webhook rules 03/01/23 11:50:15.564
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  1 11:50:15.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1230" for this suite. 03/01/23 11:50:15.584
STEP: Destroying namespace "webhook-1230-markers" for this suite. 03/01/23 11:50:15.595
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","completed":46,"skipped":893,"failed":0}
------------------------------
â€¢ [4.049 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:50:11.626
    Mar  1 11:50:11.627: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename webhook 03/01/23 11:50:11.627
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:50:11.653
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:50:11.656
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/01/23 11:50:11.675
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/01/23 11:50:12.266
    STEP: Deploying the webhook pod 03/01/23 11:50:12.279
    STEP: Wait for the deployment to be ready 03/01/23 11:50:12.294
    Mar  1 11:50:12.303: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/01/23 11:50:14.319
    STEP: Verifying the service has paired with the endpoint 03/01/23 11:50:14.337
    Mar  1 11:50:15.338: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:581
    STEP: Listing all of the created validation webhooks 03/01/23 11:50:15.425
    STEP: Creating a configMap that does not comply to the validation webhook rules 03/01/23 11:50:15.459
    STEP: Deleting the collection of validation webhooks 03/01/23 11:50:15.49
    STEP: Creating a configMap that does not comply to the validation webhook rules 03/01/23 11:50:15.564
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  1 11:50:15.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-1230" for this suite. 03/01/23 11:50:15.584
    STEP: Destroying namespace "webhook-1230-markers" for this suite. 03/01/23 11:50:15.595
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:50:15.68
Mar  1 11:50:15.680: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename limitrange 03/01/23 11:50:15.68
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:50:15.704
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:50:15.707
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
STEP: Creating a LimitRange 03/01/23 11:50:15.711
STEP: Setting up watch 03/01/23 11:50:15.711
STEP: Submitting a LimitRange 03/01/23 11:50:15.816
STEP: Verifying LimitRange creation was observed 03/01/23 11:50:15.825
STEP: Fetching the LimitRange to ensure it has proper values 03/01/23 11:50:15.825
Mar  1 11:50:15.829: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Mar  1 11:50:15.829: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 03/01/23 11:50:15.829
STEP: Ensuring Pod has resource requirements applied from LimitRange 03/01/23 11:50:15.838
Mar  1 11:50:15.844: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Mar  1 11:50:15.844: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 03/01/23 11:50:15.844
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 03/01/23 11:50:15.853
Mar  1 11:50:15.858: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Mar  1 11:50:15.858: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 03/01/23 11:50:15.859
STEP: Failing to create a Pod with more than max resources 03/01/23 11:50:15.861
STEP: Updating a LimitRange 03/01/23 11:50:15.865
STEP: Verifying LimitRange updating is effective 03/01/23 11:50:15.871
STEP: Creating a Pod with less than former min resources 03/01/23 11:50:17.878
STEP: Failing to create a Pod with more than max resources 03/01/23 11:50:17.89
STEP: Deleting a LimitRange 03/01/23 11:50:17.894
STEP: Verifying the LimitRange was deleted 03/01/23 11:50:17.913
Mar  1 11:50:22.919: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 03/01/23 11:50:22.919
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:187
Mar  1 11:50:22.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-9429" for this suite. 03/01/23 11:50:22.943
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","completed":47,"skipped":923,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.272 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:50:15.68
    Mar  1 11:50:15.680: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename limitrange 03/01/23 11:50:15.68
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:50:15.704
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:50:15.707
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:57
    STEP: Creating a LimitRange 03/01/23 11:50:15.711
    STEP: Setting up watch 03/01/23 11:50:15.711
    STEP: Submitting a LimitRange 03/01/23 11:50:15.816
    STEP: Verifying LimitRange creation was observed 03/01/23 11:50:15.825
    STEP: Fetching the LimitRange to ensure it has proper values 03/01/23 11:50:15.825
    Mar  1 11:50:15.829: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Mar  1 11:50:15.829: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 03/01/23 11:50:15.829
    STEP: Ensuring Pod has resource requirements applied from LimitRange 03/01/23 11:50:15.838
    Mar  1 11:50:15.844: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Mar  1 11:50:15.844: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 03/01/23 11:50:15.844
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 03/01/23 11:50:15.853
    Mar  1 11:50:15.858: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Mar  1 11:50:15.858: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 03/01/23 11:50:15.859
    STEP: Failing to create a Pod with more than max resources 03/01/23 11:50:15.861
    STEP: Updating a LimitRange 03/01/23 11:50:15.865
    STEP: Verifying LimitRange updating is effective 03/01/23 11:50:15.871
    STEP: Creating a Pod with less than former min resources 03/01/23 11:50:17.878
    STEP: Failing to create a Pod with more than max resources 03/01/23 11:50:17.89
    STEP: Deleting a LimitRange 03/01/23 11:50:17.894
    STEP: Verifying the LimitRange was deleted 03/01/23 11:50:17.913
    Mar  1 11:50:22.919: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 03/01/23 11:50:22.919
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:187
    Mar  1 11:50:22.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "limitrange-9429" for this suite. 03/01/23 11:50:22.943
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:50:22.955
Mar  1 11:50:22.955: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename kubelet-test 03/01/23 11:50:22.956
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:50:22.978
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:50:22.982
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Mar  1 11:50:23.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-329" for this suite. 03/01/23 11:50:23.015
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","completed":48,"skipped":937,"failed":0}
------------------------------
â€¢ [0.070 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:50:22.955
    Mar  1 11:50:22.955: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename kubelet-test 03/01/23 11:50:22.956
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:50:22.978
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:50:22.982
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Mar  1 11:50:23.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-329" for this suite. 03/01/23 11:50:23.015
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:50:23.032
Mar  1 11:50:23.032: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename custom-resource-definition 03/01/23 11:50:23.033
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:50:23.054
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:50:23.058
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 03/01/23 11:50:23.061
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 03/01/23 11:50:23.063
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 03/01/23 11:50:23.063
STEP: fetching the /apis/apiextensions.k8s.io discovery document 03/01/23 11:50:23.063
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 03/01/23 11:50:23.065
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 03/01/23 11:50:23.065
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 03/01/23 11:50:23.066
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  1 11:50:23.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6586" for this suite. 03/01/23 11:50:23.073
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","completed":49,"skipped":996,"failed":0}
------------------------------
â€¢ [0.053 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:50:23.032
    Mar  1 11:50:23.032: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename custom-resource-definition 03/01/23 11:50:23.033
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:50:23.054
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:50:23.058
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 03/01/23 11:50:23.061
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 03/01/23 11:50:23.063
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 03/01/23 11:50:23.063
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 03/01/23 11:50:23.063
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 03/01/23 11:50:23.065
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 03/01/23 11:50:23.065
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 03/01/23 11:50:23.066
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  1 11:50:23.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-6586" for this suite. 03/01/23 11:50:23.073
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:50:23.089
Mar  1 11:50:23.090: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename container-lifecycle-hook 03/01/23 11:50:23.09
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:50:23.115
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:50:23.119
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 03/01/23 11:50:23.131
Mar  1 11:50:23.141: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-1072" to be "running and ready"
Mar  1 11:50:23.147: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 6.221347ms
Mar  1 11:50:23.147: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar  1 11:50:25.153: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.01226792s
Mar  1 11:50:25.153: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Mar  1 11:50:25.153: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
STEP: create the pod with lifecycle hook 03/01/23 11:50:25.16
Mar  1 11:50:25.169: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-1072" to be "running and ready"
Mar  1 11:50:25.176: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 6.829442ms
Mar  1 11:50:25.176: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Mar  1 11:50:27.182: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.013471007s
Mar  1 11:50:27.182: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Mar  1 11:50:27.183: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 03/01/23 11:50:27.187
STEP: delete the pod with lifecycle hook 03/01/23 11:50:27.206
Mar  1 11:50:27.217: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  1 11:50:27.233: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  1 11:50:29.234: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  1 11:50:29.239: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  1 11:50:31.234: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  1 11:50:31.240: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Mar  1 11:50:31.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1072" for this suite. 03/01/23 11:50:31.25
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","completed":50,"skipped":1027,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.170 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:50:23.089
    Mar  1 11:50:23.090: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename container-lifecycle-hook 03/01/23 11:50:23.09
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:50:23.115
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:50:23.119
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 03/01/23 11:50:23.131
    Mar  1 11:50:23.141: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-1072" to be "running and ready"
    Mar  1 11:50:23.147: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 6.221347ms
    Mar  1 11:50:23.147: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Mar  1 11:50:25.153: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.01226792s
    Mar  1 11:50:25.153: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Mar  1 11:50:25.153: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:97
    STEP: create the pod with lifecycle hook 03/01/23 11:50:25.16
    Mar  1 11:50:25.169: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-1072" to be "running and ready"
    Mar  1 11:50:25.176: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 6.829442ms
    Mar  1 11:50:25.176: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Mar  1 11:50:27.182: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.013471007s
    Mar  1 11:50:27.182: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Mar  1 11:50:27.183: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 03/01/23 11:50:27.187
    STEP: delete the pod with lifecycle hook 03/01/23 11:50:27.206
    Mar  1 11:50:27.217: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Mar  1 11:50:27.233: INFO: Pod pod-with-poststart-exec-hook still exists
    Mar  1 11:50:29.234: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Mar  1 11:50:29.239: INFO: Pod pod-with-poststart-exec-hook still exists
    Mar  1 11:50:31.234: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Mar  1 11:50:31.240: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Mar  1 11:50:31.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-1072" for this suite. 03/01/23 11:50:31.25
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:50:31.263
Mar  1 11:50:31.263: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename daemonsets 03/01/23 11:50:31.264
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:50:31.286
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:50:31.289
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
Mar  1 11:50:31.322: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 03/01/23 11:50:31.33
Mar  1 11:50:31.335: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 11:50:31.335: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 11:50:31.335: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 11:50:31.343: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  1 11:50:31.343: INFO: Node lab1-k8s-node-1 is running 0 daemon pod, expected 1
Mar  1 11:50:32.355: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 11:50:32.356: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 11:50:32.356: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 11:50:32.361: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar  1 11:50:32.361: INFO: Node lab1-k8s-node-2 is running 0 daemon pod, expected 1
Mar  1 11:50:33.352: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 11:50:33.352: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 11:50:33.353: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 11:50:33.359: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar  1 11:50:33.359: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Update daemon pods image. 03/01/23 11:50:33.379
STEP: Check that daemon pods images are updated. 03/01/23 11:50:33.396
Mar  1 11:50:33.401: INFO: Wrong image for pod: daemon-set-4skff. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar  1 11:50:33.401: INFO: Wrong image for pod: daemon-set-cd6jv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar  1 11:50:33.401: INFO: Wrong image for pod: daemon-set-ttnd2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar  1 11:50:33.411: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 11:50:33.411: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 11:50:33.411: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 11:50:34.417: INFO: Wrong image for pod: daemon-set-4skff. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar  1 11:50:34.417: INFO: Wrong image for pod: daemon-set-ttnd2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar  1 11:50:34.424: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 11:50:34.424: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 11:50:34.424: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 11:50:35.417: INFO: Wrong image for pod: daemon-set-4skff. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar  1 11:50:35.417: INFO: Pod daemon-set-99hzb is not available
Mar  1 11:50:35.417: INFO: Wrong image for pod: daemon-set-ttnd2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar  1 11:50:35.423: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 11:50:35.423: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 11:50:35.423: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 11:50:36.417: INFO: Wrong image for pod: daemon-set-ttnd2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar  1 11:50:36.427: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 11:50:36.427: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 11:50:36.427: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 11:50:37.417: INFO: Wrong image for pod: daemon-set-ttnd2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar  1 11:50:37.424: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 11:50:37.424: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 11:50:37.424: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 11:50:38.418: INFO: Pod daemon-set-kfgpz is not available
Mar  1 11:50:38.418: INFO: Wrong image for pod: daemon-set-ttnd2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar  1 11:50:38.424: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 11:50:38.424: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 11:50:38.424: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 11:50:39.417: INFO: Pod daemon-set-wc5xk is not available
Mar  1 11:50:39.425: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 11:50:39.426: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 11:50:39.426: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster. 03/01/23 11:50:39.426
Mar  1 11:50:39.432: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 11:50:39.432: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 11:50:39.432: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 11:50:39.438: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar  1 11:50:39.438: INFO: Node lab1-k8s-node-3 is running 0 daemon pod, expected 1
Mar  1 11:50:40.446: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 11:50:40.446: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 11:50:40.446: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 11:50:40.451: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar  1 11:50:40.451: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 03/01/23 11:50:40.474
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5265, will wait for the garbage collector to delete the pods 03/01/23 11:50:40.474
Mar  1 11:50:40.538: INFO: Deleting DaemonSet.extensions daemon-set took: 9.401193ms
Mar  1 11:50:40.639: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.567305ms
Mar  1 11:50:43.446: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  1 11:50:43.446: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar  1 11:50:43.450: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"7593"},"items":null}

Mar  1 11:50:43.454: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"7593"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Mar  1 11:50:43.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5265" for this suite. 03/01/23 11:50:43.478
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","completed":51,"skipped":1051,"failed":0}
------------------------------
â€¢ [SLOW TEST] [12.224 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:50:31.263
    Mar  1 11:50:31.263: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename daemonsets 03/01/23 11:50:31.264
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:50:31.286
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:50:31.289
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:373
    Mar  1 11:50:31.322: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 03/01/23 11:50:31.33
    Mar  1 11:50:31.335: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 11:50:31.335: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 11:50:31.335: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 11:50:31.343: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  1 11:50:31.343: INFO: Node lab1-k8s-node-1 is running 0 daemon pod, expected 1
    Mar  1 11:50:32.355: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 11:50:32.356: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 11:50:32.356: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 11:50:32.361: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar  1 11:50:32.361: INFO: Node lab1-k8s-node-2 is running 0 daemon pod, expected 1
    Mar  1 11:50:33.352: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 11:50:33.352: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 11:50:33.353: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 11:50:33.359: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar  1 11:50:33.359: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Update daemon pods image. 03/01/23 11:50:33.379
    STEP: Check that daemon pods images are updated. 03/01/23 11:50:33.396
    Mar  1 11:50:33.401: INFO: Wrong image for pod: daemon-set-4skff. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar  1 11:50:33.401: INFO: Wrong image for pod: daemon-set-cd6jv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar  1 11:50:33.401: INFO: Wrong image for pod: daemon-set-ttnd2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar  1 11:50:33.411: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 11:50:33.411: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 11:50:33.411: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 11:50:34.417: INFO: Wrong image for pod: daemon-set-4skff. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar  1 11:50:34.417: INFO: Wrong image for pod: daemon-set-ttnd2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar  1 11:50:34.424: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 11:50:34.424: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 11:50:34.424: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 11:50:35.417: INFO: Wrong image for pod: daemon-set-4skff. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar  1 11:50:35.417: INFO: Pod daemon-set-99hzb is not available
    Mar  1 11:50:35.417: INFO: Wrong image for pod: daemon-set-ttnd2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar  1 11:50:35.423: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 11:50:35.423: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 11:50:35.423: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 11:50:36.417: INFO: Wrong image for pod: daemon-set-ttnd2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar  1 11:50:36.427: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 11:50:36.427: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 11:50:36.427: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 11:50:37.417: INFO: Wrong image for pod: daemon-set-ttnd2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar  1 11:50:37.424: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 11:50:37.424: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 11:50:37.424: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 11:50:38.418: INFO: Pod daemon-set-kfgpz is not available
    Mar  1 11:50:38.418: INFO: Wrong image for pod: daemon-set-ttnd2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar  1 11:50:38.424: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 11:50:38.424: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 11:50:38.424: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 11:50:39.417: INFO: Pod daemon-set-wc5xk is not available
    Mar  1 11:50:39.425: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 11:50:39.426: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 11:50:39.426: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    STEP: Check that daemon pods are still running on every node of the cluster. 03/01/23 11:50:39.426
    Mar  1 11:50:39.432: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 11:50:39.432: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 11:50:39.432: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 11:50:39.438: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar  1 11:50:39.438: INFO: Node lab1-k8s-node-3 is running 0 daemon pod, expected 1
    Mar  1 11:50:40.446: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 11:50:40.446: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 11:50:40.446: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 11:50:40.451: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar  1 11:50:40.451: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 03/01/23 11:50:40.474
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5265, will wait for the garbage collector to delete the pods 03/01/23 11:50:40.474
    Mar  1 11:50:40.538: INFO: Deleting DaemonSet.extensions daemon-set took: 9.401193ms
    Mar  1 11:50:40.639: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.567305ms
    Mar  1 11:50:43.446: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  1 11:50:43.446: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Mar  1 11:50:43.450: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"7593"},"items":null}

    Mar  1 11:50:43.454: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"7593"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Mar  1 11:50:43.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-5265" for this suite. 03/01/23 11:50:43.478
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:50:43.491
Mar  1 11:50:43.491: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename configmap 03/01/23 11:50:43.492
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:50:43.514
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:50:43.518
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Mar  1 11:50:43.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2326" for this suite. 03/01/23 11:50:43.578
{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","completed":52,"skipped":1072,"failed":0}
------------------------------
â€¢ [0.096 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:50:43.491
    Mar  1 11:50:43.491: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename configmap 03/01/23 11:50:43.492
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:50:43.514
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:50:43.518
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:503
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Mar  1 11:50:43.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-2326" for this suite. 03/01/23 11:50:43.578
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:50:43.589
Mar  1 11:50:43.589: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename secrets 03/01/23 11:50:43.59
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:50:43.616
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:50:43.62
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Mar  1 11:50:43.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5838" for this suite. 03/01/23 11:50:43.68
{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","completed":53,"skipped":1089,"failed":0}
------------------------------
â€¢ [0.101 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:50:43.589
    Mar  1 11:50:43.589: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename secrets 03/01/23 11:50:43.59
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:50:43.616
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:50:43.62
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:385
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Mar  1 11:50:43.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-5838" for this suite. 03/01/23 11:50:43.68
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:50:43.694
Mar  1 11:50:43.694: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename services 03/01/23 11:50:43.695
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:50:43.718
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:50:43.721
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
STEP: creating a service externalname-service with the type=ExternalName in namespace services-4079 03/01/23 11:50:43.725
STEP: changing the ExternalName service to type=NodePort 03/01/23 11:50:43.732
STEP: creating replication controller externalname-service in namespace services-4079 03/01/23 11:50:43.766
I0301 11:50:43.774174      19 runners.go:193] Created replication controller with name: externalname-service, namespace: services-4079, replica count: 2
I0301 11:50:46.826117      19 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  1 11:50:46.826: INFO: Creating new exec pod
Mar  1 11:50:46.838: INFO: Waiting up to 5m0s for pod "execpodc6qh9" in namespace "services-4079" to be "running"
Mar  1 11:50:46.842: INFO: Pod "execpodc6qh9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.334516ms
Mar  1 11:50:48.849: INFO: Pod "execpodc6qh9": Phase="Running", Reason="", readiness=true. Elapsed: 2.010882452s
Mar  1 11:50:48.849: INFO: Pod "execpodc6qh9" satisfied condition "running"
Mar  1 11:50:49.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-4079 exec execpodc6qh9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Mar  1 11:50:50.001: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar  1 11:50:50.001: INFO: stdout: ""
Mar  1 11:50:51.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-4079 exec execpodc6qh9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Mar  1 11:50:51.141: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar  1 11:50:51.141: INFO: stdout: "externalname-service-77r44"
Mar  1 11:50:51.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-4079 exec execpodc6qh9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.42.92 80'
Mar  1 11:50:51.265: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.42.92 80\nConnection to 10.233.42.92 80 port [tcp/http] succeeded!\n"
Mar  1 11:50:51.266: INFO: stdout: "externalname-service-77r44"
Mar  1 11:50:51.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-4079 exec execpodc6qh9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.128.2.241 30328'
Mar  1 11:50:51.402: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.128.2.241 30328\nConnection to 10.128.2.241 30328 port [tcp/*] succeeded!\n"
Mar  1 11:50:51.402: INFO: stdout: "externalname-service-x5mv8"
Mar  1 11:50:51.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-4079 exec execpodc6qh9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.128.0.178 30328'
Mar  1 11:50:51.535: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.128.0.178 30328\nConnection to 10.128.0.178 30328 port [tcp/*] succeeded!\n"
Mar  1 11:50:51.536: INFO: stdout: "externalname-service-x5mv8"
Mar  1 11:50:51.536: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar  1 11:50:51.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4079" for this suite. 03/01/23 11:50:51.582
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","completed":54,"skipped":1091,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.898 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:50:43.694
    Mar  1 11:50:43.694: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename services 03/01/23 11:50:43.695
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:50:43.718
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:50:43.721
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1443
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-4079 03/01/23 11:50:43.725
    STEP: changing the ExternalName service to type=NodePort 03/01/23 11:50:43.732
    STEP: creating replication controller externalname-service in namespace services-4079 03/01/23 11:50:43.766
    I0301 11:50:43.774174      19 runners.go:193] Created replication controller with name: externalname-service, namespace: services-4079, replica count: 2
    I0301 11:50:46.826117      19 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar  1 11:50:46.826: INFO: Creating new exec pod
    Mar  1 11:50:46.838: INFO: Waiting up to 5m0s for pod "execpodc6qh9" in namespace "services-4079" to be "running"
    Mar  1 11:50:46.842: INFO: Pod "execpodc6qh9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.334516ms
    Mar  1 11:50:48.849: INFO: Pod "execpodc6qh9": Phase="Running", Reason="", readiness=true. Elapsed: 2.010882452s
    Mar  1 11:50:48.849: INFO: Pod "execpodc6qh9" satisfied condition "running"
    Mar  1 11:50:49.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-4079 exec execpodc6qh9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Mar  1 11:50:50.001: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Mar  1 11:50:50.001: INFO: stdout: ""
    Mar  1 11:50:51.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-4079 exec execpodc6qh9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Mar  1 11:50:51.141: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Mar  1 11:50:51.141: INFO: stdout: "externalname-service-77r44"
    Mar  1 11:50:51.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-4079 exec execpodc6qh9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.42.92 80'
    Mar  1 11:50:51.265: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.42.92 80\nConnection to 10.233.42.92 80 port [tcp/http] succeeded!\n"
    Mar  1 11:50:51.266: INFO: stdout: "externalname-service-77r44"
    Mar  1 11:50:51.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-4079 exec execpodc6qh9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.128.2.241 30328'
    Mar  1 11:50:51.402: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.128.2.241 30328\nConnection to 10.128.2.241 30328 port [tcp/*] succeeded!\n"
    Mar  1 11:50:51.402: INFO: stdout: "externalname-service-x5mv8"
    Mar  1 11:50:51.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-4079 exec execpodc6qh9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.128.0.178 30328'
    Mar  1 11:50:51.535: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.128.0.178 30328\nConnection to 10.128.0.178 30328 port [tcp/*] succeeded!\n"
    Mar  1 11:50:51.536: INFO: stdout: "externalname-service-x5mv8"
    Mar  1 11:50:51.536: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar  1 11:50:51.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4079" for this suite. 03/01/23 11:50:51.582
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1810
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:50:51.592
Mar  1 11:50:51.592: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename kubectl 03/01/23 11:50:51.594
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:50:51.617
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:50:51.621
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1810
STEP: Starting the proxy 03/01/23 11:50:51.624
Mar  1 11:50:51.624: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-9967 proxy --unix-socket=/tmp/kubectl-proxy-unix804811638/test'
STEP: retrieving proxy /api/ output 03/01/23 11:50:51.666
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar  1 11:50:51.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9967" for this suite. 03/01/23 11:50:51.674
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","completed":55,"skipped":1092,"failed":0}
------------------------------
â€¢ [0.092 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:50:51.592
    Mar  1 11:50:51.592: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename kubectl 03/01/23 11:50:51.594
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:50:51.617
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:50:51.621
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1810
    STEP: Starting the proxy 03/01/23 11:50:51.624
    Mar  1 11:50:51.624: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-9967 proxy --unix-socket=/tmp/kubectl-proxy-unix804811638/test'
    STEP: retrieving proxy /api/ output 03/01/23 11:50:51.666
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar  1 11:50:51.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-9967" for this suite. 03/01/23 11:50:51.674
  << End Captured GinkgoWriter Output
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:50:51.684
Mar  1 11:50:51.684: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename kubectl 03/01/23 11:50:51.684
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:50:51.706
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:50:51.709
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1570
STEP: creating an pod 03/01/23 11:50:51.713
Mar  1 11:50:51.713: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-387 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Mar  1 11:50:51.787: INFO: stderr: ""
Mar  1 11:50:51.787: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
STEP: Waiting for log generator to start. 03/01/23 11:50:51.787
Mar  1 11:50:51.787: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Mar  1 11:50:51.787: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-387" to be "running and ready, or succeeded"
Mar  1 11:50:51.792: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.941058ms
Mar  1 11:50:51.792: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'lab1-k8s-node-3' to be 'Running' but was 'Pending'
Mar  1 11:50:53.798: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.01117766s
Mar  1 11:50:53.798: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Mar  1 11:50:53.798: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 03/01/23 11:50:53.798
Mar  1 11:50:53.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-387 logs logs-generator logs-generator'
Mar  1 11:50:53.876: INFO: stderr: ""
Mar  1 11:50:53.876: INFO: stdout: "I0301 11:50:52.527387       1 logs_generator.go:76] 0 GET /api/v1/namespaces/kube-system/pods/qrl 263\nI0301 11:50:52.727507       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/dwtz 518\nI0301 11:50:52.928001       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/jzf 596\nI0301 11:50:53.128163       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/vp7 458\nI0301 11:50:53.327429       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/kube-system/pods/59n 256\nI0301 11:50:53.527801       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/r7s 342\nI0301 11:50:53.728178       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/zr9 366\n"
STEP: limiting log lines 03/01/23 11:50:53.876
Mar  1 11:50:53.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-387 logs logs-generator logs-generator --tail=1'
Mar  1 11:50:53.945: INFO: stderr: ""
Mar  1 11:50:53.945: INFO: stdout: "I0301 11:50:53.927479       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/bj9 428\n"
Mar  1 11:50:53.945: INFO: got output "I0301 11:50:53.927479       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/bj9 428\n"
STEP: limiting log bytes 03/01/23 11:50:53.945
Mar  1 11:50:53.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-387 logs logs-generator logs-generator --limit-bytes=1'
Mar  1 11:50:54.032: INFO: stderr: ""
Mar  1 11:50:54.032: INFO: stdout: "I"
Mar  1 11:50:54.032: INFO: got output "I"
STEP: exposing timestamps 03/01/23 11:50:54.032
Mar  1 11:50:54.033: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-387 logs logs-generator logs-generator --tail=1 --timestamps'
Mar  1 11:50:54.107: INFO: stderr: ""
Mar  1 11:50:54.107: INFO: stdout: "2023-03-01T11:50:53.927590554Z I0301 11:50:53.927479       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/bj9 428\n"
Mar  1 11:50:54.107: INFO: got output "2023-03-01T11:50:53.927590554Z I0301 11:50:53.927479       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/bj9 428\n"
STEP: restricting to a time range 03/01/23 11:50:54.107
Mar  1 11:50:56.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-387 logs logs-generator logs-generator --since=1s'
Mar  1 11:50:56.685: INFO: stderr: ""
Mar  1 11:50:56.685: INFO: stdout: "I0301 11:50:55.728469       1 logs_generator.go:76] 16 POST /api/v1/namespaces/default/pods/g6nr 222\nI0301 11:50:55.927812       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/dlfj 483\nI0301 11:50:56.128151       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/mfzb 282\nI0301 11:50:56.327484       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/xfz5 496\nI0301 11:50:56.527789       1 logs_generator.go:76] 20 GET /api/v1/namespaces/default/pods/7ggr 404\n"
Mar  1 11:50:56.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-387 logs logs-generator logs-generator --since=24h'
Mar  1 11:50:56.780: INFO: stderr: ""
Mar  1 11:50:56.780: INFO: stdout: "I0301 11:50:52.527387       1 logs_generator.go:76] 0 GET /api/v1/namespaces/kube-system/pods/qrl 263\nI0301 11:50:52.727507       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/dwtz 518\nI0301 11:50:52.928001       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/jzf 596\nI0301 11:50:53.128163       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/vp7 458\nI0301 11:50:53.327429       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/kube-system/pods/59n 256\nI0301 11:50:53.527801       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/r7s 342\nI0301 11:50:53.728178       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/zr9 366\nI0301 11:50:53.927479       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/bj9 428\nI0301 11:50:54.127808       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/gsr2 203\nI0301 11:50:54.328127       1 logs_generator.go:76] 9 GET /api/v1/namespaces/default/pods/ndl 468\nI0301 11:50:54.528481       1 logs_generator.go:76] 10 GET /api/v1/namespaces/kube-system/pods/ppsx 489\nI0301 11:50:54.727799       1 logs_generator.go:76] 11 GET /api/v1/namespaces/ns/pods/q8jv 532\nI0301 11:50:54.928149       1 logs_generator.go:76] 12 POST /api/v1/namespaces/default/pods/ljq2 350\nI0301 11:50:55.128486       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/default/pods/4ptn 407\nI0301 11:50:55.327810       1 logs_generator.go:76] 14 POST /api/v1/namespaces/ns/pods/mf9r 492\nI0301 11:50:55.528123       1 logs_generator.go:76] 15 POST /api/v1/namespaces/ns/pods/659t 568\nI0301 11:50:55.728469       1 logs_generator.go:76] 16 POST /api/v1/namespaces/default/pods/g6nr 222\nI0301 11:50:55.927812       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/dlfj 483\nI0301 11:50:56.128151       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/mfzb 282\nI0301 11:50:56.327484       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/xfz5 496\nI0301 11:50:56.527789       1 logs_generator.go:76] 20 GET /api/v1/namespaces/default/pods/7ggr 404\nI0301 11:50:56.728094       1 logs_generator.go:76] 21 GET /api/v1/namespaces/default/pods/m28 250\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1575
Mar  1 11:50:56.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-387 delete pod logs-generator'
Mar  1 11:50:57.396: INFO: stderr: ""
Mar  1 11:50:57.396: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar  1 11:50:57.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-387" for this suite. 03/01/23 11:50:57.403
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","completed":56,"skipped":1092,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.728 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1567
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1590

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:50:51.684
    Mar  1 11:50:51.684: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename kubectl 03/01/23 11:50:51.684
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:50:51.706
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:50:51.709
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1570
    STEP: creating an pod 03/01/23 11:50:51.713
    Mar  1 11:50:51.713: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-387 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Mar  1 11:50:51.787: INFO: stderr: ""
    Mar  1 11:50:51.787: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1590
    STEP: Waiting for log generator to start. 03/01/23 11:50:51.787
    Mar  1 11:50:51.787: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Mar  1 11:50:51.787: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-387" to be "running and ready, or succeeded"
    Mar  1 11:50:51.792: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.941058ms
    Mar  1 11:50:51.792: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'lab1-k8s-node-3' to be 'Running' but was 'Pending'
    Mar  1 11:50:53.798: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.01117766s
    Mar  1 11:50:53.798: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Mar  1 11:50:53.798: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 03/01/23 11:50:53.798
    Mar  1 11:50:53.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-387 logs logs-generator logs-generator'
    Mar  1 11:50:53.876: INFO: stderr: ""
    Mar  1 11:50:53.876: INFO: stdout: "I0301 11:50:52.527387       1 logs_generator.go:76] 0 GET /api/v1/namespaces/kube-system/pods/qrl 263\nI0301 11:50:52.727507       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/dwtz 518\nI0301 11:50:52.928001       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/jzf 596\nI0301 11:50:53.128163       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/vp7 458\nI0301 11:50:53.327429       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/kube-system/pods/59n 256\nI0301 11:50:53.527801       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/r7s 342\nI0301 11:50:53.728178       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/zr9 366\n"
    STEP: limiting log lines 03/01/23 11:50:53.876
    Mar  1 11:50:53.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-387 logs logs-generator logs-generator --tail=1'
    Mar  1 11:50:53.945: INFO: stderr: ""
    Mar  1 11:50:53.945: INFO: stdout: "I0301 11:50:53.927479       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/bj9 428\n"
    Mar  1 11:50:53.945: INFO: got output "I0301 11:50:53.927479       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/bj9 428\n"
    STEP: limiting log bytes 03/01/23 11:50:53.945
    Mar  1 11:50:53.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-387 logs logs-generator logs-generator --limit-bytes=1'
    Mar  1 11:50:54.032: INFO: stderr: ""
    Mar  1 11:50:54.032: INFO: stdout: "I"
    Mar  1 11:50:54.032: INFO: got output "I"
    STEP: exposing timestamps 03/01/23 11:50:54.032
    Mar  1 11:50:54.033: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-387 logs logs-generator logs-generator --tail=1 --timestamps'
    Mar  1 11:50:54.107: INFO: stderr: ""
    Mar  1 11:50:54.107: INFO: stdout: "2023-03-01T11:50:53.927590554Z I0301 11:50:53.927479       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/bj9 428\n"
    Mar  1 11:50:54.107: INFO: got output "2023-03-01T11:50:53.927590554Z I0301 11:50:53.927479       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/bj9 428\n"
    STEP: restricting to a time range 03/01/23 11:50:54.107
    Mar  1 11:50:56.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-387 logs logs-generator logs-generator --since=1s'
    Mar  1 11:50:56.685: INFO: stderr: ""
    Mar  1 11:50:56.685: INFO: stdout: "I0301 11:50:55.728469       1 logs_generator.go:76] 16 POST /api/v1/namespaces/default/pods/g6nr 222\nI0301 11:50:55.927812       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/dlfj 483\nI0301 11:50:56.128151       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/mfzb 282\nI0301 11:50:56.327484       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/xfz5 496\nI0301 11:50:56.527789       1 logs_generator.go:76] 20 GET /api/v1/namespaces/default/pods/7ggr 404\n"
    Mar  1 11:50:56.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-387 logs logs-generator logs-generator --since=24h'
    Mar  1 11:50:56.780: INFO: stderr: ""
    Mar  1 11:50:56.780: INFO: stdout: "I0301 11:50:52.527387       1 logs_generator.go:76] 0 GET /api/v1/namespaces/kube-system/pods/qrl 263\nI0301 11:50:52.727507       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/dwtz 518\nI0301 11:50:52.928001       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/jzf 596\nI0301 11:50:53.128163       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/vp7 458\nI0301 11:50:53.327429       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/kube-system/pods/59n 256\nI0301 11:50:53.527801       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/r7s 342\nI0301 11:50:53.728178       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/zr9 366\nI0301 11:50:53.927479       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/bj9 428\nI0301 11:50:54.127808       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/gsr2 203\nI0301 11:50:54.328127       1 logs_generator.go:76] 9 GET /api/v1/namespaces/default/pods/ndl 468\nI0301 11:50:54.528481       1 logs_generator.go:76] 10 GET /api/v1/namespaces/kube-system/pods/ppsx 489\nI0301 11:50:54.727799       1 logs_generator.go:76] 11 GET /api/v1/namespaces/ns/pods/q8jv 532\nI0301 11:50:54.928149       1 logs_generator.go:76] 12 POST /api/v1/namespaces/default/pods/ljq2 350\nI0301 11:50:55.128486       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/default/pods/4ptn 407\nI0301 11:50:55.327810       1 logs_generator.go:76] 14 POST /api/v1/namespaces/ns/pods/mf9r 492\nI0301 11:50:55.528123       1 logs_generator.go:76] 15 POST /api/v1/namespaces/ns/pods/659t 568\nI0301 11:50:55.728469       1 logs_generator.go:76] 16 POST /api/v1/namespaces/default/pods/g6nr 222\nI0301 11:50:55.927812       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/dlfj 483\nI0301 11:50:56.128151       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/mfzb 282\nI0301 11:50:56.327484       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/xfz5 496\nI0301 11:50:56.527789       1 logs_generator.go:76] 20 GET /api/v1/namespaces/default/pods/7ggr 404\nI0301 11:50:56.728094       1 logs_generator.go:76] 21 GET /api/v1/namespaces/default/pods/m28 250\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1575
    Mar  1 11:50:56.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-387 delete pod logs-generator'
    Mar  1 11:50:57.396: INFO: stderr: ""
    Mar  1 11:50:57.396: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar  1 11:50:57.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-387" for this suite. 03/01/23 11:50:57.403
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:50:57.412
Mar  1 11:50:57.412: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename disruption 03/01/23 11:50:57.413
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:50:57.434
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:50:57.437
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
STEP: creating the pdb 03/01/23 11:50:57.44
STEP: Waiting for the pdb to be processed 03/01/23 11:50:57.446
STEP: updating the pdb 03/01/23 11:50:59.459
STEP: Waiting for the pdb to be processed 03/01/23 11:50:59.471
STEP: patching the pdb 03/01/23 11:50:59.48
STEP: Waiting for the pdb to be processed 03/01/23 11:50:59.491
STEP: Waiting for the pdb to be deleted 03/01/23 11:51:01.512
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Mar  1 11:51:01.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-5026" for this suite. 03/01/23 11:51:01.525
{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","completed":57,"skipped":1100,"failed":0}
------------------------------
â€¢ [4.121 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:50:57.412
    Mar  1 11:50:57.412: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename disruption 03/01/23 11:50:57.413
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:50:57.434
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:50:57.437
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:107
    STEP: creating the pdb 03/01/23 11:50:57.44
    STEP: Waiting for the pdb to be processed 03/01/23 11:50:57.446
    STEP: updating the pdb 03/01/23 11:50:59.459
    STEP: Waiting for the pdb to be processed 03/01/23 11:50:59.471
    STEP: patching the pdb 03/01/23 11:50:59.48
    STEP: Waiting for the pdb to be processed 03/01/23 11:50:59.491
    STEP: Waiting for the pdb to be deleted 03/01/23 11:51:01.512
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Mar  1 11:51:01.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-5026" for this suite. 03/01/23 11:51:01.525
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:51:01.534
Mar  1 11:51:01.534: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename endpointslice 03/01/23 11:51:01.534
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:51:01.557
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:51:01.56
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
STEP: getting /apis 03/01/23 11:51:01.563
STEP: getting /apis/discovery.k8s.io 03/01/23 11:51:01.566
STEP: getting /apis/discovery.k8s.iov1 03/01/23 11:51:01.567
STEP: creating 03/01/23 11:51:01.568
STEP: getting 03/01/23 11:51:01.591
STEP: listing 03/01/23 11:51:01.595
STEP: watching 03/01/23 11:51:01.6
Mar  1 11:51:01.600: INFO: starting watch
STEP: cluster-wide listing 03/01/23 11:51:01.602
STEP: cluster-wide watching 03/01/23 11:51:01.606
Mar  1 11:51:01.606: INFO: starting watch
STEP: patching 03/01/23 11:51:01.607
STEP: updating 03/01/23 11:51:01.615
Mar  1 11:51:01.626: INFO: waiting for watch events with expected annotations
Mar  1 11:51:01.626: INFO: saw patched and updated annotations
STEP: deleting 03/01/23 11:51:01.626
STEP: deleting a collection 03/01/23 11:51:01.644
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Mar  1 11:51:01.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-4354" for this suite. 03/01/23 11:51:01.673
{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","completed":58,"skipped":1100,"failed":0}
------------------------------
â€¢ [0.148 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:51:01.534
    Mar  1 11:51:01.534: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename endpointslice 03/01/23 11:51:01.534
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:51:01.557
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:51:01.56
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:352
    STEP: getting /apis 03/01/23 11:51:01.563
    STEP: getting /apis/discovery.k8s.io 03/01/23 11:51:01.566
    STEP: getting /apis/discovery.k8s.iov1 03/01/23 11:51:01.567
    STEP: creating 03/01/23 11:51:01.568
    STEP: getting 03/01/23 11:51:01.591
    STEP: listing 03/01/23 11:51:01.595
    STEP: watching 03/01/23 11:51:01.6
    Mar  1 11:51:01.600: INFO: starting watch
    STEP: cluster-wide listing 03/01/23 11:51:01.602
    STEP: cluster-wide watching 03/01/23 11:51:01.606
    Mar  1 11:51:01.606: INFO: starting watch
    STEP: patching 03/01/23 11:51:01.607
    STEP: updating 03/01/23 11:51:01.615
    Mar  1 11:51:01.626: INFO: waiting for watch events with expected annotations
    Mar  1 11:51:01.626: INFO: saw patched and updated annotations
    STEP: deleting 03/01/23 11:51:01.626
    STEP: deleting a collection 03/01/23 11:51:01.644
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Mar  1 11:51:01.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-4354" for this suite. 03/01/23 11:51:01.673
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:51:01.682
Mar  1 11:51:01.682: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename pods 03/01/23 11:51:01.682
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:51:01.705
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:51:01.709
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
STEP: Create set of pods 03/01/23 11:51:01.712
Mar  1 11:51:01.722: INFO: created test-pod-1
Mar  1 11:51:01.731: INFO: created test-pod-2
Mar  1 11:51:01.740: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 03/01/23 11:51:01.741
Mar  1 11:51:01.741: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-6661' to be running and ready
Mar  1 11:51:01.770: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Mar  1 11:51:01.770: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Mar  1 11:51:01.770: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Mar  1 11:51:01.771: INFO: 0 / 3 pods in namespace 'pods-6661' are running and ready (0 seconds elapsed)
Mar  1 11:51:01.771: INFO: expected 0 pod replicas in namespace 'pods-6661', 0 are Running and Ready.
Mar  1 11:51:01.771: INFO: POD         NODE             PHASE    GRACE  CONDITIONS
Mar  1 11:51:01.771: INFO: test-pod-1  lab1-k8s-node-3  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-01 11:51:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-01 11:51:01 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-01 11:51:01 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-01 11:51:01 +0000 UTC  }]
Mar  1 11:51:01.771: INFO: test-pod-2  lab1-k8s-node-3  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-01 11:51:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-01 11:51:01 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-01 11:51:01 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-01 11:51:01 +0000 UTC  }]
Mar  1 11:51:01.771: INFO: test-pod-3  lab1-k8s-node-1  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-01 11:51:01 +0000 UTC  }]
Mar  1 11:51:01.771: INFO: 
Mar  1 11:51:03.786: INFO: 3 / 3 pods in namespace 'pods-6661' are running and ready (2 seconds elapsed)
Mar  1 11:51:03.787: INFO: expected 0 pod replicas in namespace 'pods-6661', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 03/01/23 11:51:03.815
Mar  1 11:51:03.820: INFO: Pod quantity 3 is different from expected quantity 0
Mar  1 11:51:04.826: INFO: Pod quantity 3 is different from expected quantity 0
Mar  1 11:51:05.827: INFO: Pod quantity 3 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Mar  1 11:51:06.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6661" for this suite. 03/01/23 11:51:06.833
{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","completed":59,"skipped":1108,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.171 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:51:01.682
    Mar  1 11:51:01.682: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename pods 03/01/23 11:51:01.682
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:51:01.705
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:51:01.709
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:844
    STEP: Create set of pods 03/01/23 11:51:01.712
    Mar  1 11:51:01.722: INFO: created test-pod-1
    Mar  1 11:51:01.731: INFO: created test-pod-2
    Mar  1 11:51:01.740: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 03/01/23 11:51:01.741
    Mar  1 11:51:01.741: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-6661' to be running and ready
    Mar  1 11:51:01.770: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Mar  1 11:51:01.770: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Mar  1 11:51:01.770: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Mar  1 11:51:01.771: INFO: 0 / 3 pods in namespace 'pods-6661' are running and ready (0 seconds elapsed)
    Mar  1 11:51:01.771: INFO: expected 0 pod replicas in namespace 'pods-6661', 0 are Running and Ready.
    Mar  1 11:51:01.771: INFO: POD         NODE             PHASE    GRACE  CONDITIONS
    Mar  1 11:51:01.771: INFO: test-pod-1  lab1-k8s-node-3  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-01 11:51:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-01 11:51:01 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-01 11:51:01 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-01 11:51:01 +0000 UTC  }]
    Mar  1 11:51:01.771: INFO: test-pod-2  lab1-k8s-node-3  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-01 11:51:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-01 11:51:01 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-01 11:51:01 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-01 11:51:01 +0000 UTC  }]
    Mar  1 11:51:01.771: INFO: test-pod-3  lab1-k8s-node-1  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-01 11:51:01 +0000 UTC  }]
    Mar  1 11:51:01.771: INFO: 
    Mar  1 11:51:03.786: INFO: 3 / 3 pods in namespace 'pods-6661' are running and ready (2 seconds elapsed)
    Mar  1 11:51:03.787: INFO: expected 0 pod replicas in namespace 'pods-6661', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 03/01/23 11:51:03.815
    Mar  1 11:51:03.820: INFO: Pod quantity 3 is different from expected quantity 0
    Mar  1 11:51:04.826: INFO: Pod quantity 3 is different from expected quantity 0
    Mar  1 11:51:05.827: INFO: Pod quantity 3 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Mar  1 11:51:06.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-6661" for this suite. 03/01/23 11:51:06.833
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:51:06.853
Mar  1 11:51:06.853: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename container-probe 03/01/23 11:51:06.854
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:51:06.879
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:51:06.883
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Mar  1 11:52:06.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3564" for this suite. 03/01/23 11:52:06.911
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","completed":60,"skipped":1112,"failed":0}
------------------------------
â€¢ [SLOW TEST] [60.067 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:51:06.853
    Mar  1 11:51:06.853: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename container-probe 03/01/23 11:51:06.854
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:51:06.879
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:51:06.883
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:104
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Mar  1 11:52:06.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-3564" for this suite. 03/01/23 11:52:06.911
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:52:06.926
Mar  1 11:52:06.926: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename runtimeclass 03/01/23 11:52:06.927
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:52:06.951
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:52:06.955
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 03/01/23 11:52:06.959
STEP: getting /apis/node.k8s.io 03/01/23 11:52:06.962
STEP: getting /apis/node.k8s.io/v1 03/01/23 11:52:06.964
STEP: creating 03/01/23 11:52:06.965
STEP: watching 03/01/23 11:52:06.988
Mar  1 11:52:06.988: INFO: starting watch
STEP: getting 03/01/23 11:52:06.995
STEP: listing 03/01/23 11:52:06.999
STEP: patching 03/01/23 11:52:07.005
STEP: updating 03/01/23 11:52:07.011
Mar  1 11:52:07.017: INFO: waiting for watch events with expected annotations
STEP: deleting 03/01/23 11:52:07.017
STEP: deleting a collection 03/01/23 11:52:07.037
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Mar  1 11:52:07.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-3283" for this suite. 03/01/23 11:52:07.067
{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","completed":61,"skipped":1135,"failed":0}
------------------------------
â€¢ [0.149 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:52:06.926
    Mar  1 11:52:06.926: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename runtimeclass 03/01/23 11:52:06.927
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:52:06.951
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:52:06.955
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 03/01/23 11:52:06.959
    STEP: getting /apis/node.k8s.io 03/01/23 11:52:06.962
    STEP: getting /apis/node.k8s.io/v1 03/01/23 11:52:06.964
    STEP: creating 03/01/23 11:52:06.965
    STEP: watching 03/01/23 11:52:06.988
    Mar  1 11:52:06.988: INFO: starting watch
    STEP: getting 03/01/23 11:52:06.995
    STEP: listing 03/01/23 11:52:06.999
    STEP: patching 03/01/23 11:52:07.005
    STEP: updating 03/01/23 11:52:07.011
    Mar  1 11:52:07.017: INFO: waiting for watch events with expected annotations
    STEP: deleting 03/01/23 11:52:07.017
    STEP: deleting a collection 03/01/23 11:52:07.037
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Mar  1 11:52:07.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-3283" for this suite. 03/01/23 11:52:07.067
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:52:07.075
Mar  1 11:52:07.075: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename projected 03/01/23 11:52:07.076
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:52:07.11
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:52:07.114
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
STEP: Creating a pod to test downward API volume plugin 03/01/23 11:52:07.118
Mar  1 11:52:07.128: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e0e630ef-bcc8-40b2-a069-a51139a8160a" in namespace "projected-346" to be "Succeeded or Failed"
Mar  1 11:52:07.136: INFO: Pod "downwardapi-volume-e0e630ef-bcc8-40b2-a069-a51139a8160a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.86217ms
Mar  1 11:52:09.141: INFO: Pod "downwardapi-volume-e0e630ef-bcc8-40b2-a069-a51139a8160a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013278124s
Mar  1 11:52:11.143: INFO: Pod "downwardapi-volume-e0e630ef-bcc8-40b2-a069-a51139a8160a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014664796s
STEP: Saw pod success 03/01/23 11:52:11.143
Mar  1 11:52:11.143: INFO: Pod "downwardapi-volume-e0e630ef-bcc8-40b2-a069-a51139a8160a" satisfied condition "Succeeded or Failed"
Mar  1 11:52:11.148: INFO: Trying to get logs from node lab1-k8s-node-3 pod downwardapi-volume-e0e630ef-bcc8-40b2-a069-a51139a8160a container client-container: <nil>
STEP: delete the pod 03/01/23 11:52:11.157
Mar  1 11:52:11.173: INFO: Waiting for pod downwardapi-volume-e0e630ef-bcc8-40b2-a069-a51139a8160a to disappear
Mar  1 11:52:11.177: INFO: Pod downwardapi-volume-e0e630ef-bcc8-40b2-a069-a51139a8160a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Mar  1 11:52:11.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-346" for this suite. 03/01/23 11:52:11.186
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","completed":62,"skipped":1147,"failed":0}
------------------------------
â€¢ [4.120 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:52:07.075
    Mar  1 11:52:07.075: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename projected 03/01/23 11:52:07.076
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:52:07.11
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:52:07.114
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:206
    STEP: Creating a pod to test downward API volume plugin 03/01/23 11:52:07.118
    Mar  1 11:52:07.128: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e0e630ef-bcc8-40b2-a069-a51139a8160a" in namespace "projected-346" to be "Succeeded or Failed"
    Mar  1 11:52:07.136: INFO: Pod "downwardapi-volume-e0e630ef-bcc8-40b2-a069-a51139a8160a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.86217ms
    Mar  1 11:52:09.141: INFO: Pod "downwardapi-volume-e0e630ef-bcc8-40b2-a069-a51139a8160a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013278124s
    Mar  1 11:52:11.143: INFO: Pod "downwardapi-volume-e0e630ef-bcc8-40b2-a069-a51139a8160a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014664796s
    STEP: Saw pod success 03/01/23 11:52:11.143
    Mar  1 11:52:11.143: INFO: Pod "downwardapi-volume-e0e630ef-bcc8-40b2-a069-a51139a8160a" satisfied condition "Succeeded or Failed"
    Mar  1 11:52:11.148: INFO: Trying to get logs from node lab1-k8s-node-3 pod downwardapi-volume-e0e630ef-bcc8-40b2-a069-a51139a8160a container client-container: <nil>
    STEP: delete the pod 03/01/23 11:52:11.157
    Mar  1 11:52:11.173: INFO: Waiting for pod downwardapi-volume-e0e630ef-bcc8-40b2-a069-a51139a8160a to disappear
    Mar  1 11:52:11.177: INFO: Pod downwardapi-volume-e0e630ef-bcc8-40b2-a069-a51139a8160a no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Mar  1 11:52:11.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-346" for this suite. 03/01/23 11:52:11.186
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:52:11.197
Mar  1 11:52:11.198: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename cronjob 03/01/23 11:52:11.198
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:52:11.22
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:52:11.223
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 03/01/23 11:52:11.226
STEP: Ensuring a job is scheduled 03/01/23 11:52:11.239
STEP: Ensuring exactly one is scheduled 03/01/23 11:53:01.245
STEP: Ensuring exactly one running job exists by listing jobs explicitly 03/01/23 11:53:01.252
STEP: Ensuring the job is replaced with a new one 03/01/23 11:53:01.256
STEP: Removing cronjob 03/01/23 11:54:01.263
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Mar  1 11:54:01.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-2457" for this suite. 03/01/23 11:54:01.281
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","completed":63,"skipped":1148,"failed":0}
------------------------------
â€¢ [SLOW TEST] [110.094 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:52:11.197
    Mar  1 11:52:11.198: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename cronjob 03/01/23 11:52:11.198
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:52:11.22
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:52:11.223
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 03/01/23 11:52:11.226
    STEP: Ensuring a job is scheduled 03/01/23 11:52:11.239
    STEP: Ensuring exactly one is scheduled 03/01/23 11:53:01.245
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 03/01/23 11:53:01.252
    STEP: Ensuring the job is replaced with a new one 03/01/23 11:53:01.256
    STEP: Removing cronjob 03/01/23 11:54:01.263
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Mar  1 11:54:01.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-2457" for this suite. 03/01/23 11:54:01.281
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:54:01.293
Mar  1 11:54:01.293: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename custom-resource-definition 03/01/23 11:54:01.294
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:54:01.328
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:54:01.332
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Mar  1 11:54:01.335: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  1 11:54:07.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-214" for this suite. 03/01/23 11:54:07.375
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","completed":64,"skipped":1188,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.091 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:54:01.293
    Mar  1 11:54:01.293: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename custom-resource-definition 03/01/23 11:54:01.294
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:54:01.328
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:54:01.332
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Mar  1 11:54:01.335: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  1 11:54:07.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-214" for this suite. 03/01/23 11:54:07.375
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:54:07.389
Mar  1 11:54:07.389: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename configmap 03/01/23 11:54:07.39
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:54:07.412
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:54:07.415
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
STEP: Creating configMap with name cm-test-opt-del-4c2564b9-6b16-491b-a92f-d7a3d3ddf590 03/01/23 11:54:07.425
STEP: Creating configMap with name cm-test-opt-upd-66545dd6-b06b-4d54-96cd-6d1ff1d7f797 03/01/23 11:54:07.431
STEP: Creating the pod 03/01/23 11:54:07.437
Mar  1 11:54:07.449: INFO: Waiting up to 5m0s for pod "pod-configmaps-88ba6138-7edf-44c9-9303-7c8bfa06de5e" in namespace "configmap-8063" to be "running and ready"
Mar  1 11:54:07.455: INFO: Pod "pod-configmaps-88ba6138-7edf-44c9-9303-7c8bfa06de5e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.959172ms
Mar  1 11:54:07.455: INFO: The phase of Pod pod-configmaps-88ba6138-7edf-44c9-9303-7c8bfa06de5e is Pending, waiting for it to be Running (with Ready = true)
Mar  1 11:54:09.462: INFO: Pod "pod-configmaps-88ba6138-7edf-44c9-9303-7c8bfa06de5e": Phase="Running", Reason="", readiness=true. Elapsed: 2.013419701s
Mar  1 11:54:09.463: INFO: The phase of Pod pod-configmaps-88ba6138-7edf-44c9-9303-7c8bfa06de5e is Running (Ready = true)
Mar  1 11:54:09.463: INFO: Pod "pod-configmaps-88ba6138-7edf-44c9-9303-7c8bfa06de5e" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-4c2564b9-6b16-491b-a92f-d7a3d3ddf590 03/01/23 11:54:09.507
STEP: Updating configmap cm-test-opt-upd-66545dd6-b06b-4d54-96cd-6d1ff1d7f797 03/01/23 11:54:09.516
STEP: Creating configMap with name cm-test-opt-create-b79456fb-2469-4396-b886-f36646739fee 03/01/23 11:54:09.522
STEP: waiting to observe update in volume 03/01/23 11:54:09.529
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Mar  1 11:54:11.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8063" for this suite. 03/01/23 11:54:11.583
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":65,"skipped":1216,"failed":0}
------------------------------
â€¢ [4.203 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:54:07.389
    Mar  1 11:54:07.389: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename configmap 03/01/23 11:54:07.39
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:54:07.412
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:54:07.415
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:239
    STEP: Creating configMap with name cm-test-opt-del-4c2564b9-6b16-491b-a92f-d7a3d3ddf590 03/01/23 11:54:07.425
    STEP: Creating configMap with name cm-test-opt-upd-66545dd6-b06b-4d54-96cd-6d1ff1d7f797 03/01/23 11:54:07.431
    STEP: Creating the pod 03/01/23 11:54:07.437
    Mar  1 11:54:07.449: INFO: Waiting up to 5m0s for pod "pod-configmaps-88ba6138-7edf-44c9-9303-7c8bfa06de5e" in namespace "configmap-8063" to be "running and ready"
    Mar  1 11:54:07.455: INFO: Pod "pod-configmaps-88ba6138-7edf-44c9-9303-7c8bfa06de5e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.959172ms
    Mar  1 11:54:07.455: INFO: The phase of Pod pod-configmaps-88ba6138-7edf-44c9-9303-7c8bfa06de5e is Pending, waiting for it to be Running (with Ready = true)
    Mar  1 11:54:09.462: INFO: Pod "pod-configmaps-88ba6138-7edf-44c9-9303-7c8bfa06de5e": Phase="Running", Reason="", readiness=true. Elapsed: 2.013419701s
    Mar  1 11:54:09.463: INFO: The phase of Pod pod-configmaps-88ba6138-7edf-44c9-9303-7c8bfa06de5e is Running (Ready = true)
    Mar  1 11:54:09.463: INFO: Pod "pod-configmaps-88ba6138-7edf-44c9-9303-7c8bfa06de5e" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-4c2564b9-6b16-491b-a92f-d7a3d3ddf590 03/01/23 11:54:09.507
    STEP: Updating configmap cm-test-opt-upd-66545dd6-b06b-4d54-96cd-6d1ff1d7f797 03/01/23 11:54:09.516
    STEP: Creating configMap with name cm-test-opt-create-b79456fb-2469-4396-b886-f36646739fee 03/01/23 11:54:09.522
    STEP: waiting to observe update in volume 03/01/23 11:54:09.529
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Mar  1 11:54:11.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-8063" for this suite. 03/01/23 11:54:11.583
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:54:11.592
Mar  1 11:54:11.592: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename dns 03/01/23 11:54:11.593
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:54:11.612
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:54:11.616
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 03/01/23 11:54:11.619
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1008 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1008;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1008 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1008;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1008.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1008.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1008.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1008.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1008.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1008.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1008.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1008.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1008.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1008.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1008.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1008.svc;check="$$(dig +notcp +noall +answer +search 99.16.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.16.99_udp@PTR;check="$$(dig +tcp +noall +answer +search 99.16.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.16.99_tcp@PTR;sleep 1; done
 03/01/23 11:54:11.643
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1008 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1008;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1008 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1008;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1008.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1008.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1008.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1008.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1008.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1008.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1008.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1008.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1008.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1008.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1008.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1008.svc;check="$$(dig +notcp +noall +answer +search 99.16.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.16.99_udp@PTR;check="$$(dig +tcp +noall +answer +search 99.16.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.16.99_tcp@PTR;sleep 1; done
 03/01/23 11:54:11.644
STEP: creating a pod to probe DNS 03/01/23 11:54:11.644
STEP: submitting the pod to kubernetes 03/01/23 11:54:11.644
Mar  1 11:54:11.661: INFO: Waiting up to 15m0s for pod "dns-test-3b09ac06-c956-41a4-ba18-d7f50fe8c7b3" in namespace "dns-1008" to be "running"
Mar  1 11:54:11.668: INFO: Pod "dns-test-3b09ac06-c956-41a4-ba18-d7f50fe8c7b3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.159098ms
Mar  1 11:54:13.675: INFO: Pod "dns-test-3b09ac06-c956-41a4-ba18-d7f50fe8c7b3": Phase="Running", Reason="", readiness=true. Elapsed: 2.013616355s
Mar  1 11:54:13.675: INFO: Pod "dns-test-3b09ac06-c956-41a4-ba18-d7f50fe8c7b3" satisfied condition "running"
STEP: retrieving the pod 03/01/23 11:54:13.675
STEP: looking for the results for each expected name from probers 03/01/23 11:54:13.68
Mar  1 11:54:13.686: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1008/dns-test-3b09ac06-c956-41a4-ba18-d7f50fe8c7b3: the server could not find the requested resource (get pods dns-test-3b09ac06-c956-41a4-ba18-d7f50fe8c7b3)
Mar  1 11:54:13.692: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1008/dns-test-3b09ac06-c956-41a4-ba18-d7f50fe8c7b3: the server could not find the requested resource (get pods dns-test-3b09ac06-c956-41a4-ba18-d7f50fe8c7b3)
Mar  1 11:54:13.697: INFO: Unable to read wheezy_udp@dns-test-service.dns-1008 from pod dns-1008/dns-test-3b09ac06-c956-41a4-ba18-d7f50fe8c7b3: the server could not find the requested resource (get pods dns-test-3b09ac06-c956-41a4-ba18-d7f50fe8c7b3)
Mar  1 11:54:13.702: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1008 from pod dns-1008/dns-test-3b09ac06-c956-41a4-ba18-d7f50fe8c7b3: the server could not find the requested resource (get pods dns-test-3b09ac06-c956-41a4-ba18-d7f50fe8c7b3)
Mar  1 11:54:13.708: INFO: Unable to read wheezy_udp@dns-test-service.dns-1008.svc from pod dns-1008/dns-test-3b09ac06-c956-41a4-ba18-d7f50fe8c7b3: the server could not find the requested resource (get pods dns-test-3b09ac06-c956-41a4-ba18-d7f50fe8c7b3)
Mar  1 11:54:13.712: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1008.svc from pod dns-1008/dns-test-3b09ac06-c956-41a4-ba18-d7f50fe8c7b3: the server could not find the requested resource (get pods dns-test-3b09ac06-c956-41a4-ba18-d7f50fe8c7b3)
Mar  1 11:54:13.718: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1008.svc from pod dns-1008/dns-test-3b09ac06-c956-41a4-ba18-d7f50fe8c7b3: the server could not find the requested resource (get pods dns-test-3b09ac06-c956-41a4-ba18-d7f50fe8c7b3)
Mar  1 11:54:13.723: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1008.svc from pod dns-1008/dns-test-3b09ac06-c956-41a4-ba18-d7f50fe8c7b3: the server could not find the requested resource (get pods dns-test-3b09ac06-c956-41a4-ba18-d7f50fe8c7b3)
Mar  1 11:54:13.747: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1008/dns-test-3b09ac06-c956-41a4-ba18-d7f50fe8c7b3: the server could not find the requested resource (get pods dns-test-3b09ac06-c956-41a4-ba18-d7f50fe8c7b3)
Mar  1 11:54:13.753: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1008/dns-test-3b09ac06-c956-41a4-ba18-d7f50fe8c7b3: the server could not find the requested resource (get pods dns-test-3b09ac06-c956-41a4-ba18-d7f50fe8c7b3)
Mar  1 11:54:13.758: INFO: Unable to read jessie_udp@dns-test-service.dns-1008 from pod dns-1008/dns-test-3b09ac06-c956-41a4-ba18-d7f50fe8c7b3: the server could not find the requested resource (get pods dns-test-3b09ac06-c956-41a4-ba18-d7f50fe8c7b3)
Mar  1 11:54:13.762: INFO: Unable to read jessie_tcp@dns-test-service.dns-1008 from pod dns-1008/dns-test-3b09ac06-c956-41a4-ba18-d7f50fe8c7b3: the server could not find the requested resource (get pods dns-test-3b09ac06-c956-41a4-ba18-d7f50fe8c7b3)
Mar  1 11:54:13.768: INFO: Unable to read jessie_udp@dns-test-service.dns-1008.svc from pod dns-1008/dns-test-3b09ac06-c956-41a4-ba18-d7f50fe8c7b3: the server could not find the requested resource (get pods dns-test-3b09ac06-c956-41a4-ba18-d7f50fe8c7b3)
Mar  1 11:54:13.773: INFO: Unable to read jessie_tcp@dns-test-service.dns-1008.svc from pod dns-1008/dns-test-3b09ac06-c956-41a4-ba18-d7f50fe8c7b3: the server could not find the requested resource (get pods dns-test-3b09ac06-c956-41a4-ba18-d7f50fe8c7b3)
Mar  1 11:54:13.777: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1008.svc from pod dns-1008/dns-test-3b09ac06-c956-41a4-ba18-d7f50fe8c7b3: the server could not find the requested resource (get pods dns-test-3b09ac06-c956-41a4-ba18-d7f50fe8c7b3)
Mar  1 11:54:13.796: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1008.svc from pod dns-1008/dns-test-3b09ac06-c956-41a4-ba18-d7f50fe8c7b3: the server could not find the requested resource (get pods dns-test-3b09ac06-c956-41a4-ba18-d7f50fe8c7b3)
Mar  1 11:54:13.817: INFO: Lookups using dns-1008/dns-test-3b09ac06-c956-41a4-ba18-d7f50fe8c7b3 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1008 wheezy_tcp@dns-test-service.dns-1008 wheezy_udp@dns-test-service.dns-1008.svc wheezy_tcp@dns-test-service.dns-1008.svc wheezy_udp@_http._tcp.dns-test-service.dns-1008.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1008.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1008 jessie_tcp@dns-test-service.dns-1008 jessie_udp@dns-test-service.dns-1008.svc jessie_tcp@dns-test-service.dns-1008.svc jessie_udp@_http._tcp.dns-test-service.dns-1008.svc jessie_tcp@_http._tcp.dns-test-service.dns-1008.svc]

Mar  1 11:54:18.959: INFO: DNS probes using dns-1008/dns-test-3b09ac06-c956-41a4-ba18-d7f50fe8c7b3 succeeded

STEP: deleting the pod 03/01/23 11:54:18.959
STEP: deleting the test service 03/01/23 11:54:18.976
STEP: deleting the test headless service 03/01/23 11:54:19.011
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Mar  1 11:54:19.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1008" for this suite. 03/01/23 11:54:19.038
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","completed":66,"skipped":1217,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.458 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:54:11.592
    Mar  1 11:54:11.592: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename dns 03/01/23 11:54:11.593
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:54:11.612
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:54:11.616
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 03/01/23 11:54:11.619
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1008 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1008;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1008 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1008;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1008.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1008.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1008.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1008.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1008.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1008.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1008.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1008.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1008.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1008.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1008.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1008.svc;check="$$(dig +notcp +noall +answer +search 99.16.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.16.99_udp@PTR;check="$$(dig +tcp +noall +answer +search 99.16.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.16.99_tcp@PTR;sleep 1; done
     03/01/23 11:54:11.643
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1008 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1008;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1008 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1008;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1008.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1008.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1008.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1008.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1008.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1008.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1008.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1008.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1008.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1008.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1008.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1008.svc;check="$$(dig +notcp +noall +answer +search 99.16.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.16.99_udp@PTR;check="$$(dig +tcp +noall +answer +search 99.16.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.16.99_tcp@PTR;sleep 1; done
     03/01/23 11:54:11.644
    STEP: creating a pod to probe DNS 03/01/23 11:54:11.644
    STEP: submitting the pod to kubernetes 03/01/23 11:54:11.644
    Mar  1 11:54:11.661: INFO: Waiting up to 15m0s for pod "dns-test-3b09ac06-c956-41a4-ba18-d7f50fe8c7b3" in namespace "dns-1008" to be "running"
    Mar  1 11:54:11.668: INFO: Pod "dns-test-3b09ac06-c956-41a4-ba18-d7f50fe8c7b3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.159098ms
    Mar  1 11:54:13.675: INFO: Pod "dns-test-3b09ac06-c956-41a4-ba18-d7f50fe8c7b3": Phase="Running", Reason="", readiness=true. Elapsed: 2.013616355s
    Mar  1 11:54:13.675: INFO: Pod "dns-test-3b09ac06-c956-41a4-ba18-d7f50fe8c7b3" satisfied condition "running"
    STEP: retrieving the pod 03/01/23 11:54:13.675
    STEP: looking for the results for each expected name from probers 03/01/23 11:54:13.68
    Mar  1 11:54:13.686: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1008/dns-test-3b09ac06-c956-41a4-ba18-d7f50fe8c7b3: the server could not find the requested resource (get pods dns-test-3b09ac06-c956-41a4-ba18-d7f50fe8c7b3)
    Mar  1 11:54:13.692: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1008/dns-test-3b09ac06-c956-41a4-ba18-d7f50fe8c7b3: the server could not find the requested resource (get pods dns-test-3b09ac06-c956-41a4-ba18-d7f50fe8c7b3)
    Mar  1 11:54:13.697: INFO: Unable to read wheezy_udp@dns-test-service.dns-1008 from pod dns-1008/dns-test-3b09ac06-c956-41a4-ba18-d7f50fe8c7b3: the server could not find the requested resource (get pods dns-test-3b09ac06-c956-41a4-ba18-d7f50fe8c7b3)
    Mar  1 11:54:13.702: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1008 from pod dns-1008/dns-test-3b09ac06-c956-41a4-ba18-d7f50fe8c7b3: the server could not find the requested resource (get pods dns-test-3b09ac06-c956-41a4-ba18-d7f50fe8c7b3)
    Mar  1 11:54:13.708: INFO: Unable to read wheezy_udp@dns-test-service.dns-1008.svc from pod dns-1008/dns-test-3b09ac06-c956-41a4-ba18-d7f50fe8c7b3: the server could not find the requested resource (get pods dns-test-3b09ac06-c956-41a4-ba18-d7f50fe8c7b3)
    Mar  1 11:54:13.712: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1008.svc from pod dns-1008/dns-test-3b09ac06-c956-41a4-ba18-d7f50fe8c7b3: the server could not find the requested resource (get pods dns-test-3b09ac06-c956-41a4-ba18-d7f50fe8c7b3)
    Mar  1 11:54:13.718: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1008.svc from pod dns-1008/dns-test-3b09ac06-c956-41a4-ba18-d7f50fe8c7b3: the server could not find the requested resource (get pods dns-test-3b09ac06-c956-41a4-ba18-d7f50fe8c7b3)
    Mar  1 11:54:13.723: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1008.svc from pod dns-1008/dns-test-3b09ac06-c956-41a4-ba18-d7f50fe8c7b3: the server could not find the requested resource (get pods dns-test-3b09ac06-c956-41a4-ba18-d7f50fe8c7b3)
    Mar  1 11:54:13.747: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1008/dns-test-3b09ac06-c956-41a4-ba18-d7f50fe8c7b3: the server could not find the requested resource (get pods dns-test-3b09ac06-c956-41a4-ba18-d7f50fe8c7b3)
    Mar  1 11:54:13.753: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1008/dns-test-3b09ac06-c956-41a4-ba18-d7f50fe8c7b3: the server could not find the requested resource (get pods dns-test-3b09ac06-c956-41a4-ba18-d7f50fe8c7b3)
    Mar  1 11:54:13.758: INFO: Unable to read jessie_udp@dns-test-service.dns-1008 from pod dns-1008/dns-test-3b09ac06-c956-41a4-ba18-d7f50fe8c7b3: the server could not find the requested resource (get pods dns-test-3b09ac06-c956-41a4-ba18-d7f50fe8c7b3)
    Mar  1 11:54:13.762: INFO: Unable to read jessie_tcp@dns-test-service.dns-1008 from pod dns-1008/dns-test-3b09ac06-c956-41a4-ba18-d7f50fe8c7b3: the server could not find the requested resource (get pods dns-test-3b09ac06-c956-41a4-ba18-d7f50fe8c7b3)
    Mar  1 11:54:13.768: INFO: Unable to read jessie_udp@dns-test-service.dns-1008.svc from pod dns-1008/dns-test-3b09ac06-c956-41a4-ba18-d7f50fe8c7b3: the server could not find the requested resource (get pods dns-test-3b09ac06-c956-41a4-ba18-d7f50fe8c7b3)
    Mar  1 11:54:13.773: INFO: Unable to read jessie_tcp@dns-test-service.dns-1008.svc from pod dns-1008/dns-test-3b09ac06-c956-41a4-ba18-d7f50fe8c7b3: the server could not find the requested resource (get pods dns-test-3b09ac06-c956-41a4-ba18-d7f50fe8c7b3)
    Mar  1 11:54:13.777: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1008.svc from pod dns-1008/dns-test-3b09ac06-c956-41a4-ba18-d7f50fe8c7b3: the server could not find the requested resource (get pods dns-test-3b09ac06-c956-41a4-ba18-d7f50fe8c7b3)
    Mar  1 11:54:13.796: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1008.svc from pod dns-1008/dns-test-3b09ac06-c956-41a4-ba18-d7f50fe8c7b3: the server could not find the requested resource (get pods dns-test-3b09ac06-c956-41a4-ba18-d7f50fe8c7b3)
    Mar  1 11:54:13.817: INFO: Lookups using dns-1008/dns-test-3b09ac06-c956-41a4-ba18-d7f50fe8c7b3 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1008 wheezy_tcp@dns-test-service.dns-1008 wheezy_udp@dns-test-service.dns-1008.svc wheezy_tcp@dns-test-service.dns-1008.svc wheezy_udp@_http._tcp.dns-test-service.dns-1008.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1008.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1008 jessie_tcp@dns-test-service.dns-1008 jessie_udp@dns-test-service.dns-1008.svc jessie_tcp@dns-test-service.dns-1008.svc jessie_udp@_http._tcp.dns-test-service.dns-1008.svc jessie_tcp@_http._tcp.dns-test-service.dns-1008.svc]

    Mar  1 11:54:18.959: INFO: DNS probes using dns-1008/dns-test-3b09ac06-c956-41a4-ba18-d7f50fe8c7b3 succeeded

    STEP: deleting the pod 03/01/23 11:54:18.959
    STEP: deleting the test service 03/01/23 11:54:18.976
    STEP: deleting the test headless service 03/01/23 11:54:19.011
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Mar  1 11:54:19.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-1008" for this suite. 03/01/23 11:54:19.038
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:54:19.052
Mar  1 11:54:19.052: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename kubectl 03/01/23 11:54:19.053
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:54:19.08
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:54:19.084
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1732
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 03/01/23 11:54:19.088
Mar  1 11:54:19.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-3592 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Mar  1 11:54:19.159: INFO: stderr: ""
Mar  1 11:54:19.159: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 03/01/23 11:54:19.159
STEP: verifying the pod e2e-test-httpd-pod was created 03/01/23 11:54:24.212
Mar  1 11:54:24.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-3592 get pod e2e-test-httpd-pod -o json'
Mar  1 11:54:24.273: INFO: stderr: ""
Mar  1 11:54:24.273: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"3a86c3f834c0a1d92be5a0d6cd81cf7e12b200127ffa2fbebc5c9a92fdef7b1b\",\n            \"cni.projectcalico.org/podIP\": \"10.233.95.14/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.233.95.14/32\"\n        },\n        \"creationTimestamp\": \"2023-03-01T11:54:19Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-3592\",\n        \"resourceVersion\": \"8864\",\n        \"uid\": \"e07205ec-92e1-4b75-89b1-58da6ff6cb1d\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-gzplh\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"lab1-k8s-node-1\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-gzplh\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-01T11:54:19Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-01T11:54:20Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-01T11:54:20Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-01T11:54:19Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://97988b53f0c06cb3fc048efd8a322e3be23bea1f6cf2661bde9030427082ce39\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-03-01T11:54:19Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.128.0.178\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.233.95.14\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.233.95.14\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-03-01T11:54:19Z\"\n    }\n}\n"
STEP: replace the image in the pod 03/01/23 11:54:24.274
Mar  1 11:54:24.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-3592 replace -f -'
Mar  1 11:54:24.471: INFO: stderr: ""
Mar  1 11:54:24.471: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-2 03/01/23 11:54:24.471
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1736
Mar  1 11:54:24.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-3592 delete pods e2e-test-httpd-pod'
Mar  1 11:54:26.834: INFO: stderr: ""
Mar  1 11:54:26.834: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar  1 11:54:26.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3592" for this suite. 03/01/23 11:54:26.842
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","completed":67,"skipped":1221,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.798 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1729
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1745

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:54:19.052
    Mar  1 11:54:19.052: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename kubectl 03/01/23 11:54:19.053
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:54:19.08
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:54:19.084
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1732
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1745
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 03/01/23 11:54:19.088
    Mar  1 11:54:19.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-3592 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Mar  1 11:54:19.159: INFO: stderr: ""
    Mar  1 11:54:19.159: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 03/01/23 11:54:19.159
    STEP: verifying the pod e2e-test-httpd-pod was created 03/01/23 11:54:24.212
    Mar  1 11:54:24.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-3592 get pod e2e-test-httpd-pod -o json'
    Mar  1 11:54:24.273: INFO: stderr: ""
    Mar  1 11:54:24.273: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"3a86c3f834c0a1d92be5a0d6cd81cf7e12b200127ffa2fbebc5c9a92fdef7b1b\",\n            \"cni.projectcalico.org/podIP\": \"10.233.95.14/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.233.95.14/32\"\n        },\n        \"creationTimestamp\": \"2023-03-01T11:54:19Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-3592\",\n        \"resourceVersion\": \"8864\",\n        \"uid\": \"e07205ec-92e1-4b75-89b1-58da6ff6cb1d\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-gzplh\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"lab1-k8s-node-1\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-gzplh\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-01T11:54:19Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-01T11:54:20Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-01T11:54:20Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-01T11:54:19Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://97988b53f0c06cb3fc048efd8a322e3be23bea1f6cf2661bde9030427082ce39\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-03-01T11:54:19Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.128.0.178\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.233.95.14\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.233.95.14\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-03-01T11:54:19Z\"\n    }\n}\n"
    STEP: replace the image in the pod 03/01/23 11:54:24.274
    Mar  1 11:54:24.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-3592 replace -f -'
    Mar  1 11:54:24.471: INFO: stderr: ""
    Mar  1 11:54:24.471: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-2 03/01/23 11:54:24.471
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1736
    Mar  1 11:54:24.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-3592 delete pods e2e-test-httpd-pod'
    Mar  1 11:54:26.834: INFO: stderr: ""
    Mar  1 11:54:26.834: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar  1 11:54:26.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-3592" for this suite. 03/01/23 11:54:26.842
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:54:26.85
Mar  1 11:54:26.851: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename kubectl 03/01/23 11:54:26.852
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:54:26.875
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:54:26.878
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
STEP: creating all guestbook components 03/01/23 11:54:26.881
Mar  1 11:54:26.881: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Mar  1 11:54:26.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-2468 create -f -'
Mar  1 11:54:27.770: INFO: stderr: ""
Mar  1 11:54:27.770: INFO: stdout: "service/agnhost-replica created\n"
Mar  1 11:54:27.770: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Mar  1 11:54:27.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-2468 create -f -'
Mar  1 11:54:28.686: INFO: stderr: ""
Mar  1 11:54:28.686: INFO: stdout: "service/agnhost-primary created\n"
Mar  1 11:54:28.686: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Mar  1 11:54:28.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-2468 create -f -'
Mar  1 11:54:28.906: INFO: stderr: ""
Mar  1 11:54:28.906: INFO: stdout: "service/frontend created\n"
Mar  1 11:54:28.907: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Mar  1 11:54:28.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-2468 create -f -'
Mar  1 11:54:29.094: INFO: stderr: ""
Mar  1 11:54:29.094: INFO: stdout: "deployment.apps/frontend created\n"
Mar  1 11:54:29.094: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar  1 11:54:29.094: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-2468 create -f -'
Mar  1 11:54:29.324: INFO: stderr: ""
Mar  1 11:54:29.324: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Mar  1 11:54:29.324: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar  1 11:54:29.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-2468 create -f -'
Mar  1 11:54:29.572: INFO: stderr: ""
Mar  1 11:54:29.572: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 03/01/23 11:54:29.572
Mar  1 11:54:29.572: INFO: Waiting for all frontend pods to be Running.
Mar  1 11:54:34.623: INFO: Waiting for frontend to serve content.
Mar  1 11:54:34.637: INFO: Trying to add a new entry to the guestbook.
Mar  1 11:54:34.656: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 03/01/23 11:54:34.669
Mar  1 11:54:34.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-2468 delete --grace-period=0 --force -f -'
Mar  1 11:54:34.759: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  1 11:54:34.759: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 03/01/23 11:54:34.759
Mar  1 11:54:34.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-2468 delete --grace-period=0 --force -f -'
Mar  1 11:54:34.863: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  1 11:54:34.863: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 03/01/23 11:54:34.863
Mar  1 11:54:34.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-2468 delete --grace-period=0 --force -f -'
Mar  1 11:54:34.959: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  1 11:54:34.959: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 03/01/23 11:54:34.959
Mar  1 11:54:34.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-2468 delete --grace-period=0 --force -f -'
Mar  1 11:54:35.037: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  1 11:54:35.037: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 03/01/23 11:54:35.037
Mar  1 11:54:35.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-2468 delete --grace-period=0 --force -f -'
Mar  1 11:54:35.133: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  1 11:54:35.133: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 03/01/23 11:54:35.133
Mar  1 11:54:35.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-2468 delete --grace-period=0 --force -f -'
Mar  1 11:54:35.266: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  1 11:54:35.266: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar  1 11:54:35.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2468" for this suite. 03/01/23 11:54:35.329
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","completed":68,"skipped":1224,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.490 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:367
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:54:26.85
    Mar  1 11:54:26.851: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename kubectl 03/01/23 11:54:26.852
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:54:26.875
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:54:26.878
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:392
    STEP: creating all guestbook components 03/01/23 11:54:26.881
    Mar  1 11:54:26.881: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Mar  1 11:54:26.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-2468 create -f -'
    Mar  1 11:54:27.770: INFO: stderr: ""
    Mar  1 11:54:27.770: INFO: stdout: "service/agnhost-replica created\n"
    Mar  1 11:54:27.770: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Mar  1 11:54:27.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-2468 create -f -'
    Mar  1 11:54:28.686: INFO: stderr: ""
    Mar  1 11:54:28.686: INFO: stdout: "service/agnhost-primary created\n"
    Mar  1 11:54:28.686: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Mar  1 11:54:28.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-2468 create -f -'
    Mar  1 11:54:28.906: INFO: stderr: ""
    Mar  1 11:54:28.906: INFO: stdout: "service/frontend created\n"
    Mar  1 11:54:28.907: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Mar  1 11:54:28.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-2468 create -f -'
    Mar  1 11:54:29.094: INFO: stderr: ""
    Mar  1 11:54:29.094: INFO: stdout: "deployment.apps/frontend created\n"
    Mar  1 11:54:29.094: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Mar  1 11:54:29.094: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-2468 create -f -'
    Mar  1 11:54:29.324: INFO: stderr: ""
    Mar  1 11:54:29.324: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Mar  1 11:54:29.324: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Mar  1 11:54:29.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-2468 create -f -'
    Mar  1 11:54:29.572: INFO: stderr: ""
    Mar  1 11:54:29.572: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 03/01/23 11:54:29.572
    Mar  1 11:54:29.572: INFO: Waiting for all frontend pods to be Running.
    Mar  1 11:54:34.623: INFO: Waiting for frontend to serve content.
    Mar  1 11:54:34.637: INFO: Trying to add a new entry to the guestbook.
    Mar  1 11:54:34.656: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 03/01/23 11:54:34.669
    Mar  1 11:54:34.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-2468 delete --grace-period=0 --force -f -'
    Mar  1 11:54:34.759: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar  1 11:54:34.759: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 03/01/23 11:54:34.759
    Mar  1 11:54:34.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-2468 delete --grace-period=0 --force -f -'
    Mar  1 11:54:34.863: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar  1 11:54:34.863: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 03/01/23 11:54:34.863
    Mar  1 11:54:34.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-2468 delete --grace-period=0 --force -f -'
    Mar  1 11:54:34.959: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar  1 11:54:34.959: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 03/01/23 11:54:34.959
    Mar  1 11:54:34.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-2468 delete --grace-period=0 --force -f -'
    Mar  1 11:54:35.037: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar  1 11:54:35.037: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 03/01/23 11:54:35.037
    Mar  1 11:54:35.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-2468 delete --grace-period=0 --force -f -'
    Mar  1 11:54:35.133: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar  1 11:54:35.133: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 03/01/23 11:54:35.133
    Mar  1 11:54:35.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-2468 delete --grace-period=0 --force -f -'
    Mar  1 11:54:35.266: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar  1 11:54:35.266: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar  1 11:54:35.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-2468" for this suite. 03/01/23 11:54:35.329
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:54:35.351
Mar  1 11:54:35.352: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename services 03/01/23 11:54:35.353
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:54:35.378
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:54:35.385
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204
STEP: creating service in namespace services-892 03/01/23 11:54:35.388
STEP: creating service affinity-nodeport in namespace services-892 03/01/23 11:54:35.389
STEP: creating replication controller affinity-nodeport in namespace services-892 03/01/23 11:54:35.407
I0301 11:54:35.414798      19 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-892, replica count: 3
I0301 11:54:38.466798      19 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  1 11:54:38.484: INFO: Creating new exec pod
Mar  1 11:54:38.504: INFO: Waiting up to 5m0s for pod "execpod-affinitynlwt6" in namespace "services-892" to be "running"
Mar  1 11:54:38.510: INFO: Pod "execpod-affinitynlwt6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.882877ms
Mar  1 11:54:40.516: INFO: Pod "execpod-affinitynlwt6": Phase="Running", Reason="", readiness=true. Elapsed: 2.012826534s
Mar  1 11:54:40.517: INFO: Pod "execpod-affinitynlwt6" satisfied condition "running"
Mar  1 11:54:41.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-892 exec execpod-affinitynlwt6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Mar  1 11:54:41.667: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Mar  1 11:54:41.667: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  1 11:54:41.668: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-892 exec execpod-affinitynlwt6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.32.242 80'
Mar  1 11:54:41.803: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.32.242 80\nConnection to 10.233.32.242 80 port [tcp/http] succeeded!\n"
Mar  1 11:54:41.803: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  1 11:54:41.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-892 exec execpod-affinitynlwt6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.128.2.241 31088'
Mar  1 11:54:41.934: INFO: stderr: "+ nc -v -t -w 2 10.128.2.241 31088\nConnection to 10.128.2.241 31088 port [tcp/*] succeeded!\n+ echo hostName\n"
Mar  1 11:54:41.934: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  1 11:54:41.935: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-892 exec execpod-affinitynlwt6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.128.0.76 31088'
Mar  1 11:54:42.073: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.128.0.76 31088\nConnection to 10.128.0.76 31088 port [tcp/*] succeeded!\n"
Mar  1 11:54:42.073: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  1 11:54:42.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-892 exec execpod-affinitynlwt6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.128.0.178:31088/ ; done'
Mar  1 11:54:42.327: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31088/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31088/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31088/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31088/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31088/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31088/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31088/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31088/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31088/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31088/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31088/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31088/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31088/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31088/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31088/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31088/\n"
Mar  1 11:54:42.327: INFO: stdout: "\naffinity-nodeport-775cc\naffinity-nodeport-775cc\naffinity-nodeport-775cc\naffinity-nodeport-775cc\naffinity-nodeport-775cc\naffinity-nodeport-775cc\naffinity-nodeport-775cc\naffinity-nodeport-775cc\naffinity-nodeport-775cc\naffinity-nodeport-775cc\naffinity-nodeport-775cc\naffinity-nodeport-775cc\naffinity-nodeport-775cc\naffinity-nodeport-775cc\naffinity-nodeport-775cc\naffinity-nodeport-775cc"
Mar  1 11:54:42.327: INFO: Received response from host: affinity-nodeport-775cc
Mar  1 11:54:42.327: INFO: Received response from host: affinity-nodeport-775cc
Mar  1 11:54:42.327: INFO: Received response from host: affinity-nodeport-775cc
Mar  1 11:54:42.327: INFO: Received response from host: affinity-nodeport-775cc
Mar  1 11:54:42.327: INFO: Received response from host: affinity-nodeport-775cc
Mar  1 11:54:42.327: INFO: Received response from host: affinity-nodeport-775cc
Mar  1 11:54:42.327: INFO: Received response from host: affinity-nodeport-775cc
Mar  1 11:54:42.327: INFO: Received response from host: affinity-nodeport-775cc
Mar  1 11:54:42.327: INFO: Received response from host: affinity-nodeport-775cc
Mar  1 11:54:42.327: INFO: Received response from host: affinity-nodeport-775cc
Mar  1 11:54:42.327: INFO: Received response from host: affinity-nodeport-775cc
Mar  1 11:54:42.327: INFO: Received response from host: affinity-nodeport-775cc
Mar  1 11:54:42.327: INFO: Received response from host: affinity-nodeport-775cc
Mar  1 11:54:42.327: INFO: Received response from host: affinity-nodeport-775cc
Mar  1 11:54:42.327: INFO: Received response from host: affinity-nodeport-775cc
Mar  1 11:54:42.327: INFO: Received response from host: affinity-nodeport-775cc
Mar  1 11:54:42.327: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-892, will wait for the garbage collector to delete the pods 03/01/23 11:54:42.343
Mar  1 11:54:42.412: INFO: Deleting ReplicationController affinity-nodeport took: 9.378901ms
Mar  1 11:54:42.513: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.651342ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar  1 11:54:44.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-892" for this suite. 03/01/23 11:54:44.253
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","completed":69,"skipped":1254,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.911 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:54:35.351
    Mar  1 11:54:35.352: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename services 03/01/23 11:54:35.353
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:54:35.378
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:54:35.385
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2204
    STEP: creating service in namespace services-892 03/01/23 11:54:35.388
    STEP: creating service affinity-nodeport in namespace services-892 03/01/23 11:54:35.389
    STEP: creating replication controller affinity-nodeport in namespace services-892 03/01/23 11:54:35.407
    I0301 11:54:35.414798      19 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-892, replica count: 3
    I0301 11:54:38.466798      19 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar  1 11:54:38.484: INFO: Creating new exec pod
    Mar  1 11:54:38.504: INFO: Waiting up to 5m0s for pod "execpod-affinitynlwt6" in namespace "services-892" to be "running"
    Mar  1 11:54:38.510: INFO: Pod "execpod-affinitynlwt6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.882877ms
    Mar  1 11:54:40.516: INFO: Pod "execpod-affinitynlwt6": Phase="Running", Reason="", readiness=true. Elapsed: 2.012826534s
    Mar  1 11:54:40.517: INFO: Pod "execpod-affinitynlwt6" satisfied condition "running"
    Mar  1 11:54:41.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-892 exec execpod-affinitynlwt6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
    Mar  1 11:54:41.667: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Mar  1 11:54:41.667: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  1 11:54:41.668: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-892 exec execpod-affinitynlwt6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.32.242 80'
    Mar  1 11:54:41.803: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.32.242 80\nConnection to 10.233.32.242 80 port [tcp/http] succeeded!\n"
    Mar  1 11:54:41.803: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  1 11:54:41.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-892 exec execpod-affinitynlwt6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.128.2.241 31088'
    Mar  1 11:54:41.934: INFO: stderr: "+ nc -v -t -w 2 10.128.2.241 31088\nConnection to 10.128.2.241 31088 port [tcp/*] succeeded!\n+ echo hostName\n"
    Mar  1 11:54:41.934: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  1 11:54:41.935: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-892 exec execpod-affinitynlwt6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.128.0.76 31088'
    Mar  1 11:54:42.073: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.128.0.76 31088\nConnection to 10.128.0.76 31088 port [tcp/*] succeeded!\n"
    Mar  1 11:54:42.073: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  1 11:54:42.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-892 exec execpod-affinitynlwt6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.128.0.178:31088/ ; done'
    Mar  1 11:54:42.327: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31088/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31088/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31088/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31088/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31088/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31088/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31088/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31088/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31088/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31088/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31088/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31088/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31088/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31088/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31088/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31088/\n"
    Mar  1 11:54:42.327: INFO: stdout: "\naffinity-nodeport-775cc\naffinity-nodeport-775cc\naffinity-nodeport-775cc\naffinity-nodeport-775cc\naffinity-nodeport-775cc\naffinity-nodeport-775cc\naffinity-nodeport-775cc\naffinity-nodeport-775cc\naffinity-nodeport-775cc\naffinity-nodeport-775cc\naffinity-nodeport-775cc\naffinity-nodeport-775cc\naffinity-nodeport-775cc\naffinity-nodeport-775cc\naffinity-nodeport-775cc\naffinity-nodeport-775cc"
    Mar  1 11:54:42.327: INFO: Received response from host: affinity-nodeport-775cc
    Mar  1 11:54:42.327: INFO: Received response from host: affinity-nodeport-775cc
    Mar  1 11:54:42.327: INFO: Received response from host: affinity-nodeport-775cc
    Mar  1 11:54:42.327: INFO: Received response from host: affinity-nodeport-775cc
    Mar  1 11:54:42.327: INFO: Received response from host: affinity-nodeport-775cc
    Mar  1 11:54:42.327: INFO: Received response from host: affinity-nodeport-775cc
    Mar  1 11:54:42.327: INFO: Received response from host: affinity-nodeport-775cc
    Mar  1 11:54:42.327: INFO: Received response from host: affinity-nodeport-775cc
    Mar  1 11:54:42.327: INFO: Received response from host: affinity-nodeport-775cc
    Mar  1 11:54:42.327: INFO: Received response from host: affinity-nodeport-775cc
    Mar  1 11:54:42.327: INFO: Received response from host: affinity-nodeport-775cc
    Mar  1 11:54:42.327: INFO: Received response from host: affinity-nodeport-775cc
    Mar  1 11:54:42.327: INFO: Received response from host: affinity-nodeport-775cc
    Mar  1 11:54:42.327: INFO: Received response from host: affinity-nodeport-775cc
    Mar  1 11:54:42.327: INFO: Received response from host: affinity-nodeport-775cc
    Mar  1 11:54:42.327: INFO: Received response from host: affinity-nodeport-775cc
    Mar  1 11:54:42.327: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-892, will wait for the garbage collector to delete the pods 03/01/23 11:54:42.343
    Mar  1 11:54:42.412: INFO: Deleting ReplicationController affinity-nodeport took: 9.378901ms
    Mar  1 11:54:42.513: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.651342ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar  1 11:54:44.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-892" for this suite. 03/01/23 11:54:44.253
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:54:44.264
Mar  1 11:54:44.264: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename certificates 03/01/23 11:54:44.265
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:54:44.29
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:54:44.294
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 03/01/23 11:54:45.084
STEP: getting /apis/certificates.k8s.io 03/01/23 11:54:45.087
STEP: getting /apis/certificates.k8s.io/v1 03/01/23 11:54:45.089
STEP: creating 03/01/23 11:54:45.09
STEP: getting 03/01/23 11:54:45.111
STEP: listing 03/01/23 11:54:45.115
STEP: watching 03/01/23 11:54:45.12
Mar  1 11:54:45.120: INFO: starting watch
STEP: patching 03/01/23 11:54:45.122
STEP: updating 03/01/23 11:54:45.13
Mar  1 11:54:45.138: INFO: waiting for watch events with expected annotations
Mar  1 11:54:45.138: INFO: saw patched and updated annotations
STEP: getting /approval 03/01/23 11:54:45.138
STEP: patching /approval 03/01/23 11:54:45.142
STEP: updating /approval 03/01/23 11:54:45.152
STEP: getting /status 03/01/23 11:54:45.163
STEP: patching /status 03/01/23 11:54:45.168
STEP: updating /status 03/01/23 11:54:45.183
STEP: deleting 03/01/23 11:54:45.196
STEP: deleting a collection 03/01/23 11:54:45.239
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  1 11:54:45.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-8462" for this suite. 03/01/23 11:54:45.273
{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","completed":70,"skipped":1254,"failed":0}
------------------------------
â€¢ [1.021 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:54:44.264
    Mar  1 11:54:44.264: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename certificates 03/01/23 11:54:44.265
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:54:44.29
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:54:44.294
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 03/01/23 11:54:45.084
    STEP: getting /apis/certificates.k8s.io 03/01/23 11:54:45.087
    STEP: getting /apis/certificates.k8s.io/v1 03/01/23 11:54:45.089
    STEP: creating 03/01/23 11:54:45.09
    STEP: getting 03/01/23 11:54:45.111
    STEP: listing 03/01/23 11:54:45.115
    STEP: watching 03/01/23 11:54:45.12
    Mar  1 11:54:45.120: INFO: starting watch
    STEP: patching 03/01/23 11:54:45.122
    STEP: updating 03/01/23 11:54:45.13
    Mar  1 11:54:45.138: INFO: waiting for watch events with expected annotations
    Mar  1 11:54:45.138: INFO: saw patched and updated annotations
    STEP: getting /approval 03/01/23 11:54:45.138
    STEP: patching /approval 03/01/23 11:54:45.142
    STEP: updating /approval 03/01/23 11:54:45.152
    STEP: getting /status 03/01/23 11:54:45.163
    STEP: patching /status 03/01/23 11:54:45.168
    STEP: updating /status 03/01/23 11:54:45.183
    STEP: deleting 03/01/23 11:54:45.196
    STEP: deleting a collection 03/01/23 11:54:45.239
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  1 11:54:45.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "certificates-8462" for this suite. 03/01/23 11:54:45.273
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:54:45.288
Mar  1 11:54:45.289: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename resourcequota 03/01/23 11:54:45.289
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:54:45.312
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:54:45.316
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
STEP: Counting existing ResourceQuota 03/01/23 11:54:45.319
STEP: Creating a ResourceQuota 03/01/23 11:54:50.325
STEP: Ensuring resource quota status is calculated 03/01/23 11:54:50.33
STEP: Creating a ReplicationController 03/01/23 11:54:52.337
STEP: Ensuring resource quota status captures replication controller creation 03/01/23 11:54:52.352
STEP: Deleting a ReplicationController 03/01/23 11:54:54.357
STEP: Ensuring resource quota status released usage 03/01/23 11:54:54.367
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Mar  1 11:54:56.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5513" for this suite. 03/01/23 11:54:56.38
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","completed":71,"skipped":1289,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.102 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:54:45.288
    Mar  1 11:54:45.289: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename resourcequota 03/01/23 11:54:45.289
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:54:45.312
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:54:45.316
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:382
    STEP: Counting existing ResourceQuota 03/01/23 11:54:45.319
    STEP: Creating a ResourceQuota 03/01/23 11:54:50.325
    STEP: Ensuring resource quota status is calculated 03/01/23 11:54:50.33
    STEP: Creating a ReplicationController 03/01/23 11:54:52.337
    STEP: Ensuring resource quota status captures replication controller creation 03/01/23 11:54:52.352
    STEP: Deleting a ReplicationController 03/01/23 11:54:54.357
    STEP: Ensuring resource quota status released usage 03/01/23 11:54:54.367
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Mar  1 11:54:56.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-5513" for this suite. 03/01/23 11:54:56.38
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:54:56.392
Mar  1 11:54:56.393: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename deployment 03/01/23 11:54:56.393
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:54:56.416
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:54:56.42
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Mar  1 11:54:56.423: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Mar  1 11:54:56.435: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar  1 11:55:01.441: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 03/01/23 11:55:01.441
Mar  1 11:55:01.441: INFO: Creating deployment "test-rolling-update-deployment"
Mar  1 11:55:01.449: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Mar  1 11:55:01.461: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Mar  1 11:55:03.474: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Mar  1 11:55:03.479: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar  1 11:55:03.493: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-7384  f17120e9-81dc-4ab0-af35-f16ac50a240d 9539 1 2023-03-01 11:55:01 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-03-01 11:55:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-01 11:55:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003e85b58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-01 11:55:01 +0000 UTC,LastTransitionTime:2023-03-01 11:55:01 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-78f575d8ff" has successfully progressed.,LastUpdateTime:2023-03-01 11:55:02 +0000 UTC,LastTransitionTime:2023-03-01 11:55:01 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar  1 11:55:03.498: INFO: New ReplicaSet "test-rolling-update-deployment-78f575d8ff" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-78f575d8ff  deployment-7384  e5b12f71-2481-45f7-be9c-4e0396f947d9 9529 1 2023-03-01 11:55:01 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment f17120e9-81dc-4ab0-af35-f16ac50a240d 0xc0004c72f7 0xc0004c72f8}] [] [{kube-controller-manager Update apps/v1 2023-03-01 11:55:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f17120e9-81dc-4ab0-af35-f16ac50a240d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-01 11:55:02 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 78f575d8ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0004c7cf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar  1 11:55:03.498: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Mar  1 11:55:03.498: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-7384  3b4841d5-d3a1-461f-a960-86dad743802a 9538 2 2023-03-01 11:54:56 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment f17120e9-81dc-4ab0-af35-f16ac50a240d 0xc003e85f07 0xc003e85f08}] [] [{e2e.test Update apps/v1 2023-03-01 11:54:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-01 11:55:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f17120e9-81dc-4ab0-af35-f16ac50a240d\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-03-01 11:55:02 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003e85fc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  1 11:55:03.504: INFO: Pod "test-rolling-update-deployment-78f575d8ff-95hpg" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-78f575d8ff-95hpg test-rolling-update-deployment-78f575d8ff- deployment-7384  96cd71f6-8016-4ab5-a92d-6f38da44b56a 9528 0 2023-03-01 11:55:01 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[cni.projectcalico.org/containerID:6c78148583b8c4af35d599ba86dbdf7caca316a57de32c280f62e2fb55205a8a cni.projectcalico.org/podIP:10.233.74.62/32 cni.projectcalico.org/podIPs:10.233.74.62/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-78f575d8ff e5b12f71-2481-45f7-be9c-4e0396f947d9 0xc003aa41c7 0xc003aa41c8}] [] [{kube-controller-manager Update v1 2023-03-01 11:55:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e5b12f71-2481-45f7-be9c-4e0396f947d9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-01 11:55:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-01 11:55:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.74.62\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tzml4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tzml4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 11:55:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 11:55:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 11:55:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 11:55:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.2.241,PodIP:10.233.74.62,StartTime:2023-03-01 11:55:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-01 11:55:02 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://7ee5e833800cd94b06fc400651ce2d562250eff40350ba30d3c8d64048c108c7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.74.62,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Mar  1 11:55:03.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7384" for this suite. 03/01/23 11:55:03.513
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","completed":72,"skipped":1300,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.130 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:54:56.392
    Mar  1 11:54:56.393: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename deployment 03/01/23 11:54:56.393
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:54:56.416
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:54:56.42
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Mar  1 11:54:56.423: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    Mar  1 11:54:56.435: INFO: Pod name sample-pod: Found 0 pods out of 1
    Mar  1 11:55:01.441: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 03/01/23 11:55:01.441
    Mar  1 11:55:01.441: INFO: Creating deployment "test-rolling-update-deployment"
    Mar  1 11:55:01.449: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Mar  1 11:55:01.461: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    Mar  1 11:55:03.474: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Mar  1 11:55:03.479: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar  1 11:55:03.493: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-7384  f17120e9-81dc-4ab0-af35-f16ac50a240d 9539 1 2023-03-01 11:55:01 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-03-01 11:55:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-01 11:55:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003e85b58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-01 11:55:01 +0000 UTC,LastTransitionTime:2023-03-01 11:55:01 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-78f575d8ff" has successfully progressed.,LastUpdateTime:2023-03-01 11:55:02 +0000 UTC,LastTransitionTime:2023-03-01 11:55:01 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Mar  1 11:55:03.498: INFO: New ReplicaSet "test-rolling-update-deployment-78f575d8ff" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-78f575d8ff  deployment-7384  e5b12f71-2481-45f7-be9c-4e0396f947d9 9529 1 2023-03-01 11:55:01 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment f17120e9-81dc-4ab0-af35-f16ac50a240d 0xc0004c72f7 0xc0004c72f8}] [] [{kube-controller-manager Update apps/v1 2023-03-01 11:55:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f17120e9-81dc-4ab0-af35-f16ac50a240d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-01 11:55:02 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 78f575d8ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0004c7cf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Mar  1 11:55:03.498: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Mar  1 11:55:03.498: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-7384  3b4841d5-d3a1-461f-a960-86dad743802a 9538 2 2023-03-01 11:54:56 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment f17120e9-81dc-4ab0-af35-f16ac50a240d 0xc003e85f07 0xc003e85f08}] [] [{e2e.test Update apps/v1 2023-03-01 11:54:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-01 11:55:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f17120e9-81dc-4ab0-af35-f16ac50a240d\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-03-01 11:55:02 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003e85fc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Mar  1 11:55:03.504: INFO: Pod "test-rolling-update-deployment-78f575d8ff-95hpg" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-78f575d8ff-95hpg test-rolling-update-deployment-78f575d8ff- deployment-7384  96cd71f6-8016-4ab5-a92d-6f38da44b56a 9528 0 2023-03-01 11:55:01 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[cni.projectcalico.org/containerID:6c78148583b8c4af35d599ba86dbdf7caca316a57de32c280f62e2fb55205a8a cni.projectcalico.org/podIP:10.233.74.62/32 cni.projectcalico.org/podIPs:10.233.74.62/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-78f575d8ff e5b12f71-2481-45f7-be9c-4e0396f947d9 0xc003aa41c7 0xc003aa41c8}] [] [{kube-controller-manager Update v1 2023-03-01 11:55:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e5b12f71-2481-45f7-be9c-4e0396f947d9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-01 11:55:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-01 11:55:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.74.62\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tzml4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tzml4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 11:55:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 11:55:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 11:55:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 11:55:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.2.241,PodIP:10.233.74.62,StartTime:2023-03-01 11:55:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-01 11:55:02 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://7ee5e833800cd94b06fc400651ce2d562250eff40350ba30d3c8d64048c108c7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.74.62,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Mar  1 11:55:03.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-7384" for this suite. 03/01/23 11:55:03.513
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:55:03.528
Mar  1 11:55:03.528: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename resourcequota 03/01/23 11:55:03.529
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:55:03.551
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:55:03.555
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
STEP: Discovering how many secrets are in namespace by default 03/01/23 11:55:03.558
STEP: Counting existing ResourceQuota 03/01/23 11:55:08.565
STEP: Creating a ResourceQuota 03/01/23 11:55:13.572
STEP: Ensuring resource quota status is calculated 03/01/23 11:55:13.58
STEP: Creating a Secret 03/01/23 11:55:15.588
STEP: Ensuring resource quota status captures secret creation 03/01/23 11:55:15.603
STEP: Deleting a secret 03/01/23 11:55:17.608
STEP: Ensuring resource quota status released usage 03/01/23 11:55:17.618
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Mar  1 11:55:19.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9146" for this suite. 03/01/23 11:55:19.634
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","completed":73,"skipped":1349,"failed":0}
------------------------------
â€¢ [SLOW TEST] [16.115 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:55:03.528
    Mar  1 11:55:03.528: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename resourcequota 03/01/23 11:55:03.529
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:55:03.551
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:55:03.555
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:150
    STEP: Discovering how many secrets are in namespace by default 03/01/23 11:55:03.558
    STEP: Counting existing ResourceQuota 03/01/23 11:55:08.565
    STEP: Creating a ResourceQuota 03/01/23 11:55:13.572
    STEP: Ensuring resource quota status is calculated 03/01/23 11:55:13.58
    STEP: Creating a Secret 03/01/23 11:55:15.588
    STEP: Ensuring resource quota status captures secret creation 03/01/23 11:55:15.603
    STEP: Deleting a secret 03/01/23 11:55:17.608
    STEP: Ensuring resource quota status released usage 03/01/23 11:55:17.618
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Mar  1 11:55:19.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-9146" for this suite. 03/01/23 11:55:19.634
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:55:19.646
Mar  1 11:55:19.646: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename subpath 03/01/23 11:55:19.647
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:55:19.673
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:55:19.676
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 03/01/23 11:55:19.679
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-pmnx 03/01/23 11:55:19.691
STEP: Creating a pod to test atomic-volume-subpath 03/01/23 11:55:19.692
Mar  1 11:55:19.703: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-pmnx" in namespace "subpath-853" to be "Succeeded or Failed"
Mar  1 11:55:19.710: INFO: Pod "pod-subpath-test-downwardapi-pmnx": Phase="Pending", Reason="", readiness=false. Elapsed: 6.272461ms
Mar  1 11:55:21.716: INFO: Pod "pod-subpath-test-downwardapi-pmnx": Phase="Running", Reason="", readiness=true. Elapsed: 2.012725624s
Mar  1 11:55:23.717: INFO: Pod "pod-subpath-test-downwardapi-pmnx": Phase="Running", Reason="", readiness=true. Elapsed: 4.013370776s
Mar  1 11:55:25.716: INFO: Pod "pod-subpath-test-downwardapi-pmnx": Phase="Running", Reason="", readiness=true. Elapsed: 6.012213997s
Mar  1 11:55:27.715: INFO: Pod "pod-subpath-test-downwardapi-pmnx": Phase="Running", Reason="", readiness=true. Elapsed: 8.011647277s
Mar  1 11:55:29.716: INFO: Pod "pod-subpath-test-downwardapi-pmnx": Phase="Running", Reason="", readiness=true. Elapsed: 10.012674852s
Mar  1 11:55:31.716: INFO: Pod "pod-subpath-test-downwardapi-pmnx": Phase="Running", Reason="", readiness=true. Elapsed: 12.012469106s
Mar  1 11:55:33.717: INFO: Pod "pod-subpath-test-downwardapi-pmnx": Phase="Running", Reason="", readiness=true. Elapsed: 14.013087004s
Mar  1 11:55:35.717: INFO: Pod "pod-subpath-test-downwardapi-pmnx": Phase="Running", Reason="", readiness=true. Elapsed: 16.013357715s
Mar  1 11:55:37.716: INFO: Pod "pod-subpath-test-downwardapi-pmnx": Phase="Running", Reason="", readiness=true. Elapsed: 18.012403599s
Mar  1 11:55:39.716: INFO: Pod "pod-subpath-test-downwardapi-pmnx": Phase="Running", Reason="", readiness=true. Elapsed: 20.012591379s
Mar  1 11:55:41.716: INFO: Pod "pod-subpath-test-downwardapi-pmnx": Phase="Running", Reason="", readiness=false. Elapsed: 22.012887419s
Mar  1 11:55:43.715: INFO: Pod "pod-subpath-test-downwardapi-pmnx": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.011309303s
STEP: Saw pod success 03/01/23 11:55:43.715
Mar  1 11:55:43.715: INFO: Pod "pod-subpath-test-downwardapi-pmnx" satisfied condition "Succeeded or Failed"
Mar  1 11:55:43.720: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-subpath-test-downwardapi-pmnx container test-container-subpath-downwardapi-pmnx: <nil>
STEP: delete the pod 03/01/23 11:55:43.741
Mar  1 11:55:43.756: INFO: Waiting for pod pod-subpath-test-downwardapi-pmnx to disappear
Mar  1 11:55:43.760: INFO: Pod pod-subpath-test-downwardapi-pmnx no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-pmnx 03/01/23 11:55:43.761
Mar  1 11:55:43.761: INFO: Deleting pod "pod-subpath-test-downwardapi-pmnx" in namespace "subpath-853"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Mar  1 11:55:43.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-853" for this suite. 03/01/23 11:55:43.772
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]","completed":74,"skipped":1375,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.135 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:55:19.646
    Mar  1 11:55:19.646: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename subpath 03/01/23 11:55:19.647
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:55:19.673
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:55:19.676
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 03/01/23 11:55:19.679
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-pmnx 03/01/23 11:55:19.691
    STEP: Creating a pod to test atomic-volume-subpath 03/01/23 11:55:19.692
    Mar  1 11:55:19.703: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-pmnx" in namespace "subpath-853" to be "Succeeded or Failed"
    Mar  1 11:55:19.710: INFO: Pod "pod-subpath-test-downwardapi-pmnx": Phase="Pending", Reason="", readiness=false. Elapsed: 6.272461ms
    Mar  1 11:55:21.716: INFO: Pod "pod-subpath-test-downwardapi-pmnx": Phase="Running", Reason="", readiness=true. Elapsed: 2.012725624s
    Mar  1 11:55:23.717: INFO: Pod "pod-subpath-test-downwardapi-pmnx": Phase="Running", Reason="", readiness=true. Elapsed: 4.013370776s
    Mar  1 11:55:25.716: INFO: Pod "pod-subpath-test-downwardapi-pmnx": Phase="Running", Reason="", readiness=true. Elapsed: 6.012213997s
    Mar  1 11:55:27.715: INFO: Pod "pod-subpath-test-downwardapi-pmnx": Phase="Running", Reason="", readiness=true. Elapsed: 8.011647277s
    Mar  1 11:55:29.716: INFO: Pod "pod-subpath-test-downwardapi-pmnx": Phase="Running", Reason="", readiness=true. Elapsed: 10.012674852s
    Mar  1 11:55:31.716: INFO: Pod "pod-subpath-test-downwardapi-pmnx": Phase="Running", Reason="", readiness=true. Elapsed: 12.012469106s
    Mar  1 11:55:33.717: INFO: Pod "pod-subpath-test-downwardapi-pmnx": Phase="Running", Reason="", readiness=true. Elapsed: 14.013087004s
    Mar  1 11:55:35.717: INFO: Pod "pod-subpath-test-downwardapi-pmnx": Phase="Running", Reason="", readiness=true. Elapsed: 16.013357715s
    Mar  1 11:55:37.716: INFO: Pod "pod-subpath-test-downwardapi-pmnx": Phase="Running", Reason="", readiness=true. Elapsed: 18.012403599s
    Mar  1 11:55:39.716: INFO: Pod "pod-subpath-test-downwardapi-pmnx": Phase="Running", Reason="", readiness=true. Elapsed: 20.012591379s
    Mar  1 11:55:41.716: INFO: Pod "pod-subpath-test-downwardapi-pmnx": Phase="Running", Reason="", readiness=false. Elapsed: 22.012887419s
    Mar  1 11:55:43.715: INFO: Pod "pod-subpath-test-downwardapi-pmnx": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.011309303s
    STEP: Saw pod success 03/01/23 11:55:43.715
    Mar  1 11:55:43.715: INFO: Pod "pod-subpath-test-downwardapi-pmnx" satisfied condition "Succeeded or Failed"
    Mar  1 11:55:43.720: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-subpath-test-downwardapi-pmnx container test-container-subpath-downwardapi-pmnx: <nil>
    STEP: delete the pod 03/01/23 11:55:43.741
    Mar  1 11:55:43.756: INFO: Waiting for pod pod-subpath-test-downwardapi-pmnx to disappear
    Mar  1 11:55:43.760: INFO: Pod pod-subpath-test-downwardapi-pmnx no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-pmnx 03/01/23 11:55:43.761
    Mar  1 11:55:43.761: INFO: Deleting pod "pod-subpath-test-downwardapi-pmnx" in namespace "subpath-853"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Mar  1 11:55:43.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-853" for this suite. 03/01/23 11:55:43.772
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:55:43.783
Mar  1 11:55:43.783: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename disruption 03/01/23 11:55:43.784
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:55:43.806
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:55:43.81
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:55:43.813
Mar  1 11:55:43.813: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename disruption-2 03/01/23 11:55:43.814
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:55:43.834
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:55:43.838
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
STEP: Waiting for the pdb to be processed 03/01/23 11:55:43.847
STEP: Waiting for the pdb to be processed 03/01/23 11:55:43.86
STEP: Waiting for the pdb to be processed 03/01/23 11:55:43.875
STEP: listing a collection of PDBs across all namespaces 03/01/23 11:55:43.879
STEP: listing a collection of PDBs in namespace disruption-1399 03/01/23 11:55:43.884
STEP: deleting a collection of PDBs 03/01/23 11:55:43.888
STEP: Waiting for the PDB collection to be deleted 03/01/23 11:55:43.907
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:187
Mar  1 11:55:43.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-1332" for this suite. 03/01/23 11:55:43.917
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Mar  1 11:55:43.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-1399" for this suite. 03/01/23 11:55:43.933
{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","completed":75,"skipped":1395,"failed":0}
------------------------------
â€¢ [0.158 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:77
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:55:43.783
    Mar  1 11:55:43.783: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename disruption 03/01/23 11:55:43.784
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:55:43.806
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:55:43.81
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:55:43.813
    Mar  1 11:55:43.813: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename disruption-2 03/01/23 11:55:43.814
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:55:43.834
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:55:43.838
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:86
    STEP: Waiting for the pdb to be processed 03/01/23 11:55:43.847
    STEP: Waiting for the pdb to be processed 03/01/23 11:55:43.86
    STEP: Waiting for the pdb to be processed 03/01/23 11:55:43.875
    STEP: listing a collection of PDBs across all namespaces 03/01/23 11:55:43.879
    STEP: listing a collection of PDBs in namespace disruption-1399 03/01/23 11:55:43.884
    STEP: deleting a collection of PDBs 03/01/23 11:55:43.888
    STEP: Waiting for the PDB collection to be deleted 03/01/23 11:55:43.907
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:187
    Mar  1 11:55:43.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-2-1332" for this suite. 03/01/23 11:55:43.917
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Mar  1 11:55:43.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-1399" for this suite. 03/01/23 11:55:43.933
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:55:43.943
Mar  1 11:55:43.943: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename container-runtime 03/01/23 11:55:43.944
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:55:43.966
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:55:43.969
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
STEP: create the container 03/01/23 11:55:43.973
STEP: wait for the container to reach Failed 03/01/23 11:55:43.984
STEP: get the container status 03/01/23 11:55:48.015
STEP: the container should be terminated 03/01/23 11:55:48.019
STEP: the termination message should be set 03/01/23 11:55:48.019
Mar  1 11:55:48.019: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 03/01/23 11:55:48.019
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Mar  1 11:55:48.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1283" for this suite. 03/01/23 11:55:48.047
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":76,"skipped":1396,"failed":0}
------------------------------
â€¢ [4.113 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:55:43.943
    Mar  1 11:55:43.943: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename container-runtime 03/01/23 11:55:43.944
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:55:43.966
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:55:43.969
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215
    STEP: create the container 03/01/23 11:55:43.973
    STEP: wait for the container to reach Failed 03/01/23 11:55:43.984
    STEP: get the container status 03/01/23 11:55:48.015
    STEP: the container should be terminated 03/01/23 11:55:48.019
    STEP: the termination message should be set 03/01/23 11:55:48.019
    Mar  1 11:55:48.019: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 03/01/23 11:55:48.019
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Mar  1 11:55:48.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-1283" for this suite. 03/01/23 11:55:48.047
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:55:48.059
Mar  1 11:55:48.059: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename pods 03/01/23 11:55:48.06
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:55:48.084
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:55:48.089
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
STEP: creating pod 03/01/23 11:55:48.093
Mar  1 11:55:48.106: INFO: Waiting up to 5m0s for pod "pod-hostip-78622588-a336-4cc8-be72-3c7832fe7fdc" in namespace "pods-9268" to be "running and ready"
Mar  1 11:55:48.111: INFO: Pod "pod-hostip-78622588-a336-4cc8-be72-3c7832fe7fdc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.821062ms
Mar  1 11:55:48.111: INFO: The phase of Pod pod-hostip-78622588-a336-4cc8-be72-3c7832fe7fdc is Pending, waiting for it to be Running (with Ready = true)
Mar  1 11:55:50.118: INFO: Pod "pod-hostip-78622588-a336-4cc8-be72-3c7832fe7fdc": Phase="Running", Reason="", readiness=true. Elapsed: 2.011938251s
Mar  1 11:55:50.118: INFO: The phase of Pod pod-hostip-78622588-a336-4cc8-be72-3c7832fe7fdc is Running (Ready = true)
Mar  1 11:55:50.118: INFO: Pod "pod-hostip-78622588-a336-4cc8-be72-3c7832fe7fdc" satisfied condition "running and ready"
Mar  1 11:55:50.127: INFO: Pod pod-hostip-78622588-a336-4cc8-be72-3c7832fe7fdc has hostIP: 10.128.2.241
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Mar  1 11:55:50.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9268" for this suite. 03/01/23 11:55:50.133
{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","completed":77,"skipped":1399,"failed":0}
------------------------------
â€¢ [2.082 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:55:48.059
    Mar  1 11:55:48.059: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename pods 03/01/23 11:55:48.06
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:55:48.084
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:55:48.089
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:203
    STEP: creating pod 03/01/23 11:55:48.093
    Mar  1 11:55:48.106: INFO: Waiting up to 5m0s for pod "pod-hostip-78622588-a336-4cc8-be72-3c7832fe7fdc" in namespace "pods-9268" to be "running and ready"
    Mar  1 11:55:48.111: INFO: Pod "pod-hostip-78622588-a336-4cc8-be72-3c7832fe7fdc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.821062ms
    Mar  1 11:55:48.111: INFO: The phase of Pod pod-hostip-78622588-a336-4cc8-be72-3c7832fe7fdc is Pending, waiting for it to be Running (with Ready = true)
    Mar  1 11:55:50.118: INFO: Pod "pod-hostip-78622588-a336-4cc8-be72-3c7832fe7fdc": Phase="Running", Reason="", readiness=true. Elapsed: 2.011938251s
    Mar  1 11:55:50.118: INFO: The phase of Pod pod-hostip-78622588-a336-4cc8-be72-3c7832fe7fdc is Running (Ready = true)
    Mar  1 11:55:50.118: INFO: Pod "pod-hostip-78622588-a336-4cc8-be72-3c7832fe7fdc" satisfied condition "running and ready"
    Mar  1 11:55:50.127: INFO: Pod pod-hostip-78622588-a336-4cc8-be72-3c7832fe7fdc has hostIP: 10.128.2.241
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Mar  1 11:55:50.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-9268" for this suite. 03/01/23 11:55:50.133
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:55:50.145
Mar  1 11:55:50.145: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename services 03/01/23 11:55:50.146
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:55:50.168
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:55:50.172
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237
STEP: creating service in namespace services-9606 03/01/23 11:55:50.176
STEP: creating service affinity-nodeport-transition in namespace services-9606 03/01/23 11:55:50.176
STEP: creating replication controller affinity-nodeport-transition in namespace services-9606 03/01/23 11:55:50.2
I0301 11:55:50.210892      19 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-9606, replica count: 3
I0301 11:55:53.262761      19 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  1 11:55:53.279: INFO: Creating new exec pod
Mar  1 11:55:53.292: INFO: Waiting up to 5m0s for pod "execpod-affinityhqxlm" in namespace "services-9606" to be "running"
Mar  1 11:55:53.297: INFO: Pod "execpod-affinityhqxlm": Phase="Pending", Reason="", readiness=false. Elapsed: 5.004777ms
Mar  1 11:55:55.305: INFO: Pod "execpod-affinityhqxlm": Phase="Running", Reason="", readiness=true. Elapsed: 2.012934592s
Mar  1 11:55:55.305: INFO: Pod "execpod-affinityhqxlm" satisfied condition "running"
Mar  1 11:55:56.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-9606 exec execpod-affinityhqxlm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Mar  1 11:55:56.464: INFO: stderr: "+ nc -v -t -w 2 affinity-nodeport-transition 80\n+ echo hostName\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Mar  1 11:55:56.464: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  1 11:55:56.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-9606 exec execpod-affinityhqxlm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.40.30 80'
Mar  1 11:55:56.595: INFO: stderr: "+ nc -v -t -w 2 10.233.40.30 80\n+ echo hostName\nConnection to 10.233.40.30 80 port [tcp/http] succeeded!\n"
Mar  1 11:55:56.595: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  1 11:55:56.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-9606 exec execpod-affinityhqxlm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.128.0.76 31250'
Mar  1 11:55:56.731: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.128.0.76 31250\nConnection to 10.128.0.76 31250 port [tcp/*] succeeded!\n"
Mar  1 11:55:56.731: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  1 11:55:56.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-9606 exec execpod-affinityhqxlm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.128.2.241 31250'
Mar  1 11:55:56.868: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.128.2.241 31250\nConnection to 10.128.2.241 31250 port [tcp/*] succeeded!\n"
Mar  1 11:55:56.868: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  1 11:55:56.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-9606 exec execpod-affinityhqxlm -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.128.0.178:31250/ ; done'
Mar  1 11:55:57.150: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31250/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31250/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31250/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31250/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31250/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31250/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31250/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31250/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31250/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31250/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31250/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31250/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31250/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31250/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31250/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31250/\n"
Mar  1 11:55:57.150: INFO: stdout: "\naffinity-nodeport-transition-hf8cm\naffinity-nodeport-transition-tdxpg\naffinity-nodeport-transition-hf8cm\naffinity-nodeport-transition-hf8cm\naffinity-nodeport-transition-fdq7m\naffinity-nodeport-transition-hf8cm\naffinity-nodeport-transition-hf8cm\naffinity-nodeport-transition-hf8cm\naffinity-nodeport-transition-tdxpg\naffinity-nodeport-transition-tdxpg\naffinity-nodeport-transition-tdxpg\naffinity-nodeport-transition-fdq7m\naffinity-nodeport-transition-hf8cm\naffinity-nodeport-transition-fdq7m\naffinity-nodeport-transition-tdxpg\naffinity-nodeport-transition-hf8cm"
Mar  1 11:55:57.150: INFO: Received response from host: affinity-nodeport-transition-hf8cm
Mar  1 11:55:57.150: INFO: Received response from host: affinity-nodeport-transition-tdxpg
Mar  1 11:55:57.150: INFO: Received response from host: affinity-nodeport-transition-hf8cm
Mar  1 11:55:57.150: INFO: Received response from host: affinity-nodeport-transition-hf8cm
Mar  1 11:55:57.150: INFO: Received response from host: affinity-nodeport-transition-fdq7m
Mar  1 11:55:57.150: INFO: Received response from host: affinity-nodeport-transition-hf8cm
Mar  1 11:55:57.151: INFO: Received response from host: affinity-nodeport-transition-hf8cm
Mar  1 11:55:57.151: INFO: Received response from host: affinity-nodeport-transition-hf8cm
Mar  1 11:55:57.151: INFO: Received response from host: affinity-nodeport-transition-tdxpg
Mar  1 11:55:57.151: INFO: Received response from host: affinity-nodeport-transition-tdxpg
Mar  1 11:55:57.151: INFO: Received response from host: affinity-nodeport-transition-tdxpg
Mar  1 11:55:57.151: INFO: Received response from host: affinity-nodeport-transition-fdq7m
Mar  1 11:55:57.151: INFO: Received response from host: affinity-nodeport-transition-hf8cm
Mar  1 11:55:57.151: INFO: Received response from host: affinity-nodeport-transition-fdq7m
Mar  1 11:55:57.151: INFO: Received response from host: affinity-nodeport-transition-tdxpg
Mar  1 11:55:57.151: INFO: Received response from host: affinity-nodeport-transition-hf8cm
Mar  1 11:55:57.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-9606 exec execpod-affinityhqxlm -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.128.0.178:31250/ ; done'
Mar  1 11:55:57.428: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31250/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31250/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31250/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31250/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31250/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31250/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31250/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31250/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31250/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31250/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31250/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31250/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31250/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31250/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31250/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31250/\n"
Mar  1 11:55:57.428: INFO: stdout: "\naffinity-nodeport-transition-fdq7m\naffinity-nodeport-transition-fdq7m\naffinity-nodeport-transition-fdq7m\naffinity-nodeport-transition-fdq7m\naffinity-nodeport-transition-fdq7m\naffinity-nodeport-transition-fdq7m\naffinity-nodeport-transition-fdq7m\naffinity-nodeport-transition-fdq7m\naffinity-nodeport-transition-fdq7m\naffinity-nodeport-transition-fdq7m\naffinity-nodeport-transition-fdq7m\naffinity-nodeport-transition-fdq7m\naffinity-nodeport-transition-fdq7m\naffinity-nodeport-transition-fdq7m\naffinity-nodeport-transition-fdq7m\naffinity-nodeport-transition-fdq7m"
Mar  1 11:55:57.428: INFO: Received response from host: affinity-nodeport-transition-fdq7m
Mar  1 11:55:57.428: INFO: Received response from host: affinity-nodeport-transition-fdq7m
Mar  1 11:55:57.428: INFO: Received response from host: affinity-nodeport-transition-fdq7m
Mar  1 11:55:57.428: INFO: Received response from host: affinity-nodeport-transition-fdq7m
Mar  1 11:55:57.428: INFO: Received response from host: affinity-nodeport-transition-fdq7m
Mar  1 11:55:57.428: INFO: Received response from host: affinity-nodeport-transition-fdq7m
Mar  1 11:55:57.428: INFO: Received response from host: affinity-nodeport-transition-fdq7m
Mar  1 11:55:57.428: INFO: Received response from host: affinity-nodeport-transition-fdq7m
Mar  1 11:55:57.428: INFO: Received response from host: affinity-nodeport-transition-fdq7m
Mar  1 11:55:57.428: INFO: Received response from host: affinity-nodeport-transition-fdq7m
Mar  1 11:55:57.428: INFO: Received response from host: affinity-nodeport-transition-fdq7m
Mar  1 11:55:57.428: INFO: Received response from host: affinity-nodeport-transition-fdq7m
Mar  1 11:55:57.428: INFO: Received response from host: affinity-nodeport-transition-fdq7m
Mar  1 11:55:57.428: INFO: Received response from host: affinity-nodeport-transition-fdq7m
Mar  1 11:55:57.428: INFO: Received response from host: affinity-nodeport-transition-fdq7m
Mar  1 11:55:57.428: INFO: Received response from host: affinity-nodeport-transition-fdq7m
Mar  1 11:55:57.428: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-9606, will wait for the garbage collector to delete the pods 03/01/23 11:55:57.444
Mar  1 11:55:57.511: INFO: Deleting ReplicationController affinity-nodeport-transition took: 11.777738ms
Mar  1 11:55:57.612: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.720689ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar  1 11:55:59.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9606" for this suite. 03/01/23 11:55:59.452
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","completed":78,"skipped":1451,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.315 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:55:50.145
    Mar  1 11:55:50.145: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename services 03/01/23 11:55:50.146
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:55:50.168
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:55:50.172
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2237
    STEP: creating service in namespace services-9606 03/01/23 11:55:50.176
    STEP: creating service affinity-nodeport-transition in namespace services-9606 03/01/23 11:55:50.176
    STEP: creating replication controller affinity-nodeport-transition in namespace services-9606 03/01/23 11:55:50.2
    I0301 11:55:50.210892      19 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-9606, replica count: 3
    I0301 11:55:53.262761      19 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar  1 11:55:53.279: INFO: Creating new exec pod
    Mar  1 11:55:53.292: INFO: Waiting up to 5m0s for pod "execpod-affinityhqxlm" in namespace "services-9606" to be "running"
    Mar  1 11:55:53.297: INFO: Pod "execpod-affinityhqxlm": Phase="Pending", Reason="", readiness=false. Elapsed: 5.004777ms
    Mar  1 11:55:55.305: INFO: Pod "execpod-affinityhqxlm": Phase="Running", Reason="", readiness=true. Elapsed: 2.012934592s
    Mar  1 11:55:55.305: INFO: Pod "execpod-affinityhqxlm" satisfied condition "running"
    Mar  1 11:55:56.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-9606 exec execpod-affinityhqxlm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
    Mar  1 11:55:56.464: INFO: stderr: "+ nc -v -t -w 2 affinity-nodeport-transition 80\n+ echo hostName\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Mar  1 11:55:56.464: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  1 11:55:56.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-9606 exec execpod-affinityhqxlm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.40.30 80'
    Mar  1 11:55:56.595: INFO: stderr: "+ nc -v -t -w 2 10.233.40.30 80\n+ echo hostName\nConnection to 10.233.40.30 80 port [tcp/http] succeeded!\n"
    Mar  1 11:55:56.595: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  1 11:55:56.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-9606 exec execpod-affinityhqxlm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.128.0.76 31250'
    Mar  1 11:55:56.731: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.128.0.76 31250\nConnection to 10.128.0.76 31250 port [tcp/*] succeeded!\n"
    Mar  1 11:55:56.731: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  1 11:55:56.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-9606 exec execpod-affinityhqxlm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.128.2.241 31250'
    Mar  1 11:55:56.868: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.128.2.241 31250\nConnection to 10.128.2.241 31250 port [tcp/*] succeeded!\n"
    Mar  1 11:55:56.868: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  1 11:55:56.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-9606 exec execpod-affinityhqxlm -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.128.0.178:31250/ ; done'
    Mar  1 11:55:57.150: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31250/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31250/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31250/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31250/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31250/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31250/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31250/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31250/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31250/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31250/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31250/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31250/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31250/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31250/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31250/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31250/\n"
    Mar  1 11:55:57.150: INFO: stdout: "\naffinity-nodeport-transition-hf8cm\naffinity-nodeport-transition-tdxpg\naffinity-nodeport-transition-hf8cm\naffinity-nodeport-transition-hf8cm\naffinity-nodeport-transition-fdq7m\naffinity-nodeport-transition-hf8cm\naffinity-nodeport-transition-hf8cm\naffinity-nodeport-transition-hf8cm\naffinity-nodeport-transition-tdxpg\naffinity-nodeport-transition-tdxpg\naffinity-nodeport-transition-tdxpg\naffinity-nodeport-transition-fdq7m\naffinity-nodeport-transition-hf8cm\naffinity-nodeport-transition-fdq7m\naffinity-nodeport-transition-tdxpg\naffinity-nodeport-transition-hf8cm"
    Mar  1 11:55:57.150: INFO: Received response from host: affinity-nodeport-transition-hf8cm
    Mar  1 11:55:57.150: INFO: Received response from host: affinity-nodeport-transition-tdxpg
    Mar  1 11:55:57.150: INFO: Received response from host: affinity-nodeport-transition-hf8cm
    Mar  1 11:55:57.150: INFO: Received response from host: affinity-nodeport-transition-hf8cm
    Mar  1 11:55:57.150: INFO: Received response from host: affinity-nodeport-transition-fdq7m
    Mar  1 11:55:57.150: INFO: Received response from host: affinity-nodeport-transition-hf8cm
    Mar  1 11:55:57.151: INFO: Received response from host: affinity-nodeport-transition-hf8cm
    Mar  1 11:55:57.151: INFO: Received response from host: affinity-nodeport-transition-hf8cm
    Mar  1 11:55:57.151: INFO: Received response from host: affinity-nodeport-transition-tdxpg
    Mar  1 11:55:57.151: INFO: Received response from host: affinity-nodeport-transition-tdxpg
    Mar  1 11:55:57.151: INFO: Received response from host: affinity-nodeport-transition-tdxpg
    Mar  1 11:55:57.151: INFO: Received response from host: affinity-nodeport-transition-fdq7m
    Mar  1 11:55:57.151: INFO: Received response from host: affinity-nodeport-transition-hf8cm
    Mar  1 11:55:57.151: INFO: Received response from host: affinity-nodeport-transition-fdq7m
    Mar  1 11:55:57.151: INFO: Received response from host: affinity-nodeport-transition-tdxpg
    Mar  1 11:55:57.151: INFO: Received response from host: affinity-nodeport-transition-hf8cm
    Mar  1 11:55:57.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-9606 exec execpod-affinityhqxlm -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.128.0.178:31250/ ; done'
    Mar  1 11:55:57.428: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31250/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31250/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31250/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31250/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31250/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31250/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31250/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31250/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31250/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31250/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31250/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31250/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31250/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31250/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31250/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.178:31250/\n"
    Mar  1 11:55:57.428: INFO: stdout: "\naffinity-nodeport-transition-fdq7m\naffinity-nodeport-transition-fdq7m\naffinity-nodeport-transition-fdq7m\naffinity-nodeport-transition-fdq7m\naffinity-nodeport-transition-fdq7m\naffinity-nodeport-transition-fdq7m\naffinity-nodeport-transition-fdq7m\naffinity-nodeport-transition-fdq7m\naffinity-nodeport-transition-fdq7m\naffinity-nodeport-transition-fdq7m\naffinity-nodeport-transition-fdq7m\naffinity-nodeport-transition-fdq7m\naffinity-nodeport-transition-fdq7m\naffinity-nodeport-transition-fdq7m\naffinity-nodeport-transition-fdq7m\naffinity-nodeport-transition-fdq7m"
    Mar  1 11:55:57.428: INFO: Received response from host: affinity-nodeport-transition-fdq7m
    Mar  1 11:55:57.428: INFO: Received response from host: affinity-nodeport-transition-fdq7m
    Mar  1 11:55:57.428: INFO: Received response from host: affinity-nodeport-transition-fdq7m
    Mar  1 11:55:57.428: INFO: Received response from host: affinity-nodeport-transition-fdq7m
    Mar  1 11:55:57.428: INFO: Received response from host: affinity-nodeport-transition-fdq7m
    Mar  1 11:55:57.428: INFO: Received response from host: affinity-nodeport-transition-fdq7m
    Mar  1 11:55:57.428: INFO: Received response from host: affinity-nodeport-transition-fdq7m
    Mar  1 11:55:57.428: INFO: Received response from host: affinity-nodeport-transition-fdq7m
    Mar  1 11:55:57.428: INFO: Received response from host: affinity-nodeport-transition-fdq7m
    Mar  1 11:55:57.428: INFO: Received response from host: affinity-nodeport-transition-fdq7m
    Mar  1 11:55:57.428: INFO: Received response from host: affinity-nodeport-transition-fdq7m
    Mar  1 11:55:57.428: INFO: Received response from host: affinity-nodeport-transition-fdq7m
    Mar  1 11:55:57.428: INFO: Received response from host: affinity-nodeport-transition-fdq7m
    Mar  1 11:55:57.428: INFO: Received response from host: affinity-nodeport-transition-fdq7m
    Mar  1 11:55:57.428: INFO: Received response from host: affinity-nodeport-transition-fdq7m
    Mar  1 11:55:57.428: INFO: Received response from host: affinity-nodeport-transition-fdq7m
    Mar  1 11:55:57.428: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-9606, will wait for the garbage collector to delete the pods 03/01/23 11:55:57.444
    Mar  1 11:55:57.511: INFO: Deleting ReplicationController affinity-nodeport-transition took: 11.777738ms
    Mar  1 11:55:57.612: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.720689ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar  1 11:55:59.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-9606" for this suite. 03/01/23 11:55:59.452
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:55:59.461
Mar  1 11:55:59.461: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename pods 03/01/23 11:55:59.461
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:55:59.482
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:55:59.488
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
STEP: creating the pod 03/01/23 11:55:59.491
STEP: setting up watch 03/01/23 11:55:59.492
STEP: submitting the pod to kubernetes 03/01/23 11:55:59.597
STEP: verifying the pod is in kubernetes 03/01/23 11:55:59.609
STEP: verifying pod creation was observed 03/01/23 11:55:59.617
Mar  1 11:55:59.617: INFO: Waiting up to 5m0s for pod "pod-submit-remove-36c1ebca-adf2-41e6-bf72-6b1411dcf78b" in namespace "pods-6023" to be "running"
Mar  1 11:55:59.623: INFO: Pod "pod-submit-remove-36c1ebca-adf2-41e6-bf72-6b1411dcf78b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.286358ms
Mar  1 11:56:01.628: INFO: Pod "pod-submit-remove-36c1ebca-adf2-41e6-bf72-6b1411dcf78b": Phase="Running", Reason="", readiness=true. Elapsed: 2.011087255s
Mar  1 11:56:01.629: INFO: Pod "pod-submit-remove-36c1ebca-adf2-41e6-bf72-6b1411dcf78b" satisfied condition "running"
STEP: deleting the pod gracefully 03/01/23 11:56:01.634
STEP: verifying pod deletion was observed 03/01/23 11:56:01.645
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Mar  1 11:56:04.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6023" for this suite. 03/01/23 11:56:04.112
{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","completed":79,"skipped":1454,"failed":0}
------------------------------
â€¢ [4.660 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:55:59.461
    Mar  1 11:55:59.461: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename pods 03/01/23 11:55:59.461
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:55:59.482
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:55:59.488
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:225
    STEP: creating the pod 03/01/23 11:55:59.491
    STEP: setting up watch 03/01/23 11:55:59.492
    STEP: submitting the pod to kubernetes 03/01/23 11:55:59.597
    STEP: verifying the pod is in kubernetes 03/01/23 11:55:59.609
    STEP: verifying pod creation was observed 03/01/23 11:55:59.617
    Mar  1 11:55:59.617: INFO: Waiting up to 5m0s for pod "pod-submit-remove-36c1ebca-adf2-41e6-bf72-6b1411dcf78b" in namespace "pods-6023" to be "running"
    Mar  1 11:55:59.623: INFO: Pod "pod-submit-remove-36c1ebca-adf2-41e6-bf72-6b1411dcf78b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.286358ms
    Mar  1 11:56:01.628: INFO: Pod "pod-submit-remove-36c1ebca-adf2-41e6-bf72-6b1411dcf78b": Phase="Running", Reason="", readiness=true. Elapsed: 2.011087255s
    Mar  1 11:56:01.629: INFO: Pod "pod-submit-remove-36c1ebca-adf2-41e6-bf72-6b1411dcf78b" satisfied condition "running"
    STEP: deleting the pod gracefully 03/01/23 11:56:01.634
    STEP: verifying pod deletion was observed 03/01/23 11:56:01.645
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Mar  1 11:56:04.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-6023" for this suite. 03/01/23 11:56:04.112
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:56:04.123
Mar  1 11:56:04.123: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename secrets 03/01/23 11:56:04.124
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:56:04.158
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:56:04.161
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
STEP: Creating secret with name secret-test-66c3dc93-718f-4312-b375-35691e52f395 03/01/23 11:56:04.164
STEP: Creating a pod to test consume secrets 03/01/23 11:56:04.17
Mar  1 11:56:04.179: INFO: Waiting up to 5m0s for pod "pod-secrets-9452bcd3-fe90-4311-9941-dd9d28a955c1" in namespace "secrets-2888" to be "Succeeded or Failed"
Mar  1 11:56:04.183: INFO: Pod "pod-secrets-9452bcd3-fe90-4311-9941-dd9d28a955c1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.870177ms
Mar  1 11:56:06.190: INFO: Pod "pod-secrets-9452bcd3-fe90-4311-9941-dd9d28a955c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011443288s
Mar  1 11:56:08.189: INFO: Pod "pod-secrets-9452bcd3-fe90-4311-9941-dd9d28a955c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009848997s
STEP: Saw pod success 03/01/23 11:56:08.189
Mar  1 11:56:08.189: INFO: Pod "pod-secrets-9452bcd3-fe90-4311-9941-dd9d28a955c1" satisfied condition "Succeeded or Failed"
Mar  1 11:56:08.199: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-secrets-9452bcd3-fe90-4311-9941-dd9d28a955c1 container secret-volume-test: <nil>
STEP: delete the pod 03/01/23 11:56:08.218
Mar  1 11:56:08.237: INFO: Waiting for pod pod-secrets-9452bcd3-fe90-4311-9941-dd9d28a955c1 to disappear
Mar  1 11:56:08.243: INFO: Pod pod-secrets-9452bcd3-fe90-4311-9941-dd9d28a955c1 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Mar  1 11:56:08.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2888" for this suite. 03/01/23 11:56:08.251
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":80,"skipped":1468,"failed":0}
------------------------------
â€¢ [4.139 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:56:04.123
    Mar  1 11:56:04.123: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename secrets 03/01/23 11:56:04.124
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:56:04.158
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:56:04.161
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:67
    STEP: Creating secret with name secret-test-66c3dc93-718f-4312-b375-35691e52f395 03/01/23 11:56:04.164
    STEP: Creating a pod to test consume secrets 03/01/23 11:56:04.17
    Mar  1 11:56:04.179: INFO: Waiting up to 5m0s for pod "pod-secrets-9452bcd3-fe90-4311-9941-dd9d28a955c1" in namespace "secrets-2888" to be "Succeeded or Failed"
    Mar  1 11:56:04.183: INFO: Pod "pod-secrets-9452bcd3-fe90-4311-9941-dd9d28a955c1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.870177ms
    Mar  1 11:56:06.190: INFO: Pod "pod-secrets-9452bcd3-fe90-4311-9941-dd9d28a955c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011443288s
    Mar  1 11:56:08.189: INFO: Pod "pod-secrets-9452bcd3-fe90-4311-9941-dd9d28a955c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009848997s
    STEP: Saw pod success 03/01/23 11:56:08.189
    Mar  1 11:56:08.189: INFO: Pod "pod-secrets-9452bcd3-fe90-4311-9941-dd9d28a955c1" satisfied condition "Succeeded or Failed"
    Mar  1 11:56:08.199: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-secrets-9452bcd3-fe90-4311-9941-dd9d28a955c1 container secret-volume-test: <nil>
    STEP: delete the pod 03/01/23 11:56:08.218
    Mar  1 11:56:08.237: INFO: Waiting for pod pod-secrets-9452bcd3-fe90-4311-9941-dd9d28a955c1 to disappear
    Mar  1 11:56:08.243: INFO: Pod pod-secrets-9452bcd3-fe90-4311-9941-dd9d28a955c1 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Mar  1 11:56:08.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-2888" for this suite. 03/01/23 11:56:08.251
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:56:08.264
Mar  1 11:56:08.265: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename init-container 03/01/23 11:56:08.265
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:56:08.291
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:56:08.299
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
STEP: creating the pod 03/01/23 11:56:08.335
Mar  1 11:56:08.335: INFO: PodSpec: initContainers in spec.initContainers
Mar  1 11:56:52.190: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-aec94fb3-34c0-49b5-87ba-8f360973da00", GenerateName:"", Namespace:"init-container-6712", SelfLink:"", UID:"06d01f14-7d1a-4888-95c7-c42a15586b06", ResourceVersion:"10291", Generation:0, CreationTimestamp:time.Date(2023, time.March, 1, 11, 56, 8, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"335126508"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"219644b2c00c0b68349b151586b4bc4736437deed3ff953dbd97b4aab1d1907a", "cni.projectcalico.org/podIP":"10.233.74.70/32", "cni.projectcalico.org/podIPs":"10.233.74.70/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.March, 1, 11, 56, 8, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000ed84e0), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.March, 1, 11, 56, 8, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000ed8528), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.March, 1, 11, 56, 52, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000ed8588), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-88zkk", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc0032f6620), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-88zkk", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-88zkk", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-88zkk", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc000beaa38), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"lab1-k8s-node-3", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc003fee380), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc000beabd0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc000beabf0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc000beabf8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc000beabfc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc000b95910), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 1, 11, 56, 8, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 1, 11, 56, 8, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 1, 11, 56, 8, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 1, 11, 56, 8, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.128.2.241", PodIP:"10.233.74.70", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.233.74.70"}}, StartTime:time.Date(2023, time.March, 1, 11, 56, 8, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc003fee460)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc003fee4d0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"containerd://23e8ec92a29c83732f192dd91a48462964dd67b842b09cc51b57378f72064dcc", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0032f66a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0032f6680), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0xc000beac8f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Mar  1 11:56:52.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6712" for this suite. 03/01/23 11:56:52.198
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","completed":81,"skipped":1483,"failed":0}
------------------------------
â€¢ [SLOW TEST] [43.942 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:56:08.264
    Mar  1 11:56:08.265: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename init-container 03/01/23 11:56:08.265
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:56:08.291
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:56:08.299
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:333
    STEP: creating the pod 03/01/23 11:56:08.335
    Mar  1 11:56:08.335: INFO: PodSpec: initContainers in spec.initContainers
    Mar  1 11:56:52.190: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-aec94fb3-34c0-49b5-87ba-8f360973da00", GenerateName:"", Namespace:"init-container-6712", SelfLink:"", UID:"06d01f14-7d1a-4888-95c7-c42a15586b06", ResourceVersion:"10291", Generation:0, CreationTimestamp:time.Date(2023, time.March, 1, 11, 56, 8, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"335126508"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"219644b2c00c0b68349b151586b4bc4736437deed3ff953dbd97b4aab1d1907a", "cni.projectcalico.org/podIP":"10.233.74.70/32", "cni.projectcalico.org/podIPs":"10.233.74.70/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.March, 1, 11, 56, 8, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000ed84e0), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.March, 1, 11, 56, 8, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000ed8528), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.March, 1, 11, 56, 52, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000ed8588), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-88zkk", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc0032f6620), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-88zkk", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-88zkk", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-88zkk", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc000beaa38), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"lab1-k8s-node-3", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc003fee380), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc000beabd0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc000beabf0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc000beabf8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc000beabfc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc000b95910), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 1, 11, 56, 8, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 1, 11, 56, 8, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 1, 11, 56, 8, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 1, 11, 56, 8, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.128.2.241", PodIP:"10.233.74.70", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.233.74.70"}}, StartTime:time.Date(2023, time.March, 1, 11, 56, 8, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc003fee460)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc003fee4d0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"containerd://23e8ec92a29c83732f192dd91a48462964dd67b842b09cc51b57378f72064dcc", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0032f66a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0032f6680), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0xc000beac8f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Mar  1 11:56:52.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-6712" for this suite. 03/01/23 11:56:52.198
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:56:52.208
Mar  1 11:56:52.208: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename secrets 03/01/23 11:56:52.208
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:56:52.229
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:56:52.237
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
STEP: Creating secret with name secret-test-4acfe320-d1cd-4adb-ac82-c0622d4331f5 03/01/23 11:56:52.24
STEP: Creating a pod to test consume secrets 03/01/23 11:56:52.246
Mar  1 11:56:52.261: INFO: Waiting up to 5m0s for pod "pod-secrets-5fe74fb5-e7f1-44b5-b3d7-f9f72683e87b" in namespace "secrets-4424" to be "Succeeded or Failed"
Mar  1 11:56:52.267: INFO: Pod "pod-secrets-5fe74fb5-e7f1-44b5-b3d7-f9f72683e87b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.740821ms
Mar  1 11:56:54.271: INFO: Pod "pod-secrets-5fe74fb5-e7f1-44b5-b3d7-f9f72683e87b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010688168s
Mar  1 11:56:56.277: INFO: Pod "pod-secrets-5fe74fb5-e7f1-44b5-b3d7-f9f72683e87b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016354009s
STEP: Saw pod success 03/01/23 11:56:56.277
Mar  1 11:56:56.277: INFO: Pod "pod-secrets-5fe74fb5-e7f1-44b5-b3d7-f9f72683e87b" satisfied condition "Succeeded or Failed"
Mar  1 11:56:56.282: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-secrets-5fe74fb5-e7f1-44b5-b3d7-f9f72683e87b container secret-env-test: <nil>
STEP: delete the pod 03/01/23 11:56:56.298
Mar  1 11:56:56.316: INFO: Waiting for pod pod-secrets-5fe74fb5-e7f1-44b5-b3d7-f9f72683e87b to disappear
Mar  1 11:56:56.321: INFO: Pod pod-secrets-5fe74fb5-e7f1-44b5-b3d7-f9f72683e87b no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Mar  1 11:56:56.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4424" for this suite. 03/01/23 11:56:56.33
{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","completed":82,"skipped":1490,"failed":0}
------------------------------
â€¢ [4.131 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:56:52.208
    Mar  1 11:56:52.208: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename secrets 03/01/23 11:56:52.208
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:56:52.229
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:56:52.237
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:45
    STEP: Creating secret with name secret-test-4acfe320-d1cd-4adb-ac82-c0622d4331f5 03/01/23 11:56:52.24
    STEP: Creating a pod to test consume secrets 03/01/23 11:56:52.246
    Mar  1 11:56:52.261: INFO: Waiting up to 5m0s for pod "pod-secrets-5fe74fb5-e7f1-44b5-b3d7-f9f72683e87b" in namespace "secrets-4424" to be "Succeeded or Failed"
    Mar  1 11:56:52.267: INFO: Pod "pod-secrets-5fe74fb5-e7f1-44b5-b3d7-f9f72683e87b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.740821ms
    Mar  1 11:56:54.271: INFO: Pod "pod-secrets-5fe74fb5-e7f1-44b5-b3d7-f9f72683e87b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010688168s
    Mar  1 11:56:56.277: INFO: Pod "pod-secrets-5fe74fb5-e7f1-44b5-b3d7-f9f72683e87b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016354009s
    STEP: Saw pod success 03/01/23 11:56:56.277
    Mar  1 11:56:56.277: INFO: Pod "pod-secrets-5fe74fb5-e7f1-44b5-b3d7-f9f72683e87b" satisfied condition "Succeeded or Failed"
    Mar  1 11:56:56.282: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-secrets-5fe74fb5-e7f1-44b5-b3d7-f9f72683e87b container secret-env-test: <nil>
    STEP: delete the pod 03/01/23 11:56:56.298
    Mar  1 11:56:56.316: INFO: Waiting for pod pod-secrets-5fe74fb5-e7f1-44b5-b3d7-f9f72683e87b to disappear
    Mar  1 11:56:56.321: INFO: Pod pod-secrets-5fe74fb5-e7f1-44b5-b3d7-f9f72683e87b no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Mar  1 11:56:56.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-4424" for this suite. 03/01/23 11:56:56.33
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 11:56:56.344
Mar  1 11:56:56.344: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename container-probe 03/01/23 11:56:56.344
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:56:56.366
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:56:56.37
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
STEP: Creating pod test-webserver-cf2b9667-91d2-44fd-8029-c7e419d7c929 in namespace container-probe-1545 03/01/23 11:56:56.373
Mar  1 11:56:56.384: INFO: Waiting up to 5m0s for pod "test-webserver-cf2b9667-91d2-44fd-8029-c7e419d7c929" in namespace "container-probe-1545" to be "not pending"
Mar  1 11:56:56.390: INFO: Pod "test-webserver-cf2b9667-91d2-44fd-8029-c7e419d7c929": Phase="Pending", Reason="", readiness=false. Elapsed: 5.503458ms
Mar  1 11:56:58.395: INFO: Pod "test-webserver-cf2b9667-91d2-44fd-8029-c7e419d7c929": Phase="Running", Reason="", readiness=true. Elapsed: 2.010882915s
Mar  1 11:56:58.395: INFO: Pod "test-webserver-cf2b9667-91d2-44fd-8029-c7e419d7c929" satisfied condition "not pending"
Mar  1 11:56:58.395: INFO: Started pod test-webserver-cf2b9667-91d2-44fd-8029-c7e419d7c929 in namespace container-probe-1545
STEP: checking the pod's current state and verifying that restartCount is present 03/01/23 11:56:58.395
Mar  1 11:56:58.400: INFO: Initial restart count of pod test-webserver-cf2b9667-91d2-44fd-8029-c7e419d7c929 is 0
STEP: deleting the pod 03/01/23 12:00:59.133
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Mar  1 12:00:59.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1545" for this suite. 03/01/23 12:00:59.18
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":83,"skipped":1539,"failed":0}
------------------------------
â€¢ [SLOW TEST] [242.846 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 11:56:56.344
    Mar  1 11:56:56.344: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename container-probe 03/01/23 11:56:56.344
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 11:56:56.366
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 11:56:56.37
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:211
    STEP: Creating pod test-webserver-cf2b9667-91d2-44fd-8029-c7e419d7c929 in namespace container-probe-1545 03/01/23 11:56:56.373
    Mar  1 11:56:56.384: INFO: Waiting up to 5m0s for pod "test-webserver-cf2b9667-91d2-44fd-8029-c7e419d7c929" in namespace "container-probe-1545" to be "not pending"
    Mar  1 11:56:56.390: INFO: Pod "test-webserver-cf2b9667-91d2-44fd-8029-c7e419d7c929": Phase="Pending", Reason="", readiness=false. Elapsed: 5.503458ms
    Mar  1 11:56:58.395: INFO: Pod "test-webserver-cf2b9667-91d2-44fd-8029-c7e419d7c929": Phase="Running", Reason="", readiness=true. Elapsed: 2.010882915s
    Mar  1 11:56:58.395: INFO: Pod "test-webserver-cf2b9667-91d2-44fd-8029-c7e419d7c929" satisfied condition "not pending"
    Mar  1 11:56:58.395: INFO: Started pod test-webserver-cf2b9667-91d2-44fd-8029-c7e419d7c929 in namespace container-probe-1545
    STEP: checking the pod's current state and verifying that restartCount is present 03/01/23 11:56:58.395
    Mar  1 11:56:58.400: INFO: Initial restart count of pod test-webserver-cf2b9667-91d2-44fd-8029-c7e419d7c929 is 0
    STEP: deleting the pod 03/01/23 12:00:59.133
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Mar  1 12:00:59.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-1545" for this suite. 03/01/23 12:00:59.18
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:00:59.195
Mar  1 12:00:59.201: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename proxy 03/01/23 12:00:59.202
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:00:59.225
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:00:59.23
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Mar  1 12:00:59.234: INFO: Creating pod...
Mar  1 12:00:59.244: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-5645" to be "running"
Mar  1 12:00:59.252: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 7.45853ms
Mar  1 12:01:01.258: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.013967487s
Mar  1 12:01:01.258: INFO: Pod "agnhost" satisfied condition "running"
Mar  1 12:01:01.258: INFO: Creating service...
Mar  1 12:01:01.271: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5645/pods/agnhost/proxy?method=DELETE
Mar  1 12:01:01.277: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Mar  1 12:01:01.277: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5645/pods/agnhost/proxy?method=OPTIONS
Mar  1 12:01:01.288: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Mar  1 12:01:01.288: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5645/pods/agnhost/proxy?method=PATCH
Mar  1 12:01:01.294: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Mar  1 12:01:01.294: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5645/pods/agnhost/proxy?method=POST
Mar  1 12:01:01.299: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Mar  1 12:01:01.299: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5645/pods/agnhost/proxy?method=PUT
Mar  1 12:01:01.304: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Mar  1 12:01:01.304: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5645/services/e2e-proxy-test-service/proxy?method=DELETE
Mar  1 12:01:01.312: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Mar  1 12:01:01.312: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5645/services/e2e-proxy-test-service/proxy?method=OPTIONS
Mar  1 12:01:01.320: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Mar  1 12:01:01.320: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5645/services/e2e-proxy-test-service/proxy?method=PATCH
Mar  1 12:01:01.327: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Mar  1 12:01:01.327: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5645/services/e2e-proxy-test-service/proxy?method=POST
Mar  1 12:01:01.335: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Mar  1 12:01:01.335: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5645/services/e2e-proxy-test-service/proxy?method=PUT
Mar  1 12:01:01.343: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Mar  1 12:01:01.343: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5645/pods/agnhost/proxy?method=GET
Mar  1 12:01:01.347: INFO: http.Client request:GET StatusCode:301
Mar  1 12:01:01.347: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5645/services/e2e-proxy-test-service/proxy?method=GET
Mar  1 12:01:01.354: INFO: http.Client request:GET StatusCode:301
Mar  1 12:01:01.354: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5645/pods/agnhost/proxy?method=HEAD
Mar  1 12:01:01.358: INFO: http.Client request:HEAD StatusCode:301
Mar  1 12:01:01.358: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5645/services/e2e-proxy-test-service/proxy?method=HEAD
Mar  1 12:01:01.363: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Mar  1 12:01:01.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-5645" for this suite. 03/01/23 12:01:01.37
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]","completed":84,"skipped":1556,"failed":0}
------------------------------
â€¢ [2.185 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:00:59.195
    Mar  1 12:00:59.201: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename proxy 03/01/23 12:00:59.202
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:00:59.225
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:00:59.23
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Mar  1 12:00:59.234: INFO: Creating pod...
    Mar  1 12:00:59.244: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-5645" to be "running"
    Mar  1 12:00:59.252: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 7.45853ms
    Mar  1 12:01:01.258: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.013967487s
    Mar  1 12:01:01.258: INFO: Pod "agnhost" satisfied condition "running"
    Mar  1 12:01:01.258: INFO: Creating service...
    Mar  1 12:01:01.271: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5645/pods/agnhost/proxy?method=DELETE
    Mar  1 12:01:01.277: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Mar  1 12:01:01.277: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5645/pods/agnhost/proxy?method=OPTIONS
    Mar  1 12:01:01.288: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Mar  1 12:01:01.288: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5645/pods/agnhost/proxy?method=PATCH
    Mar  1 12:01:01.294: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Mar  1 12:01:01.294: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5645/pods/agnhost/proxy?method=POST
    Mar  1 12:01:01.299: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Mar  1 12:01:01.299: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5645/pods/agnhost/proxy?method=PUT
    Mar  1 12:01:01.304: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Mar  1 12:01:01.304: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5645/services/e2e-proxy-test-service/proxy?method=DELETE
    Mar  1 12:01:01.312: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Mar  1 12:01:01.312: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5645/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Mar  1 12:01:01.320: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Mar  1 12:01:01.320: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5645/services/e2e-proxy-test-service/proxy?method=PATCH
    Mar  1 12:01:01.327: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Mar  1 12:01:01.327: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5645/services/e2e-proxy-test-service/proxy?method=POST
    Mar  1 12:01:01.335: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Mar  1 12:01:01.335: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5645/services/e2e-proxy-test-service/proxy?method=PUT
    Mar  1 12:01:01.343: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Mar  1 12:01:01.343: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5645/pods/agnhost/proxy?method=GET
    Mar  1 12:01:01.347: INFO: http.Client request:GET StatusCode:301
    Mar  1 12:01:01.347: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5645/services/e2e-proxy-test-service/proxy?method=GET
    Mar  1 12:01:01.354: INFO: http.Client request:GET StatusCode:301
    Mar  1 12:01:01.354: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5645/pods/agnhost/proxy?method=HEAD
    Mar  1 12:01:01.358: INFO: http.Client request:HEAD StatusCode:301
    Mar  1 12:01:01.358: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5645/services/e2e-proxy-test-service/proxy?method=HEAD
    Mar  1 12:01:01.363: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Mar  1 12:01:01.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-5645" for this suite. 03/01/23 12:01:01.37
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:01:01.38
Mar  1 12:01:01.380: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename replication-controller 03/01/23 12:01:01.381
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:01:01.404
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:01:01.408
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
STEP: Creating replication controller my-hostname-basic-10ca0486-9f45-45c3-98aa-80e65e0d991d 03/01/23 12:01:01.411
Mar  1 12:01:01.423: INFO: Pod name my-hostname-basic-10ca0486-9f45-45c3-98aa-80e65e0d991d: Found 0 pods out of 1
Mar  1 12:01:06.430: INFO: Pod name my-hostname-basic-10ca0486-9f45-45c3-98aa-80e65e0d991d: Found 1 pods out of 1
Mar  1 12:01:06.430: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-10ca0486-9f45-45c3-98aa-80e65e0d991d" are running
Mar  1 12:01:06.430: INFO: Waiting up to 5m0s for pod "my-hostname-basic-10ca0486-9f45-45c3-98aa-80e65e0d991d-9lxzn" in namespace "replication-controller-4921" to be "running"
Mar  1 12:01:06.435: INFO: Pod "my-hostname-basic-10ca0486-9f45-45c3-98aa-80e65e0d991d-9lxzn": Phase="Running", Reason="", readiness=true. Elapsed: 4.496216ms
Mar  1 12:01:06.435: INFO: Pod "my-hostname-basic-10ca0486-9f45-45c3-98aa-80e65e0d991d-9lxzn" satisfied condition "running"
Mar  1 12:01:06.435: INFO: Pod "my-hostname-basic-10ca0486-9f45-45c3-98aa-80e65e0d991d-9lxzn" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-01 12:01:01 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-01 12:01:02 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-01 12:01:02 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-01 12:01:01 +0000 UTC Reason: Message:}])
Mar  1 12:01:06.435: INFO: Trying to dial the pod
Mar  1 12:01:11.452: INFO: Controller my-hostname-basic-10ca0486-9f45-45c3-98aa-80e65e0d991d: Got expected result from replica 1 [my-hostname-basic-10ca0486-9f45-45c3-98aa-80e65e0d991d-9lxzn]: "my-hostname-basic-10ca0486-9f45-45c3-98aa-80e65e0d991d-9lxzn", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Mar  1 12:01:11.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4921" for this suite. 03/01/23 12:01:11.461
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","completed":85,"skipped":1563,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.090 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:01:01.38
    Mar  1 12:01:01.380: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename replication-controller 03/01/23 12:01:01.381
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:01:01.404
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:01:01.408
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:66
    STEP: Creating replication controller my-hostname-basic-10ca0486-9f45-45c3-98aa-80e65e0d991d 03/01/23 12:01:01.411
    Mar  1 12:01:01.423: INFO: Pod name my-hostname-basic-10ca0486-9f45-45c3-98aa-80e65e0d991d: Found 0 pods out of 1
    Mar  1 12:01:06.430: INFO: Pod name my-hostname-basic-10ca0486-9f45-45c3-98aa-80e65e0d991d: Found 1 pods out of 1
    Mar  1 12:01:06.430: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-10ca0486-9f45-45c3-98aa-80e65e0d991d" are running
    Mar  1 12:01:06.430: INFO: Waiting up to 5m0s for pod "my-hostname-basic-10ca0486-9f45-45c3-98aa-80e65e0d991d-9lxzn" in namespace "replication-controller-4921" to be "running"
    Mar  1 12:01:06.435: INFO: Pod "my-hostname-basic-10ca0486-9f45-45c3-98aa-80e65e0d991d-9lxzn": Phase="Running", Reason="", readiness=true. Elapsed: 4.496216ms
    Mar  1 12:01:06.435: INFO: Pod "my-hostname-basic-10ca0486-9f45-45c3-98aa-80e65e0d991d-9lxzn" satisfied condition "running"
    Mar  1 12:01:06.435: INFO: Pod "my-hostname-basic-10ca0486-9f45-45c3-98aa-80e65e0d991d-9lxzn" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-01 12:01:01 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-01 12:01:02 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-01 12:01:02 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-01 12:01:01 +0000 UTC Reason: Message:}])
    Mar  1 12:01:06.435: INFO: Trying to dial the pod
    Mar  1 12:01:11.452: INFO: Controller my-hostname-basic-10ca0486-9f45-45c3-98aa-80e65e0d991d: Got expected result from replica 1 [my-hostname-basic-10ca0486-9f45-45c3-98aa-80e65e0d991d-9lxzn]: "my-hostname-basic-10ca0486-9f45-45c3-98aa-80e65e0d991d-9lxzn", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Mar  1 12:01:11.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-4921" for this suite. 03/01/23 12:01:11.461
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:01:11.471
Mar  1 12:01:11.471: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename var-expansion 03/01/23 12:01:11.472
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:01:11.494
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:01:11.497
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
Mar  1 12:01:11.512: INFO: Waiting up to 2m0s for pod "var-expansion-674f9883-a06a-4fe9-8e24-d006fc934389" in namespace "var-expansion-9395" to be "container 0 failed with reason CreateContainerConfigError"
Mar  1 12:01:11.518: INFO: Pod "var-expansion-674f9883-a06a-4fe9-8e24-d006fc934389": Phase="Pending", Reason="", readiness=false. Elapsed: 5.801199ms
Mar  1 12:01:13.525: INFO: Pod "var-expansion-674f9883-a06a-4fe9-8e24-d006fc934389": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01265165s
Mar  1 12:01:13.525: INFO: Pod "var-expansion-674f9883-a06a-4fe9-8e24-d006fc934389" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Mar  1 12:01:13.525: INFO: Deleting pod "var-expansion-674f9883-a06a-4fe9-8e24-d006fc934389" in namespace "var-expansion-9395"
Mar  1 12:01:13.535: INFO: Wait up to 5m0s for pod "var-expansion-674f9883-a06a-4fe9-8e24-d006fc934389" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Mar  1 12:01:17.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9395" for this suite. 03/01/23 12:01:17.556
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","completed":86,"skipped":1578,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.095 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:01:11.471
    Mar  1 12:01:11.471: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename var-expansion 03/01/23 12:01:11.472
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:01:11.494
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:01:11.497
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:185
    Mar  1 12:01:11.512: INFO: Waiting up to 2m0s for pod "var-expansion-674f9883-a06a-4fe9-8e24-d006fc934389" in namespace "var-expansion-9395" to be "container 0 failed with reason CreateContainerConfigError"
    Mar  1 12:01:11.518: INFO: Pod "var-expansion-674f9883-a06a-4fe9-8e24-d006fc934389": Phase="Pending", Reason="", readiness=false. Elapsed: 5.801199ms
    Mar  1 12:01:13.525: INFO: Pod "var-expansion-674f9883-a06a-4fe9-8e24-d006fc934389": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01265165s
    Mar  1 12:01:13.525: INFO: Pod "var-expansion-674f9883-a06a-4fe9-8e24-d006fc934389" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Mar  1 12:01:13.525: INFO: Deleting pod "var-expansion-674f9883-a06a-4fe9-8e24-d006fc934389" in namespace "var-expansion-9395"
    Mar  1 12:01:13.535: INFO: Wait up to 5m0s for pod "var-expansion-674f9883-a06a-4fe9-8e24-d006fc934389" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Mar  1 12:01:17.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-9395" for this suite. 03/01/23 12:01:17.556
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:01:17.569
Mar  1 12:01:17.569: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename secrets 03/01/23 12:01:17.57
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:01:17.595
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:01:17.599
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
STEP: Creating projection with secret that has name secret-emptykey-test-54517aa9-cb91-4308-a7cb-b63d6602d269 03/01/23 12:01:17.603
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Mar  1 12:01:17.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1846" for this suite. 03/01/23 12:01:17.61
{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","completed":87,"skipped":1590,"failed":0}
------------------------------
â€¢ [0.050 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:01:17.569
    Mar  1 12:01:17.569: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename secrets 03/01/23 12:01:17.57
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:01:17.595
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:01:17.599
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:139
    STEP: Creating projection with secret that has name secret-emptykey-test-54517aa9-cb91-4308-a7cb-b63d6602d269 03/01/23 12:01:17.603
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Mar  1 12:01:17.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-1846" for this suite. 03/01/23 12:01:17.61
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:01:17.629
Mar  1 12:01:17.629: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename downward-api 03/01/23 12:01:17.63
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:01:17.65
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:01:17.653
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
STEP: Creating a pod to test downward API volume plugin 03/01/23 12:01:17.657
Mar  1 12:01:17.669: INFO: Waiting up to 5m0s for pod "downwardapi-volume-08cee079-707c-4e35-aed4-6aad950d275e" in namespace "downward-api-1874" to be "Succeeded or Failed"
Mar  1 12:01:17.673: INFO: Pod "downwardapi-volume-08cee079-707c-4e35-aed4-6aad950d275e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.11853ms
Mar  1 12:01:19.678: INFO: Pod "downwardapi-volume-08cee079-707c-4e35-aed4-6aad950d275e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009520038s
Mar  1 12:01:21.678: INFO: Pod "downwardapi-volume-08cee079-707c-4e35-aed4-6aad950d275e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009536077s
STEP: Saw pod success 03/01/23 12:01:21.678
Mar  1 12:01:21.679: INFO: Pod "downwardapi-volume-08cee079-707c-4e35-aed4-6aad950d275e" satisfied condition "Succeeded or Failed"
Mar  1 12:01:21.684: INFO: Trying to get logs from node lab1-k8s-node-3 pod downwardapi-volume-08cee079-707c-4e35-aed4-6aad950d275e container client-container: <nil>
STEP: delete the pod 03/01/23 12:01:21.702
Mar  1 12:01:21.718: INFO: Waiting for pod downwardapi-volume-08cee079-707c-4e35-aed4-6aad950d275e to disappear
Mar  1 12:01:21.722: INFO: Pod downwardapi-volume-08cee079-707c-4e35-aed4-6aad950d275e no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Mar  1 12:01:21.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1874" for this suite. 03/01/23 12:01:21.732
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","completed":88,"skipped":1634,"failed":0}
------------------------------
â€¢ [4.112 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:01:17.629
    Mar  1 12:01:17.629: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename downward-api 03/01/23 12:01:17.63
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:01:17.65
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:01:17.653
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:192
    STEP: Creating a pod to test downward API volume plugin 03/01/23 12:01:17.657
    Mar  1 12:01:17.669: INFO: Waiting up to 5m0s for pod "downwardapi-volume-08cee079-707c-4e35-aed4-6aad950d275e" in namespace "downward-api-1874" to be "Succeeded or Failed"
    Mar  1 12:01:17.673: INFO: Pod "downwardapi-volume-08cee079-707c-4e35-aed4-6aad950d275e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.11853ms
    Mar  1 12:01:19.678: INFO: Pod "downwardapi-volume-08cee079-707c-4e35-aed4-6aad950d275e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009520038s
    Mar  1 12:01:21.678: INFO: Pod "downwardapi-volume-08cee079-707c-4e35-aed4-6aad950d275e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009536077s
    STEP: Saw pod success 03/01/23 12:01:21.678
    Mar  1 12:01:21.679: INFO: Pod "downwardapi-volume-08cee079-707c-4e35-aed4-6aad950d275e" satisfied condition "Succeeded or Failed"
    Mar  1 12:01:21.684: INFO: Trying to get logs from node lab1-k8s-node-3 pod downwardapi-volume-08cee079-707c-4e35-aed4-6aad950d275e container client-container: <nil>
    STEP: delete the pod 03/01/23 12:01:21.702
    Mar  1 12:01:21.718: INFO: Waiting for pod downwardapi-volume-08cee079-707c-4e35-aed4-6aad950d275e to disappear
    Mar  1 12:01:21.722: INFO: Pod downwardapi-volume-08cee079-707c-4e35-aed4-6aad950d275e no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Mar  1 12:01:21.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-1874" for this suite. 03/01/23 12:01:21.732
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:01:21.741
Mar  1 12:01:21.741: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename webhook 03/01/23 12:01:21.742
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:01:21.765
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:01:21.773
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/01/23 12:01:21.798
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/01/23 12:01:22.236
STEP: Deploying the webhook pod 03/01/23 12:01:22.248
STEP: Wait for the deployment to be ready 03/01/23 12:01:22.263
Mar  1 12:01:22.279: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/01/23 12:01:24.294
STEP: Verifying the service has paired with the endpoint 03/01/23 12:01:24.31
Mar  1 12:01:25.310: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 03/01/23 12:01:25.315
STEP: create a pod that should be updated by the webhook 03/01/23 12:01:25.331
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  1 12:01:25.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7853" for this suite. 03/01/23 12:01:25.368
STEP: Destroying namespace "webhook-7853-markers" for this suite. 03/01/23 12:01:25.379
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","completed":89,"skipped":1650,"failed":0}
------------------------------
â€¢ [3.705 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:01:21.741
    Mar  1 12:01:21.741: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename webhook 03/01/23 12:01:21.742
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:01:21.765
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:01:21.773
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/01/23 12:01:21.798
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/01/23 12:01:22.236
    STEP: Deploying the webhook pod 03/01/23 12:01:22.248
    STEP: Wait for the deployment to be ready 03/01/23 12:01:22.263
    Mar  1 12:01:22.279: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/01/23 12:01:24.294
    STEP: Verifying the service has paired with the endpoint 03/01/23 12:01:24.31
    Mar  1 12:01:25.310: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:263
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 03/01/23 12:01:25.315
    STEP: create a pod that should be updated by the webhook 03/01/23 12:01:25.331
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  1 12:01:25.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-7853" for this suite. 03/01/23 12:01:25.368
    STEP: Destroying namespace "webhook-7853-markers" for this suite. 03/01/23 12:01:25.379
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:01:25.447
Mar  1 12:01:25.447: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename init-container 03/01/23 12:01:25.447
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:01:25.472
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:01:25.477
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
STEP: creating the pod 03/01/23 12:01:25.481
Mar  1 12:01:25.481: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Mar  1 12:01:28.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-813" for this suite. 03/01/23 12:01:28.764
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","completed":90,"skipped":1655,"failed":0}
------------------------------
â€¢ [3.327 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:01:25.447
    Mar  1 12:01:25.447: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename init-container 03/01/23 12:01:25.447
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:01:25.472
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:01:25.477
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:254
    STEP: creating the pod 03/01/23 12:01:25.481
    Mar  1 12:01:25.481: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Mar  1 12:01:28.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-813" for this suite. 03/01/23 12:01:28.764
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:01:28.777
Mar  1 12:01:28.777: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename container-probe 03/01/23 12:01:28.778
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:01:28.802
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:01:28.806
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
STEP: Creating pod liveness-dca05494-ab0c-45ee-bfc5-e82d97d828a0 in namespace container-probe-6736 03/01/23 12:01:28.81
Mar  1 12:01:28.823: INFO: Waiting up to 5m0s for pod "liveness-dca05494-ab0c-45ee-bfc5-e82d97d828a0" in namespace "container-probe-6736" to be "not pending"
Mar  1 12:01:28.830: INFO: Pod "liveness-dca05494-ab0c-45ee-bfc5-e82d97d828a0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.865739ms
Mar  1 12:01:30.837: INFO: Pod "liveness-dca05494-ab0c-45ee-bfc5-e82d97d828a0": Phase="Running", Reason="", readiness=true. Elapsed: 2.013914457s
Mar  1 12:01:30.837: INFO: Pod "liveness-dca05494-ab0c-45ee-bfc5-e82d97d828a0" satisfied condition "not pending"
Mar  1 12:01:30.837: INFO: Started pod liveness-dca05494-ab0c-45ee-bfc5-e82d97d828a0 in namespace container-probe-6736
STEP: checking the pod's current state and verifying that restartCount is present 03/01/23 12:01:30.837
Mar  1 12:01:30.842: INFO: Initial restart count of pod liveness-dca05494-ab0c-45ee-bfc5-e82d97d828a0 is 0
STEP: deleting the pod 03/01/23 12:05:31.595
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Mar  1 12:05:31.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6736" for this suite. 03/01/23 12:05:31.627
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","completed":91,"skipped":1692,"failed":0}
------------------------------
â€¢ [SLOW TEST] [242.861 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:01:28.777
    Mar  1 12:01:28.777: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename container-probe 03/01/23 12:01:28.778
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:01:28.802
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:01:28.806
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:180
    STEP: Creating pod liveness-dca05494-ab0c-45ee-bfc5-e82d97d828a0 in namespace container-probe-6736 03/01/23 12:01:28.81
    Mar  1 12:01:28.823: INFO: Waiting up to 5m0s for pod "liveness-dca05494-ab0c-45ee-bfc5-e82d97d828a0" in namespace "container-probe-6736" to be "not pending"
    Mar  1 12:01:28.830: INFO: Pod "liveness-dca05494-ab0c-45ee-bfc5-e82d97d828a0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.865739ms
    Mar  1 12:01:30.837: INFO: Pod "liveness-dca05494-ab0c-45ee-bfc5-e82d97d828a0": Phase="Running", Reason="", readiness=true. Elapsed: 2.013914457s
    Mar  1 12:01:30.837: INFO: Pod "liveness-dca05494-ab0c-45ee-bfc5-e82d97d828a0" satisfied condition "not pending"
    Mar  1 12:01:30.837: INFO: Started pod liveness-dca05494-ab0c-45ee-bfc5-e82d97d828a0 in namespace container-probe-6736
    STEP: checking the pod's current state and verifying that restartCount is present 03/01/23 12:01:30.837
    Mar  1 12:01:30.842: INFO: Initial restart count of pod liveness-dca05494-ab0c-45ee-bfc5-e82d97d828a0 is 0
    STEP: deleting the pod 03/01/23 12:05:31.595
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Mar  1 12:05:31.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-6736" for this suite. 03/01/23 12:05:31.627
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:05:31.643
Mar  1 12:05:31.643: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename endpointslice 03/01/23 12:05:31.644
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:05:31.669
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:05:31.672
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
STEP: referencing a single matching pod 03/01/23 12:05:36.754
STEP: referencing matching pods with named port 03/01/23 12:05:41.766
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 03/01/23 12:05:46.78
STEP: recreating EndpointSlices after they've been deleted 03/01/23 12:05:51.795
Mar  1 12:05:51.822: INFO: EndpointSlice for Service endpointslice-2108/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Mar  1 12:06:01.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-2108" for this suite. 03/01/23 12:06:01.841
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","completed":92,"skipped":1700,"failed":0}
------------------------------
â€¢ [SLOW TEST] [30.207 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:05:31.643
    Mar  1 12:05:31.643: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename endpointslice 03/01/23 12:05:31.644
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:05:31.669
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:05:31.672
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:204
    STEP: referencing a single matching pod 03/01/23 12:05:36.754
    STEP: referencing matching pods with named port 03/01/23 12:05:41.766
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 03/01/23 12:05:46.78
    STEP: recreating EndpointSlices after they've been deleted 03/01/23 12:05:51.795
    Mar  1 12:05:51.822: INFO: EndpointSlice for Service endpointslice-2108/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Mar  1 12:06:01.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-2108" for this suite. 03/01/23 12:06:01.841
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:06:01.851
Mar  1 12:06:01.851: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename configmap 03/01/23 12:06:01.851
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:06:01.871
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:06:01.876
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
STEP: Creating configMap with name configmap-test-volume-1ccd03d1-9bda-4ef8-9079-7cf3c9a6313a 03/01/23 12:06:01.879
STEP: Creating a pod to test consume configMaps 03/01/23 12:06:01.885
Mar  1 12:06:01.898: INFO: Waiting up to 5m0s for pod "pod-configmaps-f1ed8c36-2f7a-4fa0-a0cd-f1e72f084235" in namespace "configmap-564" to be "Succeeded or Failed"
Mar  1 12:06:01.909: INFO: Pod "pod-configmaps-f1ed8c36-2f7a-4fa0-a0cd-f1e72f084235": Phase="Pending", Reason="", readiness=false. Elapsed: 11.105413ms
Mar  1 12:06:03.914: INFO: Pod "pod-configmaps-f1ed8c36-2f7a-4fa0-a0cd-f1e72f084235": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016761329s
Mar  1 12:06:05.918: INFO: Pod "pod-configmaps-f1ed8c36-2f7a-4fa0-a0cd-f1e72f084235": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020592194s
STEP: Saw pod success 03/01/23 12:06:05.918
Mar  1 12:06:05.919: INFO: Pod "pod-configmaps-f1ed8c36-2f7a-4fa0-a0cd-f1e72f084235" satisfied condition "Succeeded or Failed"
Mar  1 12:06:05.923: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-configmaps-f1ed8c36-2f7a-4fa0-a0cd-f1e72f084235 container agnhost-container: <nil>
STEP: delete the pod 03/01/23 12:06:05.944
Mar  1 12:06:05.960: INFO: Waiting for pod pod-configmaps-f1ed8c36-2f7a-4fa0-a0cd-f1e72f084235 to disappear
Mar  1 12:06:05.964: INFO: Pod pod-configmaps-f1ed8c36-2f7a-4fa0-a0cd-f1e72f084235 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Mar  1 12:06:05.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-564" for this suite. 03/01/23 12:06:05.97
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":93,"skipped":1702,"failed":0}
------------------------------
â€¢ [4.127 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:06:01.851
    Mar  1 12:06:01.851: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename configmap 03/01/23 12:06:01.851
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:06:01.871
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:06:01.876
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:73
    STEP: Creating configMap with name configmap-test-volume-1ccd03d1-9bda-4ef8-9079-7cf3c9a6313a 03/01/23 12:06:01.879
    STEP: Creating a pod to test consume configMaps 03/01/23 12:06:01.885
    Mar  1 12:06:01.898: INFO: Waiting up to 5m0s for pod "pod-configmaps-f1ed8c36-2f7a-4fa0-a0cd-f1e72f084235" in namespace "configmap-564" to be "Succeeded or Failed"
    Mar  1 12:06:01.909: INFO: Pod "pod-configmaps-f1ed8c36-2f7a-4fa0-a0cd-f1e72f084235": Phase="Pending", Reason="", readiness=false. Elapsed: 11.105413ms
    Mar  1 12:06:03.914: INFO: Pod "pod-configmaps-f1ed8c36-2f7a-4fa0-a0cd-f1e72f084235": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016761329s
    Mar  1 12:06:05.918: INFO: Pod "pod-configmaps-f1ed8c36-2f7a-4fa0-a0cd-f1e72f084235": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020592194s
    STEP: Saw pod success 03/01/23 12:06:05.918
    Mar  1 12:06:05.919: INFO: Pod "pod-configmaps-f1ed8c36-2f7a-4fa0-a0cd-f1e72f084235" satisfied condition "Succeeded or Failed"
    Mar  1 12:06:05.923: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-configmaps-f1ed8c36-2f7a-4fa0-a0cd-f1e72f084235 container agnhost-container: <nil>
    STEP: delete the pod 03/01/23 12:06:05.944
    Mar  1 12:06:05.960: INFO: Waiting for pod pod-configmaps-f1ed8c36-2f7a-4fa0-a0cd-f1e72f084235 to disappear
    Mar  1 12:06:05.964: INFO: Pod pod-configmaps-f1ed8c36-2f7a-4fa0-a0cd-f1e72f084235 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Mar  1 12:06:05.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-564" for this suite. 03/01/23 12:06:05.97
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:06:05.978
Mar  1 12:06:05.979: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename svc-latency 03/01/23 12:06:05.979
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:06:06.004
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:06:06.008
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Mar  1 12:06:06.012: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: creating replication controller svc-latency-rc in namespace svc-latency-5654 03/01/23 12:06:06.013
I0301 12:06:06.021935      19 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-5654, replica count: 1
I0301 12:06:07.072515      19 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0301 12:06:08.072936      19 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  1 12:06:08.190: INFO: Created: latency-svc-ht22z
Mar  1 12:06:08.198: INFO: Got endpoints: latency-svc-ht22z [25.684321ms]
Mar  1 12:06:08.229: INFO: Created: latency-svc-5hz9w
Mar  1 12:06:08.234: INFO: Created: latency-svc-pqrlr
Mar  1 12:06:08.243: INFO: Got endpoints: latency-svc-5hz9w [44.084943ms]
Mar  1 12:06:08.247: INFO: Got endpoints: latency-svc-pqrlr [47.71294ms]
Mar  1 12:06:08.249: INFO: Created: latency-svc-7sqsc
Mar  1 12:06:08.257: INFO: Got endpoints: latency-svc-7sqsc [58.137582ms]
Mar  1 12:06:08.261: INFO: Created: latency-svc-x95lb
Mar  1 12:06:08.272: INFO: Got endpoints: latency-svc-x95lb [72.432207ms]
Mar  1 12:06:08.273: INFO: Created: latency-svc-kgfcj
Mar  1 12:06:08.283: INFO: Got endpoints: latency-svc-kgfcj [83.282746ms]
Mar  1 12:06:08.287: INFO: Created: latency-svc-qj6mz
Mar  1 12:06:08.297: INFO: Got endpoints: latency-svc-qj6mz [97.359701ms]
Mar  1 12:06:08.299: INFO: Created: latency-svc-8fpl9
Mar  1 12:06:08.310: INFO: Created: latency-svc-djpgm
Mar  1 12:06:08.311: INFO: Got endpoints: latency-svc-8fpl9 [111.324683ms]
Mar  1 12:06:08.318: INFO: Got endpoints: latency-svc-djpgm [119.467602ms]
Mar  1 12:06:08.326: INFO: Created: latency-svc-vz29g
Mar  1 12:06:08.340: INFO: Got endpoints: latency-svc-vz29g [141.6682ms]
Mar  1 12:06:08.340: INFO: Created: latency-svc-mtm8p
Mar  1 12:06:08.347: INFO: Created: latency-svc-m8fnv
Mar  1 12:06:08.358: INFO: Got endpoints: latency-svc-mtm8p [158.808185ms]
Mar  1 12:06:08.358: INFO: Got endpoints: latency-svc-m8fnv [159.132727ms]
Mar  1 12:06:08.362: INFO: Created: latency-svc-fckqh
Mar  1 12:06:08.373: INFO: Got endpoints: latency-svc-fckqh [173.999831ms]
Mar  1 12:06:08.374: INFO: Created: latency-svc-2ctps
Mar  1 12:06:08.384: INFO: Created: latency-svc-lrmsf
Mar  1 12:06:08.387: INFO: Got endpoints: latency-svc-2ctps [187.833567ms]
Mar  1 12:06:08.398: INFO: Got endpoints: latency-svc-lrmsf [198.773016ms]
Mar  1 12:06:08.494: INFO: Created: latency-svc-7rnlq
Mar  1 12:06:08.494: INFO: Created: latency-svc-sgj9b
Mar  1 12:06:08.495: INFO: Created: latency-svc-jk2hs
Mar  1 12:06:08.499: INFO: Created: latency-svc-2p2cr
Mar  1 12:06:08.499: INFO: Created: latency-svc-4jrk5
Mar  1 12:06:08.500: INFO: Created: latency-svc-rdwb2
Mar  1 12:06:08.516: INFO: Created: latency-svc-98458
Mar  1 12:06:08.516: INFO: Created: latency-svc-8rlvs
Mar  1 12:06:08.517: INFO: Created: latency-svc-h9746
Mar  1 12:06:08.517: INFO: Created: latency-svc-fgczs
Mar  1 12:06:08.517: INFO: Created: latency-svc-gbxbf
Mar  1 12:06:08.517: INFO: Created: latency-svc-tlmvv
Mar  1 12:06:08.517: INFO: Created: latency-svc-dwdfw
Mar  1 12:06:08.516: INFO: Created: latency-svc-8jpvk
Mar  1 12:06:08.518: INFO: Created: latency-svc-nggr2
Mar  1 12:06:08.528: INFO: Got endpoints: latency-svc-7rnlq [256.907492ms]
Mar  1 12:06:08.529: INFO: Got endpoints: latency-svc-jk2hs [155.105033ms]
Mar  1 12:06:08.529: INFO: Got endpoints: latency-svc-sgj9b [271.742763ms]
Mar  1 12:06:08.531: INFO: Got endpoints: latency-svc-8jpvk [132.442273ms]
Mar  1 12:06:08.531: INFO: Got endpoints: latency-svc-rdwb2 [190.319564ms]
Mar  1 12:06:08.531: INFO: Got endpoints: latency-svc-2p2cr [234.444248ms]
Mar  1 12:06:08.546: INFO: Got endpoints: latency-svc-tlmvv [187.577052ms]
Mar  1 12:06:08.546: INFO: Got endpoints: latency-svc-h9746 [263.344469ms]
Mar  1 12:06:08.560: INFO: Got endpoints: latency-svc-dwdfw [249.830281ms]
Mar  1 12:06:08.561: INFO: Got endpoints: latency-svc-gbxbf [361.60564ms]
Mar  1 12:06:08.561: INFO: Got endpoints: latency-svc-98458 [202.814295ms]
Mar  1 12:06:08.561: INFO: Got endpoints: latency-svc-4jrk5 [317.798488ms]
Mar  1 12:06:08.562: INFO: Got endpoints: latency-svc-8rlvs [243.424593ms]
Mar  1 12:06:08.566: INFO: Got endpoints: latency-svc-fgczs [319.178583ms]
Mar  1 12:06:08.568: INFO: Got endpoints: latency-svc-nggr2 [181.07714ms]
Mar  1 12:06:08.578: INFO: Created: latency-svc-f4994
Mar  1 12:06:08.580: INFO: Got endpoints: latency-svc-f4994 [51.699667ms]
Mar  1 12:06:08.585: INFO: Created: latency-svc-7jdl2
Mar  1 12:06:08.590: INFO: Got endpoints: latency-svc-7jdl2 [60.834221ms]
Mar  1 12:06:08.595: INFO: Created: latency-svc-mjvv7
Mar  1 12:06:08.605: INFO: Got endpoints: latency-svc-mjvv7 [75.497003ms]
Mar  1 12:06:08.612: INFO: Created: latency-svc-78frf
Mar  1 12:06:08.617: INFO: Created: latency-svc-7trf4
Mar  1 12:06:08.620: INFO: Got endpoints: latency-svc-78frf [89.040604ms]
Mar  1 12:06:08.626: INFO: Got endpoints: latency-svc-7trf4 [94.503652ms]
Mar  1 12:06:08.630: INFO: Created: latency-svc-vrl5q
Mar  1 12:06:08.637: INFO: Got endpoints: latency-svc-vrl5q [91.028159ms]
Mar  1 12:06:08.643: INFO: Created: latency-svc-rghmp
Mar  1 12:06:08.650: INFO: Got endpoints: latency-svc-rghmp [103.299108ms]
Mar  1 12:06:08.674: INFO: Created: latency-svc-ldrr2
Mar  1 12:06:08.683: INFO: Got endpoints: latency-svc-ldrr2 [152.131098ms]
Mar  1 12:06:08.684: INFO: Created: latency-svc-t27gc
Mar  1 12:06:08.694: INFO: Got endpoints: latency-svc-t27gc [133.571616ms]
Mar  1 12:06:08.703: INFO: Created: latency-svc-4s2g4
Mar  1 12:06:08.708: INFO: Created: latency-svc-ngtbk
Mar  1 12:06:08.713: INFO: Got endpoints: latency-svc-4s2g4 [150.92899ms]
Mar  1 12:06:08.718: INFO: Created: latency-svc-ggxd9
Mar  1 12:06:08.728: INFO: Created: latency-svc-hkglr
Mar  1 12:06:08.737: INFO: Created: latency-svc-2275s
Mar  1 12:06:08.744: INFO: Created: latency-svc-mnsqw
Mar  1 12:06:08.751: INFO: Got endpoints: latency-svc-ngtbk [189.691575ms]
Mar  1 12:06:08.755: INFO: Created: latency-svc-k5n9h
Mar  1 12:06:08.764: INFO: Created: latency-svc-tbzq6
Mar  1 12:06:08.771: INFO: Created: latency-svc-8wsqr
Mar  1 12:06:08.780: INFO: Created: latency-svc-7j9gn
Mar  1 12:06:08.790: INFO: Created: latency-svc-j7rjf
Mar  1 12:06:08.799: INFO: Got endpoints: latency-svc-ggxd9 [237.831585ms]
Mar  1 12:06:08.800: INFO: Created: latency-svc-mdlmd
Mar  1 12:06:08.813: INFO: Created: latency-svc-4ms2m
Mar  1 12:06:08.819: INFO: Created: latency-svc-wq8gn
Mar  1 12:06:08.828: INFO: Created: latency-svc-l7rfd
Mar  1 12:06:08.837: INFO: Created: latency-svc-lzq9v
Mar  1 12:06:08.846: INFO: Created: latency-svc-wk6gk
Mar  1 12:06:08.851: INFO: Got endpoints: latency-svc-hkglr [289.992008ms]
Mar  1 12:06:08.855: INFO: Created: latency-svc-zmzgx
Mar  1 12:06:08.868: INFO: Created: latency-svc-ld8sq
Mar  1 12:06:08.898: INFO: Got endpoints: latency-svc-2275s [332.200557ms]
Mar  1 12:06:08.914: INFO: Created: latency-svc-57jxk
Mar  1 12:06:08.948: INFO: Got endpoints: latency-svc-mnsqw [379.78036ms]
Mar  1 12:06:08.965: INFO: Created: latency-svc-gk6h2
Mar  1 12:06:09.000: INFO: Got endpoints: latency-svc-k5n9h [419.633661ms]
Mar  1 12:06:09.015: INFO: Created: latency-svc-z7zn6
Mar  1 12:06:09.057: INFO: Got endpoints: latency-svc-tbzq6 [466.700124ms]
Mar  1 12:06:09.078: INFO: Created: latency-svc-l4w7j
Mar  1 12:06:09.098: INFO: Got endpoints: latency-svc-8wsqr [492.894072ms]
Mar  1 12:06:09.112: INFO: Created: latency-svc-9ql9b
Mar  1 12:06:09.149: INFO: Got endpoints: latency-svc-7j9gn [529.229524ms]
Mar  1 12:06:09.165: INFO: Created: latency-svc-8tv7n
Mar  1 12:06:09.198: INFO: Got endpoints: latency-svc-j7rjf [572.1497ms]
Mar  1 12:06:09.214: INFO: Created: latency-svc-85qdp
Mar  1 12:06:09.249: INFO: Got endpoints: latency-svc-mdlmd [611.571358ms]
Mar  1 12:06:09.264: INFO: Created: latency-svc-v9zn2
Mar  1 12:06:09.298: INFO: Got endpoints: latency-svc-4ms2m [647.959464ms]
Mar  1 12:06:09.314: INFO: Created: latency-svc-bhvjw
Mar  1 12:06:09.349: INFO: Got endpoints: latency-svc-wq8gn [666.583809ms]
Mar  1 12:06:09.364: INFO: Created: latency-svc-76rkt
Mar  1 12:06:09.400: INFO: Got endpoints: latency-svc-l7rfd [705.456049ms]
Mar  1 12:06:09.413: INFO: Created: latency-svc-xz6cf
Mar  1 12:06:09.448: INFO: Got endpoints: latency-svc-lzq9v [735.41745ms]
Mar  1 12:06:09.463: INFO: Created: latency-svc-d7hnt
Mar  1 12:06:09.499: INFO: Got endpoints: latency-svc-wk6gk [748.390208ms]
Mar  1 12:06:09.514: INFO: Created: latency-svc-x59sd
Mar  1 12:06:09.548: INFO: Got endpoints: latency-svc-zmzgx [749.647196ms]
Mar  1 12:06:09.562: INFO: Created: latency-svc-x8m5l
Mar  1 12:06:09.599: INFO: Got endpoints: latency-svc-ld8sq [748.443144ms]
Mar  1 12:06:09.616: INFO: Created: latency-svc-gkhlq
Mar  1 12:06:09.648: INFO: Got endpoints: latency-svc-57jxk [749.36937ms]
Mar  1 12:06:09.664: INFO: Created: latency-svc-pqjsn
Mar  1 12:06:09.700: INFO: Got endpoints: latency-svc-gk6h2 [752.070416ms]
Mar  1 12:06:09.713: INFO: Created: latency-svc-hf9lg
Mar  1 12:06:09.749: INFO: Got endpoints: latency-svc-z7zn6 [748.79269ms]
Mar  1 12:06:09.764: INFO: Created: latency-svc-dhs5b
Mar  1 12:06:09.798: INFO: Got endpoints: latency-svc-l4w7j [741.314141ms]
Mar  1 12:06:09.812: INFO: Created: latency-svc-q8wtd
Mar  1 12:06:09.849: INFO: Got endpoints: latency-svc-9ql9b [751.063022ms]
Mar  1 12:06:09.866: INFO: Created: latency-svc-bltgf
Mar  1 12:06:09.898: INFO: Got endpoints: latency-svc-8tv7n [748.750736ms]
Mar  1 12:06:09.915: INFO: Created: latency-svc-nqspf
Mar  1 12:06:09.949: INFO: Got endpoints: latency-svc-85qdp [751.139508ms]
Mar  1 12:06:09.962: INFO: Created: latency-svc-wrk9x
Mar  1 12:06:09.998: INFO: Got endpoints: latency-svc-v9zn2 [749.424512ms]
Mar  1 12:06:10.014: INFO: Created: latency-svc-cdkw7
Mar  1 12:06:10.047: INFO: Got endpoints: latency-svc-bhvjw [748.610327ms]
Mar  1 12:06:10.062: INFO: Created: latency-svc-bbmzj
Mar  1 12:06:10.099: INFO: Got endpoints: latency-svc-76rkt [749.649049ms]
Mar  1 12:06:10.115: INFO: Created: latency-svc-8l644
Mar  1 12:06:10.150: INFO: Got endpoints: latency-svc-xz6cf [750.059474ms]
Mar  1 12:06:10.163: INFO: Created: latency-svc-vmf4j
Mar  1 12:06:10.201: INFO: Got endpoints: latency-svc-d7hnt [751.941387ms]
Mar  1 12:06:10.216: INFO: Created: latency-svc-5t4x9
Mar  1 12:06:10.247: INFO: Got endpoints: latency-svc-x59sd [748.069004ms]
Mar  1 12:06:10.261: INFO: Created: latency-svc-6ts2s
Mar  1 12:06:10.300: INFO: Got endpoints: latency-svc-x8m5l [751.094482ms]
Mar  1 12:06:10.313: INFO: Created: latency-svc-478qm
Mar  1 12:06:10.349: INFO: Got endpoints: latency-svc-gkhlq [749.637316ms]
Mar  1 12:06:10.365: INFO: Created: latency-svc-9dg7g
Mar  1 12:06:10.398: INFO: Got endpoints: latency-svc-pqjsn [750.23561ms]
Mar  1 12:06:10.412: INFO: Created: latency-svc-l6n69
Mar  1 12:06:10.448: INFO: Got endpoints: latency-svc-hf9lg [747.976856ms]
Mar  1 12:06:10.461: INFO: Created: latency-svc-hk8sw
Mar  1 12:06:10.498: INFO: Got endpoints: latency-svc-dhs5b [748.604744ms]
Mar  1 12:06:10.512: INFO: Created: latency-svc-wkhrr
Mar  1 12:06:10.556: INFO: Got endpoints: latency-svc-q8wtd [758.013088ms]
Mar  1 12:06:10.570: INFO: Created: latency-svc-bcrmb
Mar  1 12:06:10.599: INFO: Got endpoints: latency-svc-bltgf [750.222953ms]
Mar  1 12:06:10.613: INFO: Created: latency-svc-7b4jj
Mar  1 12:06:10.649: INFO: Got endpoints: latency-svc-nqspf [750.904803ms]
Mar  1 12:06:10.663: INFO: Created: latency-svc-jbpkf
Mar  1 12:06:10.698: INFO: Got endpoints: latency-svc-wrk9x [748.781587ms]
Mar  1 12:06:10.712: INFO: Created: latency-svc-r92x6
Mar  1 12:06:10.747: INFO: Got endpoints: latency-svc-cdkw7 [748.431821ms]
Mar  1 12:06:10.763: INFO: Created: latency-svc-zsk42
Mar  1 12:06:10.798: INFO: Got endpoints: latency-svc-bbmzj [751.73263ms]
Mar  1 12:06:10.814: INFO: Created: latency-svc-vkb2w
Mar  1 12:06:10.849: INFO: Got endpoints: latency-svc-8l644 [749.503983ms]
Mar  1 12:06:10.864: INFO: Created: latency-svc-qtqhn
Mar  1 12:06:10.899: INFO: Got endpoints: latency-svc-vmf4j [748.896117ms]
Mar  1 12:06:10.914: INFO: Created: latency-svc-9h2dh
Mar  1 12:06:10.949: INFO: Got endpoints: latency-svc-5t4x9 [747.949877ms]
Mar  1 12:06:10.965: INFO: Created: latency-svc-4dzgz
Mar  1 12:06:10.998: INFO: Got endpoints: latency-svc-6ts2s [750.615143ms]
Mar  1 12:06:11.017: INFO: Created: latency-svc-m965w
Mar  1 12:06:11.047: INFO: Got endpoints: latency-svc-478qm [747.459662ms]
Mar  1 12:06:11.062: INFO: Created: latency-svc-r5sj8
Mar  1 12:06:11.099: INFO: Got endpoints: latency-svc-9dg7g [749.910071ms]
Mar  1 12:06:11.117: INFO: Created: latency-svc-wwgvv
Mar  1 12:06:11.150: INFO: Got endpoints: latency-svc-l6n69 [751.637307ms]
Mar  1 12:06:11.165: INFO: Created: latency-svc-rx2q4
Mar  1 12:06:11.198: INFO: Got endpoints: latency-svc-hk8sw [749.515238ms]
Mar  1 12:06:11.211: INFO: Created: latency-svc-twm8s
Mar  1 12:06:11.249: INFO: Got endpoints: latency-svc-wkhrr [751.425378ms]
Mar  1 12:06:11.264: INFO: Created: latency-svc-n4cld
Mar  1 12:06:11.300: INFO: Got endpoints: latency-svc-bcrmb [744.1748ms]
Mar  1 12:06:11.316: INFO: Created: latency-svc-h9lqx
Mar  1 12:06:11.348: INFO: Got endpoints: latency-svc-7b4jj [748.825304ms]
Mar  1 12:06:11.366: INFO: Created: latency-svc-n8f2h
Mar  1 12:06:11.397: INFO: Got endpoints: latency-svc-jbpkf [747.726077ms]
Mar  1 12:06:11.414: INFO: Created: latency-svc-n8g5t
Mar  1 12:06:11.448: INFO: Got endpoints: latency-svc-r92x6 [749.789725ms]
Mar  1 12:06:11.465: INFO: Created: latency-svc-dk8sn
Mar  1 12:06:11.498: INFO: Got endpoints: latency-svc-zsk42 [750.79133ms]
Mar  1 12:06:11.516: INFO: Created: latency-svc-nkh5h
Mar  1 12:06:11.547: INFO: Got endpoints: latency-svc-vkb2w [748.3855ms]
Mar  1 12:06:11.561: INFO: Created: latency-svc-wds2n
Mar  1 12:06:11.600: INFO: Got endpoints: latency-svc-qtqhn [751.324718ms]
Mar  1 12:06:11.615: INFO: Created: latency-svc-9vt5s
Mar  1 12:06:11.648: INFO: Got endpoints: latency-svc-9h2dh [748.852154ms]
Mar  1 12:06:11.662: INFO: Created: latency-svc-s25lx
Mar  1 12:06:11.696: INFO: Got endpoints: latency-svc-4dzgz [747.341909ms]
Mar  1 12:06:11.715: INFO: Created: latency-svc-kfnz8
Mar  1 12:06:11.748: INFO: Got endpoints: latency-svc-m965w [749.810147ms]
Mar  1 12:06:11.764: INFO: Created: latency-svc-9c7dm
Mar  1 12:06:11.798: INFO: Got endpoints: latency-svc-r5sj8 [751.124999ms]
Mar  1 12:06:11.810: INFO: Created: latency-svc-c54xk
Mar  1 12:06:11.849: INFO: Got endpoints: latency-svc-wwgvv [750.158724ms]
Mar  1 12:06:11.864: INFO: Created: latency-svc-724jb
Mar  1 12:06:11.898: INFO: Got endpoints: latency-svc-rx2q4 [747.76327ms]
Mar  1 12:06:11.911: INFO: Created: latency-svc-2l9lw
Mar  1 12:06:11.950: INFO: Got endpoints: latency-svc-twm8s [752.410413ms]
Mar  1 12:06:11.965: INFO: Created: latency-svc-pxcsx
Mar  1 12:06:11.999: INFO: Got endpoints: latency-svc-n4cld [749.611135ms]
Mar  1 12:06:12.015: INFO: Created: latency-svc-r54jp
Mar  1 12:06:12.050: INFO: Got endpoints: latency-svc-h9lqx [749.497152ms]
Mar  1 12:06:12.066: INFO: Created: latency-svc-nsxxv
Mar  1 12:06:12.098: INFO: Got endpoints: latency-svc-n8f2h [749.475096ms]
Mar  1 12:06:12.114: INFO: Created: latency-svc-qqg6z
Mar  1 12:06:12.147: INFO: Got endpoints: latency-svc-n8g5t [750.551829ms]
Mar  1 12:06:12.164: INFO: Created: latency-svc-jfq58
Mar  1 12:06:12.200: INFO: Got endpoints: latency-svc-dk8sn [752.221139ms]
Mar  1 12:06:12.214: INFO: Created: latency-svc-6wfln
Mar  1 12:06:12.249: INFO: Got endpoints: latency-svc-nkh5h [751.37435ms]
Mar  1 12:06:12.266: INFO: Created: latency-svc-52rn2
Mar  1 12:06:12.297: INFO: Got endpoints: latency-svc-wds2n [750.218326ms]
Mar  1 12:06:12.314: INFO: Created: latency-svc-q28d2
Mar  1 12:06:12.349: INFO: Got endpoints: latency-svc-9vt5s [748.321039ms]
Mar  1 12:06:12.363: INFO: Created: latency-svc-grlvp
Mar  1 12:06:12.398: INFO: Got endpoints: latency-svc-s25lx [749.983632ms]
Mar  1 12:06:12.414: INFO: Created: latency-svc-bbrw6
Mar  1 12:06:12.447: INFO: Got endpoints: latency-svc-kfnz8 [751.015134ms]
Mar  1 12:06:12.463: INFO: Created: latency-svc-pztqs
Mar  1 12:06:12.501: INFO: Got endpoints: latency-svc-9c7dm [752.220786ms]
Mar  1 12:06:12.516: INFO: Created: latency-svc-jm5q4
Mar  1 12:06:12.547: INFO: Got endpoints: latency-svc-c54xk [748.707459ms]
Mar  1 12:06:12.564: INFO: Created: latency-svc-84hq6
Mar  1 12:06:12.598: INFO: Got endpoints: latency-svc-724jb [748.497784ms]
Mar  1 12:06:12.613: INFO: Created: latency-svc-r5p5g
Mar  1 12:06:12.648: INFO: Got endpoints: latency-svc-2l9lw [750.19134ms]
Mar  1 12:06:12.666: INFO: Created: latency-svc-8tzbm
Mar  1 12:06:12.698: INFO: Got endpoints: latency-svc-pxcsx [747.320856ms]
Mar  1 12:06:12.713: INFO: Created: latency-svc-7kjvh
Mar  1 12:06:12.751: INFO: Got endpoints: latency-svc-r54jp [751.955029ms]
Mar  1 12:06:12.768: INFO: Created: latency-svc-zjm98
Mar  1 12:06:12.797: INFO: Got endpoints: latency-svc-nsxxv [747.200075ms]
Mar  1 12:06:12.812: INFO: Created: latency-svc-s4w2w
Mar  1 12:06:12.848: INFO: Got endpoints: latency-svc-qqg6z [749.57811ms]
Mar  1 12:06:12.862: INFO: Created: latency-svc-t2g7p
Mar  1 12:06:12.900: INFO: Got endpoints: latency-svc-jfq58 [752.683691ms]
Mar  1 12:06:12.918: INFO: Created: latency-svc-8j5p8
Mar  1 12:06:12.951: INFO: Got endpoints: latency-svc-6wfln [750.697033ms]
Mar  1 12:06:12.975: INFO: Created: latency-svc-h8btk
Mar  1 12:06:12.998: INFO: Got endpoints: latency-svc-52rn2 [748.204533ms]
Mar  1 12:06:13.014: INFO: Created: latency-svc-pdkjw
Mar  1 12:06:13.052: INFO: Got endpoints: latency-svc-q28d2 [754.474531ms]
Mar  1 12:06:13.073: INFO: Created: latency-svc-7hcl4
Mar  1 12:06:13.100: INFO: Got endpoints: latency-svc-grlvp [750.904274ms]
Mar  1 12:06:13.116: INFO: Created: latency-svc-h8rd8
Mar  1 12:06:13.149: INFO: Got endpoints: latency-svc-bbrw6 [750.857147ms]
Mar  1 12:06:13.163: INFO: Created: latency-svc-kqqbt
Mar  1 12:06:13.200: INFO: Got endpoints: latency-svc-pztqs [752.152672ms]
Mar  1 12:06:13.216: INFO: Created: latency-svc-4qn8c
Mar  1 12:06:13.248: INFO: Got endpoints: latency-svc-jm5q4 [747.475633ms]
Mar  1 12:06:13.265: INFO: Created: latency-svc-bmrc4
Mar  1 12:06:13.298: INFO: Got endpoints: latency-svc-84hq6 [750.811993ms]
Mar  1 12:06:13.313: INFO: Created: latency-svc-z8wp4
Mar  1 12:06:13.350: INFO: Got endpoints: latency-svc-r5p5g [752.050374ms]
Mar  1 12:06:13.366: INFO: Created: latency-svc-zzpg4
Mar  1 12:06:13.404: INFO: Got endpoints: latency-svc-8tzbm [755.930332ms]
Mar  1 12:06:13.426: INFO: Created: latency-svc-tgxfr
Mar  1 12:06:13.449: INFO: Got endpoints: latency-svc-7kjvh [751.696762ms]
Mar  1 12:06:13.464: INFO: Created: latency-svc-hxm6b
Mar  1 12:06:13.498: INFO: Got endpoints: latency-svc-zjm98 [747.264763ms]
Mar  1 12:06:13.514: INFO: Created: latency-svc-2z7pb
Mar  1 12:06:13.547: INFO: Got endpoints: latency-svc-s4w2w [750.074274ms]
Mar  1 12:06:13.562: INFO: Created: latency-svc-f9trp
Mar  1 12:06:13.598: INFO: Got endpoints: latency-svc-t2g7p [750.515921ms]
Mar  1 12:06:13.616: INFO: Created: latency-svc-sld7p
Mar  1 12:06:13.647: INFO: Got endpoints: latency-svc-8j5p8 [746.690137ms]
Mar  1 12:06:13.662: INFO: Created: latency-svc-4phl8
Mar  1 12:06:13.699: INFO: Got endpoints: latency-svc-h8btk [748.229404ms]
Mar  1 12:06:13.715: INFO: Created: latency-svc-dj8rb
Mar  1 12:06:13.749: INFO: Got endpoints: latency-svc-pdkjw [751.001658ms]
Mar  1 12:06:13.763: INFO: Created: latency-svc-62w7p
Mar  1 12:06:13.799: INFO: Got endpoints: latency-svc-7hcl4 [747.652283ms]
Mar  1 12:06:13.818: INFO: Created: latency-svc-4m2nq
Mar  1 12:06:13.850: INFO: Got endpoints: latency-svc-h8rd8 [749.807741ms]
Mar  1 12:06:13.864: INFO: Created: latency-svc-l26tf
Mar  1 12:06:13.900: INFO: Got endpoints: latency-svc-kqqbt [751.351519ms]
Mar  1 12:06:13.916: INFO: Created: latency-svc-rqh9h
Mar  1 12:06:13.947: INFO: Got endpoints: latency-svc-4qn8c [747.535012ms]
Mar  1 12:06:13.966: INFO: Created: latency-svc-jdwp6
Mar  1 12:06:14.002: INFO: Got endpoints: latency-svc-bmrc4 [753.646982ms]
Mar  1 12:06:14.017: INFO: Created: latency-svc-9bwwc
Mar  1 12:06:14.049: INFO: Got endpoints: latency-svc-z8wp4 [750.152683ms]
Mar  1 12:06:14.063: INFO: Created: latency-svc-mflbq
Mar  1 12:06:14.097: INFO: Got endpoints: latency-svc-zzpg4 [746.917372ms]
Mar  1 12:06:14.112: INFO: Created: latency-svc-8dzrz
Mar  1 12:06:14.148: INFO: Got endpoints: latency-svc-tgxfr [743.604465ms]
Mar  1 12:06:14.162: INFO: Created: latency-svc-j58n2
Mar  1 12:06:14.201: INFO: Got endpoints: latency-svc-hxm6b [751.590954ms]
Mar  1 12:06:14.215: INFO: Created: latency-svc-h7wr2
Mar  1 12:06:14.248: INFO: Got endpoints: latency-svc-2z7pb [749.979459ms]
Mar  1 12:06:14.264: INFO: Created: latency-svc-qr7z7
Mar  1 12:06:14.298: INFO: Got endpoints: latency-svc-f9trp [750.708706ms]
Mar  1 12:06:14.312: INFO: Created: latency-svc-fwtln
Mar  1 12:06:14.350: INFO: Got endpoints: latency-svc-sld7p [751.911307ms]
Mar  1 12:06:14.366: INFO: Created: latency-svc-pfq28
Mar  1 12:06:14.397: INFO: Got endpoints: latency-svc-4phl8 [749.941704ms]
Mar  1 12:06:14.414: INFO: Created: latency-svc-qxmv8
Mar  1 12:06:14.451: INFO: Got endpoints: latency-svc-dj8rb [751.435836ms]
Mar  1 12:06:14.466: INFO: Created: latency-svc-tmv59
Mar  1 12:06:14.499: INFO: Got endpoints: latency-svc-62w7p [750.694132ms]
Mar  1 12:06:14.513: INFO: Created: latency-svc-lc626
Mar  1 12:06:14.547: INFO: Got endpoints: latency-svc-4m2nq [747.683529ms]
Mar  1 12:06:14.564: INFO: Created: latency-svc-lfr59
Mar  1 12:06:14.599: INFO: Got endpoints: latency-svc-l26tf [749.054145ms]
Mar  1 12:06:14.612: INFO: Created: latency-svc-snfbk
Mar  1 12:06:14.650: INFO: Got endpoints: latency-svc-rqh9h [749.314858ms]
Mar  1 12:06:14.663: INFO: Created: latency-svc-n9dvt
Mar  1 12:06:14.697: INFO: Got endpoints: latency-svc-jdwp6 [749.305313ms]
Mar  1 12:06:14.716: INFO: Created: latency-svc-tgfq7
Mar  1 12:06:14.748: INFO: Got endpoints: latency-svc-9bwwc [745.947194ms]
Mar  1 12:06:14.762: INFO: Created: latency-svc-gh7sz
Mar  1 12:06:14.799: INFO: Got endpoints: latency-svc-mflbq [750.37661ms]
Mar  1 12:06:14.812: INFO: Created: latency-svc-d96c2
Mar  1 12:06:14.848: INFO: Got endpoints: latency-svc-8dzrz [750.676233ms]
Mar  1 12:06:14.863: INFO: Created: latency-svc-pj2wh
Mar  1 12:06:14.900: INFO: Got endpoints: latency-svc-j58n2 [751.555171ms]
Mar  1 12:06:14.916: INFO: Created: latency-svc-gc78n
Mar  1 12:06:14.949: INFO: Got endpoints: latency-svc-h7wr2 [747.598441ms]
Mar  1 12:06:14.964: INFO: Created: latency-svc-4ghjd
Mar  1 12:06:14.997: INFO: Got endpoints: latency-svc-qr7z7 [748.825816ms]
Mar  1 12:06:15.013: INFO: Created: latency-svc-v8hjf
Mar  1 12:06:15.049: INFO: Got endpoints: latency-svc-fwtln [750.474312ms]
Mar  1 12:06:15.063: INFO: Created: latency-svc-b9cjp
Mar  1 12:06:15.100: INFO: Got endpoints: latency-svc-pfq28 [749.976026ms]
Mar  1 12:06:15.115: INFO: Created: latency-svc-pq656
Mar  1 12:06:15.157: INFO: Got endpoints: latency-svc-qxmv8 [759.978933ms]
Mar  1 12:06:15.176: INFO: Created: latency-svc-v9mz9
Mar  1 12:06:15.198: INFO: Got endpoints: latency-svc-tmv59 [746.961914ms]
Mar  1 12:06:15.215: INFO: Created: latency-svc-bj6bv
Mar  1 12:06:15.250: INFO: Got endpoints: latency-svc-lc626 [751.000133ms]
Mar  1 12:06:15.264: INFO: Created: latency-svc-57szj
Mar  1 12:06:15.297: INFO: Got endpoints: latency-svc-lfr59 [749.744357ms]
Mar  1 12:06:15.313: INFO: Created: latency-svc-2mj8m
Mar  1 12:06:15.348: INFO: Got endpoints: latency-svc-snfbk [749.21507ms]
Mar  1 12:06:15.364: INFO: Created: latency-svc-bdq92
Mar  1 12:06:15.401: INFO: Got endpoints: latency-svc-n9dvt [750.922058ms]
Mar  1 12:06:15.415: INFO: Created: latency-svc-g2k58
Mar  1 12:06:15.448: INFO: Got endpoints: latency-svc-tgfq7 [750.994ms]
Mar  1 12:06:15.464: INFO: Created: latency-svc-252gq
Mar  1 12:06:15.500: INFO: Got endpoints: latency-svc-gh7sz [751.584817ms]
Mar  1 12:06:15.516: INFO: Created: latency-svc-4fwsn
Mar  1 12:06:15.548: INFO: Got endpoints: latency-svc-d96c2 [748.779826ms]
Mar  1 12:06:15.565: INFO: Created: latency-svc-kcnnq
Mar  1 12:06:15.597: INFO: Got endpoints: latency-svc-pj2wh [749.050013ms]
Mar  1 12:06:15.614: INFO: Created: latency-svc-8wh4j
Mar  1 12:06:15.649: INFO: Got endpoints: latency-svc-gc78n [748.998052ms]
Mar  1 12:06:15.663: INFO: Created: latency-svc-mht6j
Mar  1 12:06:15.698: INFO: Got endpoints: latency-svc-4ghjd [749.488371ms]
Mar  1 12:06:15.712: INFO: Created: latency-svc-qg9np
Mar  1 12:06:15.747: INFO: Got endpoints: latency-svc-v8hjf [749.922336ms]
Mar  1 12:06:15.762: INFO: Created: latency-svc-8ltdf
Mar  1 12:06:15.798: INFO: Got endpoints: latency-svc-b9cjp [748.747247ms]
Mar  1 12:06:15.814: INFO: Created: latency-svc-kc5cn
Mar  1 12:06:15.848: INFO: Got endpoints: latency-svc-pq656 [746.902168ms]
Mar  1 12:06:15.862: INFO: Created: latency-svc-d2w2c
Mar  1 12:06:15.897: INFO: Got endpoints: latency-svc-v9mz9 [739.218999ms]
Mar  1 12:06:15.914: INFO: Created: latency-svc-phw8m
Mar  1 12:06:15.951: INFO: Got endpoints: latency-svc-bj6bv [752.938741ms]
Mar  1 12:06:15.970: INFO: Created: latency-svc-qbp59
Mar  1 12:06:15.998: INFO: Got endpoints: latency-svc-57szj [748.02265ms]
Mar  1 12:06:16.015: INFO: Created: latency-svc-5hm76
Mar  1 12:06:16.052: INFO: Got endpoints: latency-svc-2mj8m [754.602051ms]
Mar  1 12:06:16.097: INFO: Got endpoints: latency-svc-bdq92 [749.182251ms]
Mar  1 12:06:16.149: INFO: Got endpoints: latency-svc-g2k58 [748.221711ms]
Mar  1 12:06:16.199: INFO: Got endpoints: latency-svc-252gq [750.445151ms]
Mar  1 12:06:16.250: INFO: Got endpoints: latency-svc-4fwsn [750.182402ms]
Mar  1 12:06:16.300: INFO: Got endpoints: latency-svc-kcnnq [751.375308ms]
Mar  1 12:06:16.346: INFO: Got endpoints: latency-svc-8wh4j [748.864536ms]
Mar  1 12:06:16.398: INFO: Got endpoints: latency-svc-mht6j [748.915268ms]
Mar  1 12:06:16.448: INFO: Got endpoints: latency-svc-qg9np [749.07958ms]
Mar  1 12:06:16.497: INFO: Got endpoints: latency-svc-8ltdf [749.880441ms]
Mar  1 12:06:16.550: INFO: Got endpoints: latency-svc-kc5cn [751.534896ms]
Mar  1 12:06:16.600: INFO: Got endpoints: latency-svc-d2w2c [751.751435ms]
Mar  1 12:06:16.649: INFO: Got endpoints: latency-svc-phw8m [752.103472ms]
Mar  1 12:06:16.699: INFO: Got endpoints: latency-svc-qbp59 [747.824268ms]
Mar  1 12:06:16.748: INFO: Got endpoints: latency-svc-5hm76 [749.51844ms]
Mar  1 12:06:16.748: INFO: Latencies: [44.084943ms 47.71294ms 51.699667ms 58.137582ms 60.834221ms 72.432207ms 75.497003ms 83.282746ms 89.040604ms 91.028159ms 94.503652ms 97.359701ms 103.299108ms 111.324683ms 119.467602ms 132.442273ms 133.571616ms 141.6682ms 150.92899ms 152.131098ms 155.105033ms 158.808185ms 159.132727ms 173.999831ms 181.07714ms 187.577052ms 187.833567ms 189.691575ms 190.319564ms 198.773016ms 202.814295ms 234.444248ms 237.831585ms 243.424593ms 249.830281ms 256.907492ms 263.344469ms 271.742763ms 289.992008ms 317.798488ms 319.178583ms 332.200557ms 361.60564ms 379.78036ms 419.633661ms 466.700124ms 492.894072ms 529.229524ms 572.1497ms 611.571358ms 647.959464ms 666.583809ms 705.456049ms 735.41745ms 739.218999ms 741.314141ms 743.604465ms 744.1748ms 745.947194ms 746.690137ms 746.902168ms 746.917372ms 746.961914ms 747.200075ms 747.264763ms 747.320856ms 747.341909ms 747.459662ms 747.475633ms 747.535012ms 747.598441ms 747.652283ms 747.683529ms 747.726077ms 747.76327ms 747.824268ms 747.949877ms 747.976856ms 748.02265ms 748.069004ms 748.204533ms 748.221711ms 748.229404ms 748.321039ms 748.3855ms 748.390208ms 748.431821ms 748.443144ms 748.497784ms 748.604744ms 748.610327ms 748.707459ms 748.747247ms 748.750736ms 748.779826ms 748.781587ms 748.79269ms 748.825304ms 748.825816ms 748.852154ms 748.864536ms 748.896117ms 748.915268ms 748.998052ms 749.050013ms 749.054145ms 749.07958ms 749.182251ms 749.21507ms 749.305313ms 749.314858ms 749.36937ms 749.424512ms 749.475096ms 749.488371ms 749.497152ms 749.503983ms 749.515238ms 749.51844ms 749.57811ms 749.611135ms 749.637316ms 749.647196ms 749.649049ms 749.744357ms 749.789725ms 749.807741ms 749.810147ms 749.880441ms 749.910071ms 749.922336ms 749.941704ms 749.976026ms 749.979459ms 749.983632ms 750.059474ms 750.074274ms 750.152683ms 750.158724ms 750.182402ms 750.19134ms 750.218326ms 750.222953ms 750.23561ms 750.37661ms 750.445151ms 750.474312ms 750.515921ms 750.551829ms 750.615143ms 750.676233ms 750.694132ms 750.697033ms 750.708706ms 750.79133ms 750.811993ms 750.857147ms 750.904274ms 750.904803ms 750.922058ms 750.994ms 751.000133ms 751.001658ms 751.015134ms 751.063022ms 751.094482ms 751.124999ms 751.139508ms 751.324718ms 751.351519ms 751.37435ms 751.375308ms 751.425378ms 751.435836ms 751.534896ms 751.555171ms 751.584817ms 751.590954ms 751.637307ms 751.696762ms 751.73263ms 751.751435ms 751.911307ms 751.941387ms 751.955029ms 752.050374ms 752.070416ms 752.103472ms 752.152672ms 752.220786ms 752.221139ms 752.410413ms 752.683691ms 752.938741ms 753.646982ms 754.474531ms 754.602051ms 755.930332ms 758.013088ms 759.978933ms]
Mar  1 12:06:16.748: INFO: 50 %ile: 748.864536ms
Mar  1 12:06:16.749: INFO: 90 %ile: 751.73263ms
Mar  1 12:06:16.749: INFO: 99 %ile: 758.013088ms
Mar  1 12:06:16.749: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:187
Mar  1 12:06:16.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-5654" for this suite. 03/01/23 12:06:16.759
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","completed":94,"skipped":1717,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.790 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:06:05.978
    Mar  1 12:06:05.979: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename svc-latency 03/01/23 12:06:05.979
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:06:06.004
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:06:06.008
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Mar  1 12:06:06.012: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-5654 03/01/23 12:06:06.013
    I0301 12:06:06.021935      19 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-5654, replica count: 1
    I0301 12:06:07.072515      19 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0301 12:06:08.072936      19 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar  1 12:06:08.190: INFO: Created: latency-svc-ht22z
    Mar  1 12:06:08.198: INFO: Got endpoints: latency-svc-ht22z [25.684321ms]
    Mar  1 12:06:08.229: INFO: Created: latency-svc-5hz9w
    Mar  1 12:06:08.234: INFO: Created: latency-svc-pqrlr
    Mar  1 12:06:08.243: INFO: Got endpoints: latency-svc-5hz9w [44.084943ms]
    Mar  1 12:06:08.247: INFO: Got endpoints: latency-svc-pqrlr [47.71294ms]
    Mar  1 12:06:08.249: INFO: Created: latency-svc-7sqsc
    Mar  1 12:06:08.257: INFO: Got endpoints: latency-svc-7sqsc [58.137582ms]
    Mar  1 12:06:08.261: INFO: Created: latency-svc-x95lb
    Mar  1 12:06:08.272: INFO: Got endpoints: latency-svc-x95lb [72.432207ms]
    Mar  1 12:06:08.273: INFO: Created: latency-svc-kgfcj
    Mar  1 12:06:08.283: INFO: Got endpoints: latency-svc-kgfcj [83.282746ms]
    Mar  1 12:06:08.287: INFO: Created: latency-svc-qj6mz
    Mar  1 12:06:08.297: INFO: Got endpoints: latency-svc-qj6mz [97.359701ms]
    Mar  1 12:06:08.299: INFO: Created: latency-svc-8fpl9
    Mar  1 12:06:08.310: INFO: Created: latency-svc-djpgm
    Mar  1 12:06:08.311: INFO: Got endpoints: latency-svc-8fpl9 [111.324683ms]
    Mar  1 12:06:08.318: INFO: Got endpoints: latency-svc-djpgm [119.467602ms]
    Mar  1 12:06:08.326: INFO: Created: latency-svc-vz29g
    Mar  1 12:06:08.340: INFO: Got endpoints: latency-svc-vz29g [141.6682ms]
    Mar  1 12:06:08.340: INFO: Created: latency-svc-mtm8p
    Mar  1 12:06:08.347: INFO: Created: latency-svc-m8fnv
    Mar  1 12:06:08.358: INFO: Got endpoints: latency-svc-mtm8p [158.808185ms]
    Mar  1 12:06:08.358: INFO: Got endpoints: latency-svc-m8fnv [159.132727ms]
    Mar  1 12:06:08.362: INFO: Created: latency-svc-fckqh
    Mar  1 12:06:08.373: INFO: Got endpoints: latency-svc-fckqh [173.999831ms]
    Mar  1 12:06:08.374: INFO: Created: latency-svc-2ctps
    Mar  1 12:06:08.384: INFO: Created: latency-svc-lrmsf
    Mar  1 12:06:08.387: INFO: Got endpoints: latency-svc-2ctps [187.833567ms]
    Mar  1 12:06:08.398: INFO: Got endpoints: latency-svc-lrmsf [198.773016ms]
    Mar  1 12:06:08.494: INFO: Created: latency-svc-7rnlq
    Mar  1 12:06:08.494: INFO: Created: latency-svc-sgj9b
    Mar  1 12:06:08.495: INFO: Created: latency-svc-jk2hs
    Mar  1 12:06:08.499: INFO: Created: latency-svc-2p2cr
    Mar  1 12:06:08.499: INFO: Created: latency-svc-4jrk5
    Mar  1 12:06:08.500: INFO: Created: latency-svc-rdwb2
    Mar  1 12:06:08.516: INFO: Created: latency-svc-98458
    Mar  1 12:06:08.516: INFO: Created: latency-svc-8rlvs
    Mar  1 12:06:08.517: INFO: Created: latency-svc-h9746
    Mar  1 12:06:08.517: INFO: Created: latency-svc-fgczs
    Mar  1 12:06:08.517: INFO: Created: latency-svc-gbxbf
    Mar  1 12:06:08.517: INFO: Created: latency-svc-tlmvv
    Mar  1 12:06:08.517: INFO: Created: latency-svc-dwdfw
    Mar  1 12:06:08.516: INFO: Created: latency-svc-8jpvk
    Mar  1 12:06:08.518: INFO: Created: latency-svc-nggr2
    Mar  1 12:06:08.528: INFO: Got endpoints: latency-svc-7rnlq [256.907492ms]
    Mar  1 12:06:08.529: INFO: Got endpoints: latency-svc-jk2hs [155.105033ms]
    Mar  1 12:06:08.529: INFO: Got endpoints: latency-svc-sgj9b [271.742763ms]
    Mar  1 12:06:08.531: INFO: Got endpoints: latency-svc-8jpvk [132.442273ms]
    Mar  1 12:06:08.531: INFO: Got endpoints: latency-svc-rdwb2 [190.319564ms]
    Mar  1 12:06:08.531: INFO: Got endpoints: latency-svc-2p2cr [234.444248ms]
    Mar  1 12:06:08.546: INFO: Got endpoints: latency-svc-tlmvv [187.577052ms]
    Mar  1 12:06:08.546: INFO: Got endpoints: latency-svc-h9746 [263.344469ms]
    Mar  1 12:06:08.560: INFO: Got endpoints: latency-svc-dwdfw [249.830281ms]
    Mar  1 12:06:08.561: INFO: Got endpoints: latency-svc-gbxbf [361.60564ms]
    Mar  1 12:06:08.561: INFO: Got endpoints: latency-svc-98458 [202.814295ms]
    Mar  1 12:06:08.561: INFO: Got endpoints: latency-svc-4jrk5 [317.798488ms]
    Mar  1 12:06:08.562: INFO: Got endpoints: latency-svc-8rlvs [243.424593ms]
    Mar  1 12:06:08.566: INFO: Got endpoints: latency-svc-fgczs [319.178583ms]
    Mar  1 12:06:08.568: INFO: Got endpoints: latency-svc-nggr2 [181.07714ms]
    Mar  1 12:06:08.578: INFO: Created: latency-svc-f4994
    Mar  1 12:06:08.580: INFO: Got endpoints: latency-svc-f4994 [51.699667ms]
    Mar  1 12:06:08.585: INFO: Created: latency-svc-7jdl2
    Mar  1 12:06:08.590: INFO: Got endpoints: latency-svc-7jdl2 [60.834221ms]
    Mar  1 12:06:08.595: INFO: Created: latency-svc-mjvv7
    Mar  1 12:06:08.605: INFO: Got endpoints: latency-svc-mjvv7 [75.497003ms]
    Mar  1 12:06:08.612: INFO: Created: latency-svc-78frf
    Mar  1 12:06:08.617: INFO: Created: latency-svc-7trf4
    Mar  1 12:06:08.620: INFO: Got endpoints: latency-svc-78frf [89.040604ms]
    Mar  1 12:06:08.626: INFO: Got endpoints: latency-svc-7trf4 [94.503652ms]
    Mar  1 12:06:08.630: INFO: Created: latency-svc-vrl5q
    Mar  1 12:06:08.637: INFO: Got endpoints: latency-svc-vrl5q [91.028159ms]
    Mar  1 12:06:08.643: INFO: Created: latency-svc-rghmp
    Mar  1 12:06:08.650: INFO: Got endpoints: latency-svc-rghmp [103.299108ms]
    Mar  1 12:06:08.674: INFO: Created: latency-svc-ldrr2
    Mar  1 12:06:08.683: INFO: Got endpoints: latency-svc-ldrr2 [152.131098ms]
    Mar  1 12:06:08.684: INFO: Created: latency-svc-t27gc
    Mar  1 12:06:08.694: INFO: Got endpoints: latency-svc-t27gc [133.571616ms]
    Mar  1 12:06:08.703: INFO: Created: latency-svc-4s2g4
    Mar  1 12:06:08.708: INFO: Created: latency-svc-ngtbk
    Mar  1 12:06:08.713: INFO: Got endpoints: latency-svc-4s2g4 [150.92899ms]
    Mar  1 12:06:08.718: INFO: Created: latency-svc-ggxd9
    Mar  1 12:06:08.728: INFO: Created: latency-svc-hkglr
    Mar  1 12:06:08.737: INFO: Created: latency-svc-2275s
    Mar  1 12:06:08.744: INFO: Created: latency-svc-mnsqw
    Mar  1 12:06:08.751: INFO: Got endpoints: latency-svc-ngtbk [189.691575ms]
    Mar  1 12:06:08.755: INFO: Created: latency-svc-k5n9h
    Mar  1 12:06:08.764: INFO: Created: latency-svc-tbzq6
    Mar  1 12:06:08.771: INFO: Created: latency-svc-8wsqr
    Mar  1 12:06:08.780: INFO: Created: latency-svc-7j9gn
    Mar  1 12:06:08.790: INFO: Created: latency-svc-j7rjf
    Mar  1 12:06:08.799: INFO: Got endpoints: latency-svc-ggxd9 [237.831585ms]
    Mar  1 12:06:08.800: INFO: Created: latency-svc-mdlmd
    Mar  1 12:06:08.813: INFO: Created: latency-svc-4ms2m
    Mar  1 12:06:08.819: INFO: Created: latency-svc-wq8gn
    Mar  1 12:06:08.828: INFO: Created: latency-svc-l7rfd
    Mar  1 12:06:08.837: INFO: Created: latency-svc-lzq9v
    Mar  1 12:06:08.846: INFO: Created: latency-svc-wk6gk
    Mar  1 12:06:08.851: INFO: Got endpoints: latency-svc-hkglr [289.992008ms]
    Mar  1 12:06:08.855: INFO: Created: latency-svc-zmzgx
    Mar  1 12:06:08.868: INFO: Created: latency-svc-ld8sq
    Mar  1 12:06:08.898: INFO: Got endpoints: latency-svc-2275s [332.200557ms]
    Mar  1 12:06:08.914: INFO: Created: latency-svc-57jxk
    Mar  1 12:06:08.948: INFO: Got endpoints: latency-svc-mnsqw [379.78036ms]
    Mar  1 12:06:08.965: INFO: Created: latency-svc-gk6h2
    Mar  1 12:06:09.000: INFO: Got endpoints: latency-svc-k5n9h [419.633661ms]
    Mar  1 12:06:09.015: INFO: Created: latency-svc-z7zn6
    Mar  1 12:06:09.057: INFO: Got endpoints: latency-svc-tbzq6 [466.700124ms]
    Mar  1 12:06:09.078: INFO: Created: latency-svc-l4w7j
    Mar  1 12:06:09.098: INFO: Got endpoints: latency-svc-8wsqr [492.894072ms]
    Mar  1 12:06:09.112: INFO: Created: latency-svc-9ql9b
    Mar  1 12:06:09.149: INFO: Got endpoints: latency-svc-7j9gn [529.229524ms]
    Mar  1 12:06:09.165: INFO: Created: latency-svc-8tv7n
    Mar  1 12:06:09.198: INFO: Got endpoints: latency-svc-j7rjf [572.1497ms]
    Mar  1 12:06:09.214: INFO: Created: latency-svc-85qdp
    Mar  1 12:06:09.249: INFO: Got endpoints: latency-svc-mdlmd [611.571358ms]
    Mar  1 12:06:09.264: INFO: Created: latency-svc-v9zn2
    Mar  1 12:06:09.298: INFO: Got endpoints: latency-svc-4ms2m [647.959464ms]
    Mar  1 12:06:09.314: INFO: Created: latency-svc-bhvjw
    Mar  1 12:06:09.349: INFO: Got endpoints: latency-svc-wq8gn [666.583809ms]
    Mar  1 12:06:09.364: INFO: Created: latency-svc-76rkt
    Mar  1 12:06:09.400: INFO: Got endpoints: latency-svc-l7rfd [705.456049ms]
    Mar  1 12:06:09.413: INFO: Created: latency-svc-xz6cf
    Mar  1 12:06:09.448: INFO: Got endpoints: latency-svc-lzq9v [735.41745ms]
    Mar  1 12:06:09.463: INFO: Created: latency-svc-d7hnt
    Mar  1 12:06:09.499: INFO: Got endpoints: latency-svc-wk6gk [748.390208ms]
    Mar  1 12:06:09.514: INFO: Created: latency-svc-x59sd
    Mar  1 12:06:09.548: INFO: Got endpoints: latency-svc-zmzgx [749.647196ms]
    Mar  1 12:06:09.562: INFO: Created: latency-svc-x8m5l
    Mar  1 12:06:09.599: INFO: Got endpoints: latency-svc-ld8sq [748.443144ms]
    Mar  1 12:06:09.616: INFO: Created: latency-svc-gkhlq
    Mar  1 12:06:09.648: INFO: Got endpoints: latency-svc-57jxk [749.36937ms]
    Mar  1 12:06:09.664: INFO: Created: latency-svc-pqjsn
    Mar  1 12:06:09.700: INFO: Got endpoints: latency-svc-gk6h2 [752.070416ms]
    Mar  1 12:06:09.713: INFO: Created: latency-svc-hf9lg
    Mar  1 12:06:09.749: INFO: Got endpoints: latency-svc-z7zn6 [748.79269ms]
    Mar  1 12:06:09.764: INFO: Created: latency-svc-dhs5b
    Mar  1 12:06:09.798: INFO: Got endpoints: latency-svc-l4w7j [741.314141ms]
    Mar  1 12:06:09.812: INFO: Created: latency-svc-q8wtd
    Mar  1 12:06:09.849: INFO: Got endpoints: latency-svc-9ql9b [751.063022ms]
    Mar  1 12:06:09.866: INFO: Created: latency-svc-bltgf
    Mar  1 12:06:09.898: INFO: Got endpoints: latency-svc-8tv7n [748.750736ms]
    Mar  1 12:06:09.915: INFO: Created: latency-svc-nqspf
    Mar  1 12:06:09.949: INFO: Got endpoints: latency-svc-85qdp [751.139508ms]
    Mar  1 12:06:09.962: INFO: Created: latency-svc-wrk9x
    Mar  1 12:06:09.998: INFO: Got endpoints: latency-svc-v9zn2 [749.424512ms]
    Mar  1 12:06:10.014: INFO: Created: latency-svc-cdkw7
    Mar  1 12:06:10.047: INFO: Got endpoints: latency-svc-bhvjw [748.610327ms]
    Mar  1 12:06:10.062: INFO: Created: latency-svc-bbmzj
    Mar  1 12:06:10.099: INFO: Got endpoints: latency-svc-76rkt [749.649049ms]
    Mar  1 12:06:10.115: INFO: Created: latency-svc-8l644
    Mar  1 12:06:10.150: INFO: Got endpoints: latency-svc-xz6cf [750.059474ms]
    Mar  1 12:06:10.163: INFO: Created: latency-svc-vmf4j
    Mar  1 12:06:10.201: INFO: Got endpoints: latency-svc-d7hnt [751.941387ms]
    Mar  1 12:06:10.216: INFO: Created: latency-svc-5t4x9
    Mar  1 12:06:10.247: INFO: Got endpoints: latency-svc-x59sd [748.069004ms]
    Mar  1 12:06:10.261: INFO: Created: latency-svc-6ts2s
    Mar  1 12:06:10.300: INFO: Got endpoints: latency-svc-x8m5l [751.094482ms]
    Mar  1 12:06:10.313: INFO: Created: latency-svc-478qm
    Mar  1 12:06:10.349: INFO: Got endpoints: latency-svc-gkhlq [749.637316ms]
    Mar  1 12:06:10.365: INFO: Created: latency-svc-9dg7g
    Mar  1 12:06:10.398: INFO: Got endpoints: latency-svc-pqjsn [750.23561ms]
    Mar  1 12:06:10.412: INFO: Created: latency-svc-l6n69
    Mar  1 12:06:10.448: INFO: Got endpoints: latency-svc-hf9lg [747.976856ms]
    Mar  1 12:06:10.461: INFO: Created: latency-svc-hk8sw
    Mar  1 12:06:10.498: INFO: Got endpoints: latency-svc-dhs5b [748.604744ms]
    Mar  1 12:06:10.512: INFO: Created: latency-svc-wkhrr
    Mar  1 12:06:10.556: INFO: Got endpoints: latency-svc-q8wtd [758.013088ms]
    Mar  1 12:06:10.570: INFO: Created: latency-svc-bcrmb
    Mar  1 12:06:10.599: INFO: Got endpoints: latency-svc-bltgf [750.222953ms]
    Mar  1 12:06:10.613: INFO: Created: latency-svc-7b4jj
    Mar  1 12:06:10.649: INFO: Got endpoints: latency-svc-nqspf [750.904803ms]
    Mar  1 12:06:10.663: INFO: Created: latency-svc-jbpkf
    Mar  1 12:06:10.698: INFO: Got endpoints: latency-svc-wrk9x [748.781587ms]
    Mar  1 12:06:10.712: INFO: Created: latency-svc-r92x6
    Mar  1 12:06:10.747: INFO: Got endpoints: latency-svc-cdkw7 [748.431821ms]
    Mar  1 12:06:10.763: INFO: Created: latency-svc-zsk42
    Mar  1 12:06:10.798: INFO: Got endpoints: latency-svc-bbmzj [751.73263ms]
    Mar  1 12:06:10.814: INFO: Created: latency-svc-vkb2w
    Mar  1 12:06:10.849: INFO: Got endpoints: latency-svc-8l644 [749.503983ms]
    Mar  1 12:06:10.864: INFO: Created: latency-svc-qtqhn
    Mar  1 12:06:10.899: INFO: Got endpoints: latency-svc-vmf4j [748.896117ms]
    Mar  1 12:06:10.914: INFO: Created: latency-svc-9h2dh
    Mar  1 12:06:10.949: INFO: Got endpoints: latency-svc-5t4x9 [747.949877ms]
    Mar  1 12:06:10.965: INFO: Created: latency-svc-4dzgz
    Mar  1 12:06:10.998: INFO: Got endpoints: latency-svc-6ts2s [750.615143ms]
    Mar  1 12:06:11.017: INFO: Created: latency-svc-m965w
    Mar  1 12:06:11.047: INFO: Got endpoints: latency-svc-478qm [747.459662ms]
    Mar  1 12:06:11.062: INFO: Created: latency-svc-r5sj8
    Mar  1 12:06:11.099: INFO: Got endpoints: latency-svc-9dg7g [749.910071ms]
    Mar  1 12:06:11.117: INFO: Created: latency-svc-wwgvv
    Mar  1 12:06:11.150: INFO: Got endpoints: latency-svc-l6n69 [751.637307ms]
    Mar  1 12:06:11.165: INFO: Created: latency-svc-rx2q4
    Mar  1 12:06:11.198: INFO: Got endpoints: latency-svc-hk8sw [749.515238ms]
    Mar  1 12:06:11.211: INFO: Created: latency-svc-twm8s
    Mar  1 12:06:11.249: INFO: Got endpoints: latency-svc-wkhrr [751.425378ms]
    Mar  1 12:06:11.264: INFO: Created: latency-svc-n4cld
    Mar  1 12:06:11.300: INFO: Got endpoints: latency-svc-bcrmb [744.1748ms]
    Mar  1 12:06:11.316: INFO: Created: latency-svc-h9lqx
    Mar  1 12:06:11.348: INFO: Got endpoints: latency-svc-7b4jj [748.825304ms]
    Mar  1 12:06:11.366: INFO: Created: latency-svc-n8f2h
    Mar  1 12:06:11.397: INFO: Got endpoints: latency-svc-jbpkf [747.726077ms]
    Mar  1 12:06:11.414: INFO: Created: latency-svc-n8g5t
    Mar  1 12:06:11.448: INFO: Got endpoints: latency-svc-r92x6 [749.789725ms]
    Mar  1 12:06:11.465: INFO: Created: latency-svc-dk8sn
    Mar  1 12:06:11.498: INFO: Got endpoints: latency-svc-zsk42 [750.79133ms]
    Mar  1 12:06:11.516: INFO: Created: latency-svc-nkh5h
    Mar  1 12:06:11.547: INFO: Got endpoints: latency-svc-vkb2w [748.3855ms]
    Mar  1 12:06:11.561: INFO: Created: latency-svc-wds2n
    Mar  1 12:06:11.600: INFO: Got endpoints: latency-svc-qtqhn [751.324718ms]
    Mar  1 12:06:11.615: INFO: Created: latency-svc-9vt5s
    Mar  1 12:06:11.648: INFO: Got endpoints: latency-svc-9h2dh [748.852154ms]
    Mar  1 12:06:11.662: INFO: Created: latency-svc-s25lx
    Mar  1 12:06:11.696: INFO: Got endpoints: latency-svc-4dzgz [747.341909ms]
    Mar  1 12:06:11.715: INFO: Created: latency-svc-kfnz8
    Mar  1 12:06:11.748: INFO: Got endpoints: latency-svc-m965w [749.810147ms]
    Mar  1 12:06:11.764: INFO: Created: latency-svc-9c7dm
    Mar  1 12:06:11.798: INFO: Got endpoints: latency-svc-r5sj8 [751.124999ms]
    Mar  1 12:06:11.810: INFO: Created: latency-svc-c54xk
    Mar  1 12:06:11.849: INFO: Got endpoints: latency-svc-wwgvv [750.158724ms]
    Mar  1 12:06:11.864: INFO: Created: latency-svc-724jb
    Mar  1 12:06:11.898: INFO: Got endpoints: latency-svc-rx2q4 [747.76327ms]
    Mar  1 12:06:11.911: INFO: Created: latency-svc-2l9lw
    Mar  1 12:06:11.950: INFO: Got endpoints: latency-svc-twm8s [752.410413ms]
    Mar  1 12:06:11.965: INFO: Created: latency-svc-pxcsx
    Mar  1 12:06:11.999: INFO: Got endpoints: latency-svc-n4cld [749.611135ms]
    Mar  1 12:06:12.015: INFO: Created: latency-svc-r54jp
    Mar  1 12:06:12.050: INFO: Got endpoints: latency-svc-h9lqx [749.497152ms]
    Mar  1 12:06:12.066: INFO: Created: latency-svc-nsxxv
    Mar  1 12:06:12.098: INFO: Got endpoints: latency-svc-n8f2h [749.475096ms]
    Mar  1 12:06:12.114: INFO: Created: latency-svc-qqg6z
    Mar  1 12:06:12.147: INFO: Got endpoints: latency-svc-n8g5t [750.551829ms]
    Mar  1 12:06:12.164: INFO: Created: latency-svc-jfq58
    Mar  1 12:06:12.200: INFO: Got endpoints: latency-svc-dk8sn [752.221139ms]
    Mar  1 12:06:12.214: INFO: Created: latency-svc-6wfln
    Mar  1 12:06:12.249: INFO: Got endpoints: latency-svc-nkh5h [751.37435ms]
    Mar  1 12:06:12.266: INFO: Created: latency-svc-52rn2
    Mar  1 12:06:12.297: INFO: Got endpoints: latency-svc-wds2n [750.218326ms]
    Mar  1 12:06:12.314: INFO: Created: latency-svc-q28d2
    Mar  1 12:06:12.349: INFO: Got endpoints: latency-svc-9vt5s [748.321039ms]
    Mar  1 12:06:12.363: INFO: Created: latency-svc-grlvp
    Mar  1 12:06:12.398: INFO: Got endpoints: latency-svc-s25lx [749.983632ms]
    Mar  1 12:06:12.414: INFO: Created: latency-svc-bbrw6
    Mar  1 12:06:12.447: INFO: Got endpoints: latency-svc-kfnz8 [751.015134ms]
    Mar  1 12:06:12.463: INFO: Created: latency-svc-pztqs
    Mar  1 12:06:12.501: INFO: Got endpoints: latency-svc-9c7dm [752.220786ms]
    Mar  1 12:06:12.516: INFO: Created: latency-svc-jm5q4
    Mar  1 12:06:12.547: INFO: Got endpoints: latency-svc-c54xk [748.707459ms]
    Mar  1 12:06:12.564: INFO: Created: latency-svc-84hq6
    Mar  1 12:06:12.598: INFO: Got endpoints: latency-svc-724jb [748.497784ms]
    Mar  1 12:06:12.613: INFO: Created: latency-svc-r5p5g
    Mar  1 12:06:12.648: INFO: Got endpoints: latency-svc-2l9lw [750.19134ms]
    Mar  1 12:06:12.666: INFO: Created: latency-svc-8tzbm
    Mar  1 12:06:12.698: INFO: Got endpoints: latency-svc-pxcsx [747.320856ms]
    Mar  1 12:06:12.713: INFO: Created: latency-svc-7kjvh
    Mar  1 12:06:12.751: INFO: Got endpoints: latency-svc-r54jp [751.955029ms]
    Mar  1 12:06:12.768: INFO: Created: latency-svc-zjm98
    Mar  1 12:06:12.797: INFO: Got endpoints: latency-svc-nsxxv [747.200075ms]
    Mar  1 12:06:12.812: INFO: Created: latency-svc-s4w2w
    Mar  1 12:06:12.848: INFO: Got endpoints: latency-svc-qqg6z [749.57811ms]
    Mar  1 12:06:12.862: INFO: Created: latency-svc-t2g7p
    Mar  1 12:06:12.900: INFO: Got endpoints: latency-svc-jfq58 [752.683691ms]
    Mar  1 12:06:12.918: INFO: Created: latency-svc-8j5p8
    Mar  1 12:06:12.951: INFO: Got endpoints: latency-svc-6wfln [750.697033ms]
    Mar  1 12:06:12.975: INFO: Created: latency-svc-h8btk
    Mar  1 12:06:12.998: INFO: Got endpoints: latency-svc-52rn2 [748.204533ms]
    Mar  1 12:06:13.014: INFO: Created: latency-svc-pdkjw
    Mar  1 12:06:13.052: INFO: Got endpoints: latency-svc-q28d2 [754.474531ms]
    Mar  1 12:06:13.073: INFO: Created: latency-svc-7hcl4
    Mar  1 12:06:13.100: INFO: Got endpoints: latency-svc-grlvp [750.904274ms]
    Mar  1 12:06:13.116: INFO: Created: latency-svc-h8rd8
    Mar  1 12:06:13.149: INFO: Got endpoints: latency-svc-bbrw6 [750.857147ms]
    Mar  1 12:06:13.163: INFO: Created: latency-svc-kqqbt
    Mar  1 12:06:13.200: INFO: Got endpoints: latency-svc-pztqs [752.152672ms]
    Mar  1 12:06:13.216: INFO: Created: latency-svc-4qn8c
    Mar  1 12:06:13.248: INFO: Got endpoints: latency-svc-jm5q4 [747.475633ms]
    Mar  1 12:06:13.265: INFO: Created: latency-svc-bmrc4
    Mar  1 12:06:13.298: INFO: Got endpoints: latency-svc-84hq6 [750.811993ms]
    Mar  1 12:06:13.313: INFO: Created: latency-svc-z8wp4
    Mar  1 12:06:13.350: INFO: Got endpoints: latency-svc-r5p5g [752.050374ms]
    Mar  1 12:06:13.366: INFO: Created: latency-svc-zzpg4
    Mar  1 12:06:13.404: INFO: Got endpoints: latency-svc-8tzbm [755.930332ms]
    Mar  1 12:06:13.426: INFO: Created: latency-svc-tgxfr
    Mar  1 12:06:13.449: INFO: Got endpoints: latency-svc-7kjvh [751.696762ms]
    Mar  1 12:06:13.464: INFO: Created: latency-svc-hxm6b
    Mar  1 12:06:13.498: INFO: Got endpoints: latency-svc-zjm98 [747.264763ms]
    Mar  1 12:06:13.514: INFO: Created: latency-svc-2z7pb
    Mar  1 12:06:13.547: INFO: Got endpoints: latency-svc-s4w2w [750.074274ms]
    Mar  1 12:06:13.562: INFO: Created: latency-svc-f9trp
    Mar  1 12:06:13.598: INFO: Got endpoints: latency-svc-t2g7p [750.515921ms]
    Mar  1 12:06:13.616: INFO: Created: latency-svc-sld7p
    Mar  1 12:06:13.647: INFO: Got endpoints: latency-svc-8j5p8 [746.690137ms]
    Mar  1 12:06:13.662: INFO: Created: latency-svc-4phl8
    Mar  1 12:06:13.699: INFO: Got endpoints: latency-svc-h8btk [748.229404ms]
    Mar  1 12:06:13.715: INFO: Created: latency-svc-dj8rb
    Mar  1 12:06:13.749: INFO: Got endpoints: latency-svc-pdkjw [751.001658ms]
    Mar  1 12:06:13.763: INFO: Created: latency-svc-62w7p
    Mar  1 12:06:13.799: INFO: Got endpoints: latency-svc-7hcl4 [747.652283ms]
    Mar  1 12:06:13.818: INFO: Created: latency-svc-4m2nq
    Mar  1 12:06:13.850: INFO: Got endpoints: latency-svc-h8rd8 [749.807741ms]
    Mar  1 12:06:13.864: INFO: Created: latency-svc-l26tf
    Mar  1 12:06:13.900: INFO: Got endpoints: latency-svc-kqqbt [751.351519ms]
    Mar  1 12:06:13.916: INFO: Created: latency-svc-rqh9h
    Mar  1 12:06:13.947: INFO: Got endpoints: latency-svc-4qn8c [747.535012ms]
    Mar  1 12:06:13.966: INFO: Created: latency-svc-jdwp6
    Mar  1 12:06:14.002: INFO: Got endpoints: latency-svc-bmrc4 [753.646982ms]
    Mar  1 12:06:14.017: INFO: Created: latency-svc-9bwwc
    Mar  1 12:06:14.049: INFO: Got endpoints: latency-svc-z8wp4 [750.152683ms]
    Mar  1 12:06:14.063: INFO: Created: latency-svc-mflbq
    Mar  1 12:06:14.097: INFO: Got endpoints: latency-svc-zzpg4 [746.917372ms]
    Mar  1 12:06:14.112: INFO: Created: latency-svc-8dzrz
    Mar  1 12:06:14.148: INFO: Got endpoints: latency-svc-tgxfr [743.604465ms]
    Mar  1 12:06:14.162: INFO: Created: latency-svc-j58n2
    Mar  1 12:06:14.201: INFO: Got endpoints: latency-svc-hxm6b [751.590954ms]
    Mar  1 12:06:14.215: INFO: Created: latency-svc-h7wr2
    Mar  1 12:06:14.248: INFO: Got endpoints: latency-svc-2z7pb [749.979459ms]
    Mar  1 12:06:14.264: INFO: Created: latency-svc-qr7z7
    Mar  1 12:06:14.298: INFO: Got endpoints: latency-svc-f9trp [750.708706ms]
    Mar  1 12:06:14.312: INFO: Created: latency-svc-fwtln
    Mar  1 12:06:14.350: INFO: Got endpoints: latency-svc-sld7p [751.911307ms]
    Mar  1 12:06:14.366: INFO: Created: latency-svc-pfq28
    Mar  1 12:06:14.397: INFO: Got endpoints: latency-svc-4phl8 [749.941704ms]
    Mar  1 12:06:14.414: INFO: Created: latency-svc-qxmv8
    Mar  1 12:06:14.451: INFO: Got endpoints: latency-svc-dj8rb [751.435836ms]
    Mar  1 12:06:14.466: INFO: Created: latency-svc-tmv59
    Mar  1 12:06:14.499: INFO: Got endpoints: latency-svc-62w7p [750.694132ms]
    Mar  1 12:06:14.513: INFO: Created: latency-svc-lc626
    Mar  1 12:06:14.547: INFO: Got endpoints: latency-svc-4m2nq [747.683529ms]
    Mar  1 12:06:14.564: INFO: Created: latency-svc-lfr59
    Mar  1 12:06:14.599: INFO: Got endpoints: latency-svc-l26tf [749.054145ms]
    Mar  1 12:06:14.612: INFO: Created: latency-svc-snfbk
    Mar  1 12:06:14.650: INFO: Got endpoints: latency-svc-rqh9h [749.314858ms]
    Mar  1 12:06:14.663: INFO: Created: latency-svc-n9dvt
    Mar  1 12:06:14.697: INFO: Got endpoints: latency-svc-jdwp6 [749.305313ms]
    Mar  1 12:06:14.716: INFO: Created: latency-svc-tgfq7
    Mar  1 12:06:14.748: INFO: Got endpoints: latency-svc-9bwwc [745.947194ms]
    Mar  1 12:06:14.762: INFO: Created: latency-svc-gh7sz
    Mar  1 12:06:14.799: INFO: Got endpoints: latency-svc-mflbq [750.37661ms]
    Mar  1 12:06:14.812: INFO: Created: latency-svc-d96c2
    Mar  1 12:06:14.848: INFO: Got endpoints: latency-svc-8dzrz [750.676233ms]
    Mar  1 12:06:14.863: INFO: Created: latency-svc-pj2wh
    Mar  1 12:06:14.900: INFO: Got endpoints: latency-svc-j58n2 [751.555171ms]
    Mar  1 12:06:14.916: INFO: Created: latency-svc-gc78n
    Mar  1 12:06:14.949: INFO: Got endpoints: latency-svc-h7wr2 [747.598441ms]
    Mar  1 12:06:14.964: INFO: Created: latency-svc-4ghjd
    Mar  1 12:06:14.997: INFO: Got endpoints: latency-svc-qr7z7 [748.825816ms]
    Mar  1 12:06:15.013: INFO: Created: latency-svc-v8hjf
    Mar  1 12:06:15.049: INFO: Got endpoints: latency-svc-fwtln [750.474312ms]
    Mar  1 12:06:15.063: INFO: Created: latency-svc-b9cjp
    Mar  1 12:06:15.100: INFO: Got endpoints: latency-svc-pfq28 [749.976026ms]
    Mar  1 12:06:15.115: INFO: Created: latency-svc-pq656
    Mar  1 12:06:15.157: INFO: Got endpoints: latency-svc-qxmv8 [759.978933ms]
    Mar  1 12:06:15.176: INFO: Created: latency-svc-v9mz9
    Mar  1 12:06:15.198: INFO: Got endpoints: latency-svc-tmv59 [746.961914ms]
    Mar  1 12:06:15.215: INFO: Created: latency-svc-bj6bv
    Mar  1 12:06:15.250: INFO: Got endpoints: latency-svc-lc626 [751.000133ms]
    Mar  1 12:06:15.264: INFO: Created: latency-svc-57szj
    Mar  1 12:06:15.297: INFO: Got endpoints: latency-svc-lfr59 [749.744357ms]
    Mar  1 12:06:15.313: INFO: Created: latency-svc-2mj8m
    Mar  1 12:06:15.348: INFO: Got endpoints: latency-svc-snfbk [749.21507ms]
    Mar  1 12:06:15.364: INFO: Created: latency-svc-bdq92
    Mar  1 12:06:15.401: INFO: Got endpoints: latency-svc-n9dvt [750.922058ms]
    Mar  1 12:06:15.415: INFO: Created: latency-svc-g2k58
    Mar  1 12:06:15.448: INFO: Got endpoints: latency-svc-tgfq7 [750.994ms]
    Mar  1 12:06:15.464: INFO: Created: latency-svc-252gq
    Mar  1 12:06:15.500: INFO: Got endpoints: latency-svc-gh7sz [751.584817ms]
    Mar  1 12:06:15.516: INFO: Created: latency-svc-4fwsn
    Mar  1 12:06:15.548: INFO: Got endpoints: latency-svc-d96c2 [748.779826ms]
    Mar  1 12:06:15.565: INFO: Created: latency-svc-kcnnq
    Mar  1 12:06:15.597: INFO: Got endpoints: latency-svc-pj2wh [749.050013ms]
    Mar  1 12:06:15.614: INFO: Created: latency-svc-8wh4j
    Mar  1 12:06:15.649: INFO: Got endpoints: latency-svc-gc78n [748.998052ms]
    Mar  1 12:06:15.663: INFO: Created: latency-svc-mht6j
    Mar  1 12:06:15.698: INFO: Got endpoints: latency-svc-4ghjd [749.488371ms]
    Mar  1 12:06:15.712: INFO: Created: latency-svc-qg9np
    Mar  1 12:06:15.747: INFO: Got endpoints: latency-svc-v8hjf [749.922336ms]
    Mar  1 12:06:15.762: INFO: Created: latency-svc-8ltdf
    Mar  1 12:06:15.798: INFO: Got endpoints: latency-svc-b9cjp [748.747247ms]
    Mar  1 12:06:15.814: INFO: Created: latency-svc-kc5cn
    Mar  1 12:06:15.848: INFO: Got endpoints: latency-svc-pq656 [746.902168ms]
    Mar  1 12:06:15.862: INFO: Created: latency-svc-d2w2c
    Mar  1 12:06:15.897: INFO: Got endpoints: latency-svc-v9mz9 [739.218999ms]
    Mar  1 12:06:15.914: INFO: Created: latency-svc-phw8m
    Mar  1 12:06:15.951: INFO: Got endpoints: latency-svc-bj6bv [752.938741ms]
    Mar  1 12:06:15.970: INFO: Created: latency-svc-qbp59
    Mar  1 12:06:15.998: INFO: Got endpoints: latency-svc-57szj [748.02265ms]
    Mar  1 12:06:16.015: INFO: Created: latency-svc-5hm76
    Mar  1 12:06:16.052: INFO: Got endpoints: latency-svc-2mj8m [754.602051ms]
    Mar  1 12:06:16.097: INFO: Got endpoints: latency-svc-bdq92 [749.182251ms]
    Mar  1 12:06:16.149: INFO: Got endpoints: latency-svc-g2k58 [748.221711ms]
    Mar  1 12:06:16.199: INFO: Got endpoints: latency-svc-252gq [750.445151ms]
    Mar  1 12:06:16.250: INFO: Got endpoints: latency-svc-4fwsn [750.182402ms]
    Mar  1 12:06:16.300: INFO: Got endpoints: latency-svc-kcnnq [751.375308ms]
    Mar  1 12:06:16.346: INFO: Got endpoints: latency-svc-8wh4j [748.864536ms]
    Mar  1 12:06:16.398: INFO: Got endpoints: latency-svc-mht6j [748.915268ms]
    Mar  1 12:06:16.448: INFO: Got endpoints: latency-svc-qg9np [749.07958ms]
    Mar  1 12:06:16.497: INFO: Got endpoints: latency-svc-8ltdf [749.880441ms]
    Mar  1 12:06:16.550: INFO: Got endpoints: latency-svc-kc5cn [751.534896ms]
    Mar  1 12:06:16.600: INFO: Got endpoints: latency-svc-d2w2c [751.751435ms]
    Mar  1 12:06:16.649: INFO: Got endpoints: latency-svc-phw8m [752.103472ms]
    Mar  1 12:06:16.699: INFO: Got endpoints: latency-svc-qbp59 [747.824268ms]
    Mar  1 12:06:16.748: INFO: Got endpoints: latency-svc-5hm76 [749.51844ms]
    Mar  1 12:06:16.748: INFO: Latencies: [44.084943ms 47.71294ms 51.699667ms 58.137582ms 60.834221ms 72.432207ms 75.497003ms 83.282746ms 89.040604ms 91.028159ms 94.503652ms 97.359701ms 103.299108ms 111.324683ms 119.467602ms 132.442273ms 133.571616ms 141.6682ms 150.92899ms 152.131098ms 155.105033ms 158.808185ms 159.132727ms 173.999831ms 181.07714ms 187.577052ms 187.833567ms 189.691575ms 190.319564ms 198.773016ms 202.814295ms 234.444248ms 237.831585ms 243.424593ms 249.830281ms 256.907492ms 263.344469ms 271.742763ms 289.992008ms 317.798488ms 319.178583ms 332.200557ms 361.60564ms 379.78036ms 419.633661ms 466.700124ms 492.894072ms 529.229524ms 572.1497ms 611.571358ms 647.959464ms 666.583809ms 705.456049ms 735.41745ms 739.218999ms 741.314141ms 743.604465ms 744.1748ms 745.947194ms 746.690137ms 746.902168ms 746.917372ms 746.961914ms 747.200075ms 747.264763ms 747.320856ms 747.341909ms 747.459662ms 747.475633ms 747.535012ms 747.598441ms 747.652283ms 747.683529ms 747.726077ms 747.76327ms 747.824268ms 747.949877ms 747.976856ms 748.02265ms 748.069004ms 748.204533ms 748.221711ms 748.229404ms 748.321039ms 748.3855ms 748.390208ms 748.431821ms 748.443144ms 748.497784ms 748.604744ms 748.610327ms 748.707459ms 748.747247ms 748.750736ms 748.779826ms 748.781587ms 748.79269ms 748.825304ms 748.825816ms 748.852154ms 748.864536ms 748.896117ms 748.915268ms 748.998052ms 749.050013ms 749.054145ms 749.07958ms 749.182251ms 749.21507ms 749.305313ms 749.314858ms 749.36937ms 749.424512ms 749.475096ms 749.488371ms 749.497152ms 749.503983ms 749.515238ms 749.51844ms 749.57811ms 749.611135ms 749.637316ms 749.647196ms 749.649049ms 749.744357ms 749.789725ms 749.807741ms 749.810147ms 749.880441ms 749.910071ms 749.922336ms 749.941704ms 749.976026ms 749.979459ms 749.983632ms 750.059474ms 750.074274ms 750.152683ms 750.158724ms 750.182402ms 750.19134ms 750.218326ms 750.222953ms 750.23561ms 750.37661ms 750.445151ms 750.474312ms 750.515921ms 750.551829ms 750.615143ms 750.676233ms 750.694132ms 750.697033ms 750.708706ms 750.79133ms 750.811993ms 750.857147ms 750.904274ms 750.904803ms 750.922058ms 750.994ms 751.000133ms 751.001658ms 751.015134ms 751.063022ms 751.094482ms 751.124999ms 751.139508ms 751.324718ms 751.351519ms 751.37435ms 751.375308ms 751.425378ms 751.435836ms 751.534896ms 751.555171ms 751.584817ms 751.590954ms 751.637307ms 751.696762ms 751.73263ms 751.751435ms 751.911307ms 751.941387ms 751.955029ms 752.050374ms 752.070416ms 752.103472ms 752.152672ms 752.220786ms 752.221139ms 752.410413ms 752.683691ms 752.938741ms 753.646982ms 754.474531ms 754.602051ms 755.930332ms 758.013088ms 759.978933ms]
    Mar  1 12:06:16.748: INFO: 50 %ile: 748.864536ms
    Mar  1 12:06:16.749: INFO: 90 %ile: 751.73263ms
    Mar  1 12:06:16.749: INFO: 99 %ile: 758.013088ms
    Mar  1 12:06:16.749: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:187
    Mar  1 12:06:16.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svc-latency-5654" for this suite. 03/01/23 12:06:16.759
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:06:16.773
Mar  1 12:06:16.773: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename secrets 03/01/23 12:06:16.773
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:06:16.797
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:06:16.8
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
STEP: Creating secret with name secret-test-73c45562-9130-461e-8b65-7c2d019e7380 03/01/23 12:06:16.803
STEP: Creating a pod to test consume secrets 03/01/23 12:06:16.811
Mar  1 12:06:16.823: INFO: Waiting up to 5m0s for pod "pod-secrets-25f1395e-3421-4789-9b80-38158ed274d2" in namespace "secrets-9266" to be "Succeeded or Failed"
Mar  1 12:06:16.829: INFO: Pod "pod-secrets-25f1395e-3421-4789-9b80-38158ed274d2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.81196ms
Mar  1 12:06:18.836: INFO: Pod "pod-secrets-25f1395e-3421-4789-9b80-38158ed274d2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013017517s
Mar  1 12:06:20.838: INFO: Pod "pod-secrets-25f1395e-3421-4789-9b80-38158ed274d2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014790429s
STEP: Saw pod success 03/01/23 12:06:20.838
Mar  1 12:06:20.838: INFO: Pod "pod-secrets-25f1395e-3421-4789-9b80-38158ed274d2" satisfied condition "Succeeded or Failed"
Mar  1 12:06:20.844: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-secrets-25f1395e-3421-4789-9b80-38158ed274d2 container secret-volume-test: <nil>
STEP: delete the pod 03/01/23 12:06:20.863
Mar  1 12:06:20.877: INFO: Waiting for pod pod-secrets-25f1395e-3421-4789-9b80-38158ed274d2 to disappear
Mar  1 12:06:20.882: INFO: Pod pod-secrets-25f1395e-3421-4789-9b80-38158ed274d2 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Mar  1 12:06:20.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9266" for this suite. 03/01/23 12:06:20.89
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":95,"skipped":1726,"failed":0}
------------------------------
â€¢ [4.126 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:06:16.773
    Mar  1 12:06:16.773: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename secrets 03/01/23 12:06:16.773
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:06:16.797
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:06:16.8
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:56
    STEP: Creating secret with name secret-test-73c45562-9130-461e-8b65-7c2d019e7380 03/01/23 12:06:16.803
    STEP: Creating a pod to test consume secrets 03/01/23 12:06:16.811
    Mar  1 12:06:16.823: INFO: Waiting up to 5m0s for pod "pod-secrets-25f1395e-3421-4789-9b80-38158ed274d2" in namespace "secrets-9266" to be "Succeeded or Failed"
    Mar  1 12:06:16.829: INFO: Pod "pod-secrets-25f1395e-3421-4789-9b80-38158ed274d2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.81196ms
    Mar  1 12:06:18.836: INFO: Pod "pod-secrets-25f1395e-3421-4789-9b80-38158ed274d2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013017517s
    Mar  1 12:06:20.838: INFO: Pod "pod-secrets-25f1395e-3421-4789-9b80-38158ed274d2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014790429s
    STEP: Saw pod success 03/01/23 12:06:20.838
    Mar  1 12:06:20.838: INFO: Pod "pod-secrets-25f1395e-3421-4789-9b80-38158ed274d2" satisfied condition "Succeeded or Failed"
    Mar  1 12:06:20.844: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-secrets-25f1395e-3421-4789-9b80-38158ed274d2 container secret-volume-test: <nil>
    STEP: delete the pod 03/01/23 12:06:20.863
    Mar  1 12:06:20.877: INFO: Waiting for pod pod-secrets-25f1395e-3421-4789-9b80-38158ed274d2 to disappear
    Mar  1 12:06:20.882: INFO: Pod pod-secrets-25f1395e-3421-4789-9b80-38158ed274d2 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Mar  1 12:06:20.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-9266" for this suite. 03/01/23 12:06:20.89
  << End Captured GinkgoWriter Output
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:06:20.899
Mar  1 12:06:20.899: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename sched-preemption 03/01/23 12:06:20.9
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:06:20.924
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:06:20.928
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Mar  1 12:06:20.947: INFO: Waiting up to 1m0s for all nodes to be ready
Mar  1 12:07:20.993: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
STEP: Create pods that use 4/5 of node resources. 03/01/23 12:07:20.998
Mar  1 12:07:21.041: INFO: Created pod: pod0-0-sched-preemption-low-priority
Mar  1 12:07:21.057: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Mar  1 12:07:21.082: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Mar  1 12:07:21.097: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Mar  1 12:07:21.121: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Mar  1 12:07:21.134: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 03/01/23 12:07:21.134
Mar  1 12:07:21.134: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-33" to be "running"
Mar  1 12:07:21.143: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 9.448224ms
Mar  1 12:07:23.150: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015899789s
Mar  1 12:07:25.156: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.021844036s
Mar  1 12:07:25.156: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Mar  1 12:07:25.156: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-33" to be "running"
Mar  1 12:07:25.161: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.913385ms
Mar  1 12:07:25.161: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Mar  1 12:07:25.161: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-33" to be "running"
Mar  1 12:07:25.165: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.651578ms
Mar  1 12:07:27.173: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011941411s
Mar  1 12:07:29.173: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012524153s
Mar  1 12:07:31.172: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.011500805s
Mar  1 12:07:31.172: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Mar  1 12:07:31.172: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-33" to be "running"
Mar  1 12:07:31.177: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.915587ms
Mar  1 12:07:31.177: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Mar  1 12:07:31.177: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-33" to be "running"
Mar  1 12:07:31.183: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.667749ms
Mar  1 12:07:31.183: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Mar  1 12:07:31.183: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-33" to be "running"
Mar  1 12:07:31.187: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.916952ms
Mar  1 12:07:31.187: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 03/01/23 12:07:31.187
Mar  1 12:07:31.193: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-33" to be "running"
Mar  1 12:07:31.198: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.291263ms
Mar  1 12:07:33.203: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009680662s
Mar  1 12:07:35.208: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.014098807s
Mar  1 12:07:35.214: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Mar  1 12:07:35.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-33" for this suite. 03/01/23 12:07:35.275
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","completed":96,"skipped":1726,"failed":0}
------------------------------
â€¢ [SLOW TEST] [74.439 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:06:20.899
    Mar  1 12:06:20.899: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename sched-preemption 03/01/23 12:06:20.9
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:06:20.924
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:06:20.928
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Mar  1 12:06:20.947: INFO: Waiting up to 1m0s for all nodes to be ready
    Mar  1 12:07:20.993: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:125
    STEP: Create pods that use 4/5 of node resources. 03/01/23 12:07:20.998
    Mar  1 12:07:21.041: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Mar  1 12:07:21.057: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Mar  1 12:07:21.082: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Mar  1 12:07:21.097: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Mar  1 12:07:21.121: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Mar  1 12:07:21.134: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 03/01/23 12:07:21.134
    Mar  1 12:07:21.134: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-33" to be "running"
    Mar  1 12:07:21.143: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 9.448224ms
    Mar  1 12:07:23.150: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015899789s
    Mar  1 12:07:25.156: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.021844036s
    Mar  1 12:07:25.156: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Mar  1 12:07:25.156: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-33" to be "running"
    Mar  1 12:07:25.161: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.913385ms
    Mar  1 12:07:25.161: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Mar  1 12:07:25.161: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-33" to be "running"
    Mar  1 12:07:25.165: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.651578ms
    Mar  1 12:07:27.173: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011941411s
    Mar  1 12:07:29.173: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012524153s
    Mar  1 12:07:31.172: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.011500805s
    Mar  1 12:07:31.172: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Mar  1 12:07:31.172: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-33" to be "running"
    Mar  1 12:07:31.177: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.915587ms
    Mar  1 12:07:31.177: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Mar  1 12:07:31.177: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-33" to be "running"
    Mar  1 12:07:31.183: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.667749ms
    Mar  1 12:07:31.183: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Mar  1 12:07:31.183: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-33" to be "running"
    Mar  1 12:07:31.187: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.916952ms
    Mar  1 12:07:31.187: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 03/01/23 12:07:31.187
    Mar  1 12:07:31.193: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-33" to be "running"
    Mar  1 12:07:31.198: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.291263ms
    Mar  1 12:07:33.203: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009680662s
    Mar  1 12:07:35.208: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.014098807s
    Mar  1 12:07:35.214: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Mar  1 12:07:35.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-33" for this suite. 03/01/23 12:07:35.275
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:07:35.346
Mar  1 12:07:35.346: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename downward-api 03/01/23 12:07:35.347
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:07:35.373
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:07:35.377
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
STEP: Creating a pod to test downward api env vars 03/01/23 12:07:35.38
Mar  1 12:07:35.390: INFO: Waiting up to 5m0s for pod "downward-api-71bf7967-2204-4993-b7d8-ddce8571be5a" in namespace "downward-api-5817" to be "Succeeded or Failed"
Mar  1 12:07:35.394: INFO: Pod "downward-api-71bf7967-2204-4993-b7d8-ddce8571be5a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027983ms
Mar  1 12:07:37.400: INFO: Pod "downward-api-71bf7967-2204-4993-b7d8-ddce8571be5a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010445077s
Mar  1 12:07:39.400: INFO: Pod "downward-api-71bf7967-2204-4993-b7d8-ddce8571be5a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009831705s
STEP: Saw pod success 03/01/23 12:07:39.4
Mar  1 12:07:39.400: INFO: Pod "downward-api-71bf7967-2204-4993-b7d8-ddce8571be5a" satisfied condition "Succeeded or Failed"
Mar  1 12:07:39.406: INFO: Trying to get logs from node lab1-k8s-node-3 pod downward-api-71bf7967-2204-4993-b7d8-ddce8571be5a container dapi-container: <nil>
STEP: delete the pod 03/01/23 12:07:39.416
Mar  1 12:07:39.431: INFO: Waiting for pod downward-api-71bf7967-2204-4993-b7d8-ddce8571be5a to disappear
Mar  1 12:07:39.436: INFO: Pod downward-api-71bf7967-2204-4993-b7d8-ddce8571be5a no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Mar  1 12:07:39.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5817" for this suite. 03/01/23 12:07:39.445
{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","completed":97,"skipped":1784,"failed":0}
------------------------------
â€¢ [4.108 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:07:35.346
    Mar  1 12:07:35.346: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename downward-api 03/01/23 12:07:35.347
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:07:35.373
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:07:35.377
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:216
    STEP: Creating a pod to test downward api env vars 03/01/23 12:07:35.38
    Mar  1 12:07:35.390: INFO: Waiting up to 5m0s for pod "downward-api-71bf7967-2204-4993-b7d8-ddce8571be5a" in namespace "downward-api-5817" to be "Succeeded or Failed"
    Mar  1 12:07:35.394: INFO: Pod "downward-api-71bf7967-2204-4993-b7d8-ddce8571be5a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027983ms
    Mar  1 12:07:37.400: INFO: Pod "downward-api-71bf7967-2204-4993-b7d8-ddce8571be5a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010445077s
    Mar  1 12:07:39.400: INFO: Pod "downward-api-71bf7967-2204-4993-b7d8-ddce8571be5a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009831705s
    STEP: Saw pod success 03/01/23 12:07:39.4
    Mar  1 12:07:39.400: INFO: Pod "downward-api-71bf7967-2204-4993-b7d8-ddce8571be5a" satisfied condition "Succeeded or Failed"
    Mar  1 12:07:39.406: INFO: Trying to get logs from node lab1-k8s-node-3 pod downward-api-71bf7967-2204-4993-b7d8-ddce8571be5a container dapi-container: <nil>
    STEP: delete the pod 03/01/23 12:07:39.416
    Mar  1 12:07:39.431: INFO: Waiting for pod downward-api-71bf7967-2204-4993-b7d8-ddce8571be5a to disappear
    Mar  1 12:07:39.436: INFO: Pod downward-api-71bf7967-2204-4993-b7d8-ddce8571be5a no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Mar  1 12:07:39.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-5817" for this suite. 03/01/23 12:07:39.445
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:07:39.455
Mar  1 12:07:39.455: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename server-version 03/01/23 12:07:39.456
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:07:39.477
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:07:39.481
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 03/01/23 12:07:39.484
STEP: Confirm major version 03/01/23 12:07:39.486
Mar  1 12:07:39.486: INFO: Major version: 1
STEP: Confirm minor version 03/01/23 12:07:39.486
Mar  1 12:07:39.486: INFO: cleanMinorVersion: 25
Mar  1 12:07:39.486: INFO: Minor version: 25
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:187
Mar  1 12:07:39.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-9425" for this suite. 03/01/23 12:07:39.495
{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","completed":98,"skipped":1805,"failed":0}
------------------------------
â€¢ [0.051 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:07:39.455
    Mar  1 12:07:39.455: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename server-version 03/01/23 12:07:39.456
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:07:39.477
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:07:39.481
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 03/01/23 12:07:39.484
    STEP: Confirm major version 03/01/23 12:07:39.486
    Mar  1 12:07:39.486: INFO: Major version: 1
    STEP: Confirm minor version 03/01/23 12:07:39.486
    Mar  1 12:07:39.486: INFO: cleanMinorVersion: 25
    Mar  1 12:07:39.486: INFO: Minor version: 25
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:187
    Mar  1 12:07:39.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "server-version-9425" for this suite. 03/01/23 12:07:39.495
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:07:39.508
Mar  1 12:07:39.508: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename statefulset 03/01/23 12:07:39.509
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:07:39.531
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:07:39.534
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-5834 03/01/23 12:07:39.538
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
STEP: Initializing watcher for selector baz=blah,foo=bar 03/01/23 12:07:39.545
STEP: Creating stateful set ss in namespace statefulset-5834 03/01/23 12:07:39.551
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5834 03/01/23 12:07:39.561
Mar  1 12:07:39.567: INFO: Found 0 stateful pods, waiting for 1
Mar  1 12:07:49.573: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 03/01/23 12:07:49.573
Mar  1 12:07:49.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=statefulset-5834 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  1 12:07:49.704: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  1 12:07:49.704: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  1 12:07:49.704: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  1 12:07:49.709: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar  1 12:07:59.716: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  1 12:07:59.716: INFO: Waiting for statefulset status.replicas updated to 0
Mar  1 12:07:59.736: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999779s
Mar  1 12:08:00.742: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.995373548s
Mar  1 12:08:01.748: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.989513427s
Mar  1 12:08:02.754: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.984000945s
Mar  1 12:08:03.759: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.977416578s
Mar  1 12:08:04.765: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.972248295s
Mar  1 12:08:05.772: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.966435842s
Mar  1 12:08:06.778: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.959761545s
Mar  1 12:08:07.783: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.953690035s
Mar  1 12:08:08.791: INFO: Verifying statefulset ss doesn't scale past 1 for another 948.020905ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5834 03/01/23 12:08:09.791
Mar  1 12:08:09.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=statefulset-5834 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 12:08:09.940: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  1 12:08:09.940: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  1 12:08:09.940: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  1 12:08:09.945: INFO: Found 1 stateful pods, waiting for 3
Mar  1 12:08:19.952: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  1 12:08:19.952: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  1 12:08:19.952: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 03/01/23 12:08:19.952
STEP: Scale down will halt with unhealthy stateful pod 03/01/23 12:08:19.952
Mar  1 12:08:19.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=statefulset-5834 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  1 12:08:20.105: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  1 12:08:20.105: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  1 12:08:20.105: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  1 12:08:20.105: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=statefulset-5834 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  1 12:08:20.254: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  1 12:08:20.254: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  1 12:08:20.254: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  1 12:08:20.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=statefulset-5834 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  1 12:08:20.405: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  1 12:08:20.405: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  1 12:08:20.405: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  1 12:08:20.405: INFO: Waiting for statefulset status.replicas updated to 0
Mar  1 12:08:20.410: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Mar  1 12:08:30.424: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  1 12:08:30.424: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar  1 12:08:30.424: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar  1 12:08:30.442: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999747s
Mar  1 12:08:31.448: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.993269479s
Mar  1 12:08:32.454: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.987039575s
Mar  1 12:08:33.461: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.981760152s
Mar  1 12:08:34.466: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.974733991s
Mar  1 12:08:35.472: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.969329957s
Mar  1 12:08:36.479: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.963577042s
Mar  1 12:08:37.485: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.956067719s
Mar  1 12:08:38.492: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.950354375s
Mar  1 12:08:39.499: INFO: Verifying statefulset ss doesn't scale past 3 for another 944.014075ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5834 03/01/23 12:08:40.499
Mar  1 12:08:40.504: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=statefulset-5834 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 12:08:40.640: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  1 12:08:40.640: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  1 12:08:40.640: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  1 12:08:40.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=statefulset-5834 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 12:08:40.766: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  1 12:08:40.766: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  1 12:08:40.766: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  1 12:08:40.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=statefulset-5834 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 12:08:40.901: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  1 12:08:40.901: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  1 12:08:40.901: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  1 12:08:40.901: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 03/01/23 12:08:50.925
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Mar  1 12:08:50.925: INFO: Deleting all statefulset in ns statefulset-5834
Mar  1 12:08:50.929: INFO: Scaling statefulset ss to 0
Mar  1 12:08:50.943: INFO: Waiting for statefulset status.replicas updated to 0
Mar  1 12:08:50.947: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Mar  1 12:08:50.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5834" for this suite. 03/01/23 12:08:50.982
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","completed":99,"skipped":1851,"failed":0}
------------------------------
â€¢ [SLOW TEST] [71.484 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:585

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:07:39.508
    Mar  1 12:07:39.508: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename statefulset 03/01/23 12:07:39.509
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:07:39.531
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:07:39.534
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-5834 03/01/23 12:07:39.538
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:585
    STEP: Initializing watcher for selector baz=blah,foo=bar 03/01/23 12:07:39.545
    STEP: Creating stateful set ss in namespace statefulset-5834 03/01/23 12:07:39.551
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5834 03/01/23 12:07:39.561
    Mar  1 12:07:39.567: INFO: Found 0 stateful pods, waiting for 1
    Mar  1 12:07:49.573: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 03/01/23 12:07:49.573
    Mar  1 12:07:49.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=statefulset-5834 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar  1 12:07:49.704: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar  1 12:07:49.704: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar  1 12:07:49.704: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar  1 12:07:49.709: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Mar  1 12:07:59.716: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Mar  1 12:07:59.716: INFO: Waiting for statefulset status.replicas updated to 0
    Mar  1 12:07:59.736: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999779s
    Mar  1 12:08:00.742: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.995373548s
    Mar  1 12:08:01.748: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.989513427s
    Mar  1 12:08:02.754: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.984000945s
    Mar  1 12:08:03.759: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.977416578s
    Mar  1 12:08:04.765: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.972248295s
    Mar  1 12:08:05.772: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.966435842s
    Mar  1 12:08:06.778: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.959761545s
    Mar  1 12:08:07.783: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.953690035s
    Mar  1 12:08:08.791: INFO: Verifying statefulset ss doesn't scale past 1 for another 948.020905ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5834 03/01/23 12:08:09.791
    Mar  1 12:08:09.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=statefulset-5834 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar  1 12:08:09.940: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar  1 12:08:09.940: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar  1 12:08:09.940: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar  1 12:08:09.945: INFO: Found 1 stateful pods, waiting for 3
    Mar  1 12:08:19.952: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Mar  1 12:08:19.952: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Mar  1 12:08:19.952: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 03/01/23 12:08:19.952
    STEP: Scale down will halt with unhealthy stateful pod 03/01/23 12:08:19.952
    Mar  1 12:08:19.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=statefulset-5834 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar  1 12:08:20.105: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar  1 12:08:20.105: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar  1 12:08:20.105: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar  1 12:08:20.105: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=statefulset-5834 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar  1 12:08:20.254: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar  1 12:08:20.254: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar  1 12:08:20.254: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar  1 12:08:20.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=statefulset-5834 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar  1 12:08:20.405: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar  1 12:08:20.405: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar  1 12:08:20.405: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar  1 12:08:20.405: INFO: Waiting for statefulset status.replicas updated to 0
    Mar  1 12:08:20.410: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
    Mar  1 12:08:30.424: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Mar  1 12:08:30.424: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Mar  1 12:08:30.424: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Mar  1 12:08:30.442: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999747s
    Mar  1 12:08:31.448: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.993269479s
    Mar  1 12:08:32.454: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.987039575s
    Mar  1 12:08:33.461: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.981760152s
    Mar  1 12:08:34.466: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.974733991s
    Mar  1 12:08:35.472: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.969329957s
    Mar  1 12:08:36.479: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.963577042s
    Mar  1 12:08:37.485: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.956067719s
    Mar  1 12:08:38.492: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.950354375s
    Mar  1 12:08:39.499: INFO: Verifying statefulset ss doesn't scale past 3 for another 944.014075ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5834 03/01/23 12:08:40.499
    Mar  1 12:08:40.504: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=statefulset-5834 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar  1 12:08:40.640: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar  1 12:08:40.640: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar  1 12:08:40.640: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar  1 12:08:40.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=statefulset-5834 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar  1 12:08:40.766: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar  1 12:08:40.766: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar  1 12:08:40.766: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar  1 12:08:40.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=statefulset-5834 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar  1 12:08:40.901: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar  1 12:08:40.901: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar  1 12:08:40.901: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar  1 12:08:40.901: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 03/01/23 12:08:50.925
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Mar  1 12:08:50.925: INFO: Deleting all statefulset in ns statefulset-5834
    Mar  1 12:08:50.929: INFO: Scaling statefulset ss to 0
    Mar  1 12:08:50.943: INFO: Waiting for statefulset status.replicas updated to 0
    Mar  1 12:08:50.947: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Mar  1 12:08:50.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-5834" for this suite. 03/01/23 12:08:50.982
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:08:50.994
Mar  1 12:08:50.994: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename webhook 03/01/23 12:08:50.995
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:08:51.014
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:08:51.017
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/01/23 12:08:51.039
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/01/23 12:08:51.441
STEP: Deploying the webhook pod 03/01/23 12:08:51.452
STEP: Wait for the deployment to be ready 03/01/23 12:08:51.466
Mar  1 12:08:51.476: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/01/23 12:08:53.491
STEP: Verifying the service has paired with the endpoint 03/01/23 12:08:53.505
Mar  1 12:08:54.505: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
Mar  1 12:08:54.510: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9489-crds.webhook.example.com via the AdmissionRegistration API 03/01/23 12:09:00.024
STEP: Creating a custom resource that should be mutated by the webhook 03/01/23 12:09:00.058
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  1 12:09:02.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-952" for this suite. 03/01/23 12:09:02.674
STEP: Destroying namespace "webhook-952-markers" for this suite. 03/01/23 12:09:02.685
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","completed":100,"skipped":1858,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.765 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:08:50.994
    Mar  1 12:08:50.994: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename webhook 03/01/23 12:08:50.995
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:08:51.014
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:08:51.017
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/01/23 12:08:51.039
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/01/23 12:08:51.441
    STEP: Deploying the webhook pod 03/01/23 12:08:51.452
    STEP: Wait for the deployment to be ready 03/01/23 12:08:51.466
    Mar  1 12:08:51.476: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/01/23 12:08:53.491
    STEP: Verifying the service has paired with the endpoint 03/01/23 12:08:53.505
    Mar  1 12:08:54.505: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:340
    Mar  1 12:08:54.510: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9489-crds.webhook.example.com via the AdmissionRegistration API 03/01/23 12:09:00.024
    STEP: Creating a custom resource that should be mutated by the webhook 03/01/23 12:09:00.058
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  1 12:09:02.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-952" for this suite. 03/01/23 12:09:02.674
    STEP: Destroying namespace "webhook-952-markers" for this suite. 03/01/23 12:09:02.685
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:09:02.766
Mar  1 12:09:02.766: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename resourcequota 03/01/23 12:09:02.767
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:09:02.793
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:09:02.797
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
STEP: Counting existing ResourceQuota 03/01/23 12:09:02.8
STEP: Creating a ResourceQuota 03/01/23 12:09:07.805
STEP: Ensuring resource quota status is calculated 03/01/23 12:09:07.814
STEP: Creating a ReplicaSet 03/01/23 12:09:09.819
STEP: Ensuring resource quota status captures replicaset creation 03/01/23 12:09:09.833
STEP: Deleting a ReplicaSet 03/01/23 12:09:11.839
STEP: Ensuring resource quota status released usage 03/01/23 12:09:11.849
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Mar  1 12:09:13.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2850" for this suite. 03/01/23 12:09:13.861
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","completed":101,"skipped":1899,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.104 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:09:02.766
    Mar  1 12:09:02.766: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename resourcequota 03/01/23 12:09:02.767
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:09:02.793
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:09:02.797
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:438
    STEP: Counting existing ResourceQuota 03/01/23 12:09:02.8
    STEP: Creating a ResourceQuota 03/01/23 12:09:07.805
    STEP: Ensuring resource quota status is calculated 03/01/23 12:09:07.814
    STEP: Creating a ReplicaSet 03/01/23 12:09:09.819
    STEP: Ensuring resource quota status captures replicaset creation 03/01/23 12:09:09.833
    STEP: Deleting a ReplicaSet 03/01/23 12:09:11.839
    STEP: Ensuring resource quota status released usage 03/01/23 12:09:11.849
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Mar  1 12:09:13.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-2850" for this suite. 03/01/23 12:09:13.861
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:09:13.872
Mar  1 12:09:13.872: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename crd-publish-openapi 03/01/23 12:09:13.873
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:09:13.895
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:09:13.899
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
Mar  1 12:09:13.903: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 03/01/23 12:09:21.589
Mar  1 12:09:21.590: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=crd-publish-openapi-3494 --namespace=crd-publish-openapi-3494 create -f -'
Mar  1 12:09:22.223: INFO: stderr: ""
Mar  1 12:09:22.223: INFO: stdout: "e2e-test-crd-publish-openapi-1931-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Mar  1 12:09:22.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=crd-publish-openapi-3494 --namespace=crd-publish-openapi-3494 delete e2e-test-crd-publish-openapi-1931-crds test-cr'
Mar  1 12:09:22.294: INFO: stderr: ""
Mar  1 12:09:22.294: INFO: stdout: "e2e-test-crd-publish-openapi-1931-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Mar  1 12:09:22.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=crd-publish-openapi-3494 --namespace=crd-publish-openapi-3494 apply -f -'
Mar  1 12:09:22.898: INFO: stderr: ""
Mar  1 12:09:22.898: INFO: stdout: "e2e-test-crd-publish-openapi-1931-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Mar  1 12:09:22.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=crd-publish-openapi-3494 --namespace=crd-publish-openapi-3494 delete e2e-test-crd-publish-openapi-1931-crds test-cr'
Mar  1 12:09:22.968: INFO: stderr: ""
Mar  1 12:09:22.968: INFO: stdout: "e2e-test-crd-publish-openapi-1931-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 03/01/23 12:09:22.968
Mar  1 12:09:22.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=crd-publish-openapi-3494 explain e2e-test-crd-publish-openapi-1931-crds'
Mar  1 12:09:23.465: INFO: stderr: ""
Mar  1 12:09:23.465: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1931-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  1 12:09:26.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3494" for this suite. 03/01/23 12:09:26.21
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","completed":102,"skipped":1905,"failed":0}
------------------------------
â€¢ [SLOW TEST] [12.348 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:09:13.872
    Mar  1 12:09:13.872: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename crd-publish-openapi 03/01/23 12:09:13.873
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:09:13.895
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:09:13.899
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:235
    Mar  1 12:09:13.903: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 03/01/23 12:09:21.589
    Mar  1 12:09:21.590: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=crd-publish-openapi-3494 --namespace=crd-publish-openapi-3494 create -f -'
    Mar  1 12:09:22.223: INFO: stderr: ""
    Mar  1 12:09:22.223: INFO: stdout: "e2e-test-crd-publish-openapi-1931-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Mar  1 12:09:22.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=crd-publish-openapi-3494 --namespace=crd-publish-openapi-3494 delete e2e-test-crd-publish-openapi-1931-crds test-cr'
    Mar  1 12:09:22.294: INFO: stderr: ""
    Mar  1 12:09:22.294: INFO: stdout: "e2e-test-crd-publish-openapi-1931-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Mar  1 12:09:22.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=crd-publish-openapi-3494 --namespace=crd-publish-openapi-3494 apply -f -'
    Mar  1 12:09:22.898: INFO: stderr: ""
    Mar  1 12:09:22.898: INFO: stdout: "e2e-test-crd-publish-openapi-1931-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Mar  1 12:09:22.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=crd-publish-openapi-3494 --namespace=crd-publish-openapi-3494 delete e2e-test-crd-publish-openapi-1931-crds test-cr'
    Mar  1 12:09:22.968: INFO: stderr: ""
    Mar  1 12:09:22.968: INFO: stdout: "e2e-test-crd-publish-openapi-1931-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 03/01/23 12:09:22.968
    Mar  1 12:09:22.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=crd-publish-openapi-3494 explain e2e-test-crd-publish-openapi-1931-crds'
    Mar  1 12:09:23.465: INFO: stderr: ""
    Mar  1 12:09:23.465: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1931-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  1 12:09:26.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-3494" for this suite. 03/01/23 12:09:26.21
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:09:26.232
Mar  1 12:09:26.232: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename pods 03/01/23 12:09:26.234
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:09:26.258
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:09:26.26
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
Mar  1 12:09:26.262: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: creating the pod 03/01/23 12:09:26.263
STEP: submitting the pod to kubernetes 03/01/23 12:09:26.263
Mar  1 12:09:26.272: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-7af7f7e2-005c-4fab-9c1c-b985363fd72e" in namespace "pods-6624" to be "running and ready"
Mar  1 12:09:26.280: INFO: Pod "pod-exec-websocket-7af7f7e2-005c-4fab-9c1c-b985363fd72e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.992537ms
Mar  1 12:09:26.280: INFO: The phase of Pod pod-exec-websocket-7af7f7e2-005c-4fab-9c1c-b985363fd72e is Pending, waiting for it to be Running (with Ready = true)
Mar  1 12:09:28.285: INFO: Pod "pod-exec-websocket-7af7f7e2-005c-4fab-9c1c-b985363fd72e": Phase="Running", Reason="", readiness=true. Elapsed: 2.012817435s
Mar  1 12:09:28.285: INFO: The phase of Pod pod-exec-websocket-7af7f7e2-005c-4fab-9c1c-b985363fd72e is Running (Ready = true)
Mar  1 12:09:28.285: INFO: Pod "pod-exec-websocket-7af7f7e2-005c-4fab-9c1c-b985363fd72e" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Mar  1 12:09:28.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6624" for this suite. 03/01/23 12:09:28.367
{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","completed":103,"skipped":1985,"failed":0}
------------------------------
â€¢ [2.145 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:09:26.232
    Mar  1 12:09:26.232: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename pods 03/01/23 12:09:26.234
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:09:26.258
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:09:26.26
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:535
    Mar  1 12:09:26.262: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: creating the pod 03/01/23 12:09:26.263
    STEP: submitting the pod to kubernetes 03/01/23 12:09:26.263
    Mar  1 12:09:26.272: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-7af7f7e2-005c-4fab-9c1c-b985363fd72e" in namespace "pods-6624" to be "running and ready"
    Mar  1 12:09:26.280: INFO: Pod "pod-exec-websocket-7af7f7e2-005c-4fab-9c1c-b985363fd72e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.992537ms
    Mar  1 12:09:26.280: INFO: The phase of Pod pod-exec-websocket-7af7f7e2-005c-4fab-9c1c-b985363fd72e is Pending, waiting for it to be Running (with Ready = true)
    Mar  1 12:09:28.285: INFO: Pod "pod-exec-websocket-7af7f7e2-005c-4fab-9c1c-b985363fd72e": Phase="Running", Reason="", readiness=true. Elapsed: 2.012817435s
    Mar  1 12:09:28.285: INFO: The phase of Pod pod-exec-websocket-7af7f7e2-005c-4fab-9c1c-b985363fd72e is Running (Ready = true)
    Mar  1 12:09:28.285: INFO: Pod "pod-exec-websocket-7af7f7e2-005c-4fab-9c1c-b985363fd72e" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Mar  1 12:09:28.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-6624" for this suite. 03/01/23 12:09:28.367
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:09:28.382
Mar  1 12:09:28.382: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename services 03/01/23 12:09:28.383
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:09:28.403
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:09:28.405
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
STEP: creating a service nodeport-service with the type=NodePort in namespace services-9115 03/01/23 12:09:28.408
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 03/01/23 12:09:28.428
STEP: creating service externalsvc in namespace services-9115 03/01/23 12:09:28.428
STEP: creating replication controller externalsvc in namespace services-9115 03/01/23 12:09:28.443
I0301 12:09:28.453960      19 runners.go:193] Created replication controller with name: externalsvc, namespace: services-9115, replica count: 2
I0301 12:09:31.505073      19 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 03/01/23 12:09:31.51
Mar  1 12:09:31.532: INFO: Creating new exec pod
Mar  1 12:09:31.544: INFO: Waiting up to 5m0s for pod "execpodtfj5x" in namespace "services-9115" to be "running"
Mar  1 12:09:31.549: INFO: Pod "execpodtfj5x": Phase="Pending", Reason="", readiness=false. Elapsed: 5.091256ms
Mar  1 12:09:33.554: INFO: Pod "execpodtfj5x": Phase="Running", Reason="", readiness=true. Elapsed: 2.010746001s
Mar  1 12:09:33.554: INFO: Pod "execpodtfj5x" satisfied condition "running"
Mar  1 12:09:33.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-9115 exec execpodtfj5x -- /bin/sh -x -c nslookup nodeport-service.services-9115.svc.cluster.local'
Mar  1 12:09:33.725: INFO: stderr: "+ nslookup nodeport-service.services-9115.svc.cluster.local\n"
Mar  1 12:09:33.725: INFO: stdout: "Server:\t\t169.254.25.10\nAddress:\t169.254.25.10#53\n\nnodeport-service.services-9115.svc.cluster.local\tcanonical name = externalsvc.services-9115.svc.cluster.local.\nName:\texternalsvc.services-9115.svc.cluster.local\nAddress: 10.233.11.172\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-9115, will wait for the garbage collector to delete the pods 03/01/23 12:09:33.725
Mar  1 12:09:33.789: INFO: Deleting ReplicationController externalsvc took: 8.809685ms
Mar  1 12:09:33.890: INFO: Terminating ReplicationController externalsvc pods took: 100.999575ms
Mar  1 12:09:35.817: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar  1 12:09:35.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9115" for this suite. 03/01/23 12:09:35.843
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","completed":104,"skipped":2019,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.471 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:09:28.382
    Mar  1 12:09:28.382: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename services 03/01/23 12:09:28.383
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:09:28.403
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:09:28.405
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1523
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-9115 03/01/23 12:09:28.408
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 03/01/23 12:09:28.428
    STEP: creating service externalsvc in namespace services-9115 03/01/23 12:09:28.428
    STEP: creating replication controller externalsvc in namespace services-9115 03/01/23 12:09:28.443
    I0301 12:09:28.453960      19 runners.go:193] Created replication controller with name: externalsvc, namespace: services-9115, replica count: 2
    I0301 12:09:31.505073      19 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 03/01/23 12:09:31.51
    Mar  1 12:09:31.532: INFO: Creating new exec pod
    Mar  1 12:09:31.544: INFO: Waiting up to 5m0s for pod "execpodtfj5x" in namespace "services-9115" to be "running"
    Mar  1 12:09:31.549: INFO: Pod "execpodtfj5x": Phase="Pending", Reason="", readiness=false. Elapsed: 5.091256ms
    Mar  1 12:09:33.554: INFO: Pod "execpodtfj5x": Phase="Running", Reason="", readiness=true. Elapsed: 2.010746001s
    Mar  1 12:09:33.554: INFO: Pod "execpodtfj5x" satisfied condition "running"
    Mar  1 12:09:33.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-9115 exec execpodtfj5x -- /bin/sh -x -c nslookup nodeport-service.services-9115.svc.cluster.local'
    Mar  1 12:09:33.725: INFO: stderr: "+ nslookup nodeport-service.services-9115.svc.cluster.local\n"
    Mar  1 12:09:33.725: INFO: stdout: "Server:\t\t169.254.25.10\nAddress:\t169.254.25.10#53\n\nnodeport-service.services-9115.svc.cluster.local\tcanonical name = externalsvc.services-9115.svc.cluster.local.\nName:\texternalsvc.services-9115.svc.cluster.local\nAddress: 10.233.11.172\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-9115, will wait for the garbage collector to delete the pods 03/01/23 12:09:33.725
    Mar  1 12:09:33.789: INFO: Deleting ReplicationController externalsvc took: 8.809685ms
    Mar  1 12:09:33.890: INFO: Terminating ReplicationController externalsvc pods took: 100.999575ms
    Mar  1 12:09:35.817: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar  1 12:09:35.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-9115" for this suite. 03/01/23 12:09:35.843
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:09:35.854
Mar  1 12:09:35.854: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename kubectl 03/01/23 12:09:35.855
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:09:35.874
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:09:35.878
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
STEP: creating a replication controller 03/01/23 12:09:35.881
Mar  1 12:09:35.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-4601 create -f -'
Mar  1 12:09:36.527: INFO: stderr: ""
Mar  1 12:09:36.527: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 03/01/23 12:09:36.527
Mar  1 12:09:36.528: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-4601 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  1 12:09:36.600: INFO: stderr: ""
Mar  1 12:09:36.600: INFO: stdout: "update-demo-nautilus-cgdzm update-demo-nautilus-ljlkp "
Mar  1 12:09:36.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-4601 get pods update-demo-nautilus-cgdzm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  1 12:09:36.659: INFO: stderr: ""
Mar  1 12:09:36.659: INFO: stdout: ""
Mar  1 12:09:36.659: INFO: update-demo-nautilus-cgdzm is created but not running
Mar  1 12:09:41.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-4601 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  1 12:09:41.730: INFO: stderr: ""
Mar  1 12:09:41.730: INFO: stdout: "update-demo-nautilus-cgdzm update-demo-nautilus-ljlkp "
Mar  1 12:09:41.730: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-4601 get pods update-demo-nautilus-cgdzm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  1 12:09:41.789: INFO: stderr: ""
Mar  1 12:09:41.789: INFO: stdout: "true"
Mar  1 12:09:41.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-4601 get pods update-demo-nautilus-cgdzm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  1 12:09:41.846: INFO: stderr: ""
Mar  1 12:09:41.846: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Mar  1 12:09:41.846: INFO: validating pod update-demo-nautilus-cgdzm
Mar  1 12:09:41.854: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  1 12:09:41.855: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  1 12:09:41.855: INFO: update-demo-nautilus-cgdzm is verified up and running
Mar  1 12:09:41.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-4601 get pods update-demo-nautilus-ljlkp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  1 12:09:41.914: INFO: stderr: ""
Mar  1 12:09:41.914: INFO: stdout: "true"
Mar  1 12:09:41.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-4601 get pods update-demo-nautilus-ljlkp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  1 12:09:41.979: INFO: stderr: ""
Mar  1 12:09:41.979: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Mar  1 12:09:41.979: INFO: validating pod update-demo-nautilus-ljlkp
Mar  1 12:09:41.987: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  1 12:09:41.987: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  1 12:09:41.987: INFO: update-demo-nautilus-ljlkp is verified up and running
STEP: scaling down the replication controller 03/01/23 12:09:41.987
Mar  1 12:09:41.988: INFO: scanned /root for discovery docs: <nil>
Mar  1 12:09:41.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-4601 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Mar  1 12:09:43.070: INFO: stderr: ""
Mar  1 12:09:43.070: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 03/01/23 12:09:43.07
Mar  1 12:09:43.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-4601 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  1 12:09:43.140: INFO: stderr: ""
Mar  1 12:09:43.140: INFO: stdout: "update-demo-nautilus-cgdzm update-demo-nautilus-ljlkp "
STEP: Replicas for name=update-demo: expected=1 actual=2 03/01/23 12:09:43.14
Mar  1 12:09:48.140: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-4601 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  1 12:09:48.204: INFO: stderr: ""
Mar  1 12:09:48.204: INFO: stdout: "update-demo-nautilus-ljlkp "
Mar  1 12:09:48.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-4601 get pods update-demo-nautilus-ljlkp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  1 12:09:48.260: INFO: stderr: ""
Mar  1 12:09:48.260: INFO: stdout: "true"
Mar  1 12:09:48.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-4601 get pods update-demo-nautilus-ljlkp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  1 12:09:48.328: INFO: stderr: ""
Mar  1 12:09:48.328: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Mar  1 12:09:48.328: INFO: validating pod update-demo-nautilus-ljlkp
Mar  1 12:09:48.334: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  1 12:09:48.334: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  1 12:09:48.334: INFO: update-demo-nautilus-ljlkp is verified up and running
STEP: scaling up the replication controller 03/01/23 12:09:48.334
Mar  1 12:09:48.335: INFO: scanned /root for discovery docs: <nil>
Mar  1 12:09:48.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-4601 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Mar  1 12:09:49.420: INFO: stderr: ""
Mar  1 12:09:49.420: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 03/01/23 12:09:49.42
Mar  1 12:09:49.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-4601 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  1 12:09:49.483: INFO: stderr: ""
Mar  1 12:09:49.483: INFO: stdout: "update-demo-nautilus-89x9r update-demo-nautilus-ljlkp "
Mar  1 12:09:49.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-4601 get pods update-demo-nautilus-89x9r -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  1 12:09:49.548: INFO: stderr: ""
Mar  1 12:09:49.548: INFO: stdout: ""
Mar  1 12:09:49.548: INFO: update-demo-nautilus-89x9r is created but not running
Mar  1 12:09:54.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-4601 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  1 12:09:54.616: INFO: stderr: ""
Mar  1 12:09:54.616: INFO: stdout: "update-demo-nautilus-89x9r update-demo-nautilus-ljlkp "
Mar  1 12:09:54.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-4601 get pods update-demo-nautilus-89x9r -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  1 12:09:54.677: INFO: stderr: ""
Mar  1 12:09:54.677: INFO: stdout: "true"
Mar  1 12:09:54.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-4601 get pods update-demo-nautilus-89x9r -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  1 12:09:54.745: INFO: stderr: ""
Mar  1 12:09:54.745: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Mar  1 12:09:54.745: INFO: validating pod update-demo-nautilus-89x9r
Mar  1 12:09:54.755: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  1 12:09:54.755: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  1 12:09:54.755: INFO: update-demo-nautilus-89x9r is verified up and running
Mar  1 12:09:54.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-4601 get pods update-demo-nautilus-ljlkp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  1 12:09:54.818: INFO: stderr: ""
Mar  1 12:09:54.818: INFO: stdout: "true"
Mar  1 12:09:54.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-4601 get pods update-demo-nautilus-ljlkp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  1 12:09:54.886: INFO: stderr: ""
Mar  1 12:09:54.886: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Mar  1 12:09:54.886: INFO: validating pod update-demo-nautilus-ljlkp
Mar  1 12:09:54.892: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  1 12:09:54.892: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  1 12:09:54.892: INFO: update-demo-nautilus-ljlkp is verified up and running
STEP: using delete to clean up resources 03/01/23 12:09:54.892
Mar  1 12:09:54.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-4601 delete --grace-period=0 --force -f -'
Mar  1 12:09:54.965: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  1 12:09:54.965: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar  1 12:09:54.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-4601 get rc,svc -l name=update-demo --no-headers'
Mar  1 12:09:55.090: INFO: stderr: "No resources found in kubectl-4601 namespace.\n"
Mar  1 12:09:55.090: INFO: stdout: ""
Mar  1 12:09:55.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-4601 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  1 12:09:55.207: INFO: stderr: ""
Mar  1 12:09:55.207: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar  1 12:09:55.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4601" for this suite. 03/01/23 12:09:55.216
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","completed":105,"skipped":2024,"failed":0}
------------------------------
â€¢ [SLOW TEST] [19.373 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:350

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:09:35.854
    Mar  1 12:09:35.854: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename kubectl 03/01/23 12:09:35.855
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:09:35.874
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:09:35.878
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:350
    STEP: creating a replication controller 03/01/23 12:09:35.881
    Mar  1 12:09:35.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-4601 create -f -'
    Mar  1 12:09:36.527: INFO: stderr: ""
    Mar  1 12:09:36.527: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 03/01/23 12:09:36.527
    Mar  1 12:09:36.528: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-4601 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar  1 12:09:36.600: INFO: stderr: ""
    Mar  1 12:09:36.600: INFO: stdout: "update-demo-nautilus-cgdzm update-demo-nautilus-ljlkp "
    Mar  1 12:09:36.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-4601 get pods update-demo-nautilus-cgdzm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar  1 12:09:36.659: INFO: stderr: ""
    Mar  1 12:09:36.659: INFO: stdout: ""
    Mar  1 12:09:36.659: INFO: update-demo-nautilus-cgdzm is created but not running
    Mar  1 12:09:41.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-4601 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar  1 12:09:41.730: INFO: stderr: ""
    Mar  1 12:09:41.730: INFO: stdout: "update-demo-nautilus-cgdzm update-demo-nautilus-ljlkp "
    Mar  1 12:09:41.730: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-4601 get pods update-demo-nautilus-cgdzm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar  1 12:09:41.789: INFO: stderr: ""
    Mar  1 12:09:41.789: INFO: stdout: "true"
    Mar  1 12:09:41.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-4601 get pods update-demo-nautilus-cgdzm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar  1 12:09:41.846: INFO: stderr: ""
    Mar  1 12:09:41.846: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Mar  1 12:09:41.846: INFO: validating pod update-demo-nautilus-cgdzm
    Mar  1 12:09:41.854: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar  1 12:09:41.855: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar  1 12:09:41.855: INFO: update-demo-nautilus-cgdzm is verified up and running
    Mar  1 12:09:41.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-4601 get pods update-demo-nautilus-ljlkp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar  1 12:09:41.914: INFO: stderr: ""
    Mar  1 12:09:41.914: INFO: stdout: "true"
    Mar  1 12:09:41.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-4601 get pods update-demo-nautilus-ljlkp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar  1 12:09:41.979: INFO: stderr: ""
    Mar  1 12:09:41.979: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Mar  1 12:09:41.979: INFO: validating pod update-demo-nautilus-ljlkp
    Mar  1 12:09:41.987: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar  1 12:09:41.987: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar  1 12:09:41.987: INFO: update-demo-nautilus-ljlkp is verified up and running
    STEP: scaling down the replication controller 03/01/23 12:09:41.987
    Mar  1 12:09:41.988: INFO: scanned /root for discovery docs: <nil>
    Mar  1 12:09:41.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-4601 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Mar  1 12:09:43.070: INFO: stderr: ""
    Mar  1 12:09:43.070: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 03/01/23 12:09:43.07
    Mar  1 12:09:43.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-4601 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar  1 12:09:43.140: INFO: stderr: ""
    Mar  1 12:09:43.140: INFO: stdout: "update-demo-nautilus-cgdzm update-demo-nautilus-ljlkp "
    STEP: Replicas for name=update-demo: expected=1 actual=2 03/01/23 12:09:43.14
    Mar  1 12:09:48.140: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-4601 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar  1 12:09:48.204: INFO: stderr: ""
    Mar  1 12:09:48.204: INFO: stdout: "update-demo-nautilus-ljlkp "
    Mar  1 12:09:48.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-4601 get pods update-demo-nautilus-ljlkp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar  1 12:09:48.260: INFO: stderr: ""
    Mar  1 12:09:48.260: INFO: stdout: "true"
    Mar  1 12:09:48.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-4601 get pods update-demo-nautilus-ljlkp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar  1 12:09:48.328: INFO: stderr: ""
    Mar  1 12:09:48.328: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Mar  1 12:09:48.328: INFO: validating pod update-demo-nautilus-ljlkp
    Mar  1 12:09:48.334: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar  1 12:09:48.334: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar  1 12:09:48.334: INFO: update-demo-nautilus-ljlkp is verified up and running
    STEP: scaling up the replication controller 03/01/23 12:09:48.334
    Mar  1 12:09:48.335: INFO: scanned /root for discovery docs: <nil>
    Mar  1 12:09:48.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-4601 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Mar  1 12:09:49.420: INFO: stderr: ""
    Mar  1 12:09:49.420: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 03/01/23 12:09:49.42
    Mar  1 12:09:49.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-4601 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar  1 12:09:49.483: INFO: stderr: ""
    Mar  1 12:09:49.483: INFO: stdout: "update-demo-nautilus-89x9r update-demo-nautilus-ljlkp "
    Mar  1 12:09:49.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-4601 get pods update-demo-nautilus-89x9r -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar  1 12:09:49.548: INFO: stderr: ""
    Mar  1 12:09:49.548: INFO: stdout: ""
    Mar  1 12:09:49.548: INFO: update-demo-nautilus-89x9r is created but not running
    Mar  1 12:09:54.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-4601 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar  1 12:09:54.616: INFO: stderr: ""
    Mar  1 12:09:54.616: INFO: stdout: "update-demo-nautilus-89x9r update-demo-nautilus-ljlkp "
    Mar  1 12:09:54.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-4601 get pods update-demo-nautilus-89x9r -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar  1 12:09:54.677: INFO: stderr: ""
    Mar  1 12:09:54.677: INFO: stdout: "true"
    Mar  1 12:09:54.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-4601 get pods update-demo-nautilus-89x9r -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar  1 12:09:54.745: INFO: stderr: ""
    Mar  1 12:09:54.745: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Mar  1 12:09:54.745: INFO: validating pod update-demo-nautilus-89x9r
    Mar  1 12:09:54.755: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar  1 12:09:54.755: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar  1 12:09:54.755: INFO: update-demo-nautilus-89x9r is verified up and running
    Mar  1 12:09:54.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-4601 get pods update-demo-nautilus-ljlkp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar  1 12:09:54.818: INFO: stderr: ""
    Mar  1 12:09:54.818: INFO: stdout: "true"
    Mar  1 12:09:54.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-4601 get pods update-demo-nautilus-ljlkp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar  1 12:09:54.886: INFO: stderr: ""
    Mar  1 12:09:54.886: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Mar  1 12:09:54.886: INFO: validating pod update-demo-nautilus-ljlkp
    Mar  1 12:09:54.892: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar  1 12:09:54.892: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar  1 12:09:54.892: INFO: update-demo-nautilus-ljlkp is verified up and running
    STEP: using delete to clean up resources 03/01/23 12:09:54.892
    Mar  1 12:09:54.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-4601 delete --grace-period=0 --force -f -'
    Mar  1 12:09:54.965: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar  1 12:09:54.965: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Mar  1 12:09:54.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-4601 get rc,svc -l name=update-demo --no-headers'
    Mar  1 12:09:55.090: INFO: stderr: "No resources found in kubectl-4601 namespace.\n"
    Mar  1 12:09:55.090: INFO: stdout: ""
    Mar  1 12:09:55.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-4601 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Mar  1 12:09:55.207: INFO: stderr: ""
    Mar  1 12:09:55.207: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar  1 12:09:55.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-4601" for this suite. 03/01/23 12:09:55.216
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:09:55.232
Mar  1 12:09:55.232: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename crd-publish-openapi 03/01/23 12:09:55.233
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:09:55.268
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:09:55.284
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
Mar  1 12:09:55.298: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 03/01/23 12:10:04.044
Mar  1 12:10:04.044: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=crd-publish-openapi-1107 --namespace=crd-publish-openapi-1107 create -f -'
Mar  1 12:10:04.788: INFO: stderr: ""
Mar  1 12:10:04.788: INFO: stdout: "e2e-test-crd-publish-openapi-6856-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Mar  1 12:10:04.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=crd-publish-openapi-1107 --namespace=crd-publish-openapi-1107 delete e2e-test-crd-publish-openapi-6856-crds test-foo'
Mar  1 12:10:04.890: INFO: stderr: ""
Mar  1 12:10:04.890: INFO: stdout: "e2e-test-crd-publish-openapi-6856-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Mar  1 12:10:04.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=crd-publish-openapi-1107 --namespace=crd-publish-openapi-1107 apply -f -'
Mar  1 12:10:05.440: INFO: stderr: ""
Mar  1 12:10:05.440: INFO: stdout: "e2e-test-crd-publish-openapi-6856-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Mar  1 12:10:05.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=crd-publish-openapi-1107 --namespace=crd-publish-openapi-1107 delete e2e-test-crd-publish-openapi-6856-crds test-foo'
Mar  1 12:10:05.514: INFO: stderr: ""
Mar  1 12:10:05.514: INFO: stdout: "e2e-test-crd-publish-openapi-6856-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 03/01/23 12:10:05.514
Mar  1 12:10:05.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=crd-publish-openapi-1107 --namespace=crd-publish-openapi-1107 create -f -'
Mar  1 12:10:05.719: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 03/01/23 12:10:05.719
Mar  1 12:10:05.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=crd-publish-openapi-1107 --namespace=crd-publish-openapi-1107 create -f -'
Mar  1 12:10:05.919: INFO: rc: 1
Mar  1 12:10:05.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=crd-publish-openapi-1107 --namespace=crd-publish-openapi-1107 apply -f -'
Mar  1 12:10:06.107: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 03/01/23 12:10:06.107
Mar  1 12:10:06.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=crd-publish-openapi-1107 --namespace=crd-publish-openapi-1107 create -f -'
Mar  1 12:10:06.284: INFO: rc: 1
Mar  1 12:10:06.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=crd-publish-openapi-1107 --namespace=crd-publish-openapi-1107 apply -f -'
Mar  1 12:10:06.476: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 03/01/23 12:10:06.476
Mar  1 12:10:06.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=crd-publish-openapi-1107 explain e2e-test-crd-publish-openapi-6856-crds'
Mar  1 12:10:06.982: INFO: stderr: ""
Mar  1 12:10:06.982: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6856-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 03/01/23 12:10:06.983
Mar  1 12:10:06.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=crd-publish-openapi-1107 explain e2e-test-crd-publish-openapi-6856-crds.metadata'
Mar  1 12:10:07.168: INFO: stderr: ""
Mar  1 12:10:07.169: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6856-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Mar  1 12:10:07.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=crd-publish-openapi-1107 explain e2e-test-crd-publish-openapi-6856-crds.spec'
Mar  1 12:10:07.345: INFO: stderr: ""
Mar  1 12:10:07.345: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6856-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Mar  1 12:10:07.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=crd-publish-openapi-1107 explain e2e-test-crd-publish-openapi-6856-crds.spec.bars'
Mar  1 12:10:07.520: INFO: stderr: ""
Mar  1 12:10:07.520: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6856-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 03/01/23 12:10:07.52
Mar  1 12:10:07.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=crd-publish-openapi-1107 explain e2e-test-crd-publish-openapi-6856-crds.spec.bars2'
Mar  1 12:10:07.710: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  1 12:10:10.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1107" for this suite. 03/01/23 12:10:10.346
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","completed":106,"skipped":2052,"failed":0}
------------------------------
â€¢ [SLOW TEST] [15.124 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:09:55.232
    Mar  1 12:09:55.232: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename crd-publish-openapi 03/01/23 12:09:55.233
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:09:55.268
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:09:55.284
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:68
    Mar  1 12:09:55.298: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 03/01/23 12:10:04.044
    Mar  1 12:10:04.044: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=crd-publish-openapi-1107 --namespace=crd-publish-openapi-1107 create -f -'
    Mar  1 12:10:04.788: INFO: stderr: ""
    Mar  1 12:10:04.788: INFO: stdout: "e2e-test-crd-publish-openapi-6856-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Mar  1 12:10:04.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=crd-publish-openapi-1107 --namespace=crd-publish-openapi-1107 delete e2e-test-crd-publish-openapi-6856-crds test-foo'
    Mar  1 12:10:04.890: INFO: stderr: ""
    Mar  1 12:10:04.890: INFO: stdout: "e2e-test-crd-publish-openapi-6856-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Mar  1 12:10:04.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=crd-publish-openapi-1107 --namespace=crd-publish-openapi-1107 apply -f -'
    Mar  1 12:10:05.440: INFO: stderr: ""
    Mar  1 12:10:05.440: INFO: stdout: "e2e-test-crd-publish-openapi-6856-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Mar  1 12:10:05.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=crd-publish-openapi-1107 --namespace=crd-publish-openapi-1107 delete e2e-test-crd-publish-openapi-6856-crds test-foo'
    Mar  1 12:10:05.514: INFO: stderr: ""
    Mar  1 12:10:05.514: INFO: stdout: "e2e-test-crd-publish-openapi-6856-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 03/01/23 12:10:05.514
    Mar  1 12:10:05.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=crd-publish-openapi-1107 --namespace=crd-publish-openapi-1107 create -f -'
    Mar  1 12:10:05.719: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 03/01/23 12:10:05.719
    Mar  1 12:10:05.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=crd-publish-openapi-1107 --namespace=crd-publish-openapi-1107 create -f -'
    Mar  1 12:10:05.919: INFO: rc: 1
    Mar  1 12:10:05.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=crd-publish-openapi-1107 --namespace=crd-publish-openapi-1107 apply -f -'
    Mar  1 12:10:06.107: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 03/01/23 12:10:06.107
    Mar  1 12:10:06.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=crd-publish-openapi-1107 --namespace=crd-publish-openapi-1107 create -f -'
    Mar  1 12:10:06.284: INFO: rc: 1
    Mar  1 12:10:06.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=crd-publish-openapi-1107 --namespace=crd-publish-openapi-1107 apply -f -'
    Mar  1 12:10:06.476: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 03/01/23 12:10:06.476
    Mar  1 12:10:06.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=crd-publish-openapi-1107 explain e2e-test-crd-publish-openapi-6856-crds'
    Mar  1 12:10:06.982: INFO: stderr: ""
    Mar  1 12:10:06.982: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6856-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 03/01/23 12:10:06.983
    Mar  1 12:10:06.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=crd-publish-openapi-1107 explain e2e-test-crd-publish-openapi-6856-crds.metadata'
    Mar  1 12:10:07.168: INFO: stderr: ""
    Mar  1 12:10:07.169: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6856-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Mar  1 12:10:07.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=crd-publish-openapi-1107 explain e2e-test-crd-publish-openapi-6856-crds.spec'
    Mar  1 12:10:07.345: INFO: stderr: ""
    Mar  1 12:10:07.345: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6856-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Mar  1 12:10:07.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=crd-publish-openapi-1107 explain e2e-test-crd-publish-openapi-6856-crds.spec.bars'
    Mar  1 12:10:07.520: INFO: stderr: ""
    Mar  1 12:10:07.520: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6856-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 03/01/23 12:10:07.52
    Mar  1 12:10:07.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=crd-publish-openapi-1107 explain e2e-test-crd-publish-openapi-6856-crds.spec.bars2'
    Mar  1 12:10:07.710: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  1 12:10:10.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-1107" for this suite. 03/01/23 12:10:10.346
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:10:10.361
Mar  1 12:10:10.361: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename container-lifecycle-hook 03/01/23 12:10:10.362
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:10:10.385
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:10:10.389
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 03/01/23 12:10:10.402
Mar  1 12:10:10.410: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-8658" to be "running and ready"
Mar  1 12:10:10.416: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 6.061283ms
Mar  1 12:10:10.416: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar  1 12:10:12.423: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.012762691s
Mar  1 12:10:12.423: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Mar  1 12:10:12.423: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
STEP: create the pod with lifecycle hook 03/01/23 12:10:12.428
Mar  1 12:10:12.435: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-8658" to be "running and ready"
Mar  1 12:10:12.442: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 6.585226ms
Mar  1 12:10:12.442: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Mar  1 12:10:14.448: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.012897071s
Mar  1 12:10:14.448: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Mar  1 12:10:14.448: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 03/01/23 12:10:14.452
Mar  1 12:10:14.467: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  1 12:10:14.471: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  1 12:10:16.472: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  1 12:10:16.480: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  1 12:10:18.471: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  1 12:10:18.477: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 03/01/23 12:10:18.477
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Mar  1 12:10:18.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8658" for this suite. 03/01/23 12:10:18.505
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","completed":107,"skipped":2095,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.156 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:114

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:10:10.361
    Mar  1 12:10:10.361: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename container-lifecycle-hook 03/01/23 12:10:10.362
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:10:10.385
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:10:10.389
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 03/01/23 12:10:10.402
    Mar  1 12:10:10.410: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-8658" to be "running and ready"
    Mar  1 12:10:10.416: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 6.061283ms
    Mar  1 12:10:10.416: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Mar  1 12:10:12.423: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.012762691s
    Mar  1 12:10:12.423: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Mar  1 12:10:12.423: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:114
    STEP: create the pod with lifecycle hook 03/01/23 12:10:12.428
    Mar  1 12:10:12.435: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-8658" to be "running and ready"
    Mar  1 12:10:12.442: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 6.585226ms
    Mar  1 12:10:12.442: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Mar  1 12:10:14.448: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.012897071s
    Mar  1 12:10:14.448: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Mar  1 12:10:14.448: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 03/01/23 12:10:14.452
    Mar  1 12:10:14.467: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Mar  1 12:10:14.471: INFO: Pod pod-with-prestop-exec-hook still exists
    Mar  1 12:10:16.472: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Mar  1 12:10:16.480: INFO: Pod pod-with-prestop-exec-hook still exists
    Mar  1 12:10:18.471: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Mar  1 12:10:18.477: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 03/01/23 12:10:18.477
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Mar  1 12:10:18.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-8658" for this suite. 03/01/23 12:10:18.505
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:10:18.517
Mar  1 12:10:18.517: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename custom-resource-definition 03/01/23 12:10:18.518
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:10:18.539
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:10:18.544
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Mar  1 12:10:18.547: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  1 12:11:18.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2885" for this suite. 03/01/23 12:11:18.959
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","completed":108,"skipped":2114,"failed":0}
------------------------------
â€¢ [SLOW TEST] [60.453 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:10:18.517
    Mar  1 12:10:18.517: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename custom-resource-definition 03/01/23 12:10:18.518
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:10:18.539
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:10:18.544
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Mar  1 12:10:18.547: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  1 12:11:18.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-2885" for this suite. 03/01/23 12:11:18.959
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:11:18.972
Mar  1 12:11:18.972: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename watch 03/01/23 12:11:18.972
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:11:18.993
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:11:18.996
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 03/01/23 12:11:19
STEP: creating a watch on configmaps with label B 03/01/23 12:11:19.002
STEP: creating a watch on configmaps with label A or B 03/01/23 12:11:19.003
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 03/01/23 12:11:19.004
Mar  1 12:11:19.011: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4784  e8da20c0-528e-4d55-b6b1-386845ec8997 16076 0 2023-03-01 12:11:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-01 12:11:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  1 12:11:19.011: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4784  e8da20c0-528e-4d55-b6b1-386845ec8997 16076 0 2023-03-01 12:11:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-01 12:11:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 03/01/23 12:11:19.011
Mar  1 12:11:19.021: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4784  e8da20c0-528e-4d55-b6b1-386845ec8997 16077 0 2023-03-01 12:11:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-01 12:11:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  1 12:11:19.022: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4784  e8da20c0-528e-4d55-b6b1-386845ec8997 16077 0 2023-03-01 12:11:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-01 12:11:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 03/01/23 12:11:19.022
Mar  1 12:11:19.032: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4784  e8da20c0-528e-4d55-b6b1-386845ec8997 16078 0 2023-03-01 12:11:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-01 12:11:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  1 12:11:19.032: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4784  e8da20c0-528e-4d55-b6b1-386845ec8997 16078 0 2023-03-01 12:11:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-01 12:11:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 03/01/23 12:11:19.033
Mar  1 12:11:19.043: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4784  e8da20c0-528e-4d55-b6b1-386845ec8997 16079 0 2023-03-01 12:11:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-01 12:11:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  1 12:11:19.043: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4784  e8da20c0-528e-4d55-b6b1-386845ec8997 16079 0 2023-03-01 12:11:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-01 12:11:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 03/01/23 12:11:19.043
Mar  1 12:11:19.049: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4784  ce483da9-587c-4a9d-a099-59f5e6b0772a 16080 0 2023-03-01 12:11:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-01 12:11:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  1 12:11:19.049: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4784  ce483da9-587c-4a9d-a099-59f5e6b0772a 16080 0 2023-03-01 12:11:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-01 12:11:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 03/01/23 12:11:29.05
Mar  1 12:11:29.066: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4784  ce483da9-587c-4a9d-a099-59f5e6b0772a 16116 0 2023-03-01 12:11:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-01 12:11:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  1 12:11:29.066: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4784  ce483da9-587c-4a9d-a099-59f5e6b0772a 16116 0 2023-03-01 12:11:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-01 12:11:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Mar  1 12:11:39.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4784" for this suite. 03/01/23 12:11:39.076
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","completed":109,"skipped":2128,"failed":0}
------------------------------
â€¢ [SLOW TEST] [20.114 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:11:18.972
    Mar  1 12:11:18.972: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename watch 03/01/23 12:11:18.972
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:11:18.993
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:11:18.996
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 03/01/23 12:11:19
    STEP: creating a watch on configmaps with label B 03/01/23 12:11:19.002
    STEP: creating a watch on configmaps with label A or B 03/01/23 12:11:19.003
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 03/01/23 12:11:19.004
    Mar  1 12:11:19.011: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4784  e8da20c0-528e-4d55-b6b1-386845ec8997 16076 0 2023-03-01 12:11:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-01 12:11:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar  1 12:11:19.011: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4784  e8da20c0-528e-4d55-b6b1-386845ec8997 16076 0 2023-03-01 12:11:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-01 12:11:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 03/01/23 12:11:19.011
    Mar  1 12:11:19.021: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4784  e8da20c0-528e-4d55-b6b1-386845ec8997 16077 0 2023-03-01 12:11:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-01 12:11:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar  1 12:11:19.022: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4784  e8da20c0-528e-4d55-b6b1-386845ec8997 16077 0 2023-03-01 12:11:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-01 12:11:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 03/01/23 12:11:19.022
    Mar  1 12:11:19.032: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4784  e8da20c0-528e-4d55-b6b1-386845ec8997 16078 0 2023-03-01 12:11:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-01 12:11:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar  1 12:11:19.032: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4784  e8da20c0-528e-4d55-b6b1-386845ec8997 16078 0 2023-03-01 12:11:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-01 12:11:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 03/01/23 12:11:19.033
    Mar  1 12:11:19.043: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4784  e8da20c0-528e-4d55-b6b1-386845ec8997 16079 0 2023-03-01 12:11:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-01 12:11:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar  1 12:11:19.043: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4784  e8da20c0-528e-4d55-b6b1-386845ec8997 16079 0 2023-03-01 12:11:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-01 12:11:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 03/01/23 12:11:19.043
    Mar  1 12:11:19.049: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4784  ce483da9-587c-4a9d-a099-59f5e6b0772a 16080 0 2023-03-01 12:11:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-01 12:11:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar  1 12:11:19.049: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4784  ce483da9-587c-4a9d-a099-59f5e6b0772a 16080 0 2023-03-01 12:11:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-01 12:11:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 03/01/23 12:11:29.05
    Mar  1 12:11:29.066: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4784  ce483da9-587c-4a9d-a099-59f5e6b0772a 16116 0 2023-03-01 12:11:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-01 12:11:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar  1 12:11:29.066: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4784  ce483da9-587c-4a9d-a099-59f5e6b0772a 16116 0 2023-03-01 12:11:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-01 12:11:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Mar  1 12:11:39.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-4784" for this suite. 03/01/23 12:11:39.076
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:11:39.089
Mar  1 12:11:39.089: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename kubectl 03/01/23 12:11:39.089
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:11:39.112
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:11:39.115
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
STEP: starting the proxy server 03/01/23 12:11:39.118
Mar  1 12:11:39.118: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-3341 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 03/01/23 12:11:39.166
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar  1 12:11:39.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3341" for this suite. 03/01/23 12:11:39.182
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","completed":110,"skipped":2173,"failed":0}
------------------------------
â€¢ [0.103 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1785

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:11:39.089
    Mar  1 12:11:39.089: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename kubectl 03/01/23 12:11:39.089
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:11:39.112
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:11:39.115
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1785
    STEP: starting the proxy server 03/01/23 12:11:39.118
    Mar  1 12:11:39.118: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-3341 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 03/01/23 12:11:39.166
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar  1 12:11:39.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-3341" for this suite. 03/01/23 12:11:39.182
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:11:39.192
Mar  1 12:11:39.192: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 03/01/23 12:11:39.193
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:11:39.211
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:11:39.214
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 03/01/23 12:11:39.217
STEP: Creating hostNetwork=false pod 03/01/23 12:11:39.218
Mar  1 12:11:39.236: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-6035" to be "running and ready"
Mar  1 12:11:39.242: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.908351ms
Mar  1 12:11:39.242: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Mar  1 12:11:41.248: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.012322824s
Mar  1 12:11:41.248: INFO: The phase of Pod test-pod is Running (Ready = true)
Mar  1 12:11:41.248: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 03/01/23 12:11:41.252
Mar  1 12:11:41.262: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-6035" to be "running and ready"
Mar  1 12:11:41.268: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.41796ms
Mar  1 12:11:41.268: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Mar  1 12:11:43.275: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.012967555s
Mar  1 12:11:43.275: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Mar  1 12:11:43.275: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 03/01/23 12:11:43.279
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 03/01/23 12:11:43.279
Mar  1 12:11:43.279: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6035 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  1 12:11:43.279: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
Mar  1 12:11:43.279: INFO: ExecWithOptions: Clientset creation
Mar  1 12:11:43.279: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6035/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Mar  1 12:11:43.349: INFO: Exec stderr: ""
Mar  1 12:11:43.349: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6035 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  1 12:11:43.349: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
Mar  1 12:11:43.349: INFO: ExecWithOptions: Clientset creation
Mar  1 12:11:43.349: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6035/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Mar  1 12:11:43.406: INFO: Exec stderr: ""
Mar  1 12:11:43.406: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6035 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  1 12:11:43.406: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
Mar  1 12:11:43.407: INFO: ExecWithOptions: Clientset creation
Mar  1 12:11:43.407: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6035/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Mar  1 12:11:43.475: INFO: Exec stderr: ""
Mar  1 12:11:43.475: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6035 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  1 12:11:43.475: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
Mar  1 12:11:43.476: INFO: ExecWithOptions: Clientset creation
Mar  1 12:11:43.476: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6035/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Mar  1 12:11:43.540: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 03/01/23 12:11:43.541
Mar  1 12:11:43.541: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6035 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  1 12:11:43.541: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
Mar  1 12:11:43.541: INFO: ExecWithOptions: Clientset creation
Mar  1 12:11:43.542: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6035/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Mar  1 12:11:43.606: INFO: Exec stderr: ""
Mar  1 12:11:43.607: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6035 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  1 12:11:43.607: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
Mar  1 12:11:43.607: INFO: ExecWithOptions: Clientset creation
Mar  1 12:11:43.608: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6035/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Mar  1 12:11:43.666: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 03/01/23 12:11:43.666
Mar  1 12:11:43.667: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6035 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  1 12:11:43.667: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
Mar  1 12:11:43.667: INFO: ExecWithOptions: Clientset creation
Mar  1 12:11:43.667: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6035/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Mar  1 12:11:43.734: INFO: Exec stderr: ""
Mar  1 12:11:43.734: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6035 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  1 12:11:43.734: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
Mar  1 12:11:43.735: INFO: ExecWithOptions: Clientset creation
Mar  1 12:11:43.735: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6035/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Mar  1 12:11:43.800: INFO: Exec stderr: ""
Mar  1 12:11:43.800: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6035 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  1 12:11:43.800: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
Mar  1 12:11:43.801: INFO: ExecWithOptions: Clientset creation
Mar  1 12:11:43.801: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6035/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Mar  1 12:11:43.872: INFO: Exec stderr: ""
Mar  1 12:11:43.872: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6035 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  1 12:11:43.872: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
Mar  1 12:11:43.873: INFO: ExecWithOptions: Clientset creation
Mar  1 12:11:43.873: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6035/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Mar  1 12:11:43.941: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:187
Mar  1 12:11:43.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-6035" for this suite. 03/01/23 12:11:43.95
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","completed":111,"skipped":2175,"failed":0}
------------------------------
â€¢ [4.767 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:11:39.192
    Mar  1 12:11:39.192: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 03/01/23 12:11:39.193
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:11:39.211
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:11:39.214
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 03/01/23 12:11:39.217
    STEP: Creating hostNetwork=false pod 03/01/23 12:11:39.218
    Mar  1 12:11:39.236: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-6035" to be "running and ready"
    Mar  1 12:11:39.242: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.908351ms
    Mar  1 12:11:39.242: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Mar  1 12:11:41.248: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.012322824s
    Mar  1 12:11:41.248: INFO: The phase of Pod test-pod is Running (Ready = true)
    Mar  1 12:11:41.248: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 03/01/23 12:11:41.252
    Mar  1 12:11:41.262: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-6035" to be "running and ready"
    Mar  1 12:11:41.268: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.41796ms
    Mar  1 12:11:41.268: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Mar  1 12:11:43.275: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.012967555s
    Mar  1 12:11:43.275: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Mar  1 12:11:43.275: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 03/01/23 12:11:43.279
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 03/01/23 12:11:43.279
    Mar  1 12:11:43.279: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6035 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  1 12:11:43.279: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    Mar  1 12:11:43.279: INFO: ExecWithOptions: Clientset creation
    Mar  1 12:11:43.279: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6035/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Mar  1 12:11:43.349: INFO: Exec stderr: ""
    Mar  1 12:11:43.349: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6035 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  1 12:11:43.349: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    Mar  1 12:11:43.349: INFO: ExecWithOptions: Clientset creation
    Mar  1 12:11:43.349: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6035/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Mar  1 12:11:43.406: INFO: Exec stderr: ""
    Mar  1 12:11:43.406: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6035 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  1 12:11:43.406: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    Mar  1 12:11:43.407: INFO: ExecWithOptions: Clientset creation
    Mar  1 12:11:43.407: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6035/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Mar  1 12:11:43.475: INFO: Exec stderr: ""
    Mar  1 12:11:43.475: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6035 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  1 12:11:43.475: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    Mar  1 12:11:43.476: INFO: ExecWithOptions: Clientset creation
    Mar  1 12:11:43.476: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6035/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Mar  1 12:11:43.540: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 03/01/23 12:11:43.541
    Mar  1 12:11:43.541: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6035 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  1 12:11:43.541: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    Mar  1 12:11:43.541: INFO: ExecWithOptions: Clientset creation
    Mar  1 12:11:43.542: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6035/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Mar  1 12:11:43.606: INFO: Exec stderr: ""
    Mar  1 12:11:43.607: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6035 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  1 12:11:43.607: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    Mar  1 12:11:43.607: INFO: ExecWithOptions: Clientset creation
    Mar  1 12:11:43.608: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6035/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Mar  1 12:11:43.666: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 03/01/23 12:11:43.666
    Mar  1 12:11:43.667: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6035 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  1 12:11:43.667: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    Mar  1 12:11:43.667: INFO: ExecWithOptions: Clientset creation
    Mar  1 12:11:43.667: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6035/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Mar  1 12:11:43.734: INFO: Exec stderr: ""
    Mar  1 12:11:43.734: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6035 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  1 12:11:43.734: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    Mar  1 12:11:43.735: INFO: ExecWithOptions: Clientset creation
    Mar  1 12:11:43.735: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6035/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Mar  1 12:11:43.800: INFO: Exec stderr: ""
    Mar  1 12:11:43.800: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6035 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  1 12:11:43.800: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    Mar  1 12:11:43.801: INFO: ExecWithOptions: Clientset creation
    Mar  1 12:11:43.801: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6035/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Mar  1 12:11:43.872: INFO: Exec stderr: ""
    Mar  1 12:11:43.872: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6035 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  1 12:11:43.872: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    Mar  1 12:11:43.873: INFO: ExecWithOptions: Clientset creation
    Mar  1 12:11:43.873: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6035/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Mar  1 12:11:43.941: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:187
    Mar  1 12:11:43.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-6035" for this suite. 03/01/23 12:11:43.95
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Services
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:11:43.959
Mar  1 12:11:43.959: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename services 03/01/23 12:11:43.96
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:11:43.979
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:11:43.982
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173
STEP: creating service in namespace services-9742 03/01/23 12:11:43.985
Mar  1 12:11:43.994: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-9742" to be "running and ready"
Mar  1 12:11:44.001: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 6.633415ms
Mar  1 12:11:44.001: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Mar  1 12:11:46.007: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.01298202s
Mar  1 12:11:46.007: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
Mar  1 12:11:46.007: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
Mar  1 12:11:46.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-9742 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Mar  1 12:11:46.170: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Mar  1 12:11:46.170: INFO: stdout: "iptables"
Mar  1 12:11:46.170: INFO: proxyMode: iptables
Mar  1 12:11:46.185: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Mar  1 12:11:46.189: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-9742 03/01/23 12:11:46.189
STEP: creating replication controller affinity-clusterip-timeout in namespace services-9742 03/01/23 12:11:46.205
I0301 12:11:46.215104      19 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-9742, replica count: 3
I0301 12:11:49.266592      19 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  1 12:11:49.278: INFO: Creating new exec pod
Mar  1 12:11:49.283: INFO: Waiting up to 5m0s for pod "execpod-affinityvshfc" in namespace "services-9742" to be "running"
Mar  1 12:11:49.290: INFO: Pod "execpod-affinityvshfc": Phase="Pending", Reason="", readiness=false. Elapsed: 7.19024ms
Mar  1 12:11:51.295: INFO: Pod "execpod-affinityvshfc": Phase="Running", Reason="", readiness=true. Elapsed: 2.01214211s
Mar  1 12:11:51.296: INFO: Pod "execpod-affinityvshfc" satisfied condition "running"
Mar  1 12:11:52.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-9742 exec execpod-affinityvshfc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
Mar  1 12:11:52.440: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Mar  1 12:11:52.440: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  1 12:11:52.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-9742 exec execpod-affinityvshfc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.36.150 80'
Mar  1 12:11:52.570: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.36.150 80\nConnection to 10.233.36.150 80 port [tcp/http] succeeded!\n"
Mar  1 12:11:52.570: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  1 12:11:52.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-9742 exec execpod-affinityvshfc -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.36.150:80/ ; done'
Mar  1 12:11:52.796: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.150:80/\n"
Mar  1 12:11:52.796: INFO: stdout: "\naffinity-clusterip-timeout-s4rnc\naffinity-clusterip-timeout-s4rnc\naffinity-clusterip-timeout-s4rnc\naffinity-clusterip-timeout-s4rnc\naffinity-clusterip-timeout-s4rnc\naffinity-clusterip-timeout-s4rnc\naffinity-clusterip-timeout-s4rnc\naffinity-clusterip-timeout-s4rnc\naffinity-clusterip-timeout-s4rnc\naffinity-clusterip-timeout-s4rnc\naffinity-clusterip-timeout-s4rnc\naffinity-clusterip-timeout-s4rnc\naffinity-clusterip-timeout-s4rnc\naffinity-clusterip-timeout-s4rnc\naffinity-clusterip-timeout-s4rnc\naffinity-clusterip-timeout-s4rnc"
Mar  1 12:11:52.796: INFO: Received response from host: affinity-clusterip-timeout-s4rnc
Mar  1 12:11:52.796: INFO: Received response from host: affinity-clusterip-timeout-s4rnc
Mar  1 12:11:52.796: INFO: Received response from host: affinity-clusterip-timeout-s4rnc
Mar  1 12:11:52.796: INFO: Received response from host: affinity-clusterip-timeout-s4rnc
Mar  1 12:11:52.796: INFO: Received response from host: affinity-clusterip-timeout-s4rnc
Mar  1 12:11:52.796: INFO: Received response from host: affinity-clusterip-timeout-s4rnc
Mar  1 12:11:52.796: INFO: Received response from host: affinity-clusterip-timeout-s4rnc
Mar  1 12:11:52.796: INFO: Received response from host: affinity-clusterip-timeout-s4rnc
Mar  1 12:11:52.796: INFO: Received response from host: affinity-clusterip-timeout-s4rnc
Mar  1 12:11:52.796: INFO: Received response from host: affinity-clusterip-timeout-s4rnc
Mar  1 12:11:52.796: INFO: Received response from host: affinity-clusterip-timeout-s4rnc
Mar  1 12:11:52.796: INFO: Received response from host: affinity-clusterip-timeout-s4rnc
Mar  1 12:11:52.796: INFO: Received response from host: affinity-clusterip-timeout-s4rnc
Mar  1 12:11:52.796: INFO: Received response from host: affinity-clusterip-timeout-s4rnc
Mar  1 12:11:52.796: INFO: Received response from host: affinity-clusterip-timeout-s4rnc
Mar  1 12:11:52.796: INFO: Received response from host: affinity-clusterip-timeout-s4rnc
Mar  1 12:11:52.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-9742 exec execpod-affinityvshfc -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.233.36.150:80/'
Mar  1 12:11:52.944: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.233.36.150:80/\n"
Mar  1 12:11:52.944: INFO: stdout: "affinity-clusterip-timeout-s4rnc"
Mar  1 12:12:12.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-9742 exec execpod-affinityvshfc -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.233.36.150:80/'
Mar  1 12:12:13.075: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.233.36.150:80/\n"
Mar  1 12:12:13.075: INFO: stdout: "affinity-clusterip-timeout-wrbrj"
Mar  1 12:12:13.075: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-9742, will wait for the garbage collector to delete the pods 03/01/23 12:12:13.09
Mar  1 12:12:13.155: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 8.596071ms
Mar  1 12:12:13.256: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 101.028216ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar  1 12:12:15.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9742" for this suite. 03/01/23 12:12:15.214
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","completed":112,"skipped":2181,"failed":0}
------------------------------
â€¢ [SLOW TEST] [31.280 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:11:43.959
    Mar  1 12:11:43.959: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename services 03/01/23 12:11:43.96
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:11:43.979
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:11:43.982
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2173
    STEP: creating service in namespace services-9742 03/01/23 12:11:43.985
    Mar  1 12:11:43.994: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-9742" to be "running and ready"
    Mar  1 12:11:44.001: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 6.633415ms
    Mar  1 12:11:44.001: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
    Mar  1 12:11:46.007: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.01298202s
    Mar  1 12:11:46.007: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
    Mar  1 12:11:46.007: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
    Mar  1 12:11:46.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-9742 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
    Mar  1 12:11:46.170: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
    Mar  1 12:11:46.170: INFO: stdout: "iptables"
    Mar  1 12:11:46.170: INFO: proxyMode: iptables
    Mar  1 12:11:46.185: INFO: Waiting for pod kube-proxy-mode-detector to disappear
    Mar  1 12:11:46.189: INFO: Pod kube-proxy-mode-detector no longer exists
    STEP: creating service affinity-clusterip-timeout in namespace services-9742 03/01/23 12:11:46.189
    STEP: creating replication controller affinity-clusterip-timeout in namespace services-9742 03/01/23 12:11:46.205
    I0301 12:11:46.215104      19 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-9742, replica count: 3
    I0301 12:11:49.266592      19 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar  1 12:11:49.278: INFO: Creating new exec pod
    Mar  1 12:11:49.283: INFO: Waiting up to 5m0s for pod "execpod-affinityvshfc" in namespace "services-9742" to be "running"
    Mar  1 12:11:49.290: INFO: Pod "execpod-affinityvshfc": Phase="Pending", Reason="", readiness=false. Elapsed: 7.19024ms
    Mar  1 12:11:51.295: INFO: Pod "execpod-affinityvshfc": Phase="Running", Reason="", readiness=true. Elapsed: 2.01214211s
    Mar  1 12:11:51.296: INFO: Pod "execpod-affinityvshfc" satisfied condition "running"
    Mar  1 12:11:52.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-9742 exec execpod-affinityvshfc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
    Mar  1 12:11:52.440: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
    Mar  1 12:11:52.440: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  1 12:11:52.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-9742 exec execpod-affinityvshfc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.36.150 80'
    Mar  1 12:11:52.570: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.36.150 80\nConnection to 10.233.36.150 80 port [tcp/http] succeeded!\n"
    Mar  1 12:11:52.570: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  1 12:11:52.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-9742 exec execpod-affinityvshfc -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.36.150:80/ ; done'
    Mar  1 12:11:52.796: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.36.150:80/\n"
    Mar  1 12:11:52.796: INFO: stdout: "\naffinity-clusterip-timeout-s4rnc\naffinity-clusterip-timeout-s4rnc\naffinity-clusterip-timeout-s4rnc\naffinity-clusterip-timeout-s4rnc\naffinity-clusterip-timeout-s4rnc\naffinity-clusterip-timeout-s4rnc\naffinity-clusterip-timeout-s4rnc\naffinity-clusterip-timeout-s4rnc\naffinity-clusterip-timeout-s4rnc\naffinity-clusterip-timeout-s4rnc\naffinity-clusterip-timeout-s4rnc\naffinity-clusterip-timeout-s4rnc\naffinity-clusterip-timeout-s4rnc\naffinity-clusterip-timeout-s4rnc\naffinity-clusterip-timeout-s4rnc\naffinity-clusterip-timeout-s4rnc"
    Mar  1 12:11:52.796: INFO: Received response from host: affinity-clusterip-timeout-s4rnc
    Mar  1 12:11:52.796: INFO: Received response from host: affinity-clusterip-timeout-s4rnc
    Mar  1 12:11:52.796: INFO: Received response from host: affinity-clusterip-timeout-s4rnc
    Mar  1 12:11:52.796: INFO: Received response from host: affinity-clusterip-timeout-s4rnc
    Mar  1 12:11:52.796: INFO: Received response from host: affinity-clusterip-timeout-s4rnc
    Mar  1 12:11:52.796: INFO: Received response from host: affinity-clusterip-timeout-s4rnc
    Mar  1 12:11:52.796: INFO: Received response from host: affinity-clusterip-timeout-s4rnc
    Mar  1 12:11:52.796: INFO: Received response from host: affinity-clusterip-timeout-s4rnc
    Mar  1 12:11:52.796: INFO: Received response from host: affinity-clusterip-timeout-s4rnc
    Mar  1 12:11:52.796: INFO: Received response from host: affinity-clusterip-timeout-s4rnc
    Mar  1 12:11:52.796: INFO: Received response from host: affinity-clusterip-timeout-s4rnc
    Mar  1 12:11:52.796: INFO: Received response from host: affinity-clusterip-timeout-s4rnc
    Mar  1 12:11:52.796: INFO: Received response from host: affinity-clusterip-timeout-s4rnc
    Mar  1 12:11:52.796: INFO: Received response from host: affinity-clusterip-timeout-s4rnc
    Mar  1 12:11:52.796: INFO: Received response from host: affinity-clusterip-timeout-s4rnc
    Mar  1 12:11:52.796: INFO: Received response from host: affinity-clusterip-timeout-s4rnc
    Mar  1 12:11:52.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-9742 exec execpod-affinityvshfc -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.233.36.150:80/'
    Mar  1 12:11:52.944: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.233.36.150:80/\n"
    Mar  1 12:11:52.944: INFO: stdout: "affinity-clusterip-timeout-s4rnc"
    Mar  1 12:12:12.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-9742 exec execpod-affinityvshfc -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.233.36.150:80/'
    Mar  1 12:12:13.075: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.233.36.150:80/\n"
    Mar  1 12:12:13.075: INFO: stdout: "affinity-clusterip-timeout-wrbrj"
    Mar  1 12:12:13.075: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-9742, will wait for the garbage collector to delete the pods 03/01/23 12:12:13.09
    Mar  1 12:12:13.155: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 8.596071ms
    Mar  1 12:12:13.256: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 101.028216ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar  1 12:12:15.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-9742" for this suite. 03/01/23 12:12:15.214
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:12:15.24
Mar  1 12:12:15.240: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename gc 03/01/23 12:12:15.241
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:12:15.259
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:12:15.266
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 03/01/23 12:12:15.277
STEP: create the rc2 03/01/23 12:12:15.287
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 03/01/23 12:12:20.307
STEP: delete the rc simpletest-rc-to-be-deleted 03/01/23 12:12:21.2
STEP: wait for the rc to be deleted 03/01/23 12:12:21.214
Mar  1 12:12:26.236: INFO: 71 pods remaining
Mar  1 12:12:26.236: INFO: 71 pods has nil DeletionTimestamp
Mar  1 12:12:26.236: INFO: 
STEP: Gathering metrics 03/01/23 12:12:31.235
Mar  1 12:12:31.273: INFO: Waiting up to 5m0s for pod "kube-controller-manager-lab1-k8s-master-3" in namespace "kube-system" to be "running and ready"
Mar  1 12:12:31.280: INFO: Pod "kube-controller-manager-lab1-k8s-master-3": Phase="Running", Reason="", readiness=true. Elapsed: 6.926458ms
Mar  1 12:12:31.280: INFO: The phase of Pod kube-controller-manager-lab1-k8s-master-3 is Running (Ready = true)
Mar  1 12:12:31.280: INFO: Pod "kube-controller-manager-lab1-k8s-master-3" satisfied condition "running and ready"
Mar  1 12:12:31.339: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Mar  1 12:12:31.339: INFO: Deleting pod "simpletest-rc-to-be-deleted-25mt4" in namespace "gc-3218"
Mar  1 12:12:31.355: INFO: Deleting pod "simpletest-rc-to-be-deleted-4d68m" in namespace "gc-3218"
Mar  1 12:12:31.372: INFO: Deleting pod "simpletest-rc-to-be-deleted-4f44q" in namespace "gc-3218"
Mar  1 12:12:31.387: INFO: Deleting pod "simpletest-rc-to-be-deleted-4fzbm" in namespace "gc-3218"
Mar  1 12:12:31.402: INFO: Deleting pod "simpletest-rc-to-be-deleted-4ggnt" in namespace "gc-3218"
Mar  1 12:12:31.419: INFO: Deleting pod "simpletest-rc-to-be-deleted-4zx87" in namespace "gc-3218"
Mar  1 12:12:31.443: INFO: Deleting pod "simpletest-rc-to-be-deleted-568hq" in namespace "gc-3218"
Mar  1 12:12:31.467: INFO: Deleting pod "simpletest-rc-to-be-deleted-5b8q6" in namespace "gc-3218"
Mar  1 12:12:31.494: INFO: Deleting pod "simpletest-rc-to-be-deleted-5jdjs" in namespace "gc-3218"
Mar  1 12:12:31.520: INFO: Deleting pod "simpletest-rc-to-be-deleted-5sb9z" in namespace "gc-3218"
Mar  1 12:12:31.544: INFO: Deleting pod "simpletest-rc-to-be-deleted-6b4d5" in namespace "gc-3218"
Mar  1 12:12:31.572: INFO: Deleting pod "simpletest-rc-to-be-deleted-6pxtv" in namespace "gc-3218"
Mar  1 12:12:31.590: INFO: Deleting pod "simpletest-rc-to-be-deleted-75dlk" in namespace "gc-3218"
Mar  1 12:12:31.608: INFO: Deleting pod "simpletest-rc-to-be-deleted-7g57t" in namespace "gc-3218"
Mar  1 12:12:31.626: INFO: Deleting pod "simpletest-rc-to-be-deleted-82tdk" in namespace "gc-3218"
Mar  1 12:12:31.642: INFO: Deleting pod "simpletest-rc-to-be-deleted-8ghm5" in namespace "gc-3218"
Mar  1 12:12:31.656: INFO: Deleting pod "simpletest-rc-to-be-deleted-8l7gz" in namespace "gc-3218"
Mar  1 12:12:31.684: INFO: Deleting pod "simpletest-rc-to-be-deleted-8nxpz" in namespace "gc-3218"
Mar  1 12:12:31.702: INFO: Deleting pod "simpletest-rc-to-be-deleted-8qb5v" in namespace "gc-3218"
Mar  1 12:12:31.737: INFO: Deleting pod "simpletest-rc-to-be-deleted-8rvjh" in namespace "gc-3218"
Mar  1 12:12:31.761: INFO: Deleting pod "simpletest-rc-to-be-deleted-94n5p" in namespace "gc-3218"
Mar  1 12:12:31.776: INFO: Deleting pod "simpletest-rc-to-be-deleted-9ggpk" in namespace "gc-3218"
Mar  1 12:12:31.793: INFO: Deleting pod "simpletest-rc-to-be-deleted-9qqzc" in namespace "gc-3218"
Mar  1 12:12:31.810: INFO: Deleting pod "simpletest-rc-to-be-deleted-9skk7" in namespace "gc-3218"
Mar  1 12:12:31.830: INFO: Deleting pod "simpletest-rc-to-be-deleted-9wd9p" in namespace "gc-3218"
Mar  1 12:12:31.848: INFO: Deleting pod "simpletest-rc-to-be-deleted-bgzfp" in namespace "gc-3218"
Mar  1 12:12:31.870: INFO: Deleting pod "simpletest-rc-to-be-deleted-bq29h" in namespace "gc-3218"
Mar  1 12:12:31.885: INFO: Deleting pod "simpletest-rc-to-be-deleted-cn62s" in namespace "gc-3218"
Mar  1 12:12:31.901: INFO: Deleting pod "simpletest-rc-to-be-deleted-crc9w" in namespace "gc-3218"
Mar  1 12:12:31.925: INFO: Deleting pod "simpletest-rc-to-be-deleted-csxpv" in namespace "gc-3218"
Mar  1 12:12:31.945: INFO: Deleting pod "simpletest-rc-to-be-deleted-dh7c2" in namespace "gc-3218"
Mar  1 12:12:31.967: INFO: Deleting pod "simpletest-rc-to-be-deleted-f92kv" in namespace "gc-3218"
Mar  1 12:12:31.994: INFO: Deleting pod "simpletest-rc-to-be-deleted-f9cs2" in namespace "gc-3218"
Mar  1 12:12:32.012: INFO: Deleting pod "simpletest-rc-to-be-deleted-fbg7v" in namespace "gc-3218"
Mar  1 12:12:32.038: INFO: Deleting pod "simpletest-rc-to-be-deleted-fd49z" in namespace "gc-3218"
Mar  1 12:12:32.050: INFO: Deleting pod "simpletest-rc-to-be-deleted-flkck" in namespace "gc-3218"
Mar  1 12:12:32.080: INFO: Deleting pod "simpletest-rc-to-be-deleted-fpgw8" in namespace "gc-3218"
Mar  1 12:12:32.108: INFO: Deleting pod "simpletest-rc-to-be-deleted-fsr7g" in namespace "gc-3218"
Mar  1 12:12:32.132: INFO: Deleting pod "simpletest-rc-to-be-deleted-fxwgj" in namespace "gc-3218"
Mar  1 12:12:32.159: INFO: Deleting pod "simpletest-rc-to-be-deleted-fzvfn" in namespace "gc-3218"
Mar  1 12:12:32.179: INFO: Deleting pod "simpletest-rc-to-be-deleted-gqmpc" in namespace "gc-3218"
Mar  1 12:12:32.197: INFO: Deleting pod "simpletest-rc-to-be-deleted-hjvxn" in namespace "gc-3218"
Mar  1 12:12:32.213: INFO: Deleting pod "simpletest-rc-to-be-deleted-hmfg8" in namespace "gc-3218"
Mar  1 12:12:32.232: INFO: Deleting pod "simpletest-rc-to-be-deleted-hq5h5" in namespace "gc-3218"
Mar  1 12:12:32.250: INFO: Deleting pod "simpletest-rc-to-be-deleted-hzng8" in namespace "gc-3218"
Mar  1 12:12:32.284: INFO: Deleting pod "simpletest-rc-to-be-deleted-j5vd2" in namespace "gc-3218"
Mar  1 12:12:32.304: INFO: Deleting pod "simpletest-rc-to-be-deleted-jdbtn" in namespace "gc-3218"
Mar  1 12:12:32.320: INFO: Deleting pod "simpletest-rc-to-be-deleted-jqdxx" in namespace "gc-3218"
Mar  1 12:12:32.350: INFO: Deleting pod "simpletest-rc-to-be-deleted-l7xj6" in namespace "gc-3218"
Mar  1 12:12:32.367: INFO: Deleting pod "simpletest-rc-to-be-deleted-lbb82" in namespace "gc-3218"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Mar  1 12:12:32.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3218" for this suite. 03/01/23 12:12:32.393
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","completed":113,"skipped":2187,"failed":0}
------------------------------
â€¢ [SLOW TEST] [17.174 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:12:15.24
    Mar  1 12:12:15.240: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename gc 03/01/23 12:12:15.241
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:12:15.259
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:12:15.266
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 03/01/23 12:12:15.277
    STEP: create the rc2 03/01/23 12:12:15.287
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 03/01/23 12:12:20.307
    STEP: delete the rc simpletest-rc-to-be-deleted 03/01/23 12:12:21.2
    STEP: wait for the rc to be deleted 03/01/23 12:12:21.214
    Mar  1 12:12:26.236: INFO: 71 pods remaining
    Mar  1 12:12:26.236: INFO: 71 pods has nil DeletionTimestamp
    Mar  1 12:12:26.236: INFO: 
    STEP: Gathering metrics 03/01/23 12:12:31.235
    Mar  1 12:12:31.273: INFO: Waiting up to 5m0s for pod "kube-controller-manager-lab1-k8s-master-3" in namespace "kube-system" to be "running and ready"
    Mar  1 12:12:31.280: INFO: Pod "kube-controller-manager-lab1-k8s-master-3": Phase="Running", Reason="", readiness=true. Elapsed: 6.926458ms
    Mar  1 12:12:31.280: INFO: The phase of Pod kube-controller-manager-lab1-k8s-master-3 is Running (Ready = true)
    Mar  1 12:12:31.280: INFO: Pod "kube-controller-manager-lab1-k8s-master-3" satisfied condition "running and ready"
    Mar  1 12:12:31.339: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Mar  1 12:12:31.339: INFO: Deleting pod "simpletest-rc-to-be-deleted-25mt4" in namespace "gc-3218"
    Mar  1 12:12:31.355: INFO: Deleting pod "simpletest-rc-to-be-deleted-4d68m" in namespace "gc-3218"
    Mar  1 12:12:31.372: INFO: Deleting pod "simpletest-rc-to-be-deleted-4f44q" in namespace "gc-3218"
    Mar  1 12:12:31.387: INFO: Deleting pod "simpletest-rc-to-be-deleted-4fzbm" in namespace "gc-3218"
    Mar  1 12:12:31.402: INFO: Deleting pod "simpletest-rc-to-be-deleted-4ggnt" in namespace "gc-3218"
    Mar  1 12:12:31.419: INFO: Deleting pod "simpletest-rc-to-be-deleted-4zx87" in namespace "gc-3218"
    Mar  1 12:12:31.443: INFO: Deleting pod "simpletest-rc-to-be-deleted-568hq" in namespace "gc-3218"
    Mar  1 12:12:31.467: INFO: Deleting pod "simpletest-rc-to-be-deleted-5b8q6" in namespace "gc-3218"
    Mar  1 12:12:31.494: INFO: Deleting pod "simpletest-rc-to-be-deleted-5jdjs" in namespace "gc-3218"
    Mar  1 12:12:31.520: INFO: Deleting pod "simpletest-rc-to-be-deleted-5sb9z" in namespace "gc-3218"
    Mar  1 12:12:31.544: INFO: Deleting pod "simpletest-rc-to-be-deleted-6b4d5" in namespace "gc-3218"
    Mar  1 12:12:31.572: INFO: Deleting pod "simpletest-rc-to-be-deleted-6pxtv" in namespace "gc-3218"
    Mar  1 12:12:31.590: INFO: Deleting pod "simpletest-rc-to-be-deleted-75dlk" in namespace "gc-3218"
    Mar  1 12:12:31.608: INFO: Deleting pod "simpletest-rc-to-be-deleted-7g57t" in namespace "gc-3218"
    Mar  1 12:12:31.626: INFO: Deleting pod "simpletest-rc-to-be-deleted-82tdk" in namespace "gc-3218"
    Mar  1 12:12:31.642: INFO: Deleting pod "simpletest-rc-to-be-deleted-8ghm5" in namespace "gc-3218"
    Mar  1 12:12:31.656: INFO: Deleting pod "simpletest-rc-to-be-deleted-8l7gz" in namespace "gc-3218"
    Mar  1 12:12:31.684: INFO: Deleting pod "simpletest-rc-to-be-deleted-8nxpz" in namespace "gc-3218"
    Mar  1 12:12:31.702: INFO: Deleting pod "simpletest-rc-to-be-deleted-8qb5v" in namespace "gc-3218"
    Mar  1 12:12:31.737: INFO: Deleting pod "simpletest-rc-to-be-deleted-8rvjh" in namespace "gc-3218"
    Mar  1 12:12:31.761: INFO: Deleting pod "simpletest-rc-to-be-deleted-94n5p" in namespace "gc-3218"
    Mar  1 12:12:31.776: INFO: Deleting pod "simpletest-rc-to-be-deleted-9ggpk" in namespace "gc-3218"
    Mar  1 12:12:31.793: INFO: Deleting pod "simpletest-rc-to-be-deleted-9qqzc" in namespace "gc-3218"
    Mar  1 12:12:31.810: INFO: Deleting pod "simpletest-rc-to-be-deleted-9skk7" in namespace "gc-3218"
    Mar  1 12:12:31.830: INFO: Deleting pod "simpletest-rc-to-be-deleted-9wd9p" in namespace "gc-3218"
    Mar  1 12:12:31.848: INFO: Deleting pod "simpletest-rc-to-be-deleted-bgzfp" in namespace "gc-3218"
    Mar  1 12:12:31.870: INFO: Deleting pod "simpletest-rc-to-be-deleted-bq29h" in namespace "gc-3218"
    Mar  1 12:12:31.885: INFO: Deleting pod "simpletest-rc-to-be-deleted-cn62s" in namespace "gc-3218"
    Mar  1 12:12:31.901: INFO: Deleting pod "simpletest-rc-to-be-deleted-crc9w" in namespace "gc-3218"
    Mar  1 12:12:31.925: INFO: Deleting pod "simpletest-rc-to-be-deleted-csxpv" in namespace "gc-3218"
    Mar  1 12:12:31.945: INFO: Deleting pod "simpletest-rc-to-be-deleted-dh7c2" in namespace "gc-3218"
    Mar  1 12:12:31.967: INFO: Deleting pod "simpletest-rc-to-be-deleted-f92kv" in namespace "gc-3218"
    Mar  1 12:12:31.994: INFO: Deleting pod "simpletest-rc-to-be-deleted-f9cs2" in namespace "gc-3218"
    Mar  1 12:12:32.012: INFO: Deleting pod "simpletest-rc-to-be-deleted-fbg7v" in namespace "gc-3218"
    Mar  1 12:12:32.038: INFO: Deleting pod "simpletest-rc-to-be-deleted-fd49z" in namespace "gc-3218"
    Mar  1 12:12:32.050: INFO: Deleting pod "simpletest-rc-to-be-deleted-flkck" in namespace "gc-3218"
    Mar  1 12:12:32.080: INFO: Deleting pod "simpletest-rc-to-be-deleted-fpgw8" in namespace "gc-3218"
    Mar  1 12:12:32.108: INFO: Deleting pod "simpletest-rc-to-be-deleted-fsr7g" in namespace "gc-3218"
    Mar  1 12:12:32.132: INFO: Deleting pod "simpletest-rc-to-be-deleted-fxwgj" in namespace "gc-3218"
    Mar  1 12:12:32.159: INFO: Deleting pod "simpletest-rc-to-be-deleted-fzvfn" in namespace "gc-3218"
    Mar  1 12:12:32.179: INFO: Deleting pod "simpletest-rc-to-be-deleted-gqmpc" in namespace "gc-3218"
    Mar  1 12:12:32.197: INFO: Deleting pod "simpletest-rc-to-be-deleted-hjvxn" in namespace "gc-3218"
    Mar  1 12:12:32.213: INFO: Deleting pod "simpletest-rc-to-be-deleted-hmfg8" in namespace "gc-3218"
    Mar  1 12:12:32.232: INFO: Deleting pod "simpletest-rc-to-be-deleted-hq5h5" in namespace "gc-3218"
    Mar  1 12:12:32.250: INFO: Deleting pod "simpletest-rc-to-be-deleted-hzng8" in namespace "gc-3218"
    Mar  1 12:12:32.284: INFO: Deleting pod "simpletest-rc-to-be-deleted-j5vd2" in namespace "gc-3218"
    Mar  1 12:12:32.304: INFO: Deleting pod "simpletest-rc-to-be-deleted-jdbtn" in namespace "gc-3218"
    Mar  1 12:12:32.320: INFO: Deleting pod "simpletest-rc-to-be-deleted-jqdxx" in namespace "gc-3218"
    Mar  1 12:12:32.350: INFO: Deleting pod "simpletest-rc-to-be-deleted-l7xj6" in namespace "gc-3218"
    Mar  1 12:12:32.367: INFO: Deleting pod "simpletest-rc-to-be-deleted-lbb82" in namespace "gc-3218"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Mar  1 12:12:32.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-3218" for this suite. 03/01/23 12:12:32.393
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:12:32.417
Mar  1 12:12:32.425: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename container-probe 03/01/23 12:12:32.426
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:12:32.449
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:12:32.459
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
Mar  1 12:12:32.505: INFO: Waiting up to 5m0s for pod "test-webserver-ca81b06e-65cf-4692-88bb-ab4ace7b6f57" in namespace "container-probe-9418" to be "running and ready"
Mar  1 12:12:32.512: INFO: Pod "test-webserver-ca81b06e-65cf-4692-88bb-ab4ace7b6f57": Phase="Pending", Reason="", readiness=false. Elapsed: 7.682568ms
Mar  1 12:12:32.512: INFO: The phase of Pod test-webserver-ca81b06e-65cf-4692-88bb-ab4ace7b6f57 is Pending, waiting for it to be Running (with Ready = true)
Mar  1 12:12:34.518: INFO: Pod "test-webserver-ca81b06e-65cf-4692-88bb-ab4ace7b6f57": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012842958s
Mar  1 12:12:34.518: INFO: The phase of Pod test-webserver-ca81b06e-65cf-4692-88bb-ab4ace7b6f57 is Pending, waiting for it to be Running (with Ready = true)
Mar  1 12:12:36.518: INFO: Pod "test-webserver-ca81b06e-65cf-4692-88bb-ab4ace7b6f57": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01324214s
Mar  1 12:12:36.518: INFO: The phase of Pod test-webserver-ca81b06e-65cf-4692-88bb-ab4ace7b6f57 is Pending, waiting for it to be Running (with Ready = true)
Mar  1 12:12:38.519: INFO: Pod "test-webserver-ca81b06e-65cf-4692-88bb-ab4ace7b6f57": Phase="Running", Reason="", readiness=false. Elapsed: 6.014324383s
Mar  1 12:12:38.519: INFO: The phase of Pod test-webserver-ca81b06e-65cf-4692-88bb-ab4ace7b6f57 is Running (Ready = false)
Mar  1 12:12:40.518: INFO: Pod "test-webserver-ca81b06e-65cf-4692-88bb-ab4ace7b6f57": Phase="Running", Reason="", readiness=false. Elapsed: 8.013793485s
Mar  1 12:12:40.519: INFO: The phase of Pod test-webserver-ca81b06e-65cf-4692-88bb-ab4ace7b6f57 is Running (Ready = false)
Mar  1 12:12:42.518: INFO: Pod "test-webserver-ca81b06e-65cf-4692-88bb-ab4ace7b6f57": Phase="Running", Reason="", readiness=false. Elapsed: 10.013649186s
Mar  1 12:12:42.518: INFO: The phase of Pod test-webserver-ca81b06e-65cf-4692-88bb-ab4ace7b6f57 is Running (Ready = false)
Mar  1 12:12:44.518: INFO: Pod "test-webserver-ca81b06e-65cf-4692-88bb-ab4ace7b6f57": Phase="Running", Reason="", readiness=false. Elapsed: 12.013274802s
Mar  1 12:12:44.518: INFO: The phase of Pod test-webserver-ca81b06e-65cf-4692-88bb-ab4ace7b6f57 is Running (Ready = false)
Mar  1 12:12:46.520: INFO: Pod "test-webserver-ca81b06e-65cf-4692-88bb-ab4ace7b6f57": Phase="Running", Reason="", readiness=false. Elapsed: 14.015032403s
Mar  1 12:12:46.520: INFO: The phase of Pod test-webserver-ca81b06e-65cf-4692-88bb-ab4ace7b6f57 is Running (Ready = false)
Mar  1 12:12:48.519: INFO: Pod "test-webserver-ca81b06e-65cf-4692-88bb-ab4ace7b6f57": Phase="Running", Reason="", readiness=false. Elapsed: 16.014636727s
Mar  1 12:12:48.519: INFO: The phase of Pod test-webserver-ca81b06e-65cf-4692-88bb-ab4ace7b6f57 is Running (Ready = false)
Mar  1 12:12:50.517: INFO: Pod "test-webserver-ca81b06e-65cf-4692-88bb-ab4ace7b6f57": Phase="Running", Reason="", readiness=false. Elapsed: 18.012703346s
Mar  1 12:12:50.518: INFO: The phase of Pod test-webserver-ca81b06e-65cf-4692-88bb-ab4ace7b6f57 is Running (Ready = false)
Mar  1 12:12:52.519: INFO: Pod "test-webserver-ca81b06e-65cf-4692-88bb-ab4ace7b6f57": Phase="Running", Reason="", readiness=false. Elapsed: 20.014059061s
Mar  1 12:12:52.519: INFO: The phase of Pod test-webserver-ca81b06e-65cf-4692-88bb-ab4ace7b6f57 is Running (Ready = false)
Mar  1 12:12:54.520: INFO: Pod "test-webserver-ca81b06e-65cf-4692-88bb-ab4ace7b6f57": Phase="Running", Reason="", readiness=true. Elapsed: 22.014869289s
Mar  1 12:12:54.520: INFO: The phase of Pod test-webserver-ca81b06e-65cf-4692-88bb-ab4ace7b6f57 is Running (Ready = true)
Mar  1 12:12:54.520: INFO: Pod "test-webserver-ca81b06e-65cf-4692-88bb-ab4ace7b6f57" satisfied condition "running and ready"
Mar  1 12:12:54.525: INFO: Container started at 2023-03-01 12:12:34 +0000 UTC, pod became ready at 2023-03-01 12:12:53 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Mar  1 12:12:54.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9418" for this suite. 03/01/23 12:12:54.534
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","completed":114,"skipped":2219,"failed":0}
------------------------------
â€¢ [SLOW TEST] [22.128 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:12:32.417
    Mar  1 12:12:32.425: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename container-probe 03/01/23 12:12:32.426
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:12:32.449
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:12:32.459
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:68
    Mar  1 12:12:32.505: INFO: Waiting up to 5m0s for pod "test-webserver-ca81b06e-65cf-4692-88bb-ab4ace7b6f57" in namespace "container-probe-9418" to be "running and ready"
    Mar  1 12:12:32.512: INFO: Pod "test-webserver-ca81b06e-65cf-4692-88bb-ab4ace7b6f57": Phase="Pending", Reason="", readiness=false. Elapsed: 7.682568ms
    Mar  1 12:12:32.512: INFO: The phase of Pod test-webserver-ca81b06e-65cf-4692-88bb-ab4ace7b6f57 is Pending, waiting for it to be Running (with Ready = true)
    Mar  1 12:12:34.518: INFO: Pod "test-webserver-ca81b06e-65cf-4692-88bb-ab4ace7b6f57": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012842958s
    Mar  1 12:12:34.518: INFO: The phase of Pod test-webserver-ca81b06e-65cf-4692-88bb-ab4ace7b6f57 is Pending, waiting for it to be Running (with Ready = true)
    Mar  1 12:12:36.518: INFO: Pod "test-webserver-ca81b06e-65cf-4692-88bb-ab4ace7b6f57": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01324214s
    Mar  1 12:12:36.518: INFO: The phase of Pod test-webserver-ca81b06e-65cf-4692-88bb-ab4ace7b6f57 is Pending, waiting for it to be Running (with Ready = true)
    Mar  1 12:12:38.519: INFO: Pod "test-webserver-ca81b06e-65cf-4692-88bb-ab4ace7b6f57": Phase="Running", Reason="", readiness=false. Elapsed: 6.014324383s
    Mar  1 12:12:38.519: INFO: The phase of Pod test-webserver-ca81b06e-65cf-4692-88bb-ab4ace7b6f57 is Running (Ready = false)
    Mar  1 12:12:40.518: INFO: Pod "test-webserver-ca81b06e-65cf-4692-88bb-ab4ace7b6f57": Phase="Running", Reason="", readiness=false. Elapsed: 8.013793485s
    Mar  1 12:12:40.519: INFO: The phase of Pod test-webserver-ca81b06e-65cf-4692-88bb-ab4ace7b6f57 is Running (Ready = false)
    Mar  1 12:12:42.518: INFO: Pod "test-webserver-ca81b06e-65cf-4692-88bb-ab4ace7b6f57": Phase="Running", Reason="", readiness=false. Elapsed: 10.013649186s
    Mar  1 12:12:42.518: INFO: The phase of Pod test-webserver-ca81b06e-65cf-4692-88bb-ab4ace7b6f57 is Running (Ready = false)
    Mar  1 12:12:44.518: INFO: Pod "test-webserver-ca81b06e-65cf-4692-88bb-ab4ace7b6f57": Phase="Running", Reason="", readiness=false. Elapsed: 12.013274802s
    Mar  1 12:12:44.518: INFO: The phase of Pod test-webserver-ca81b06e-65cf-4692-88bb-ab4ace7b6f57 is Running (Ready = false)
    Mar  1 12:12:46.520: INFO: Pod "test-webserver-ca81b06e-65cf-4692-88bb-ab4ace7b6f57": Phase="Running", Reason="", readiness=false. Elapsed: 14.015032403s
    Mar  1 12:12:46.520: INFO: The phase of Pod test-webserver-ca81b06e-65cf-4692-88bb-ab4ace7b6f57 is Running (Ready = false)
    Mar  1 12:12:48.519: INFO: Pod "test-webserver-ca81b06e-65cf-4692-88bb-ab4ace7b6f57": Phase="Running", Reason="", readiness=false. Elapsed: 16.014636727s
    Mar  1 12:12:48.519: INFO: The phase of Pod test-webserver-ca81b06e-65cf-4692-88bb-ab4ace7b6f57 is Running (Ready = false)
    Mar  1 12:12:50.517: INFO: Pod "test-webserver-ca81b06e-65cf-4692-88bb-ab4ace7b6f57": Phase="Running", Reason="", readiness=false. Elapsed: 18.012703346s
    Mar  1 12:12:50.518: INFO: The phase of Pod test-webserver-ca81b06e-65cf-4692-88bb-ab4ace7b6f57 is Running (Ready = false)
    Mar  1 12:12:52.519: INFO: Pod "test-webserver-ca81b06e-65cf-4692-88bb-ab4ace7b6f57": Phase="Running", Reason="", readiness=false. Elapsed: 20.014059061s
    Mar  1 12:12:52.519: INFO: The phase of Pod test-webserver-ca81b06e-65cf-4692-88bb-ab4ace7b6f57 is Running (Ready = false)
    Mar  1 12:12:54.520: INFO: Pod "test-webserver-ca81b06e-65cf-4692-88bb-ab4ace7b6f57": Phase="Running", Reason="", readiness=true. Elapsed: 22.014869289s
    Mar  1 12:12:54.520: INFO: The phase of Pod test-webserver-ca81b06e-65cf-4692-88bb-ab4ace7b6f57 is Running (Ready = true)
    Mar  1 12:12:54.520: INFO: Pod "test-webserver-ca81b06e-65cf-4692-88bb-ab4ace7b6f57" satisfied condition "running and ready"
    Mar  1 12:12:54.525: INFO: Container started at 2023-03-01 12:12:34 +0000 UTC, pod became ready at 2023-03-01 12:12:53 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Mar  1 12:12:54.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-9418" for this suite. 03/01/23 12:12:54.534
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:12:54.546
Mar  1 12:12:54.546: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename namespaces 03/01/23 12:12:54.547
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:12:54.567
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:12:54.571
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
STEP: Read namespace status 03/01/23 12:12:54.574
Mar  1 12:12:54.579: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 03/01/23 12:12:54.579
Mar  1 12:12:54.586: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 03/01/23 12:12:54.586
Mar  1 12:12:54.599: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Mar  1 12:12:54.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-7592" for this suite. 03/01/23 12:12:54.62
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]","completed":115,"skipped":2257,"failed":0}
------------------------------
â€¢ [0.083 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:12:54.546
    Mar  1 12:12:54.546: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename namespaces 03/01/23 12:12:54.547
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:12:54.567
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:12:54.571
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:298
    STEP: Read namespace status 03/01/23 12:12:54.574
    Mar  1 12:12:54.579: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 03/01/23 12:12:54.579
    Mar  1 12:12:54.586: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 03/01/23 12:12:54.586
    Mar  1 12:12:54.599: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Mar  1 12:12:54.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-7592" for this suite. 03/01/23 12:12:54.62
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:12:54.632
Mar  1 12:12:54.632: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename container-lifecycle-hook 03/01/23 12:12:54.633
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:12:54.652
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:12:54.656
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 03/01/23 12:12:54.664
Mar  1 12:12:54.673: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-8744" to be "running and ready"
Mar  1 12:12:54.686: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 12.900376ms
Mar  1 12:12:54.686: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar  1 12:12:56.693: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.019413594s
Mar  1 12:12:56.693: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Mar  1 12:12:56.693: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
STEP: create the pod with lifecycle hook 03/01/23 12:12:56.699
Mar  1 12:12:56.704: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-8744" to be "running and ready"
Mar  1 12:12:56.713: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 9.134757ms
Mar  1 12:12:56.713: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Mar  1 12:12:58.720: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.016098468s
Mar  1 12:12:58.720: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Mar  1 12:12:58.720: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 03/01/23 12:12:58.726
Mar  1 12:12:58.739: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  1 12:12:58.745: INFO: Pod pod-with-prestop-http-hook still exists
Mar  1 12:13:00.745: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  1 12:13:00.751: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 03/01/23 12:13:00.751
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Mar  1 12:13:00.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8744" for this suite. 03/01/23 12:13:00.781
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","completed":116,"skipped":2264,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.157 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:12:54.632
    Mar  1 12:12:54.632: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename container-lifecycle-hook 03/01/23 12:12:54.633
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:12:54.652
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:12:54.656
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 03/01/23 12:12:54.664
    Mar  1 12:12:54.673: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-8744" to be "running and ready"
    Mar  1 12:12:54.686: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 12.900376ms
    Mar  1 12:12:54.686: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Mar  1 12:12:56.693: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.019413594s
    Mar  1 12:12:56.693: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Mar  1 12:12:56.693: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:152
    STEP: create the pod with lifecycle hook 03/01/23 12:12:56.699
    Mar  1 12:12:56.704: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-8744" to be "running and ready"
    Mar  1 12:12:56.713: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 9.134757ms
    Mar  1 12:12:56.713: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Mar  1 12:12:58.720: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.016098468s
    Mar  1 12:12:58.720: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Mar  1 12:12:58.720: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 03/01/23 12:12:58.726
    Mar  1 12:12:58.739: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Mar  1 12:12:58.745: INFO: Pod pod-with-prestop-http-hook still exists
    Mar  1 12:13:00.745: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Mar  1 12:13:00.751: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 03/01/23 12:13:00.751
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Mar  1 12:13:00.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-8744" for this suite. 03/01/23 12:13:00.781
  << End Captured GinkgoWriter Output
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:13:00.789
Mar  1 12:13:00.790: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename svcaccounts 03/01/23 12:13:00.791
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:13:00.816
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:13:00.82
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
Mar  1 12:13:00.843: INFO: created pod
Mar  1 12:13:00.843: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-6730" to be "Succeeded or Failed"
Mar  1 12:13:00.855: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 11.897999ms
Mar  1 12:13:02.860: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017533812s
Mar  1 12:13:04.862: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019182937s
STEP: Saw pod success 03/01/23 12:13:04.862
Mar  1 12:13:04.862: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Mar  1 12:13:34.863: INFO: polling logs
Mar  1 12:13:34.884: INFO: Pod logs: 
I0301 12:13:01.621364       1 log.go:195] OK: Got token
I0301 12:13:01.621391       1 log.go:195] validating with in-cluster discovery
I0301 12:13:01.621630       1 log.go:195] OK: got issuer https://kubernetes.default.svc.cluster.local
I0301 12:13:01.621652       1 log.go:195] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-6730:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1677673381, NotBefore:1677672781, IssuedAt:1677672781, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-6730", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"3c4cf2a5-d7c0-4083-86f7-2b5f40b13e3a"}}}
I0301 12:13:01.676025       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
I0301 12:13:01.689614       1 log.go:195] OK: Validated signature on JWT
I0301 12:13:01.689754       1 log.go:195] OK: Got valid claims from token!
I0301 12:13:01.689785       1 log.go:195] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-6730:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1677673381, NotBefore:1677672781, IssuedAt:1677672781, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-6730", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"3c4cf2a5-d7c0-4083-86f7-2b5f40b13e3a"}}}

Mar  1 12:13:34.884: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Mar  1 12:13:34.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6730" for this suite. 03/01/23 12:13:34.903
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","completed":117,"skipped":2264,"failed":0}
------------------------------
â€¢ [SLOW TEST] [34.124 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:13:00.789
    Mar  1 12:13:00.790: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename svcaccounts 03/01/23 12:13:00.791
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:13:00.816
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:13:00.82
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:528
    Mar  1 12:13:00.843: INFO: created pod
    Mar  1 12:13:00.843: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-6730" to be "Succeeded or Failed"
    Mar  1 12:13:00.855: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 11.897999ms
    Mar  1 12:13:02.860: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017533812s
    Mar  1 12:13:04.862: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019182937s
    STEP: Saw pod success 03/01/23 12:13:04.862
    Mar  1 12:13:04.862: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Mar  1 12:13:34.863: INFO: polling logs
    Mar  1 12:13:34.884: INFO: Pod logs: 
    I0301 12:13:01.621364       1 log.go:195] OK: Got token
    I0301 12:13:01.621391       1 log.go:195] validating with in-cluster discovery
    I0301 12:13:01.621630       1 log.go:195] OK: got issuer https://kubernetes.default.svc.cluster.local
    I0301 12:13:01.621652       1 log.go:195] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-6730:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1677673381, NotBefore:1677672781, IssuedAt:1677672781, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-6730", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"3c4cf2a5-d7c0-4083-86f7-2b5f40b13e3a"}}}
    I0301 12:13:01.676025       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
    I0301 12:13:01.689614       1 log.go:195] OK: Validated signature on JWT
    I0301 12:13:01.689754       1 log.go:195] OK: Got valid claims from token!
    I0301 12:13:01.689785       1 log.go:195] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-6730:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1677673381, NotBefore:1677672781, IssuedAt:1677672781, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-6730", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"3c4cf2a5-d7c0-4083-86f7-2b5f40b13e3a"}}}

    Mar  1 12:13:34.884: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Mar  1 12:13:34.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-6730" for this suite. 03/01/23 12:13:34.903
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:13:34.914
Mar  1 12:13:34.914: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename sched-preemption 03/01/23 12:13:34.915
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:13:34.937
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:13:34.941
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Mar  1 12:13:34.960: INFO: Waiting up to 1m0s for all nodes to be ready
Mar  1 12:14:35.006: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
STEP: Create pods that use 4/5 of node resources. 03/01/23 12:14:35.012
Mar  1 12:14:35.041: INFO: Created pod: pod0-0-sched-preemption-low-priority
Mar  1 12:14:35.057: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Mar  1 12:14:35.083: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Mar  1 12:14:35.094: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Mar  1 12:14:35.114: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Mar  1 12:14:35.122: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 03/01/23 12:14:35.122
Mar  1 12:14:35.122: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-5293" to be "running"
Mar  1 12:14:35.136: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 13.76858ms
Mar  1 12:14:37.143: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.020654619s
Mar  1 12:14:37.143: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Mar  1 12:14:37.143: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-5293" to be "running"
Mar  1 12:14:37.150: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.452372ms
Mar  1 12:14:37.150: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Mar  1 12:14:37.150: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-5293" to be "running"
Mar  1 12:14:37.155: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.646034ms
Mar  1 12:14:37.155: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Mar  1 12:14:37.155: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-5293" to be "running"
Mar  1 12:14:37.160: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.422259ms
Mar  1 12:14:37.160: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Mar  1 12:14:37.160: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-5293" to be "running"
Mar  1 12:14:37.170: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 9.451744ms
Mar  1 12:14:37.170: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Mar  1 12:14:37.170: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-5293" to be "running"
Mar  1 12:14:37.174: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.385792ms
Mar  1 12:14:37.174: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 03/01/23 12:14:37.174
Mar  1 12:14:37.190: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Mar  1 12:14:37.197: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.971997ms
Mar  1 12:14:39.203: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012267981s
Mar  1 12:14:41.203: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012400732s
Mar  1 12:14:43.204: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.012918565s
Mar  1 12:14:43.204: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Mar  1 12:14:43.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-5293" for this suite. 03/01/23 12:14:43.275
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","completed":118,"skipped":2271,"failed":0}
------------------------------
â€¢ [SLOW TEST] [68.439 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:13:34.914
    Mar  1 12:13:34.914: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename sched-preemption 03/01/23 12:13:34.915
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:13:34.937
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:13:34.941
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Mar  1 12:13:34.960: INFO: Waiting up to 1m0s for all nodes to be ready
    Mar  1 12:14:35.006: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:218
    STEP: Create pods that use 4/5 of node resources. 03/01/23 12:14:35.012
    Mar  1 12:14:35.041: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Mar  1 12:14:35.057: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Mar  1 12:14:35.083: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Mar  1 12:14:35.094: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Mar  1 12:14:35.114: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Mar  1 12:14:35.122: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 03/01/23 12:14:35.122
    Mar  1 12:14:35.122: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-5293" to be "running"
    Mar  1 12:14:35.136: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 13.76858ms
    Mar  1 12:14:37.143: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.020654619s
    Mar  1 12:14:37.143: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Mar  1 12:14:37.143: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-5293" to be "running"
    Mar  1 12:14:37.150: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.452372ms
    Mar  1 12:14:37.150: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Mar  1 12:14:37.150: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-5293" to be "running"
    Mar  1 12:14:37.155: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.646034ms
    Mar  1 12:14:37.155: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Mar  1 12:14:37.155: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-5293" to be "running"
    Mar  1 12:14:37.160: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.422259ms
    Mar  1 12:14:37.160: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Mar  1 12:14:37.160: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-5293" to be "running"
    Mar  1 12:14:37.170: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 9.451744ms
    Mar  1 12:14:37.170: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Mar  1 12:14:37.170: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-5293" to be "running"
    Mar  1 12:14:37.174: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.385792ms
    Mar  1 12:14:37.174: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 03/01/23 12:14:37.174
    Mar  1 12:14:37.190: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Mar  1 12:14:37.197: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.971997ms
    Mar  1 12:14:39.203: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012267981s
    Mar  1 12:14:41.203: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012400732s
    Mar  1 12:14:43.204: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.012918565s
    Mar  1 12:14:43.204: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Mar  1 12:14:43.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-5293" for this suite. 03/01/23 12:14:43.275
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:14:43.364
Mar  1 12:14:43.364: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename emptydir 03/01/23 12:14:43.365
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:14:43.389
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:14:43.392
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
STEP: Creating a pod to test emptydir volume type on tmpfs 03/01/23 12:14:43.395
Mar  1 12:14:43.403: INFO: Waiting up to 5m0s for pod "pod-f7da1326-48d4-42a4-a989-9594d6b71943" in namespace "emptydir-2780" to be "Succeeded or Failed"
Mar  1 12:14:43.412: INFO: Pod "pod-f7da1326-48d4-42a4-a989-9594d6b71943": Phase="Pending", Reason="", readiness=false. Elapsed: 8.852997ms
Mar  1 12:14:45.417: INFO: Pod "pod-f7da1326-48d4-42a4-a989-9594d6b71943": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013884221s
Mar  1 12:14:47.419: INFO: Pod "pod-f7da1326-48d4-42a4-a989-9594d6b71943": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015613053s
STEP: Saw pod success 03/01/23 12:14:47.419
Mar  1 12:14:47.419: INFO: Pod "pod-f7da1326-48d4-42a4-a989-9594d6b71943" satisfied condition "Succeeded or Failed"
Mar  1 12:14:47.423: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-f7da1326-48d4-42a4-a989-9594d6b71943 container test-container: <nil>
STEP: delete the pod 03/01/23 12:14:47.433
Mar  1 12:14:47.453: INFO: Waiting for pod pod-f7da1326-48d4-42a4-a989-9594d6b71943 to disappear
Mar  1 12:14:47.456: INFO: Pod pod-f7da1326-48d4-42a4-a989-9594d6b71943 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar  1 12:14:47.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2780" for this suite. 03/01/23 12:14:47.462
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":119,"skipped":2282,"failed":0}
------------------------------
â€¢ [4.108 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:14:43.364
    Mar  1 12:14:43.364: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename emptydir 03/01/23 12:14:43.365
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:14:43.389
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:14:43.392
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:86
    STEP: Creating a pod to test emptydir volume type on tmpfs 03/01/23 12:14:43.395
    Mar  1 12:14:43.403: INFO: Waiting up to 5m0s for pod "pod-f7da1326-48d4-42a4-a989-9594d6b71943" in namespace "emptydir-2780" to be "Succeeded or Failed"
    Mar  1 12:14:43.412: INFO: Pod "pod-f7da1326-48d4-42a4-a989-9594d6b71943": Phase="Pending", Reason="", readiness=false. Elapsed: 8.852997ms
    Mar  1 12:14:45.417: INFO: Pod "pod-f7da1326-48d4-42a4-a989-9594d6b71943": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013884221s
    Mar  1 12:14:47.419: INFO: Pod "pod-f7da1326-48d4-42a4-a989-9594d6b71943": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015613053s
    STEP: Saw pod success 03/01/23 12:14:47.419
    Mar  1 12:14:47.419: INFO: Pod "pod-f7da1326-48d4-42a4-a989-9594d6b71943" satisfied condition "Succeeded or Failed"
    Mar  1 12:14:47.423: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-f7da1326-48d4-42a4-a989-9594d6b71943 container test-container: <nil>
    STEP: delete the pod 03/01/23 12:14:47.433
    Mar  1 12:14:47.453: INFO: Waiting for pod pod-f7da1326-48d4-42a4-a989-9594d6b71943 to disappear
    Mar  1 12:14:47.456: INFO: Pod pod-f7da1326-48d4-42a4-a989-9594d6b71943 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar  1 12:14:47.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-2780" for this suite. 03/01/23 12:14:47.462
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:14:47.474
Mar  1 12:14:47.474: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename security-context-test 03/01/23 12:14:47.475
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:14:47.498
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:14:47.502
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
Mar  1 12:14:47.519: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-33707910-3e03-4172-960d-55a1ecb6fb5a" in namespace "security-context-test-2660" to be "Succeeded or Failed"
Mar  1 12:14:47.526: INFO: Pod "busybox-privileged-false-33707910-3e03-4172-960d-55a1ecb6fb5a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.887522ms
Mar  1 12:14:49.535: INFO: Pod "busybox-privileged-false-33707910-3e03-4172-960d-55a1ecb6fb5a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01627085s
Mar  1 12:14:51.532: INFO: Pod "busybox-privileged-false-33707910-3e03-4172-960d-55a1ecb6fb5a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012918371s
Mar  1 12:14:51.532: INFO: Pod "busybox-privileged-false-33707910-3e03-4172-960d-55a1ecb6fb5a" satisfied condition "Succeeded or Failed"
Mar  1 12:14:51.542: INFO: Got logs for pod "busybox-privileged-false-33707910-3e03-4172-960d-55a1ecb6fb5a": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Mar  1 12:14:51.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2660" for this suite. 03/01/23 12:14:51.552
{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","completed":120,"skipped":2297,"failed":0}
------------------------------
â€¢ [4.087 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:490
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:527

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:14:47.474
    Mar  1 12:14:47.474: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename security-context-test 03/01/23 12:14:47.475
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:14:47.498
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:14:47.502
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:527
    Mar  1 12:14:47.519: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-33707910-3e03-4172-960d-55a1ecb6fb5a" in namespace "security-context-test-2660" to be "Succeeded or Failed"
    Mar  1 12:14:47.526: INFO: Pod "busybox-privileged-false-33707910-3e03-4172-960d-55a1ecb6fb5a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.887522ms
    Mar  1 12:14:49.535: INFO: Pod "busybox-privileged-false-33707910-3e03-4172-960d-55a1ecb6fb5a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01627085s
    Mar  1 12:14:51.532: INFO: Pod "busybox-privileged-false-33707910-3e03-4172-960d-55a1ecb6fb5a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012918371s
    Mar  1 12:14:51.532: INFO: Pod "busybox-privileged-false-33707910-3e03-4172-960d-55a1ecb6fb5a" satisfied condition "Succeeded or Failed"
    Mar  1 12:14:51.542: INFO: Got logs for pod "busybox-privileged-false-33707910-3e03-4172-960d-55a1ecb6fb5a": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Mar  1 12:14:51.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-2660" for this suite. 03/01/23 12:14:51.552
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:14:51.562
Mar  1 12:14:51.563: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename aggregator 03/01/23 12:14:51.563
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:14:51.585
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:14:51.589
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Mar  1 12:14:51.592: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 03/01/23 12:14:51.593
Mar  1 12:14:52.115: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 12:14:54.122: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 12:14:56.120: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 12:14:58.120: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 12:15:00.122: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 12:15:02.122: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 12:15:04.121: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 12:15:06.122: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 12:15:08.122: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 12:15:10.119: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 12:15:12.122: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 12:15:14.122: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 12:15:16.271: INFO: Waited 129.936778ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 03/01/23 12:15:16.33
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 03/01/23 12:15:16.336
STEP: List APIServices 03/01/23 12:15:16.346
Mar  1 12:15:16.355: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:187
Mar  1 12:15:16.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-8977" for this suite. 03/01/23 12:15:16.655
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","completed":121,"skipped":2310,"failed":0}
------------------------------
â€¢ [SLOW TEST] [25.145 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:14:51.562
    Mar  1 12:14:51.563: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename aggregator 03/01/23 12:14:51.563
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:14:51.585
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:14:51.589
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Mar  1 12:14:51.592: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 03/01/23 12:14:51.593
    Mar  1 12:14:52.115: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar  1 12:14:54.122: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar  1 12:14:56.120: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar  1 12:14:58.120: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar  1 12:15:00.122: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar  1 12:15:02.122: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar  1 12:15:04.121: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar  1 12:15:06.122: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar  1 12:15:08.122: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar  1 12:15:10.119: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar  1 12:15:12.122: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar  1 12:15:14.122: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 1, 12, 14, 52, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar  1 12:15:16.271: INFO: Waited 129.936778ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 03/01/23 12:15:16.33
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 03/01/23 12:15:16.336
    STEP: List APIServices 03/01/23 12:15:16.346
    Mar  1 12:15:16.355: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:187
    Mar  1 12:15:16.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "aggregator-8977" for this suite. 03/01/23 12:15:16.655
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:15:16.71
Mar  1 12:15:16.710: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename cronjob 03/01/23 12:15:16.711
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:15:16.73
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:15:16.734
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 03/01/23 12:15:16.737
STEP: Ensuring more than one job is running at a time 03/01/23 12:15:16.744
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 03/01/23 12:17:00.751
STEP: Removing cronjob 03/01/23 12:17:00.757
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Mar  1 12:17:00.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-6374" for this suite. 03/01/23 12:17:00.777
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","completed":122,"skipped":2332,"failed":0}
------------------------------
â€¢ [SLOW TEST] [104.078 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:15:16.71
    Mar  1 12:15:16.710: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename cronjob 03/01/23 12:15:16.711
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:15:16.73
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:15:16.734
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 03/01/23 12:15:16.737
    STEP: Ensuring more than one job is running at a time 03/01/23 12:15:16.744
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 03/01/23 12:17:00.751
    STEP: Removing cronjob 03/01/23 12:17:00.757
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Mar  1 12:17:00.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-6374" for this suite. 03/01/23 12:17:00.777
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:17:00.794
Mar  1 12:17:00.794: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename sysctl 03/01/23 12:17:00.794
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:17:00.82
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:17:00.834
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 03/01/23 12:17:00.841
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
Mar  1 12:17:00.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-8055" for this suite. 03/01/23 12:17:00.855
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":123,"skipped":2352,"failed":0}
------------------------------
â€¢ [0.071 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:17:00.794
    Mar  1 12:17:00.794: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename sysctl 03/01/23 12:17:00.794
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:17:00.82
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:17:00.834
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 03/01/23 12:17:00.841
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    Mar  1 12:17:00.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-8055" for this suite. 03/01/23 12:17:00.855
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:17:00.866
Mar  1 12:17:00.866: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename kubectl 03/01/23 12:17:00.867
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:17:00.893
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:17:00.897
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 03/01/23 12:17:00.901
Mar  1 12:17:00.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-5600 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Mar  1 12:17:00.982: INFO: stderr: ""
Mar  1 12:17:00.982: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 03/01/23 12:17:00.982
Mar  1 12:17:00.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-5600 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
Mar  1 12:17:01.855: INFO: stderr: ""
Mar  1 12:17:01.855: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 03/01/23 12:17:01.855
Mar  1 12:17:01.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-5600 delete pods e2e-test-httpd-pod'
Mar  1 12:17:04.104: INFO: stderr: ""
Mar  1 12:17:04.104: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar  1 12:17:04.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5600" for this suite. 03/01/23 12:17:04.11
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","completed":124,"skipped":2363,"failed":0}
------------------------------
â€¢ [3.253 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:954
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:960

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:17:00.866
    Mar  1 12:17:00.866: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename kubectl 03/01/23 12:17:00.867
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:17:00.893
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:17:00.897
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:960
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 03/01/23 12:17:00.901
    Mar  1 12:17:00.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-5600 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Mar  1 12:17:00.982: INFO: stderr: ""
    Mar  1 12:17:00.982: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 03/01/23 12:17:00.982
    Mar  1 12:17:00.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-5600 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
    Mar  1 12:17:01.855: INFO: stderr: ""
    Mar  1 12:17:01.855: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 03/01/23 12:17:01.855
    Mar  1 12:17:01.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-5600 delete pods e2e-test-httpd-pod'
    Mar  1 12:17:04.104: INFO: stderr: ""
    Mar  1 12:17:04.104: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar  1 12:17:04.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-5600" for this suite. 03/01/23 12:17:04.11
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:17:04.119
Mar  1 12:17:04.120: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename cronjob 03/01/23 12:17:04.12
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:17:04.138
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:17:04.141
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 03/01/23 12:17:04.144
STEP: creating 03/01/23 12:17:04.144
STEP: getting 03/01/23 12:17:04.151
STEP: listing 03/01/23 12:17:04.156
STEP: watching 03/01/23 12:17:04.16
Mar  1 12:17:04.160: INFO: starting watch
STEP: cluster-wide listing 03/01/23 12:17:04.162
STEP: cluster-wide watching 03/01/23 12:17:04.167
Mar  1 12:17:04.167: INFO: starting watch
STEP: patching 03/01/23 12:17:04.169
STEP: updating 03/01/23 12:17:04.178
Mar  1 12:17:04.189: INFO: waiting for watch events with expected annotations
Mar  1 12:17:04.189: INFO: saw patched and updated annotations
STEP: patching /status 03/01/23 12:17:04.189
STEP: updating /status 03/01/23 12:17:04.198
STEP: get /status 03/01/23 12:17:04.208
STEP: deleting 03/01/23 12:17:04.214
STEP: deleting a collection 03/01/23 12:17:04.233
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Mar  1 12:17:04.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-113" for this suite. 03/01/23 12:17:04.259
{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","completed":125,"skipped":2372,"failed":0}
------------------------------
â€¢ [0.149 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:17:04.119
    Mar  1 12:17:04.120: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename cronjob 03/01/23 12:17:04.12
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:17:04.138
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:17:04.141
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 03/01/23 12:17:04.144
    STEP: creating 03/01/23 12:17:04.144
    STEP: getting 03/01/23 12:17:04.151
    STEP: listing 03/01/23 12:17:04.156
    STEP: watching 03/01/23 12:17:04.16
    Mar  1 12:17:04.160: INFO: starting watch
    STEP: cluster-wide listing 03/01/23 12:17:04.162
    STEP: cluster-wide watching 03/01/23 12:17:04.167
    Mar  1 12:17:04.167: INFO: starting watch
    STEP: patching 03/01/23 12:17:04.169
    STEP: updating 03/01/23 12:17:04.178
    Mar  1 12:17:04.189: INFO: waiting for watch events with expected annotations
    Mar  1 12:17:04.189: INFO: saw patched and updated annotations
    STEP: patching /status 03/01/23 12:17:04.189
    STEP: updating /status 03/01/23 12:17:04.198
    STEP: get /status 03/01/23 12:17:04.208
    STEP: deleting 03/01/23 12:17:04.214
    STEP: deleting a collection 03/01/23 12:17:04.233
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Mar  1 12:17:04.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-113" for this suite. 03/01/23 12:17:04.259
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:17:04.27
Mar  1 12:17:04.271: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename replication-controller 03/01/23 12:17:04.271
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:17:04.29
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:17:04.294
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
STEP: Given a ReplicationController is created 03/01/23 12:17:04.298
STEP: When the matched label of one of its pods change 03/01/23 12:17:04.305
Mar  1 12:17:04.310: INFO: Pod name pod-release: Found 0 pods out of 1
Mar  1 12:17:09.316: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 03/01/23 12:17:09.329
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Mar  1 12:17:10.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1388" for this suite. 03/01/23 12:17:10.352
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","completed":126,"skipped":2391,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.095 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:17:04.27
    Mar  1 12:17:04.271: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename replication-controller 03/01/23 12:17:04.271
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:17:04.29
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:17:04.294
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:100
    STEP: Given a ReplicationController is created 03/01/23 12:17:04.298
    STEP: When the matched label of one of its pods change 03/01/23 12:17:04.305
    Mar  1 12:17:04.310: INFO: Pod name pod-release: Found 0 pods out of 1
    Mar  1 12:17:09.316: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 03/01/23 12:17:09.329
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Mar  1 12:17:10.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-1388" for this suite. 03/01/23 12:17:10.352
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:17:10.372
Mar  1 12:17:10.372: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename svcaccounts 03/01/23 12:17:10.373
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:17:10.392
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:17:10.396
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
STEP: Creating a pod to test service account token:  03/01/23 12:17:10.4
Mar  1 12:17:10.414: INFO: Waiting up to 5m0s for pod "test-pod-739fb41b-a674-4cba-a128-c42281b5e0b1" in namespace "svcaccounts-7985" to be "Succeeded or Failed"
Mar  1 12:17:10.421: INFO: Pod "test-pod-739fb41b-a674-4cba-a128-c42281b5e0b1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.725083ms
Mar  1 12:17:12.429: INFO: Pod "test-pod-739fb41b-a674-4cba-a128-c42281b5e0b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014477698s
Mar  1 12:17:14.427: INFO: Pod "test-pod-739fb41b-a674-4cba-a128-c42281b5e0b1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012839134s
STEP: Saw pod success 03/01/23 12:17:14.428
Mar  1 12:17:14.428: INFO: Pod "test-pod-739fb41b-a674-4cba-a128-c42281b5e0b1" satisfied condition "Succeeded or Failed"
Mar  1 12:17:14.433: INFO: Trying to get logs from node lab1-k8s-node-1 pod test-pod-739fb41b-a674-4cba-a128-c42281b5e0b1 container agnhost-container: <nil>
STEP: delete the pod 03/01/23 12:17:14.449
Mar  1 12:17:14.469: INFO: Waiting for pod test-pod-739fb41b-a674-4cba-a128-c42281b5e0b1 to disappear
Mar  1 12:17:14.474: INFO: Pod test-pod-739fb41b-a674-4cba-a128-c42281b5e0b1 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Mar  1 12:17:14.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7985" for this suite. 03/01/23 12:17:14.483
{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","completed":127,"skipped":2491,"failed":0}
------------------------------
â€¢ [4.120 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:17:10.372
    Mar  1 12:17:10.372: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename svcaccounts 03/01/23 12:17:10.373
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:17:10.392
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:17:10.396
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:272
    STEP: Creating a pod to test service account token:  03/01/23 12:17:10.4
    Mar  1 12:17:10.414: INFO: Waiting up to 5m0s for pod "test-pod-739fb41b-a674-4cba-a128-c42281b5e0b1" in namespace "svcaccounts-7985" to be "Succeeded or Failed"
    Mar  1 12:17:10.421: INFO: Pod "test-pod-739fb41b-a674-4cba-a128-c42281b5e0b1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.725083ms
    Mar  1 12:17:12.429: INFO: Pod "test-pod-739fb41b-a674-4cba-a128-c42281b5e0b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014477698s
    Mar  1 12:17:14.427: INFO: Pod "test-pod-739fb41b-a674-4cba-a128-c42281b5e0b1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012839134s
    STEP: Saw pod success 03/01/23 12:17:14.428
    Mar  1 12:17:14.428: INFO: Pod "test-pod-739fb41b-a674-4cba-a128-c42281b5e0b1" satisfied condition "Succeeded or Failed"
    Mar  1 12:17:14.433: INFO: Trying to get logs from node lab1-k8s-node-1 pod test-pod-739fb41b-a674-4cba-a128-c42281b5e0b1 container agnhost-container: <nil>
    STEP: delete the pod 03/01/23 12:17:14.449
    Mar  1 12:17:14.469: INFO: Waiting for pod test-pod-739fb41b-a674-4cba-a128-c42281b5e0b1 to disappear
    Mar  1 12:17:14.474: INFO: Pod test-pod-739fb41b-a674-4cba-a128-c42281b5e0b1 no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Mar  1 12:17:14.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-7985" for this suite. 03/01/23 12:17:14.483
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:17:14.492
Mar  1 12:17:14.493: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename statefulset 03/01/23 12:17:14.493
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:17:14.511
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:17:14.515
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-607 03/01/23 12:17:14.518
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
STEP: Creating statefulset ss in namespace statefulset-607 03/01/23 12:17:14.531
Mar  1 12:17:14.551: INFO: Found 0 stateful pods, waiting for 1
Mar  1 12:17:24.558: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 03/01/23 12:17:24.567
STEP: Getting /status 03/01/23 12:17:24.58
Mar  1 12:17:24.586: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 03/01/23 12:17:24.586
Mar  1 12:17:24.598: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 03/01/23 12:17:24.598
Mar  1 12:17:24.601: INFO: Observed &StatefulSet event: ADDED
Mar  1 12:17:24.601: INFO: Found Statefulset ss in namespace statefulset-607 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Mar  1 12:17:24.601: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 03/01/23 12:17:24.601
Mar  1 12:17:24.601: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Mar  1 12:17:24.612: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 03/01/23 12:17:24.612
Mar  1 12:17:24.614: INFO: Observed &StatefulSet event: ADDED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Mar  1 12:17:24.615: INFO: Deleting all statefulset in ns statefulset-607
Mar  1 12:17:24.620: INFO: Scaling statefulset ss to 0
Mar  1 12:17:34.645: INFO: Waiting for statefulset status.replicas updated to 0
Mar  1 12:17:34.650: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Mar  1 12:17:34.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-607" for this suite. 03/01/23 12:17:34.68
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","completed":128,"skipped":2493,"failed":0}
------------------------------
â€¢ [SLOW TEST] [20.203 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:975

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:17:14.492
    Mar  1 12:17:14.493: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename statefulset 03/01/23 12:17:14.493
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:17:14.511
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:17:14.515
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-607 03/01/23 12:17:14.518
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:975
    STEP: Creating statefulset ss in namespace statefulset-607 03/01/23 12:17:14.531
    Mar  1 12:17:14.551: INFO: Found 0 stateful pods, waiting for 1
    Mar  1 12:17:24.558: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 03/01/23 12:17:24.567
    STEP: Getting /status 03/01/23 12:17:24.58
    Mar  1 12:17:24.586: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 03/01/23 12:17:24.586
    Mar  1 12:17:24.598: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 03/01/23 12:17:24.598
    Mar  1 12:17:24.601: INFO: Observed &StatefulSet event: ADDED
    Mar  1 12:17:24.601: INFO: Found Statefulset ss in namespace statefulset-607 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Mar  1 12:17:24.601: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 03/01/23 12:17:24.601
    Mar  1 12:17:24.601: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Mar  1 12:17:24.612: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 03/01/23 12:17:24.612
    Mar  1 12:17:24.614: INFO: Observed &StatefulSet event: ADDED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Mar  1 12:17:24.615: INFO: Deleting all statefulset in ns statefulset-607
    Mar  1 12:17:24.620: INFO: Scaling statefulset ss to 0
    Mar  1 12:17:34.645: INFO: Waiting for statefulset status.replicas updated to 0
    Mar  1 12:17:34.650: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Mar  1 12:17:34.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-607" for this suite. 03/01/23 12:17:34.68
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:17:34.698
Mar  1 12:17:34.698: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename taint-multiple-pods 03/01/23 12:17:34.699
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:17:34.717
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:17:34.721
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:348
Mar  1 12:17:34.725: INFO: Waiting up to 1m0s for all nodes to be ready
Mar  1 12:18:34.765: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
Mar  1 12:18:34.771: INFO: Starting informer...
STEP: Starting pods... 03/01/23 12:18:34.771
Mar  1 12:18:34.991: INFO: Pod1 is running on lab1-k8s-node-3. Tainting Node
Mar  1 12:18:35.223: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-2388" to be "running"
Mar  1 12:18:35.230: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.395044ms
Mar  1 12:18:37.236: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.012983764s
Mar  1 12:18:37.237: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Mar  1 12:18:37.237: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-2388" to be "running"
Mar  1 12:18:37.241: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 4.735759ms
Mar  1 12:18:37.241: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Mar  1 12:18:37.241: INFO: Pod2 is running on lab1-k8s-node-3. Tainting Node
STEP: Trying to apply a taint on the Node 03/01/23 12:18:37.241
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/01/23 12:18:37.257
STEP: Waiting for Pod1 and Pod2 to be deleted 03/01/23 12:18:37.274
Mar  1 12:18:42.923: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Mar  1 12:19:02.959: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/01/23 12:19:02.983
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:187
Mar  1 12:19:02.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-2388" for this suite. 03/01/23 12:19:02.997
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","completed":129,"skipped":2499,"failed":0}
------------------------------
â€¢ [SLOW TEST] [88.326 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:17:34.698
    Mar  1 12:17:34.698: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename taint-multiple-pods 03/01/23 12:17:34.699
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:17:34.717
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:17:34.721
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:348
    Mar  1 12:17:34.725: INFO: Waiting up to 1m0s for all nodes to be ready
    Mar  1 12:18:34.765: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:420
    Mar  1 12:18:34.771: INFO: Starting informer...
    STEP: Starting pods... 03/01/23 12:18:34.771
    Mar  1 12:18:34.991: INFO: Pod1 is running on lab1-k8s-node-3. Tainting Node
    Mar  1 12:18:35.223: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-2388" to be "running"
    Mar  1 12:18:35.230: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.395044ms
    Mar  1 12:18:37.236: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.012983764s
    Mar  1 12:18:37.237: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Mar  1 12:18:37.237: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-2388" to be "running"
    Mar  1 12:18:37.241: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 4.735759ms
    Mar  1 12:18:37.241: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Mar  1 12:18:37.241: INFO: Pod2 is running on lab1-k8s-node-3. Tainting Node
    STEP: Trying to apply a taint on the Node 03/01/23 12:18:37.241
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/01/23 12:18:37.257
    STEP: Waiting for Pod1 and Pod2 to be deleted 03/01/23 12:18:37.274
    Mar  1 12:18:42.923: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Mar  1 12:19:02.959: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/01/23 12:19:02.983
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:187
    Mar  1 12:19:02.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "taint-multiple-pods-2388" for this suite. 03/01/23 12:19:02.997
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:19:03.026
Mar  1 12:19:03.026: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename projected 03/01/23 12:19:03.027
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:19:03.054
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:19:03.065
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
STEP: Creating projection with secret that has name projected-secret-test-f90fca7b-d461-4b83-9fcf-d540fcdf6d3d 03/01/23 12:19:03.069
STEP: Creating a pod to test consume secrets 03/01/23 12:19:03.074
Mar  1 12:19:03.084: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a37498f0-932f-48ff-84b1-2753c6d41a7b" in namespace "projected-6583" to be "Succeeded or Failed"
Mar  1 12:19:03.103: INFO: Pod "pod-projected-secrets-a37498f0-932f-48ff-84b1-2753c6d41a7b": Phase="Pending", Reason="", readiness=false. Elapsed: 18.596177ms
Mar  1 12:19:05.112: INFO: Pod "pod-projected-secrets-a37498f0-932f-48ff-84b1-2753c6d41a7b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027637494s
Mar  1 12:19:07.111: INFO: Pod "pod-projected-secrets-a37498f0-932f-48ff-84b1-2753c6d41a7b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026859854s
STEP: Saw pod success 03/01/23 12:19:07.111
Mar  1 12:19:07.112: INFO: Pod "pod-projected-secrets-a37498f0-932f-48ff-84b1-2753c6d41a7b" satisfied condition "Succeeded or Failed"
Mar  1 12:19:07.117: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-projected-secrets-a37498f0-932f-48ff-84b1-2753c6d41a7b container projected-secret-volume-test: <nil>
STEP: delete the pod 03/01/23 12:19:07.138
Mar  1 12:19:07.157: INFO: Waiting for pod pod-projected-secrets-a37498f0-932f-48ff-84b1-2753c6d41a7b to disappear
Mar  1 12:19:07.162: INFO: Pod pod-projected-secrets-a37498f0-932f-48ff-84b1-2753c6d41a7b no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Mar  1 12:19:07.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6583" for this suite. 03/01/23 12:19:07.175
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":130,"skipped":2504,"failed":0}
------------------------------
â€¢ [4.161 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:19:03.026
    Mar  1 12:19:03.026: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename projected 03/01/23 12:19:03.027
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:19:03.054
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:19:03.065
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:55
    STEP: Creating projection with secret that has name projected-secret-test-f90fca7b-d461-4b83-9fcf-d540fcdf6d3d 03/01/23 12:19:03.069
    STEP: Creating a pod to test consume secrets 03/01/23 12:19:03.074
    Mar  1 12:19:03.084: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a37498f0-932f-48ff-84b1-2753c6d41a7b" in namespace "projected-6583" to be "Succeeded or Failed"
    Mar  1 12:19:03.103: INFO: Pod "pod-projected-secrets-a37498f0-932f-48ff-84b1-2753c6d41a7b": Phase="Pending", Reason="", readiness=false. Elapsed: 18.596177ms
    Mar  1 12:19:05.112: INFO: Pod "pod-projected-secrets-a37498f0-932f-48ff-84b1-2753c6d41a7b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027637494s
    Mar  1 12:19:07.111: INFO: Pod "pod-projected-secrets-a37498f0-932f-48ff-84b1-2753c6d41a7b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026859854s
    STEP: Saw pod success 03/01/23 12:19:07.111
    Mar  1 12:19:07.112: INFO: Pod "pod-projected-secrets-a37498f0-932f-48ff-84b1-2753c6d41a7b" satisfied condition "Succeeded or Failed"
    Mar  1 12:19:07.117: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-projected-secrets-a37498f0-932f-48ff-84b1-2753c6d41a7b container projected-secret-volume-test: <nil>
    STEP: delete the pod 03/01/23 12:19:07.138
    Mar  1 12:19:07.157: INFO: Waiting for pod pod-projected-secrets-a37498f0-932f-48ff-84b1-2753c6d41a7b to disappear
    Mar  1 12:19:07.162: INFO: Pod pod-projected-secrets-a37498f0-932f-48ff-84b1-2753c6d41a7b no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Mar  1 12:19:07.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6583" for this suite. 03/01/23 12:19:07.175
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:19:07.189
Mar  1 12:19:07.189: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename projected 03/01/23 12:19:07.189
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:19:07.207
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:19:07.21
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
STEP: Creating a pod to test downward API volume plugin 03/01/23 12:19:07.214
Mar  1 12:19:07.224: INFO: Waiting up to 5m0s for pod "downwardapi-volume-56fdfcdd-53ac-4b0c-a6ac-d351ac4e17cd" in namespace "projected-1929" to be "Succeeded or Failed"
Mar  1 12:19:07.229: INFO: Pod "downwardapi-volume-56fdfcdd-53ac-4b0c-a6ac-d351ac4e17cd": Phase="Pending", Reason="", readiness=false. Elapsed: 5.071783ms
Mar  1 12:19:09.236: INFO: Pod "downwardapi-volume-56fdfcdd-53ac-4b0c-a6ac-d351ac4e17cd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012188638s
Mar  1 12:19:11.236: INFO: Pod "downwardapi-volume-56fdfcdd-53ac-4b0c-a6ac-d351ac4e17cd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012485013s
STEP: Saw pod success 03/01/23 12:19:11.237
Mar  1 12:19:11.237: INFO: Pod "downwardapi-volume-56fdfcdd-53ac-4b0c-a6ac-d351ac4e17cd" satisfied condition "Succeeded or Failed"
Mar  1 12:19:11.241: INFO: Trying to get logs from node lab1-k8s-node-3 pod downwardapi-volume-56fdfcdd-53ac-4b0c-a6ac-d351ac4e17cd container client-container: <nil>
STEP: delete the pod 03/01/23 12:19:11.249
Mar  1 12:19:11.268: INFO: Waiting for pod downwardapi-volume-56fdfcdd-53ac-4b0c-a6ac-d351ac4e17cd to disappear
Mar  1 12:19:11.271: INFO: Pod downwardapi-volume-56fdfcdd-53ac-4b0c-a6ac-d351ac4e17cd no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Mar  1 12:19:11.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1929" for this suite. 03/01/23 12:19:11.279
{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":131,"skipped":2540,"failed":0}
------------------------------
â€¢ [4.099 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:19:07.189
    Mar  1 12:19:07.189: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename projected 03/01/23 12:19:07.189
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:19:07.207
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:19:07.21
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:83
    STEP: Creating a pod to test downward API volume plugin 03/01/23 12:19:07.214
    Mar  1 12:19:07.224: INFO: Waiting up to 5m0s for pod "downwardapi-volume-56fdfcdd-53ac-4b0c-a6ac-d351ac4e17cd" in namespace "projected-1929" to be "Succeeded or Failed"
    Mar  1 12:19:07.229: INFO: Pod "downwardapi-volume-56fdfcdd-53ac-4b0c-a6ac-d351ac4e17cd": Phase="Pending", Reason="", readiness=false. Elapsed: 5.071783ms
    Mar  1 12:19:09.236: INFO: Pod "downwardapi-volume-56fdfcdd-53ac-4b0c-a6ac-d351ac4e17cd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012188638s
    Mar  1 12:19:11.236: INFO: Pod "downwardapi-volume-56fdfcdd-53ac-4b0c-a6ac-d351ac4e17cd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012485013s
    STEP: Saw pod success 03/01/23 12:19:11.237
    Mar  1 12:19:11.237: INFO: Pod "downwardapi-volume-56fdfcdd-53ac-4b0c-a6ac-d351ac4e17cd" satisfied condition "Succeeded or Failed"
    Mar  1 12:19:11.241: INFO: Trying to get logs from node lab1-k8s-node-3 pod downwardapi-volume-56fdfcdd-53ac-4b0c-a6ac-d351ac4e17cd container client-container: <nil>
    STEP: delete the pod 03/01/23 12:19:11.249
    Mar  1 12:19:11.268: INFO: Waiting for pod downwardapi-volume-56fdfcdd-53ac-4b0c-a6ac-d351ac4e17cd to disappear
    Mar  1 12:19:11.271: INFO: Pod downwardapi-volume-56fdfcdd-53ac-4b0c-a6ac-d351ac4e17cd no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Mar  1 12:19:11.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1929" for this suite. 03/01/23 12:19:11.279
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:19:11.288
Mar  1 12:19:11.288: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename downward-api 03/01/23 12:19:11.289
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:19:11.308
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:19:11.311
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
STEP: Creating a pod to test downward API volume plugin 03/01/23 12:19:11.315
Mar  1 12:19:11.327: INFO: Waiting up to 5m0s for pod "downwardapi-volume-138a8ce7-48bf-4951-8591-d9b3ebe4eeed" in namespace "downward-api-431" to be "Succeeded or Failed"
Mar  1 12:19:11.332: INFO: Pod "downwardapi-volume-138a8ce7-48bf-4951-8591-d9b3ebe4eeed": Phase="Pending", Reason="", readiness=false. Elapsed: 5.112899ms
Mar  1 12:19:13.339: INFO: Pod "downwardapi-volume-138a8ce7-48bf-4951-8591-d9b3ebe4eeed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011832852s
Mar  1 12:19:15.340: INFO: Pod "downwardapi-volume-138a8ce7-48bf-4951-8591-d9b3ebe4eeed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012868389s
STEP: Saw pod success 03/01/23 12:19:15.34
Mar  1 12:19:15.340: INFO: Pod "downwardapi-volume-138a8ce7-48bf-4951-8591-d9b3ebe4eeed" satisfied condition "Succeeded or Failed"
Mar  1 12:19:15.344: INFO: Trying to get logs from node lab1-k8s-node-3 pod downwardapi-volume-138a8ce7-48bf-4951-8591-d9b3ebe4eeed container client-container: <nil>
STEP: delete the pod 03/01/23 12:19:15.354
Mar  1 12:19:15.373: INFO: Waiting for pod downwardapi-volume-138a8ce7-48bf-4951-8591-d9b3ebe4eeed to disappear
Mar  1 12:19:15.377: INFO: Pod downwardapi-volume-138a8ce7-48bf-4951-8591-d9b3ebe4eeed no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Mar  1 12:19:15.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-431" for this suite. 03/01/23 12:19:15.386
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","completed":132,"skipped":2541,"failed":0}
------------------------------
â€¢ [4.107 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:19:11.288
    Mar  1 12:19:11.288: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename downward-api 03/01/23 12:19:11.289
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:19:11.308
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:19:11.311
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:206
    STEP: Creating a pod to test downward API volume plugin 03/01/23 12:19:11.315
    Mar  1 12:19:11.327: INFO: Waiting up to 5m0s for pod "downwardapi-volume-138a8ce7-48bf-4951-8591-d9b3ebe4eeed" in namespace "downward-api-431" to be "Succeeded or Failed"
    Mar  1 12:19:11.332: INFO: Pod "downwardapi-volume-138a8ce7-48bf-4951-8591-d9b3ebe4eeed": Phase="Pending", Reason="", readiness=false. Elapsed: 5.112899ms
    Mar  1 12:19:13.339: INFO: Pod "downwardapi-volume-138a8ce7-48bf-4951-8591-d9b3ebe4eeed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011832852s
    Mar  1 12:19:15.340: INFO: Pod "downwardapi-volume-138a8ce7-48bf-4951-8591-d9b3ebe4eeed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012868389s
    STEP: Saw pod success 03/01/23 12:19:15.34
    Mar  1 12:19:15.340: INFO: Pod "downwardapi-volume-138a8ce7-48bf-4951-8591-d9b3ebe4eeed" satisfied condition "Succeeded or Failed"
    Mar  1 12:19:15.344: INFO: Trying to get logs from node lab1-k8s-node-3 pod downwardapi-volume-138a8ce7-48bf-4951-8591-d9b3ebe4eeed container client-container: <nil>
    STEP: delete the pod 03/01/23 12:19:15.354
    Mar  1 12:19:15.373: INFO: Waiting for pod downwardapi-volume-138a8ce7-48bf-4951-8591-d9b3ebe4eeed to disappear
    Mar  1 12:19:15.377: INFO: Pod downwardapi-volume-138a8ce7-48bf-4951-8591-d9b3ebe4eeed no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Mar  1 12:19:15.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-431" for this suite. 03/01/23 12:19:15.386
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:19:15.395
Mar  1 12:19:15.395: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename services 03/01/23 12:19:15.396
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:19:15.417
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:19:15.421
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641
STEP: creating a collection of services 03/01/23 12:19:15.424
Mar  1 12:19:15.424: INFO: Creating e2e-svc-a-gmz27
Mar  1 12:19:15.438: INFO: Creating e2e-svc-b-78xld
Mar  1 12:19:15.452: INFO: Creating e2e-svc-c-lq2c2
STEP: deleting service collection 03/01/23 12:19:15.473
Mar  1 12:19:15.525: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar  1 12:19:15.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9998" for this suite. 03/01/23 12:19:15.533
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should delete a collection of services [Conformance]","completed":133,"skipped":2544,"failed":0}
------------------------------
â€¢ [0.147 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:19:15.395
    Mar  1 12:19:15.395: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename services 03/01/23 12:19:15.396
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:19:15.417
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:19:15.421
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3641
    STEP: creating a collection of services 03/01/23 12:19:15.424
    Mar  1 12:19:15.424: INFO: Creating e2e-svc-a-gmz27
    Mar  1 12:19:15.438: INFO: Creating e2e-svc-b-78xld
    Mar  1 12:19:15.452: INFO: Creating e2e-svc-c-lq2c2
    STEP: deleting service collection 03/01/23 12:19:15.473
    Mar  1 12:19:15.525: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar  1 12:19:15.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-9998" for this suite. 03/01/23 12:19:15.533
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:19:15.547
Mar  1 12:19:15.547: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename containers 03/01/23 12:19:15.548
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:19:15.568
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:19:15.573
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
STEP: Creating a pod to test override command 03/01/23 12:19:15.576
Mar  1 12:19:15.588: INFO: Waiting up to 5m0s for pod "client-containers-9c343b9a-3461-4345-8ea5-f8ef7df59c7d" in namespace "containers-2530" to be "Succeeded or Failed"
Mar  1 12:19:15.596: INFO: Pod "client-containers-9c343b9a-3461-4345-8ea5-f8ef7df59c7d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.668073ms
Mar  1 12:19:17.604: INFO: Pod "client-containers-9c343b9a-3461-4345-8ea5-f8ef7df59c7d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01581858s
Mar  1 12:19:19.602: INFO: Pod "client-containers-9c343b9a-3461-4345-8ea5-f8ef7df59c7d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014151459s
STEP: Saw pod success 03/01/23 12:19:19.602
Mar  1 12:19:19.603: INFO: Pod "client-containers-9c343b9a-3461-4345-8ea5-f8ef7df59c7d" satisfied condition "Succeeded or Failed"
Mar  1 12:19:19.608: INFO: Trying to get logs from node lab1-k8s-node-3 pod client-containers-9c343b9a-3461-4345-8ea5-f8ef7df59c7d container agnhost-container: <nil>
STEP: delete the pod 03/01/23 12:19:19.615
Mar  1 12:19:19.634: INFO: Waiting for pod client-containers-9c343b9a-3461-4345-8ea5-f8ef7df59c7d to disappear
Mar  1 12:19:19.639: INFO: Pod client-containers-9c343b9a-3461-4345-8ea5-f8ef7df59c7d no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Mar  1 12:19:19.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2530" for this suite. 03/01/23 12:19:19.648
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]","completed":134,"skipped":2566,"failed":0}
------------------------------
â€¢ [4.114 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:19:15.547
    Mar  1 12:19:15.547: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename containers 03/01/23 12:19:15.548
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:19:15.568
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:19:15.573
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:72
    STEP: Creating a pod to test override command 03/01/23 12:19:15.576
    Mar  1 12:19:15.588: INFO: Waiting up to 5m0s for pod "client-containers-9c343b9a-3461-4345-8ea5-f8ef7df59c7d" in namespace "containers-2530" to be "Succeeded or Failed"
    Mar  1 12:19:15.596: INFO: Pod "client-containers-9c343b9a-3461-4345-8ea5-f8ef7df59c7d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.668073ms
    Mar  1 12:19:17.604: INFO: Pod "client-containers-9c343b9a-3461-4345-8ea5-f8ef7df59c7d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01581858s
    Mar  1 12:19:19.602: INFO: Pod "client-containers-9c343b9a-3461-4345-8ea5-f8ef7df59c7d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014151459s
    STEP: Saw pod success 03/01/23 12:19:19.602
    Mar  1 12:19:19.603: INFO: Pod "client-containers-9c343b9a-3461-4345-8ea5-f8ef7df59c7d" satisfied condition "Succeeded or Failed"
    Mar  1 12:19:19.608: INFO: Trying to get logs from node lab1-k8s-node-3 pod client-containers-9c343b9a-3461-4345-8ea5-f8ef7df59c7d container agnhost-container: <nil>
    STEP: delete the pod 03/01/23 12:19:19.615
    Mar  1 12:19:19.634: INFO: Waiting for pod client-containers-9c343b9a-3461-4345-8ea5-f8ef7df59c7d to disappear
    Mar  1 12:19:19.639: INFO: Pod client-containers-9c343b9a-3461-4345-8ea5-f8ef7df59c7d no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Mar  1 12:19:19.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-2530" for this suite. 03/01/23 12:19:19.648
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:19:19.666
Mar  1 12:19:19.666: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename services 03/01/23 12:19:19.667
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:19:19.685
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:19:19.688
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
STEP: creating service endpoint-test2 in namespace services-677 03/01/23 12:19:19.691
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-677 to expose endpoints map[] 03/01/23 12:19:19.704
Mar  1 12:19:19.708: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Mar  1 12:19:20.734: INFO: successfully validated that service endpoint-test2 in namespace services-677 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-677 03/01/23 12:19:20.734
Mar  1 12:19:20.743: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-677" to be "running and ready"
Mar  1 12:19:20.751: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 7.828578ms
Mar  1 12:19:20.751: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Mar  1 12:19:22.758: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.015395297s
Mar  1 12:19:22.758: INFO: The phase of Pod pod1 is Running (Ready = true)
Mar  1 12:19:22.758: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-677 to expose endpoints map[pod1:[80]] 03/01/23 12:19:22.764
Mar  1 12:19:22.778: INFO: successfully validated that service endpoint-test2 in namespace services-677 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 03/01/23 12:19:22.778
Mar  1 12:19:22.778: INFO: Creating new exec pod
Mar  1 12:19:22.787: INFO: Waiting up to 5m0s for pod "execpodh9mmd" in namespace "services-677" to be "running"
Mar  1 12:19:22.793: INFO: Pod "execpodh9mmd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.296731ms
Mar  1 12:19:24.800: INFO: Pod "execpodh9mmd": Phase="Running", Reason="", readiness=true. Elapsed: 2.013035245s
Mar  1 12:19:24.800: INFO: Pod "execpodh9mmd" satisfied condition "running"
Mar  1 12:19:25.801: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-677 exec execpodh9mmd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Mar  1 12:19:25.969: INFO: stderr: "+ echo+  hostNamenc\n -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Mar  1 12:19:25.969: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  1 12:19:25.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-677 exec execpodh9mmd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.5.244 80'
Mar  1 12:19:26.104: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.5.244 80\nConnection to 10.233.5.244 80 port [tcp/http] succeeded!\n"
Mar  1 12:19:26.104: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-677 03/01/23 12:19:26.104
Mar  1 12:19:26.111: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-677" to be "running and ready"
Mar  1 12:19:26.120: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.829658ms
Mar  1 12:19:26.120: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Mar  1 12:19:28.126: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.014724067s
Mar  1 12:19:28.126: INFO: The phase of Pod pod2 is Running (Ready = true)
Mar  1 12:19:28.126: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-677 to expose endpoints map[pod1:[80] pod2:[80]] 03/01/23 12:19:28.131
Mar  1 12:19:28.151: INFO: successfully validated that service endpoint-test2 in namespace services-677 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 03/01/23 12:19:28.151
Mar  1 12:19:29.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-677 exec execpodh9mmd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Mar  1 12:19:29.290: INFO: stderr: "+ nc -v -t -w 2 endpoint-test2 80\n+ echo hostName\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Mar  1 12:19:29.290: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  1 12:19:29.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-677 exec execpodh9mmd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.5.244 80'
Mar  1 12:19:29.428: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.5.244 80\nConnection to 10.233.5.244 80 port [tcp/http] succeeded!\n"
Mar  1 12:19:29.428: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-677 03/01/23 12:19:29.428
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-677 to expose endpoints map[pod2:[80]] 03/01/23 12:19:29.455
Mar  1 12:19:29.476: INFO: successfully validated that service endpoint-test2 in namespace services-677 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 03/01/23 12:19:29.476
Mar  1 12:19:30.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-677 exec execpodh9mmd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Mar  1 12:19:30.612: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Mar  1 12:19:30.612: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  1 12:19:30.613: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-677 exec execpodh9mmd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.5.244 80'
Mar  1 12:19:30.737: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.5.244 80\nConnection to 10.233.5.244 80 port [tcp/http] succeeded!\n"
Mar  1 12:19:30.737: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-677 03/01/23 12:19:30.737
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-677 to expose endpoints map[] 03/01/23 12:19:30.772
Mar  1 12:19:30.803: INFO: successfully validated that service endpoint-test2 in namespace services-677 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar  1 12:19:30.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-677" for this suite. 03/01/23 12:19:30.846
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","completed":135,"skipped":2636,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.188 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:19:19.666
    Mar  1 12:19:19.666: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename services 03/01/23 12:19:19.667
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:19:19.685
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:19:19.688
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:791
    STEP: creating service endpoint-test2 in namespace services-677 03/01/23 12:19:19.691
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-677 to expose endpoints map[] 03/01/23 12:19:19.704
    Mar  1 12:19:19.708: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
    Mar  1 12:19:20.734: INFO: successfully validated that service endpoint-test2 in namespace services-677 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-677 03/01/23 12:19:20.734
    Mar  1 12:19:20.743: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-677" to be "running and ready"
    Mar  1 12:19:20.751: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 7.828578ms
    Mar  1 12:19:20.751: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Mar  1 12:19:22.758: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.015395297s
    Mar  1 12:19:22.758: INFO: The phase of Pod pod1 is Running (Ready = true)
    Mar  1 12:19:22.758: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-677 to expose endpoints map[pod1:[80]] 03/01/23 12:19:22.764
    Mar  1 12:19:22.778: INFO: successfully validated that service endpoint-test2 in namespace services-677 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 03/01/23 12:19:22.778
    Mar  1 12:19:22.778: INFO: Creating new exec pod
    Mar  1 12:19:22.787: INFO: Waiting up to 5m0s for pod "execpodh9mmd" in namespace "services-677" to be "running"
    Mar  1 12:19:22.793: INFO: Pod "execpodh9mmd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.296731ms
    Mar  1 12:19:24.800: INFO: Pod "execpodh9mmd": Phase="Running", Reason="", readiness=true. Elapsed: 2.013035245s
    Mar  1 12:19:24.800: INFO: Pod "execpodh9mmd" satisfied condition "running"
    Mar  1 12:19:25.801: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-677 exec execpodh9mmd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Mar  1 12:19:25.969: INFO: stderr: "+ echo+  hostNamenc\n -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Mar  1 12:19:25.969: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  1 12:19:25.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-677 exec execpodh9mmd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.5.244 80'
    Mar  1 12:19:26.104: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.5.244 80\nConnection to 10.233.5.244 80 port [tcp/http] succeeded!\n"
    Mar  1 12:19:26.104: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Creating pod pod2 in namespace services-677 03/01/23 12:19:26.104
    Mar  1 12:19:26.111: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-677" to be "running and ready"
    Mar  1 12:19:26.120: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.829658ms
    Mar  1 12:19:26.120: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Mar  1 12:19:28.126: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.014724067s
    Mar  1 12:19:28.126: INFO: The phase of Pod pod2 is Running (Ready = true)
    Mar  1 12:19:28.126: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-677 to expose endpoints map[pod1:[80] pod2:[80]] 03/01/23 12:19:28.131
    Mar  1 12:19:28.151: INFO: successfully validated that service endpoint-test2 in namespace services-677 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 03/01/23 12:19:28.151
    Mar  1 12:19:29.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-677 exec execpodh9mmd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Mar  1 12:19:29.290: INFO: stderr: "+ nc -v -t -w 2 endpoint-test2 80\n+ echo hostName\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Mar  1 12:19:29.290: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  1 12:19:29.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-677 exec execpodh9mmd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.5.244 80'
    Mar  1 12:19:29.428: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.5.244 80\nConnection to 10.233.5.244 80 port [tcp/http] succeeded!\n"
    Mar  1 12:19:29.428: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-677 03/01/23 12:19:29.428
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-677 to expose endpoints map[pod2:[80]] 03/01/23 12:19:29.455
    Mar  1 12:19:29.476: INFO: successfully validated that service endpoint-test2 in namespace services-677 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 03/01/23 12:19:29.476
    Mar  1 12:19:30.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-677 exec execpodh9mmd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Mar  1 12:19:30.612: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Mar  1 12:19:30.612: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  1 12:19:30.613: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-677 exec execpodh9mmd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.5.244 80'
    Mar  1 12:19:30.737: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.5.244 80\nConnection to 10.233.5.244 80 port [tcp/http] succeeded!\n"
    Mar  1 12:19:30.737: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod2 in namespace services-677 03/01/23 12:19:30.737
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-677 to expose endpoints map[] 03/01/23 12:19:30.772
    Mar  1 12:19:30.803: INFO: successfully validated that service endpoint-test2 in namespace services-677 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar  1 12:19:30.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-677" for this suite. 03/01/23 12:19:30.846
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:19:30.855
Mar  1 12:19:30.855: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename secrets 03/01/23 12:19:30.856
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:19:30.876
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:19:30.879
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
STEP: Creating secret with name secret-test-615d2797-c406-40cd-b17d-b7bfbaf626fe 03/01/23 12:19:30.883
STEP: Creating a pod to test consume secrets 03/01/23 12:19:30.89
Mar  1 12:19:30.899: INFO: Waiting up to 5m0s for pod "pod-secrets-eb9550fb-e882-4599-81e8-bdb0467c6f21" in namespace "secrets-195" to be "Succeeded or Failed"
Mar  1 12:19:30.905: INFO: Pod "pod-secrets-eb9550fb-e882-4599-81e8-bdb0467c6f21": Phase="Pending", Reason="", readiness=false. Elapsed: 5.522571ms
Mar  1 12:19:32.910: INFO: Pod "pod-secrets-eb9550fb-e882-4599-81e8-bdb0467c6f21": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010299036s
Mar  1 12:19:34.912: INFO: Pod "pod-secrets-eb9550fb-e882-4599-81e8-bdb0467c6f21": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012505225s
STEP: Saw pod success 03/01/23 12:19:34.912
Mar  1 12:19:34.912: INFO: Pod "pod-secrets-eb9550fb-e882-4599-81e8-bdb0467c6f21" satisfied condition "Succeeded or Failed"
Mar  1 12:19:34.918: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-secrets-eb9550fb-e882-4599-81e8-bdb0467c6f21 container secret-volume-test: <nil>
STEP: delete the pod 03/01/23 12:19:34.928
Mar  1 12:19:34.950: INFO: Waiting for pod pod-secrets-eb9550fb-e882-4599-81e8-bdb0467c6f21 to disappear
Mar  1 12:19:34.956: INFO: Pod pod-secrets-eb9550fb-e882-4599-81e8-bdb0467c6f21 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Mar  1 12:19:34.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-195" for this suite. 03/01/23 12:19:34.965
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","completed":136,"skipped":2640,"failed":0}
------------------------------
â€¢ [4.120 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:19:30.855
    Mar  1 12:19:30.855: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename secrets 03/01/23 12:19:30.856
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:19:30.876
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:19:30.879
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:46
    STEP: Creating secret with name secret-test-615d2797-c406-40cd-b17d-b7bfbaf626fe 03/01/23 12:19:30.883
    STEP: Creating a pod to test consume secrets 03/01/23 12:19:30.89
    Mar  1 12:19:30.899: INFO: Waiting up to 5m0s for pod "pod-secrets-eb9550fb-e882-4599-81e8-bdb0467c6f21" in namespace "secrets-195" to be "Succeeded or Failed"
    Mar  1 12:19:30.905: INFO: Pod "pod-secrets-eb9550fb-e882-4599-81e8-bdb0467c6f21": Phase="Pending", Reason="", readiness=false. Elapsed: 5.522571ms
    Mar  1 12:19:32.910: INFO: Pod "pod-secrets-eb9550fb-e882-4599-81e8-bdb0467c6f21": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010299036s
    Mar  1 12:19:34.912: INFO: Pod "pod-secrets-eb9550fb-e882-4599-81e8-bdb0467c6f21": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012505225s
    STEP: Saw pod success 03/01/23 12:19:34.912
    Mar  1 12:19:34.912: INFO: Pod "pod-secrets-eb9550fb-e882-4599-81e8-bdb0467c6f21" satisfied condition "Succeeded or Failed"
    Mar  1 12:19:34.918: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-secrets-eb9550fb-e882-4599-81e8-bdb0467c6f21 container secret-volume-test: <nil>
    STEP: delete the pod 03/01/23 12:19:34.928
    Mar  1 12:19:34.950: INFO: Waiting for pod pod-secrets-eb9550fb-e882-4599-81e8-bdb0467c6f21 to disappear
    Mar  1 12:19:34.956: INFO: Pod pod-secrets-eb9550fb-e882-4599-81e8-bdb0467c6f21 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Mar  1 12:19:34.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-195" for this suite. 03/01/23 12:19:34.965
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:19:34.976
Mar  1 12:19:34.976: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename resourcequota 03/01/23 12:19:34.976
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:19:34.998
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:19:35.001
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
STEP: Creating a ResourceQuota with best effort scope 03/01/23 12:19:35.005
STEP: Ensuring ResourceQuota status is calculated 03/01/23 12:19:35.015
STEP: Creating a ResourceQuota with not best effort scope 03/01/23 12:19:37.02
STEP: Ensuring ResourceQuota status is calculated 03/01/23 12:19:37.029
STEP: Creating a best-effort pod 03/01/23 12:19:39.035
STEP: Ensuring resource quota with best effort scope captures the pod usage 03/01/23 12:19:39.053
STEP: Ensuring resource quota with not best effort ignored the pod usage 03/01/23 12:19:41.064
STEP: Deleting the pod 03/01/23 12:19:43.069
STEP: Ensuring resource quota status released the pod usage 03/01/23 12:19:43.09
STEP: Creating a not best-effort pod 03/01/23 12:19:45.097
STEP: Ensuring resource quota with not best effort scope captures the pod usage 03/01/23 12:19:45.11
STEP: Ensuring resource quota with best effort scope ignored the pod usage 03/01/23 12:19:47.115
STEP: Deleting the pod 03/01/23 12:19:49.121
STEP: Ensuring resource quota status released the pod usage 03/01/23 12:19:49.136
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Mar  1 12:19:51.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8777" for this suite. 03/01/23 12:19:51.151
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","completed":137,"skipped":2662,"failed":0}
------------------------------
â€¢ [SLOW TEST] [16.183 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:19:34.976
    Mar  1 12:19:34.976: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename resourcequota 03/01/23 12:19:34.976
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:19:34.998
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:19:35.001
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:793
    STEP: Creating a ResourceQuota with best effort scope 03/01/23 12:19:35.005
    STEP: Ensuring ResourceQuota status is calculated 03/01/23 12:19:35.015
    STEP: Creating a ResourceQuota with not best effort scope 03/01/23 12:19:37.02
    STEP: Ensuring ResourceQuota status is calculated 03/01/23 12:19:37.029
    STEP: Creating a best-effort pod 03/01/23 12:19:39.035
    STEP: Ensuring resource quota with best effort scope captures the pod usage 03/01/23 12:19:39.053
    STEP: Ensuring resource quota with not best effort ignored the pod usage 03/01/23 12:19:41.064
    STEP: Deleting the pod 03/01/23 12:19:43.069
    STEP: Ensuring resource quota status released the pod usage 03/01/23 12:19:43.09
    STEP: Creating a not best-effort pod 03/01/23 12:19:45.097
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 03/01/23 12:19:45.11
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 03/01/23 12:19:47.115
    STEP: Deleting the pod 03/01/23 12:19:49.121
    STEP: Ensuring resource quota status released the pod usage 03/01/23 12:19:49.136
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Mar  1 12:19:51.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-8777" for this suite. 03/01/23 12:19:51.151
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:19:51.159
Mar  1 12:19:51.159: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename crd-publish-openapi 03/01/23 12:19:51.16
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:19:51.18
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:19:51.183
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 03/01/23 12:19:51.186
Mar  1 12:19:51.187: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
Mar  1 12:19:59.856: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  1 12:20:16.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4022" for this suite. 03/01/23 12:20:16.731
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","completed":138,"skipped":2668,"failed":0}
------------------------------
â€¢ [SLOW TEST] [25.581 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:19:51.159
    Mar  1 12:19:51.159: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename crd-publish-openapi 03/01/23 12:19:51.16
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:19:51.18
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:19:51.183
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:356
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 03/01/23 12:19:51.186
    Mar  1 12:19:51.187: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    Mar  1 12:19:59.856: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  1 12:20:16.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-4022" for this suite. 03/01/23 12:20:16.731
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:20:16.74
Mar  1 12:20:16.741: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename secrets 03/01/23 12:20:16.741
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:20:16.764
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:20:16.768
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
STEP: Creating secret with name secret-test-5fd1712d-7283-493c-a1fa-e868a3a287ae 03/01/23 12:20:16.772
STEP: Creating a pod to test consume secrets 03/01/23 12:20:16.778
Mar  1 12:20:16.792: INFO: Waiting up to 5m0s for pod "pod-secrets-350ad28c-b2bc-423b-96bb-26d888c3ba5e" in namespace "secrets-8072" to be "Succeeded or Failed"
Mar  1 12:20:16.798: INFO: Pod "pod-secrets-350ad28c-b2bc-423b-96bb-26d888c3ba5e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.873641ms
Mar  1 12:20:18.809: INFO: Pod "pod-secrets-350ad28c-b2bc-423b-96bb-26d888c3ba5e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017118293s
Mar  1 12:20:20.805: INFO: Pod "pod-secrets-350ad28c-b2bc-423b-96bb-26d888c3ba5e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012951859s
STEP: Saw pod success 03/01/23 12:20:20.805
Mar  1 12:20:20.805: INFO: Pod "pod-secrets-350ad28c-b2bc-423b-96bb-26d888c3ba5e" satisfied condition "Succeeded or Failed"
Mar  1 12:20:20.810: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-secrets-350ad28c-b2bc-423b-96bb-26d888c3ba5e container secret-volume-test: <nil>
STEP: delete the pod 03/01/23 12:20:20.826
Mar  1 12:20:20.847: INFO: Waiting for pod pod-secrets-350ad28c-b2bc-423b-96bb-26d888c3ba5e to disappear
Mar  1 12:20:20.851: INFO: Pod pod-secrets-350ad28c-b2bc-423b-96bb-26d888c3ba5e no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Mar  1 12:20:20.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8072" for this suite. 03/01/23 12:20:20.858
{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":139,"skipped":2670,"failed":0}
------------------------------
â€¢ [4.130 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:20:16.74
    Mar  1 12:20:16.741: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename secrets 03/01/23 12:20:16.741
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:20:16.764
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:20:16.768
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:124
    STEP: Creating secret with name secret-test-5fd1712d-7283-493c-a1fa-e868a3a287ae 03/01/23 12:20:16.772
    STEP: Creating a pod to test consume secrets 03/01/23 12:20:16.778
    Mar  1 12:20:16.792: INFO: Waiting up to 5m0s for pod "pod-secrets-350ad28c-b2bc-423b-96bb-26d888c3ba5e" in namespace "secrets-8072" to be "Succeeded or Failed"
    Mar  1 12:20:16.798: INFO: Pod "pod-secrets-350ad28c-b2bc-423b-96bb-26d888c3ba5e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.873641ms
    Mar  1 12:20:18.809: INFO: Pod "pod-secrets-350ad28c-b2bc-423b-96bb-26d888c3ba5e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017118293s
    Mar  1 12:20:20.805: INFO: Pod "pod-secrets-350ad28c-b2bc-423b-96bb-26d888c3ba5e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012951859s
    STEP: Saw pod success 03/01/23 12:20:20.805
    Mar  1 12:20:20.805: INFO: Pod "pod-secrets-350ad28c-b2bc-423b-96bb-26d888c3ba5e" satisfied condition "Succeeded or Failed"
    Mar  1 12:20:20.810: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-secrets-350ad28c-b2bc-423b-96bb-26d888c3ba5e container secret-volume-test: <nil>
    STEP: delete the pod 03/01/23 12:20:20.826
    Mar  1 12:20:20.847: INFO: Waiting for pod pod-secrets-350ad28c-b2bc-423b-96bb-26d888c3ba5e to disappear
    Mar  1 12:20:20.851: INFO: Pod pod-secrets-350ad28c-b2bc-423b-96bb-26d888c3ba5e no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Mar  1 12:20:20.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-8072" for this suite. 03/01/23 12:20:20.858
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:20:20.873
Mar  1 12:20:20.873: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename tables 03/01/23 12:20:20.874
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:20:20.895
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:20:20.899
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:187
Mar  1 12:20:20.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-552" for this suite. 03/01/23 12:20:20.911
{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","completed":140,"skipped":2677,"failed":0}
------------------------------
â€¢ [0.049 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:20:20.873
    Mar  1 12:20:20.873: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename tables 03/01/23 12:20:20.874
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:20:20.895
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:20:20.899
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:187
    Mar  1 12:20:20.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "tables-552" for this suite. 03/01/23 12:20:20.911
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:20:20.922
Mar  1 12:20:20.922: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename configmap 03/01/23 12:20:20.923
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:20:20.945
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:20:20.948
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
STEP: Creating configMap with name configmap-test-upd-84c1e22f-5236-43cb-a927-ff966533a3b9 03/01/23 12:20:20.959
STEP: Creating the pod 03/01/23 12:20:20.965
Mar  1 12:20:20.974: INFO: Waiting up to 5m0s for pod "pod-configmaps-6c3384c1-103d-4eb8-a74f-28f40083a557" in namespace "configmap-8642" to be "running and ready"
Mar  1 12:20:20.980: INFO: Pod "pod-configmaps-6c3384c1-103d-4eb8-a74f-28f40083a557": Phase="Pending", Reason="", readiness=false. Elapsed: 6.678409ms
Mar  1 12:20:20.980: INFO: The phase of Pod pod-configmaps-6c3384c1-103d-4eb8-a74f-28f40083a557 is Pending, waiting for it to be Running (with Ready = true)
Mar  1 12:20:22.986: INFO: Pod "pod-configmaps-6c3384c1-103d-4eb8-a74f-28f40083a557": Phase="Running", Reason="", readiness=true. Elapsed: 2.012183882s
Mar  1 12:20:22.986: INFO: The phase of Pod pod-configmaps-6c3384c1-103d-4eb8-a74f-28f40083a557 is Running (Ready = true)
Mar  1 12:20:22.986: INFO: Pod "pod-configmaps-6c3384c1-103d-4eb8-a74f-28f40083a557" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-84c1e22f-5236-43cb-a927-ff966533a3b9 03/01/23 12:20:23
STEP: waiting to observe update in volume 03/01/23 12:20:23.007
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Mar  1 12:21:47.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8642" for this suite. 03/01/23 12:21:47.486
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":141,"skipped":2677,"failed":0}
------------------------------
â€¢ [SLOW TEST] [86.574 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:20:20.922
    Mar  1 12:20:20.922: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename configmap 03/01/23 12:20:20.923
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:20:20.945
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:20:20.948
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:123
    STEP: Creating configMap with name configmap-test-upd-84c1e22f-5236-43cb-a927-ff966533a3b9 03/01/23 12:20:20.959
    STEP: Creating the pod 03/01/23 12:20:20.965
    Mar  1 12:20:20.974: INFO: Waiting up to 5m0s for pod "pod-configmaps-6c3384c1-103d-4eb8-a74f-28f40083a557" in namespace "configmap-8642" to be "running and ready"
    Mar  1 12:20:20.980: INFO: Pod "pod-configmaps-6c3384c1-103d-4eb8-a74f-28f40083a557": Phase="Pending", Reason="", readiness=false. Elapsed: 6.678409ms
    Mar  1 12:20:20.980: INFO: The phase of Pod pod-configmaps-6c3384c1-103d-4eb8-a74f-28f40083a557 is Pending, waiting for it to be Running (with Ready = true)
    Mar  1 12:20:22.986: INFO: Pod "pod-configmaps-6c3384c1-103d-4eb8-a74f-28f40083a557": Phase="Running", Reason="", readiness=true. Elapsed: 2.012183882s
    Mar  1 12:20:22.986: INFO: The phase of Pod pod-configmaps-6c3384c1-103d-4eb8-a74f-28f40083a557 is Running (Ready = true)
    Mar  1 12:20:22.986: INFO: Pod "pod-configmaps-6c3384c1-103d-4eb8-a74f-28f40083a557" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-84c1e22f-5236-43cb-a927-ff966533a3b9 03/01/23 12:20:23
    STEP: waiting to observe update in volume 03/01/23 12:20:23.007
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Mar  1 12:21:47.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-8642" for this suite. 03/01/23 12:21:47.486
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:21:47.497
Mar  1 12:21:47.497: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename webhook 03/01/23 12:21:47.499
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:21:47.52
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:21:47.523
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/01/23 12:21:47.543
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/01/23 12:21:47.902
STEP: Deploying the webhook pod 03/01/23 12:21:47.915
STEP: Wait for the deployment to be ready 03/01/23 12:21:47.93
Mar  1 12:21:47.941: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/01/23 12:21:49.957
STEP: Verifying the service has paired with the endpoint 03/01/23 12:21:49.971
Mar  1 12:21:50.971: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
STEP: Creating a mutating webhook configuration 03/01/23 12:21:50.978
STEP: Updating a mutating webhook configuration's rules to not include the create operation 03/01/23 12:21:51
STEP: Creating a configMap that should not be mutated 03/01/23 12:21:51.008
STEP: Patching a mutating webhook configuration's rules to include the create operation 03/01/23 12:21:51.022
STEP: Creating a configMap that should be mutated 03/01/23 12:21:51.032
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  1 12:21:51.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-816" for this suite. 03/01/23 12:21:51.071
STEP: Destroying namespace "webhook-816-markers" for this suite. 03/01/23 12:21:51.079
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","completed":142,"skipped":2677,"failed":0}
------------------------------
â€¢ [3.645 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:21:47.497
    Mar  1 12:21:47.497: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename webhook 03/01/23 12:21:47.499
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:21:47.52
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:21:47.523
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/01/23 12:21:47.543
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/01/23 12:21:47.902
    STEP: Deploying the webhook pod 03/01/23 12:21:47.915
    STEP: Wait for the deployment to be ready 03/01/23 12:21:47.93
    Mar  1 12:21:47.941: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/01/23 12:21:49.957
    STEP: Verifying the service has paired with the endpoint 03/01/23 12:21:49.971
    Mar  1 12:21:50.971: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:507
    STEP: Creating a mutating webhook configuration 03/01/23 12:21:50.978
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 03/01/23 12:21:51
    STEP: Creating a configMap that should not be mutated 03/01/23 12:21:51.008
    STEP: Patching a mutating webhook configuration's rules to include the create operation 03/01/23 12:21:51.022
    STEP: Creating a configMap that should be mutated 03/01/23 12:21:51.032
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  1 12:21:51.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-816" for this suite. 03/01/23 12:21:51.071
    STEP: Destroying namespace "webhook-816-markers" for this suite. 03/01/23 12:21:51.079
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:21:51.144
Mar  1 12:21:51.144: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename subpath 03/01/23 12:21:51.145
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:21:51.171
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:21:51.177
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 03/01/23 12:21:51.18
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-5g6f 03/01/23 12:21:51.191
STEP: Creating a pod to test atomic-volume-subpath 03/01/23 12:21:51.191
Mar  1 12:21:51.206: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-5g6f" in namespace "subpath-5853" to be "Succeeded or Failed"
Mar  1 12:21:51.212: INFO: Pod "pod-subpath-test-configmap-5g6f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.958525ms
Mar  1 12:21:53.218: INFO: Pod "pod-subpath-test-configmap-5g6f": Phase="Running", Reason="", readiness=true. Elapsed: 2.012351019s
Mar  1 12:21:55.219: INFO: Pod "pod-subpath-test-configmap-5g6f": Phase="Running", Reason="", readiness=true. Elapsed: 4.013311391s
Mar  1 12:21:57.217: INFO: Pod "pod-subpath-test-configmap-5g6f": Phase="Running", Reason="", readiness=true. Elapsed: 6.011353277s
Mar  1 12:21:59.219: INFO: Pod "pod-subpath-test-configmap-5g6f": Phase="Running", Reason="", readiness=true. Elapsed: 8.012815429s
Mar  1 12:22:01.219: INFO: Pod "pod-subpath-test-configmap-5g6f": Phase="Running", Reason="", readiness=true. Elapsed: 10.013274737s
Mar  1 12:22:03.218: INFO: Pod "pod-subpath-test-configmap-5g6f": Phase="Running", Reason="", readiness=true. Elapsed: 12.012614932s
Mar  1 12:22:05.240: INFO: Pod "pod-subpath-test-configmap-5g6f": Phase="Running", Reason="", readiness=true. Elapsed: 14.034174525s
Mar  1 12:22:07.218: INFO: Pod "pod-subpath-test-configmap-5g6f": Phase="Running", Reason="", readiness=true. Elapsed: 16.012564438s
Mar  1 12:22:09.220: INFO: Pod "pod-subpath-test-configmap-5g6f": Phase="Running", Reason="", readiness=true. Elapsed: 18.013897849s
Mar  1 12:22:11.218: INFO: Pod "pod-subpath-test-configmap-5g6f": Phase="Running", Reason="", readiness=true. Elapsed: 20.012359007s
Mar  1 12:22:13.217: INFO: Pod "pod-subpath-test-configmap-5g6f": Phase="Running", Reason="", readiness=false. Elapsed: 22.01079609s
Mar  1 12:22:15.225: INFO: Pod "pod-subpath-test-configmap-5g6f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.018801893s
STEP: Saw pod success 03/01/23 12:22:15.225
Mar  1 12:22:15.225: INFO: Pod "pod-subpath-test-configmap-5g6f" satisfied condition "Succeeded or Failed"
Mar  1 12:22:15.235: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-subpath-test-configmap-5g6f container test-container-subpath-configmap-5g6f: <nil>
STEP: delete the pod 03/01/23 12:22:15.245
Mar  1 12:22:15.264: INFO: Waiting for pod pod-subpath-test-configmap-5g6f to disappear
Mar  1 12:22:15.269: INFO: Pod pod-subpath-test-configmap-5g6f no longer exists
STEP: Deleting pod pod-subpath-test-configmap-5g6f 03/01/23 12:22:15.269
Mar  1 12:22:15.269: INFO: Deleting pod "pod-subpath-test-configmap-5g6f" in namespace "subpath-5853"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Mar  1 12:22:15.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5853" for this suite. 03/01/23 12:22:15.281
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]","completed":143,"skipped":2691,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.145 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:21:51.144
    Mar  1 12:21:51.144: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename subpath 03/01/23 12:21:51.145
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:21:51.171
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:21:51.177
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 03/01/23 12:21:51.18
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-5g6f 03/01/23 12:21:51.191
    STEP: Creating a pod to test atomic-volume-subpath 03/01/23 12:21:51.191
    Mar  1 12:21:51.206: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-5g6f" in namespace "subpath-5853" to be "Succeeded or Failed"
    Mar  1 12:21:51.212: INFO: Pod "pod-subpath-test-configmap-5g6f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.958525ms
    Mar  1 12:21:53.218: INFO: Pod "pod-subpath-test-configmap-5g6f": Phase="Running", Reason="", readiness=true. Elapsed: 2.012351019s
    Mar  1 12:21:55.219: INFO: Pod "pod-subpath-test-configmap-5g6f": Phase="Running", Reason="", readiness=true. Elapsed: 4.013311391s
    Mar  1 12:21:57.217: INFO: Pod "pod-subpath-test-configmap-5g6f": Phase="Running", Reason="", readiness=true. Elapsed: 6.011353277s
    Mar  1 12:21:59.219: INFO: Pod "pod-subpath-test-configmap-5g6f": Phase="Running", Reason="", readiness=true. Elapsed: 8.012815429s
    Mar  1 12:22:01.219: INFO: Pod "pod-subpath-test-configmap-5g6f": Phase="Running", Reason="", readiness=true. Elapsed: 10.013274737s
    Mar  1 12:22:03.218: INFO: Pod "pod-subpath-test-configmap-5g6f": Phase="Running", Reason="", readiness=true. Elapsed: 12.012614932s
    Mar  1 12:22:05.240: INFO: Pod "pod-subpath-test-configmap-5g6f": Phase="Running", Reason="", readiness=true. Elapsed: 14.034174525s
    Mar  1 12:22:07.218: INFO: Pod "pod-subpath-test-configmap-5g6f": Phase="Running", Reason="", readiness=true. Elapsed: 16.012564438s
    Mar  1 12:22:09.220: INFO: Pod "pod-subpath-test-configmap-5g6f": Phase="Running", Reason="", readiness=true. Elapsed: 18.013897849s
    Mar  1 12:22:11.218: INFO: Pod "pod-subpath-test-configmap-5g6f": Phase="Running", Reason="", readiness=true. Elapsed: 20.012359007s
    Mar  1 12:22:13.217: INFO: Pod "pod-subpath-test-configmap-5g6f": Phase="Running", Reason="", readiness=false. Elapsed: 22.01079609s
    Mar  1 12:22:15.225: INFO: Pod "pod-subpath-test-configmap-5g6f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.018801893s
    STEP: Saw pod success 03/01/23 12:22:15.225
    Mar  1 12:22:15.225: INFO: Pod "pod-subpath-test-configmap-5g6f" satisfied condition "Succeeded or Failed"
    Mar  1 12:22:15.235: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-subpath-test-configmap-5g6f container test-container-subpath-configmap-5g6f: <nil>
    STEP: delete the pod 03/01/23 12:22:15.245
    Mar  1 12:22:15.264: INFO: Waiting for pod pod-subpath-test-configmap-5g6f to disappear
    Mar  1 12:22:15.269: INFO: Pod pod-subpath-test-configmap-5g6f no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-5g6f 03/01/23 12:22:15.269
    Mar  1 12:22:15.269: INFO: Deleting pod "pod-subpath-test-configmap-5g6f" in namespace "subpath-5853"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Mar  1 12:22:15.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-5853" for this suite. 03/01/23 12:22:15.281
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:22:15.293
Mar  1 12:22:15.293: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename statefulset 03/01/23 12:22:15.295
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:22:15.316
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:22:15.319
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-4970 03/01/23 12:22:15.322
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
Mar  1 12:22:15.345: INFO: Found 0 stateful pods, waiting for 1
Mar  1 12:22:25.352: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 03/01/23 12:22:25.362
W0301 12:22:25.376012      19 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Mar  1 12:22:25.385: INFO: Found 1 stateful pods, waiting for 2
Mar  1 12:22:35.394: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  1 12:22:35.394: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 03/01/23 12:22:35.404
STEP: Delete all of the StatefulSets 03/01/23 12:22:35.409
STEP: Verify that StatefulSets have been deleted 03/01/23 12:22:35.421
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Mar  1 12:22:35.429: INFO: Deleting all statefulset in ns statefulset-4970
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Mar  1 12:22:35.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4970" for this suite. 03/01/23 12:22:35.452
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","completed":144,"skipped":2692,"failed":0}
------------------------------
â€¢ [SLOW TEST] [20.176 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:906

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:22:15.293
    Mar  1 12:22:15.293: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename statefulset 03/01/23 12:22:15.295
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:22:15.316
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:22:15.319
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-4970 03/01/23 12:22:15.322
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:906
    Mar  1 12:22:15.345: INFO: Found 0 stateful pods, waiting for 1
    Mar  1 12:22:25.352: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 03/01/23 12:22:25.362
    W0301 12:22:25.376012      19 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Mar  1 12:22:25.385: INFO: Found 1 stateful pods, waiting for 2
    Mar  1 12:22:35.394: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Mar  1 12:22:35.394: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 03/01/23 12:22:35.404
    STEP: Delete all of the StatefulSets 03/01/23 12:22:35.409
    STEP: Verify that StatefulSets have been deleted 03/01/23 12:22:35.421
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Mar  1 12:22:35.429: INFO: Deleting all statefulset in ns statefulset-4970
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Mar  1 12:22:35.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-4970" for this suite. 03/01/23 12:22:35.452
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:22:35.469
Mar  1 12:22:35.470: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename emptydir 03/01/23 12:22:35.475
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:22:35.503
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:22:35.507
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
STEP: Creating a pod to test emptydir 0666 on node default medium 03/01/23 12:22:35.511
Mar  1 12:22:35.524: INFO: Waiting up to 5m0s for pod "pod-4670b4c4-9689-4632-a8f3-158e1cc49b87" in namespace "emptydir-4290" to be "Succeeded or Failed"
Mar  1 12:22:35.530: INFO: Pod "pod-4670b4c4-9689-4632-a8f3-158e1cc49b87": Phase="Pending", Reason="", readiness=false. Elapsed: 5.980201ms
Mar  1 12:22:37.536: INFO: Pod "pod-4670b4c4-9689-4632-a8f3-158e1cc49b87": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011286999s
Mar  1 12:22:39.539: INFO: Pod "pod-4670b4c4-9689-4632-a8f3-158e1cc49b87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014398581s
STEP: Saw pod success 03/01/23 12:22:39.539
Mar  1 12:22:39.539: INFO: Pod "pod-4670b4c4-9689-4632-a8f3-158e1cc49b87" satisfied condition "Succeeded or Failed"
Mar  1 12:22:39.543: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-4670b4c4-9689-4632-a8f3-158e1cc49b87 container test-container: <nil>
STEP: delete the pod 03/01/23 12:22:39.553
Mar  1 12:22:39.571: INFO: Waiting for pod pod-4670b4c4-9689-4632-a8f3-158e1cc49b87 to disappear
Mar  1 12:22:39.576: INFO: Pod pod-4670b4c4-9689-4632-a8f3-158e1cc49b87 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar  1 12:22:39.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4290" for this suite. 03/01/23 12:22:39.583
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":145,"skipped":2702,"failed":0}
------------------------------
â€¢ [4.124 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:22:35.469
    Mar  1 12:22:35.470: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename emptydir 03/01/23 12:22:35.475
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:22:35.503
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:22:35.507
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:176
    STEP: Creating a pod to test emptydir 0666 on node default medium 03/01/23 12:22:35.511
    Mar  1 12:22:35.524: INFO: Waiting up to 5m0s for pod "pod-4670b4c4-9689-4632-a8f3-158e1cc49b87" in namespace "emptydir-4290" to be "Succeeded or Failed"
    Mar  1 12:22:35.530: INFO: Pod "pod-4670b4c4-9689-4632-a8f3-158e1cc49b87": Phase="Pending", Reason="", readiness=false. Elapsed: 5.980201ms
    Mar  1 12:22:37.536: INFO: Pod "pod-4670b4c4-9689-4632-a8f3-158e1cc49b87": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011286999s
    Mar  1 12:22:39.539: INFO: Pod "pod-4670b4c4-9689-4632-a8f3-158e1cc49b87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014398581s
    STEP: Saw pod success 03/01/23 12:22:39.539
    Mar  1 12:22:39.539: INFO: Pod "pod-4670b4c4-9689-4632-a8f3-158e1cc49b87" satisfied condition "Succeeded or Failed"
    Mar  1 12:22:39.543: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-4670b4c4-9689-4632-a8f3-158e1cc49b87 container test-container: <nil>
    STEP: delete the pod 03/01/23 12:22:39.553
    Mar  1 12:22:39.571: INFO: Waiting for pod pod-4670b4c4-9689-4632-a8f3-158e1cc49b87 to disappear
    Mar  1 12:22:39.576: INFO: Pod pod-4670b4c4-9689-4632-a8f3-158e1cc49b87 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar  1 12:22:39.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-4290" for this suite. 03/01/23 12:22:39.583
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:22:39.594
Mar  1 12:22:39.594: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename kubectl 03/01/23 12:22:39.594
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:22:39.617
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:22:39.62
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
STEP: validating api versions 03/01/23 12:22:39.623
Mar  1 12:22:39.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-4591 api-versions'
Mar  1 12:22:39.678: INFO: stderr: ""
Mar  1 12:22:39.678: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nsnapshot.storage.k8s.io/v1\nsnapshot.storage.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar  1 12:22:39.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4591" for this suite. 03/01/23 12:22:39.686
{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","completed":146,"skipped":2704,"failed":0}
------------------------------
â€¢ [0.102 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:816
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:22:39.594
    Mar  1 12:22:39.594: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename kubectl 03/01/23 12:22:39.594
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:22:39.617
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:22:39.62
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:822
    STEP: validating api versions 03/01/23 12:22:39.623
    Mar  1 12:22:39.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-4591 api-versions'
    Mar  1 12:22:39.678: INFO: stderr: ""
    Mar  1 12:22:39.678: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nsnapshot.storage.k8s.io/v1\nsnapshot.storage.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar  1 12:22:39.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-4591" for this suite. 03/01/23 12:22:39.686
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:22:39.696
Mar  1 12:22:39.696: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename configmap 03/01/23 12:22:39.697
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:22:39.717
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:22:39.721
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
STEP: Creating configMap that has name configmap-test-emptyKey-955d532b-c69f-47f7-b666-aea3a9fbf2d4 03/01/23 12:22:39.724
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Mar  1 12:22:39.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2320" for this suite. 03/01/23 12:22:39.732
{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","completed":147,"skipped":2732,"failed":0}
------------------------------
â€¢ [0.048 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:22:39.696
    Mar  1 12:22:39.696: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename configmap 03/01/23 12:22:39.697
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:22:39.717
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:22:39.721
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:137
    STEP: Creating configMap that has name configmap-test-emptyKey-955d532b-c69f-47f7-b666-aea3a9fbf2d4 03/01/23 12:22:39.724
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Mar  1 12:22:39.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-2320" for this suite. 03/01/23 12:22:39.732
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:22:39.745
Mar  1 12:22:39.745: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename gc 03/01/23 12:22:39.746
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:22:39.77
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:22:39.773
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 03/01/23 12:22:39.782
STEP: delete the rc 03/01/23 12:22:44.793
STEP: wait for the rc to be deleted 03/01/23 12:22:44.807
Mar  1 12:22:45.828: INFO: 80 pods remaining
Mar  1 12:22:45.828: INFO: 80 pods has nil DeletionTimestamp
Mar  1 12:22:45.828: INFO: 
Mar  1 12:22:46.834: INFO: 71 pods remaining
Mar  1 12:22:46.834: INFO: 70 pods has nil DeletionTimestamp
Mar  1 12:22:46.834: INFO: 
Mar  1 12:22:47.835: INFO: 60 pods remaining
Mar  1 12:22:47.835: INFO: 60 pods has nil DeletionTimestamp
Mar  1 12:22:47.835: INFO: 
Mar  1 12:22:48.826: INFO: 40 pods remaining
Mar  1 12:22:48.826: INFO: 40 pods has nil DeletionTimestamp
Mar  1 12:22:48.826: INFO: 
Mar  1 12:22:49.841: INFO: 32 pods remaining
Mar  1 12:22:49.841: INFO: 32 pods has nil DeletionTimestamp
Mar  1 12:22:49.841: INFO: 
Mar  1 12:22:50.826: INFO: 20 pods remaining
Mar  1 12:22:50.827: INFO: 20 pods has nil DeletionTimestamp
Mar  1 12:22:50.827: INFO: 
STEP: Gathering metrics 03/01/23 12:22:51.82
Mar  1 12:22:51.856: INFO: Waiting up to 5m0s for pod "kube-controller-manager-lab1-k8s-master-3" in namespace "kube-system" to be "running and ready"
Mar  1 12:22:51.863: INFO: Pod "kube-controller-manager-lab1-k8s-master-3": Phase="Running", Reason="", readiness=true. Elapsed: 6.482413ms
Mar  1 12:22:51.863: INFO: The phase of Pod kube-controller-manager-lab1-k8s-master-3 is Running (Ready = true)
Mar  1 12:22:51.863: INFO: Pod "kube-controller-manager-lab1-k8s-master-3" satisfied condition "running and ready"
Mar  1 12:22:51.923: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Mar  1 12:22:51.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6315" for this suite. 03/01/23 12:22:51.931
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","completed":148,"skipped":2741,"failed":0}
------------------------------
â€¢ [SLOW TEST] [12.201 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:22:39.745
    Mar  1 12:22:39.745: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename gc 03/01/23 12:22:39.746
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:22:39.77
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:22:39.773
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 03/01/23 12:22:39.782
    STEP: delete the rc 03/01/23 12:22:44.793
    STEP: wait for the rc to be deleted 03/01/23 12:22:44.807
    Mar  1 12:22:45.828: INFO: 80 pods remaining
    Mar  1 12:22:45.828: INFO: 80 pods has nil DeletionTimestamp
    Mar  1 12:22:45.828: INFO: 
    Mar  1 12:22:46.834: INFO: 71 pods remaining
    Mar  1 12:22:46.834: INFO: 70 pods has nil DeletionTimestamp
    Mar  1 12:22:46.834: INFO: 
    Mar  1 12:22:47.835: INFO: 60 pods remaining
    Mar  1 12:22:47.835: INFO: 60 pods has nil DeletionTimestamp
    Mar  1 12:22:47.835: INFO: 
    Mar  1 12:22:48.826: INFO: 40 pods remaining
    Mar  1 12:22:48.826: INFO: 40 pods has nil DeletionTimestamp
    Mar  1 12:22:48.826: INFO: 
    Mar  1 12:22:49.841: INFO: 32 pods remaining
    Mar  1 12:22:49.841: INFO: 32 pods has nil DeletionTimestamp
    Mar  1 12:22:49.841: INFO: 
    Mar  1 12:22:50.826: INFO: 20 pods remaining
    Mar  1 12:22:50.827: INFO: 20 pods has nil DeletionTimestamp
    Mar  1 12:22:50.827: INFO: 
    STEP: Gathering metrics 03/01/23 12:22:51.82
    Mar  1 12:22:51.856: INFO: Waiting up to 5m0s for pod "kube-controller-manager-lab1-k8s-master-3" in namespace "kube-system" to be "running and ready"
    Mar  1 12:22:51.863: INFO: Pod "kube-controller-manager-lab1-k8s-master-3": Phase="Running", Reason="", readiness=true. Elapsed: 6.482413ms
    Mar  1 12:22:51.863: INFO: The phase of Pod kube-controller-manager-lab1-k8s-master-3 is Running (Ready = true)
    Mar  1 12:22:51.863: INFO: Pod "kube-controller-manager-lab1-k8s-master-3" satisfied condition "running and ready"
    Mar  1 12:22:51.923: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Mar  1 12:22:51.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-6315" for this suite. 03/01/23 12:22:51.931
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:22:51.946
Mar  1 12:22:51.946: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename gc 03/01/23 12:22:51.947
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:22:51.977
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:22:51.981
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 03/01/23 12:22:51.991
STEP: delete the rc 03/01/23 12:23:02.006
STEP: wait for the rc to be deleted 03/01/23 12:23:02.016
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 03/01/23 12:23:07.024
STEP: Gathering metrics 03/01/23 12:23:37.044
Mar  1 12:23:37.074: INFO: Waiting up to 5m0s for pod "kube-controller-manager-lab1-k8s-master-3" in namespace "kube-system" to be "running and ready"
Mar  1 12:23:37.080: INFO: Pod "kube-controller-manager-lab1-k8s-master-3": Phase="Running", Reason="", readiness=true. Elapsed: 5.9607ms
Mar  1 12:23:37.080: INFO: The phase of Pod kube-controller-manager-lab1-k8s-master-3 is Running (Ready = true)
Mar  1 12:23:37.080: INFO: Pod "kube-controller-manager-lab1-k8s-master-3" satisfied condition "running and ready"
Mar  1 12:23:37.134: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Mar  1 12:23:37.135: INFO: Deleting pod "simpletest.rc-2j99s" in namespace "gc-1672"
Mar  1 12:23:37.152: INFO: Deleting pod "simpletest.rc-2kr4p" in namespace "gc-1672"
Mar  1 12:23:37.174: INFO: Deleting pod "simpletest.rc-2vwwr" in namespace "gc-1672"
Mar  1 12:23:37.198: INFO: Deleting pod "simpletest.rc-2zfjx" in namespace "gc-1672"
Mar  1 12:23:37.214: INFO: Deleting pod "simpletest.rc-5bqxn" in namespace "gc-1672"
Mar  1 12:23:37.232: INFO: Deleting pod "simpletest.rc-5gkzn" in namespace "gc-1672"
Mar  1 12:23:37.259: INFO: Deleting pod "simpletest.rc-5vs8x" in namespace "gc-1672"
Mar  1 12:23:37.278: INFO: Deleting pod "simpletest.rc-6c86b" in namespace "gc-1672"
Mar  1 12:23:37.293: INFO: Deleting pod "simpletest.rc-6ds54" in namespace "gc-1672"
Mar  1 12:23:37.318: INFO: Deleting pod "simpletest.rc-6fz85" in namespace "gc-1672"
Mar  1 12:23:37.356: INFO: Deleting pod "simpletest.rc-794v4" in namespace "gc-1672"
Mar  1 12:23:37.380: INFO: Deleting pod "simpletest.rc-7glp6" in namespace "gc-1672"
Mar  1 12:23:37.398: INFO: Deleting pod "simpletest.rc-7kprl" in namespace "gc-1672"
Mar  1 12:23:37.411: INFO: Deleting pod "simpletest.rc-7l9vb" in namespace "gc-1672"
Mar  1 12:23:37.433: INFO: Deleting pod "simpletest.rc-85cx4" in namespace "gc-1672"
Mar  1 12:23:37.453: INFO: Deleting pod "simpletest.rc-87fss" in namespace "gc-1672"
Mar  1 12:23:37.484: INFO: Deleting pod "simpletest.rc-8m9l2" in namespace "gc-1672"
Mar  1 12:23:37.498: INFO: Deleting pod "simpletest.rc-8mkx6" in namespace "gc-1672"
Mar  1 12:23:37.515: INFO: Deleting pod "simpletest.rc-b72hf" in namespace "gc-1672"
Mar  1 12:23:37.531: INFO: Deleting pod "simpletest.rc-bdfkq" in namespace "gc-1672"
Mar  1 12:23:37.548: INFO: Deleting pod "simpletest.rc-bng8h" in namespace "gc-1672"
Mar  1 12:23:37.567: INFO: Deleting pod "simpletest.rc-bsqbj" in namespace "gc-1672"
Mar  1 12:23:37.595: INFO: Deleting pod "simpletest.rc-btmbq" in namespace "gc-1672"
Mar  1 12:23:37.619: INFO: Deleting pod "simpletest.rc-bwhrs" in namespace "gc-1672"
Mar  1 12:23:37.642: INFO: Deleting pod "simpletest.rc-c72zv" in namespace "gc-1672"
Mar  1 12:23:37.671: INFO: Deleting pod "simpletest.rc-cjg5h" in namespace "gc-1672"
Mar  1 12:23:37.688: INFO: Deleting pod "simpletest.rc-cts9x" in namespace "gc-1672"
Mar  1 12:23:37.713: INFO: Deleting pod "simpletest.rc-d7rlm" in namespace "gc-1672"
Mar  1 12:23:37.731: INFO: Deleting pod "simpletest.rc-dcftt" in namespace "gc-1672"
Mar  1 12:23:37.746: INFO: Deleting pod "simpletest.rc-dfpk5" in namespace "gc-1672"
Mar  1 12:23:37.760: INFO: Deleting pod "simpletest.rc-dj5gp" in namespace "gc-1672"
Mar  1 12:23:37.775: INFO: Deleting pod "simpletest.rc-dk6v6" in namespace "gc-1672"
Mar  1 12:23:37.791: INFO: Deleting pod "simpletest.rc-ffkh6" in namespace "gc-1672"
Mar  1 12:23:37.813: INFO: Deleting pod "simpletest.rc-fw7rm" in namespace "gc-1672"
Mar  1 12:23:37.852: INFO: Deleting pod "simpletest.rc-fwj7d" in namespace "gc-1672"
Mar  1 12:23:37.882: INFO: Deleting pod "simpletest.rc-fzkgz" in namespace "gc-1672"
Mar  1 12:23:37.906: INFO: Deleting pod "simpletest.rc-fzzkn" in namespace "gc-1672"
Mar  1 12:23:37.943: INFO: Deleting pod "simpletest.rc-gfkpp" in namespace "gc-1672"
Mar  1 12:23:37.965: INFO: Deleting pod "simpletest.rc-gmqqm" in namespace "gc-1672"
Mar  1 12:23:37.985: INFO: Deleting pod "simpletest.rc-hrdgd" in namespace "gc-1672"
Mar  1 12:23:37.998: INFO: Deleting pod "simpletest.rc-hsdvv" in namespace "gc-1672"
Mar  1 12:23:38.017: INFO: Deleting pod "simpletest.rc-hwzvp" in namespace "gc-1672"
Mar  1 12:23:38.038: INFO: Deleting pod "simpletest.rc-j7sl2" in namespace "gc-1672"
Mar  1 12:23:38.061: INFO: Deleting pod "simpletest.rc-j8dhq" in namespace "gc-1672"
Mar  1 12:23:38.084: INFO: Deleting pod "simpletest.rc-jgr8p" in namespace "gc-1672"
Mar  1 12:23:38.102: INFO: Deleting pod "simpletest.rc-jtq48" in namespace "gc-1672"
Mar  1 12:23:38.116: INFO: Deleting pod "simpletest.rc-k4sbl" in namespace "gc-1672"
Mar  1 12:23:38.135: INFO: Deleting pod "simpletest.rc-kcqn8" in namespace "gc-1672"
Mar  1 12:23:38.149: INFO: Deleting pod "simpletest.rc-kkr88" in namespace "gc-1672"
Mar  1 12:23:38.180: INFO: Deleting pod "simpletest.rc-kls7r" in namespace "gc-1672"
Mar  1 12:23:38.206: INFO: Deleting pod "simpletest.rc-kqcgc" in namespace "gc-1672"
Mar  1 12:23:38.238: INFO: Deleting pod "simpletest.rc-l8snb" in namespace "gc-1672"
Mar  1 12:23:38.279: INFO: Deleting pod "simpletest.rc-lglq4" in namespace "gc-1672"
Mar  1 12:23:38.296: INFO: Deleting pod "simpletest.rc-lmz4j" in namespace "gc-1672"
Mar  1 12:23:38.314: INFO: Deleting pod "simpletest.rc-ln58n" in namespace "gc-1672"
Mar  1 12:23:38.329: INFO: Deleting pod "simpletest.rc-lngdf" in namespace "gc-1672"
Mar  1 12:23:38.345: INFO: Deleting pod "simpletest.rc-lrkrl" in namespace "gc-1672"
Mar  1 12:23:38.367: INFO: Deleting pod "simpletest.rc-m82fc" in namespace "gc-1672"
Mar  1 12:23:38.401: INFO: Deleting pod "simpletest.rc-md74t" in namespace "gc-1672"
Mar  1 12:23:38.436: INFO: Deleting pod "simpletest.rc-mmf8s" in namespace "gc-1672"
Mar  1 12:23:38.451: INFO: Deleting pod "simpletest.rc-mmhsz" in namespace "gc-1672"
Mar  1 12:23:38.466: INFO: Deleting pod "simpletest.rc-mr4wh" in namespace "gc-1672"
Mar  1 12:23:38.517: INFO: Deleting pod "simpletest.rc-mwl5t" in namespace "gc-1672"
Mar  1 12:23:38.534: INFO: Deleting pod "simpletest.rc-mznsw" in namespace "gc-1672"
Mar  1 12:23:38.555: INFO: Deleting pod "simpletest.rc-n5pzp" in namespace "gc-1672"
Mar  1 12:23:38.573: INFO: Deleting pod "simpletest.rc-n7mvm" in namespace "gc-1672"
Mar  1 12:23:38.600: INFO: Deleting pod "simpletest.rc-nddm9" in namespace "gc-1672"
Mar  1 12:23:38.627: INFO: Deleting pod "simpletest.rc-nkljp" in namespace "gc-1672"
Mar  1 12:23:38.642: INFO: Deleting pod "simpletest.rc-nxxp4" in namespace "gc-1672"
Mar  1 12:23:38.660: INFO: Deleting pod "simpletest.rc-p5lpw" in namespace "gc-1672"
Mar  1 12:23:38.674: INFO: Deleting pod "simpletest.rc-pc56r" in namespace "gc-1672"
Mar  1 12:23:38.695: INFO: Deleting pod "simpletest.rc-pqmbd" in namespace "gc-1672"
Mar  1 12:23:38.742: INFO: Deleting pod "simpletest.rc-pttlv" in namespace "gc-1672"
Mar  1 12:23:38.760: INFO: Deleting pod "simpletest.rc-q2xml" in namespace "gc-1672"
Mar  1 12:23:38.779: INFO: Deleting pod "simpletest.rc-q6b9f" in namespace "gc-1672"
Mar  1 12:23:38.812: INFO: Deleting pod "simpletest.rc-q7dgg" in namespace "gc-1672"
Mar  1 12:23:38.834: INFO: Deleting pod "simpletest.rc-qd27s" in namespace "gc-1672"
Mar  1 12:23:38.849: INFO: Deleting pod "simpletest.rc-qdcsz" in namespace "gc-1672"
Mar  1 12:23:38.863: INFO: Deleting pod "simpletest.rc-qrdqb" in namespace "gc-1672"
Mar  1 12:23:38.881: INFO: Deleting pod "simpletest.rc-qw9bz" in namespace "gc-1672"
Mar  1 12:23:38.900: INFO: Deleting pod "simpletest.rc-r4xqp" in namespace "gc-1672"
Mar  1 12:23:38.918: INFO: Deleting pod "simpletest.rc-rhl7n" in namespace "gc-1672"
Mar  1 12:23:38.936: INFO: Deleting pod "simpletest.rc-rqbrg" in namespace "gc-1672"
Mar  1 12:23:38.952: INFO: Deleting pod "simpletest.rc-rvv4l" in namespace "gc-1672"
Mar  1 12:23:38.971: INFO: Deleting pod "simpletest.rc-s6v5m" in namespace "gc-1672"
Mar  1 12:23:38.991: INFO: Deleting pod "simpletest.rc-scm5t" in namespace "gc-1672"
Mar  1 12:23:39.055: INFO: Deleting pod "simpletest.rc-sdkjr" in namespace "gc-1672"
Mar  1 12:23:39.091: INFO: Deleting pod "simpletest.rc-sffbb" in namespace "gc-1672"
Mar  1 12:23:39.144: INFO: Deleting pod "simpletest.rc-snwwr" in namespace "gc-1672"
Mar  1 12:23:39.197: INFO: Deleting pod "simpletest.rc-svwhr" in namespace "gc-1672"
Mar  1 12:23:39.240: INFO: Deleting pod "simpletest.rc-t49xd" in namespace "gc-1672"
Mar  1 12:23:39.297: INFO: Deleting pod "simpletest.rc-tp9w9" in namespace "gc-1672"
Mar  1 12:23:39.356: INFO: Deleting pod "simpletest.rc-vtsxg" in namespace "gc-1672"
Mar  1 12:23:39.394: INFO: Deleting pod "simpletest.rc-vxcg6" in namespace "gc-1672"
Mar  1 12:23:39.442: INFO: Deleting pod "simpletest.rc-wcd8z" in namespace "gc-1672"
Mar  1 12:23:39.492: INFO: Deleting pod "simpletest.rc-x82c8" in namespace "gc-1672"
Mar  1 12:23:39.548: INFO: Deleting pod "simpletest.rc-xzppl" in namespace "gc-1672"
Mar  1 12:23:39.599: INFO: Deleting pod "simpletest.rc-znfbc" in namespace "gc-1672"
Mar  1 12:23:39.649: INFO: Deleting pod "simpletest.rc-zvnsb" in namespace "gc-1672"
Mar  1 12:23:39.698: INFO: Deleting pod "simpletest.rc-zxx44" in namespace "gc-1672"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Mar  1 12:23:39.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1672" for this suite. 03/01/23 12:23:39.791
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","completed":149,"skipped":2758,"failed":0}
------------------------------
â€¢ [SLOW TEST] [47.888 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:22:51.946
    Mar  1 12:22:51.946: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename gc 03/01/23 12:22:51.947
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:22:51.977
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:22:51.981
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 03/01/23 12:22:51.991
    STEP: delete the rc 03/01/23 12:23:02.006
    STEP: wait for the rc to be deleted 03/01/23 12:23:02.016
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 03/01/23 12:23:07.024
    STEP: Gathering metrics 03/01/23 12:23:37.044
    Mar  1 12:23:37.074: INFO: Waiting up to 5m0s for pod "kube-controller-manager-lab1-k8s-master-3" in namespace "kube-system" to be "running and ready"
    Mar  1 12:23:37.080: INFO: Pod "kube-controller-manager-lab1-k8s-master-3": Phase="Running", Reason="", readiness=true. Elapsed: 5.9607ms
    Mar  1 12:23:37.080: INFO: The phase of Pod kube-controller-manager-lab1-k8s-master-3 is Running (Ready = true)
    Mar  1 12:23:37.080: INFO: Pod "kube-controller-manager-lab1-k8s-master-3" satisfied condition "running and ready"
    Mar  1 12:23:37.134: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Mar  1 12:23:37.135: INFO: Deleting pod "simpletest.rc-2j99s" in namespace "gc-1672"
    Mar  1 12:23:37.152: INFO: Deleting pod "simpletest.rc-2kr4p" in namespace "gc-1672"
    Mar  1 12:23:37.174: INFO: Deleting pod "simpletest.rc-2vwwr" in namespace "gc-1672"
    Mar  1 12:23:37.198: INFO: Deleting pod "simpletest.rc-2zfjx" in namespace "gc-1672"
    Mar  1 12:23:37.214: INFO: Deleting pod "simpletest.rc-5bqxn" in namespace "gc-1672"
    Mar  1 12:23:37.232: INFO: Deleting pod "simpletest.rc-5gkzn" in namespace "gc-1672"
    Mar  1 12:23:37.259: INFO: Deleting pod "simpletest.rc-5vs8x" in namespace "gc-1672"
    Mar  1 12:23:37.278: INFO: Deleting pod "simpletest.rc-6c86b" in namespace "gc-1672"
    Mar  1 12:23:37.293: INFO: Deleting pod "simpletest.rc-6ds54" in namespace "gc-1672"
    Mar  1 12:23:37.318: INFO: Deleting pod "simpletest.rc-6fz85" in namespace "gc-1672"
    Mar  1 12:23:37.356: INFO: Deleting pod "simpletest.rc-794v4" in namespace "gc-1672"
    Mar  1 12:23:37.380: INFO: Deleting pod "simpletest.rc-7glp6" in namespace "gc-1672"
    Mar  1 12:23:37.398: INFO: Deleting pod "simpletest.rc-7kprl" in namespace "gc-1672"
    Mar  1 12:23:37.411: INFO: Deleting pod "simpletest.rc-7l9vb" in namespace "gc-1672"
    Mar  1 12:23:37.433: INFO: Deleting pod "simpletest.rc-85cx4" in namespace "gc-1672"
    Mar  1 12:23:37.453: INFO: Deleting pod "simpletest.rc-87fss" in namespace "gc-1672"
    Mar  1 12:23:37.484: INFO: Deleting pod "simpletest.rc-8m9l2" in namespace "gc-1672"
    Mar  1 12:23:37.498: INFO: Deleting pod "simpletest.rc-8mkx6" in namespace "gc-1672"
    Mar  1 12:23:37.515: INFO: Deleting pod "simpletest.rc-b72hf" in namespace "gc-1672"
    Mar  1 12:23:37.531: INFO: Deleting pod "simpletest.rc-bdfkq" in namespace "gc-1672"
    Mar  1 12:23:37.548: INFO: Deleting pod "simpletest.rc-bng8h" in namespace "gc-1672"
    Mar  1 12:23:37.567: INFO: Deleting pod "simpletest.rc-bsqbj" in namespace "gc-1672"
    Mar  1 12:23:37.595: INFO: Deleting pod "simpletest.rc-btmbq" in namespace "gc-1672"
    Mar  1 12:23:37.619: INFO: Deleting pod "simpletest.rc-bwhrs" in namespace "gc-1672"
    Mar  1 12:23:37.642: INFO: Deleting pod "simpletest.rc-c72zv" in namespace "gc-1672"
    Mar  1 12:23:37.671: INFO: Deleting pod "simpletest.rc-cjg5h" in namespace "gc-1672"
    Mar  1 12:23:37.688: INFO: Deleting pod "simpletest.rc-cts9x" in namespace "gc-1672"
    Mar  1 12:23:37.713: INFO: Deleting pod "simpletest.rc-d7rlm" in namespace "gc-1672"
    Mar  1 12:23:37.731: INFO: Deleting pod "simpletest.rc-dcftt" in namespace "gc-1672"
    Mar  1 12:23:37.746: INFO: Deleting pod "simpletest.rc-dfpk5" in namespace "gc-1672"
    Mar  1 12:23:37.760: INFO: Deleting pod "simpletest.rc-dj5gp" in namespace "gc-1672"
    Mar  1 12:23:37.775: INFO: Deleting pod "simpletest.rc-dk6v6" in namespace "gc-1672"
    Mar  1 12:23:37.791: INFO: Deleting pod "simpletest.rc-ffkh6" in namespace "gc-1672"
    Mar  1 12:23:37.813: INFO: Deleting pod "simpletest.rc-fw7rm" in namespace "gc-1672"
    Mar  1 12:23:37.852: INFO: Deleting pod "simpletest.rc-fwj7d" in namespace "gc-1672"
    Mar  1 12:23:37.882: INFO: Deleting pod "simpletest.rc-fzkgz" in namespace "gc-1672"
    Mar  1 12:23:37.906: INFO: Deleting pod "simpletest.rc-fzzkn" in namespace "gc-1672"
    Mar  1 12:23:37.943: INFO: Deleting pod "simpletest.rc-gfkpp" in namespace "gc-1672"
    Mar  1 12:23:37.965: INFO: Deleting pod "simpletest.rc-gmqqm" in namespace "gc-1672"
    Mar  1 12:23:37.985: INFO: Deleting pod "simpletest.rc-hrdgd" in namespace "gc-1672"
    Mar  1 12:23:37.998: INFO: Deleting pod "simpletest.rc-hsdvv" in namespace "gc-1672"
    Mar  1 12:23:38.017: INFO: Deleting pod "simpletest.rc-hwzvp" in namespace "gc-1672"
    Mar  1 12:23:38.038: INFO: Deleting pod "simpletest.rc-j7sl2" in namespace "gc-1672"
    Mar  1 12:23:38.061: INFO: Deleting pod "simpletest.rc-j8dhq" in namespace "gc-1672"
    Mar  1 12:23:38.084: INFO: Deleting pod "simpletest.rc-jgr8p" in namespace "gc-1672"
    Mar  1 12:23:38.102: INFO: Deleting pod "simpletest.rc-jtq48" in namespace "gc-1672"
    Mar  1 12:23:38.116: INFO: Deleting pod "simpletest.rc-k4sbl" in namespace "gc-1672"
    Mar  1 12:23:38.135: INFO: Deleting pod "simpletest.rc-kcqn8" in namespace "gc-1672"
    Mar  1 12:23:38.149: INFO: Deleting pod "simpletest.rc-kkr88" in namespace "gc-1672"
    Mar  1 12:23:38.180: INFO: Deleting pod "simpletest.rc-kls7r" in namespace "gc-1672"
    Mar  1 12:23:38.206: INFO: Deleting pod "simpletest.rc-kqcgc" in namespace "gc-1672"
    Mar  1 12:23:38.238: INFO: Deleting pod "simpletest.rc-l8snb" in namespace "gc-1672"
    Mar  1 12:23:38.279: INFO: Deleting pod "simpletest.rc-lglq4" in namespace "gc-1672"
    Mar  1 12:23:38.296: INFO: Deleting pod "simpletest.rc-lmz4j" in namespace "gc-1672"
    Mar  1 12:23:38.314: INFO: Deleting pod "simpletest.rc-ln58n" in namespace "gc-1672"
    Mar  1 12:23:38.329: INFO: Deleting pod "simpletest.rc-lngdf" in namespace "gc-1672"
    Mar  1 12:23:38.345: INFO: Deleting pod "simpletest.rc-lrkrl" in namespace "gc-1672"
    Mar  1 12:23:38.367: INFO: Deleting pod "simpletest.rc-m82fc" in namespace "gc-1672"
    Mar  1 12:23:38.401: INFO: Deleting pod "simpletest.rc-md74t" in namespace "gc-1672"
    Mar  1 12:23:38.436: INFO: Deleting pod "simpletest.rc-mmf8s" in namespace "gc-1672"
    Mar  1 12:23:38.451: INFO: Deleting pod "simpletest.rc-mmhsz" in namespace "gc-1672"
    Mar  1 12:23:38.466: INFO: Deleting pod "simpletest.rc-mr4wh" in namespace "gc-1672"
    Mar  1 12:23:38.517: INFO: Deleting pod "simpletest.rc-mwl5t" in namespace "gc-1672"
    Mar  1 12:23:38.534: INFO: Deleting pod "simpletest.rc-mznsw" in namespace "gc-1672"
    Mar  1 12:23:38.555: INFO: Deleting pod "simpletest.rc-n5pzp" in namespace "gc-1672"
    Mar  1 12:23:38.573: INFO: Deleting pod "simpletest.rc-n7mvm" in namespace "gc-1672"
    Mar  1 12:23:38.600: INFO: Deleting pod "simpletest.rc-nddm9" in namespace "gc-1672"
    Mar  1 12:23:38.627: INFO: Deleting pod "simpletest.rc-nkljp" in namespace "gc-1672"
    Mar  1 12:23:38.642: INFO: Deleting pod "simpletest.rc-nxxp4" in namespace "gc-1672"
    Mar  1 12:23:38.660: INFO: Deleting pod "simpletest.rc-p5lpw" in namespace "gc-1672"
    Mar  1 12:23:38.674: INFO: Deleting pod "simpletest.rc-pc56r" in namespace "gc-1672"
    Mar  1 12:23:38.695: INFO: Deleting pod "simpletest.rc-pqmbd" in namespace "gc-1672"
    Mar  1 12:23:38.742: INFO: Deleting pod "simpletest.rc-pttlv" in namespace "gc-1672"
    Mar  1 12:23:38.760: INFO: Deleting pod "simpletest.rc-q2xml" in namespace "gc-1672"
    Mar  1 12:23:38.779: INFO: Deleting pod "simpletest.rc-q6b9f" in namespace "gc-1672"
    Mar  1 12:23:38.812: INFO: Deleting pod "simpletest.rc-q7dgg" in namespace "gc-1672"
    Mar  1 12:23:38.834: INFO: Deleting pod "simpletest.rc-qd27s" in namespace "gc-1672"
    Mar  1 12:23:38.849: INFO: Deleting pod "simpletest.rc-qdcsz" in namespace "gc-1672"
    Mar  1 12:23:38.863: INFO: Deleting pod "simpletest.rc-qrdqb" in namespace "gc-1672"
    Mar  1 12:23:38.881: INFO: Deleting pod "simpletest.rc-qw9bz" in namespace "gc-1672"
    Mar  1 12:23:38.900: INFO: Deleting pod "simpletest.rc-r4xqp" in namespace "gc-1672"
    Mar  1 12:23:38.918: INFO: Deleting pod "simpletest.rc-rhl7n" in namespace "gc-1672"
    Mar  1 12:23:38.936: INFO: Deleting pod "simpletest.rc-rqbrg" in namespace "gc-1672"
    Mar  1 12:23:38.952: INFO: Deleting pod "simpletest.rc-rvv4l" in namespace "gc-1672"
    Mar  1 12:23:38.971: INFO: Deleting pod "simpletest.rc-s6v5m" in namespace "gc-1672"
    Mar  1 12:23:38.991: INFO: Deleting pod "simpletest.rc-scm5t" in namespace "gc-1672"
    Mar  1 12:23:39.055: INFO: Deleting pod "simpletest.rc-sdkjr" in namespace "gc-1672"
    Mar  1 12:23:39.091: INFO: Deleting pod "simpletest.rc-sffbb" in namespace "gc-1672"
    Mar  1 12:23:39.144: INFO: Deleting pod "simpletest.rc-snwwr" in namespace "gc-1672"
    Mar  1 12:23:39.197: INFO: Deleting pod "simpletest.rc-svwhr" in namespace "gc-1672"
    Mar  1 12:23:39.240: INFO: Deleting pod "simpletest.rc-t49xd" in namespace "gc-1672"
    Mar  1 12:23:39.297: INFO: Deleting pod "simpletest.rc-tp9w9" in namespace "gc-1672"
    Mar  1 12:23:39.356: INFO: Deleting pod "simpletest.rc-vtsxg" in namespace "gc-1672"
    Mar  1 12:23:39.394: INFO: Deleting pod "simpletest.rc-vxcg6" in namespace "gc-1672"
    Mar  1 12:23:39.442: INFO: Deleting pod "simpletest.rc-wcd8z" in namespace "gc-1672"
    Mar  1 12:23:39.492: INFO: Deleting pod "simpletest.rc-x82c8" in namespace "gc-1672"
    Mar  1 12:23:39.548: INFO: Deleting pod "simpletest.rc-xzppl" in namespace "gc-1672"
    Mar  1 12:23:39.599: INFO: Deleting pod "simpletest.rc-znfbc" in namespace "gc-1672"
    Mar  1 12:23:39.649: INFO: Deleting pod "simpletest.rc-zvnsb" in namespace "gc-1672"
    Mar  1 12:23:39.698: INFO: Deleting pod "simpletest.rc-zxx44" in namespace "gc-1672"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Mar  1 12:23:39.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-1672" for this suite. 03/01/23 12:23:39.791
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:23:39.836
Mar  1 12:23:39.836: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename lease-test 03/01/23 12:23:39.836
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:23:39.864
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:23:39.869
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/framework.go:187
Mar  1 12:23:39.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-6906" for this suite. 03/01/23 12:23:39.964
{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","completed":150,"skipped":2762,"failed":0}
------------------------------
â€¢ [0.138 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:23:39.836
    Mar  1 12:23:39.836: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename lease-test 03/01/23 12:23:39.836
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:23:39.864
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:23:39.869
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/framework.go:187
    Mar  1 12:23:39.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "lease-test-6906" for this suite. 03/01/23 12:23:39.964
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:23:39.974
Mar  1 12:23:39.974: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename endpointslicemirroring 03/01/23 12:23:39.975
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:23:40.001
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:23:40.006
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 03/01/23 12:23:40.024
Mar  1 12:23:40.035: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 03/01/23 12:23:42.045
Mar  1 12:23:42.057: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint 03/01/23 12:23:44.063
Mar  1 12:23:44.085: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:187
Mar  1 12:23:46.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-5764" for this suite. 03/01/23 12:23:46.102
{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","completed":151,"skipped":2766,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.138 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:23:39.974
    Mar  1 12:23:39.974: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename endpointslicemirroring 03/01/23 12:23:39.975
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:23:40.001
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:23:40.006
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 03/01/23 12:23:40.024
    Mar  1 12:23:40.035: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 03/01/23 12:23:42.045
    Mar  1 12:23:42.057: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
    STEP: mirroring deletion of a custom Endpoint 03/01/23 12:23:44.063
    Mar  1 12:23:44.085: INFO: Waiting for 0 EndpointSlices to exist, got 1
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:187
    Mar  1 12:23:46.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslicemirroring-5764" for this suite. 03/01/23 12:23:46.102
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:23:46.115
Mar  1 12:23:46.115: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename watch 03/01/23 12:23:46.116
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:23:46.143
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:23:46.151
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 03/01/23 12:23:46.155
STEP: starting a background goroutine to produce watch events 03/01/23 12:23:46.158
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 03/01/23 12:23:46.158
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Mar  1 12:23:48.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3141" for this suite. 03/01/23 12:23:48.973
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","completed":152,"skipped":2771,"failed":0}
------------------------------
â€¢ [2.911 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:23:46.115
    Mar  1 12:23:46.115: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename watch 03/01/23 12:23:46.116
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:23:46.143
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:23:46.151
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 03/01/23 12:23:46.155
    STEP: starting a background goroutine to produce watch events 03/01/23 12:23:46.158
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 03/01/23 12:23:46.158
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Mar  1 12:23:48.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-3141" for this suite. 03/01/23 12:23:48.973
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:23:49.025
Mar  1 12:23:49.025: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename job 03/01/23 12:23:49.026
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:23:49.053
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:23:49.057
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
STEP: Creating a job 03/01/23 12:23:49.06
STEP: Ensuring job reaches completions 03/01/23 12:23:49.07
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Mar  1 12:24:01.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5481" for this suite. 03/01/23 12:24:01.084
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","completed":153,"skipped":2775,"failed":0}
------------------------------
â€¢ [SLOW TEST] [12.068 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:23:49.025
    Mar  1 12:23:49.025: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename job 03/01/23 12:23:49.026
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:23:49.053
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:23:49.057
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:254
    STEP: Creating a job 03/01/23 12:23:49.06
    STEP: Ensuring job reaches completions 03/01/23 12:23:49.07
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Mar  1 12:24:01.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-5481" for this suite. 03/01/23 12:24:01.084
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:24:01.096
Mar  1 12:24:01.096: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename custom-resource-definition 03/01/23 12:24:01.097
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:24:01.124
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:24:01.129
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Mar  1 12:24:01.132: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  1 12:24:09.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8156" for this suite. 03/01/23 12:24:09.403
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","completed":154,"skipped":2800,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.316 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:24:01.096
    Mar  1 12:24:01.096: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename custom-resource-definition 03/01/23 12:24:01.097
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:24:01.124
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:24:01.129
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Mar  1 12:24:01.132: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  1 12:24:09.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-8156" for this suite. 03/01/23 12:24:09.403
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:24:09.415
Mar  1 12:24:09.415: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename configmap 03/01/23 12:24:09.416
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:24:09.437
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:24:09.441
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
STEP: Creating configMap with name configmap-test-upd-799aa70b-a981-4fa9-9d31-5b96cf681ae7 03/01/23 12:24:09.45
STEP: Creating the pod 03/01/23 12:24:09.456
Mar  1 12:24:09.469: INFO: Waiting up to 5m0s for pod "pod-configmaps-a737792b-515e-49bc-af4f-022ff167db5c" in namespace "configmap-8042" to be "running"
Mar  1 12:24:09.477: INFO: Pod "pod-configmaps-a737792b-515e-49bc-af4f-022ff167db5c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.358768ms
Mar  1 12:24:11.484: INFO: Pod "pod-configmaps-a737792b-515e-49bc-af4f-022ff167db5c": Phase="Running", Reason="", readiness=false. Elapsed: 2.015316645s
Mar  1 12:24:11.484: INFO: Pod "pod-configmaps-a737792b-515e-49bc-af4f-022ff167db5c" satisfied condition "running"
STEP: Waiting for pod with text data 03/01/23 12:24:11.484
STEP: Waiting for pod with binary data 03/01/23 12:24:11.502
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Mar  1 12:24:11.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8042" for this suite. 03/01/23 12:24:11.53
{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","completed":155,"skipped":2822,"failed":0}
------------------------------
â€¢ [2.125 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:24:09.415
    Mar  1 12:24:09.415: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename configmap 03/01/23 12:24:09.416
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:24:09.437
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:24:09.441
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:174
    STEP: Creating configMap with name configmap-test-upd-799aa70b-a981-4fa9-9d31-5b96cf681ae7 03/01/23 12:24:09.45
    STEP: Creating the pod 03/01/23 12:24:09.456
    Mar  1 12:24:09.469: INFO: Waiting up to 5m0s for pod "pod-configmaps-a737792b-515e-49bc-af4f-022ff167db5c" in namespace "configmap-8042" to be "running"
    Mar  1 12:24:09.477: INFO: Pod "pod-configmaps-a737792b-515e-49bc-af4f-022ff167db5c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.358768ms
    Mar  1 12:24:11.484: INFO: Pod "pod-configmaps-a737792b-515e-49bc-af4f-022ff167db5c": Phase="Running", Reason="", readiness=false. Elapsed: 2.015316645s
    Mar  1 12:24:11.484: INFO: Pod "pod-configmaps-a737792b-515e-49bc-af4f-022ff167db5c" satisfied condition "running"
    STEP: Waiting for pod with text data 03/01/23 12:24:11.484
    STEP: Waiting for pod with binary data 03/01/23 12:24:11.502
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Mar  1 12:24:11.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-8042" for this suite. 03/01/23 12:24:11.53
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:24:11.541
Mar  1 12:24:11.541: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename conformance-tests 03/01/23 12:24:11.542
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:24:11.569
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:24:11.573
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 03/01/23 12:24:11.576
Mar  1 12:24:11.576: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:187
Mar  1 12:24:11.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "conformance-tests-1245" for this suite. 03/01/23 12:24:11.596
{"msg":"PASSED [sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]","completed":156,"skipped":2853,"failed":0}
------------------------------
â€¢ [0.064 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:24:11.541
    Mar  1 12:24:11.541: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename conformance-tests 03/01/23 12:24:11.542
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:24:11.569
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:24:11.573
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 03/01/23 12:24:11.576
    Mar  1 12:24:11.576: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:187
    Mar  1 12:24:11.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "conformance-tests-1245" for this suite. 03/01/23 12:24:11.596
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:24:11.605
Mar  1 12:24:11.606: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename services 03/01/23 12:24:11.607
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:24:11.627
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:24:11.63
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
STEP: creating service in namespace services-6084 03/01/23 12:24:11.634
STEP: creating service affinity-clusterip in namespace services-6084 03/01/23 12:24:11.634
STEP: creating replication controller affinity-clusterip in namespace services-6084 03/01/23 12:24:11.648
I0301 12:24:11.654618      19 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-6084, replica count: 3
I0301 12:24:14.705323      19 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  1 12:24:14.714: INFO: Creating new exec pod
Mar  1 12:24:14.725: INFO: Waiting up to 5m0s for pod "execpod-affinity767rm" in namespace "services-6084" to be "running"
Mar  1 12:24:14.731: INFO: Pod "execpod-affinity767rm": Phase="Pending", Reason="", readiness=false. Elapsed: 6.31772ms
Mar  1 12:24:16.738: INFO: Pod "execpod-affinity767rm": Phase="Running", Reason="", readiness=true. Elapsed: 2.013616067s
Mar  1 12:24:16.738: INFO: Pod "execpod-affinity767rm" satisfied condition "running"
Mar  1 12:24:17.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-6084 exec execpod-affinity767rm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Mar  1 12:24:17.898: INFO: stderr: "+ nc -v -t -w 2 affinity-clusterip 80\n+ echo hostName\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Mar  1 12:24:17.898: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  1 12:24:17.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-6084 exec execpod-affinity767rm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.9.162 80'
Mar  1 12:24:18.037: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.9.162 80\nConnection to 10.233.9.162 80 port [tcp/http] succeeded!\n"
Mar  1 12:24:18.037: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  1 12:24:18.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-6084 exec execpod-affinity767rm -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.9.162:80/ ; done'
Mar  1 12:24:18.253: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.9.162:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.9.162:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.9.162:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.9.162:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.9.162:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.9.162:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.9.162:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.9.162:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.9.162:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.9.162:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.9.162:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.9.162:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.9.162:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.9.162:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.9.162:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.9.162:80/\n"
Mar  1 12:24:18.253: INFO: stdout: "\naffinity-clusterip-9rbbs\naffinity-clusterip-9rbbs\naffinity-clusterip-9rbbs\naffinity-clusterip-9rbbs\naffinity-clusterip-9rbbs\naffinity-clusterip-9rbbs\naffinity-clusterip-9rbbs\naffinity-clusterip-9rbbs\naffinity-clusterip-9rbbs\naffinity-clusterip-9rbbs\naffinity-clusterip-9rbbs\naffinity-clusterip-9rbbs\naffinity-clusterip-9rbbs\naffinity-clusterip-9rbbs\naffinity-clusterip-9rbbs\naffinity-clusterip-9rbbs"
Mar  1 12:24:18.253: INFO: Received response from host: affinity-clusterip-9rbbs
Mar  1 12:24:18.253: INFO: Received response from host: affinity-clusterip-9rbbs
Mar  1 12:24:18.253: INFO: Received response from host: affinity-clusterip-9rbbs
Mar  1 12:24:18.253: INFO: Received response from host: affinity-clusterip-9rbbs
Mar  1 12:24:18.253: INFO: Received response from host: affinity-clusterip-9rbbs
Mar  1 12:24:18.253: INFO: Received response from host: affinity-clusterip-9rbbs
Mar  1 12:24:18.253: INFO: Received response from host: affinity-clusterip-9rbbs
Mar  1 12:24:18.253: INFO: Received response from host: affinity-clusterip-9rbbs
Mar  1 12:24:18.253: INFO: Received response from host: affinity-clusterip-9rbbs
Mar  1 12:24:18.253: INFO: Received response from host: affinity-clusterip-9rbbs
Mar  1 12:24:18.253: INFO: Received response from host: affinity-clusterip-9rbbs
Mar  1 12:24:18.253: INFO: Received response from host: affinity-clusterip-9rbbs
Mar  1 12:24:18.253: INFO: Received response from host: affinity-clusterip-9rbbs
Mar  1 12:24:18.253: INFO: Received response from host: affinity-clusterip-9rbbs
Mar  1 12:24:18.253: INFO: Received response from host: affinity-clusterip-9rbbs
Mar  1 12:24:18.253: INFO: Received response from host: affinity-clusterip-9rbbs
Mar  1 12:24:18.253: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-6084, will wait for the garbage collector to delete the pods 03/01/23 12:24:18.269
Mar  1 12:24:18.335: INFO: Deleting ReplicationController affinity-clusterip took: 9.712485ms
Mar  1 12:24:18.436: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.687088ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar  1 12:24:20.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6084" for this suite. 03/01/23 12:24:20.869
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","completed":157,"skipped":2858,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.274 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:24:11.605
    Mar  1 12:24:11.606: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename services 03/01/23 12:24:11.607
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:24:11.627
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:24:11.63
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2157
    STEP: creating service in namespace services-6084 03/01/23 12:24:11.634
    STEP: creating service affinity-clusterip in namespace services-6084 03/01/23 12:24:11.634
    STEP: creating replication controller affinity-clusterip in namespace services-6084 03/01/23 12:24:11.648
    I0301 12:24:11.654618      19 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-6084, replica count: 3
    I0301 12:24:14.705323      19 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar  1 12:24:14.714: INFO: Creating new exec pod
    Mar  1 12:24:14.725: INFO: Waiting up to 5m0s for pod "execpod-affinity767rm" in namespace "services-6084" to be "running"
    Mar  1 12:24:14.731: INFO: Pod "execpod-affinity767rm": Phase="Pending", Reason="", readiness=false. Elapsed: 6.31772ms
    Mar  1 12:24:16.738: INFO: Pod "execpod-affinity767rm": Phase="Running", Reason="", readiness=true. Elapsed: 2.013616067s
    Mar  1 12:24:16.738: INFO: Pod "execpod-affinity767rm" satisfied condition "running"
    Mar  1 12:24:17.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-6084 exec execpod-affinity767rm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
    Mar  1 12:24:17.898: INFO: stderr: "+ nc -v -t -w 2 affinity-clusterip 80\n+ echo hostName\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Mar  1 12:24:17.898: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  1 12:24:17.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-6084 exec execpod-affinity767rm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.9.162 80'
    Mar  1 12:24:18.037: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.9.162 80\nConnection to 10.233.9.162 80 port [tcp/http] succeeded!\n"
    Mar  1 12:24:18.037: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  1 12:24:18.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-6084 exec execpod-affinity767rm -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.9.162:80/ ; done'
    Mar  1 12:24:18.253: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.9.162:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.9.162:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.9.162:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.9.162:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.9.162:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.9.162:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.9.162:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.9.162:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.9.162:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.9.162:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.9.162:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.9.162:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.9.162:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.9.162:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.9.162:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.9.162:80/\n"
    Mar  1 12:24:18.253: INFO: stdout: "\naffinity-clusterip-9rbbs\naffinity-clusterip-9rbbs\naffinity-clusterip-9rbbs\naffinity-clusterip-9rbbs\naffinity-clusterip-9rbbs\naffinity-clusterip-9rbbs\naffinity-clusterip-9rbbs\naffinity-clusterip-9rbbs\naffinity-clusterip-9rbbs\naffinity-clusterip-9rbbs\naffinity-clusterip-9rbbs\naffinity-clusterip-9rbbs\naffinity-clusterip-9rbbs\naffinity-clusterip-9rbbs\naffinity-clusterip-9rbbs\naffinity-clusterip-9rbbs"
    Mar  1 12:24:18.253: INFO: Received response from host: affinity-clusterip-9rbbs
    Mar  1 12:24:18.253: INFO: Received response from host: affinity-clusterip-9rbbs
    Mar  1 12:24:18.253: INFO: Received response from host: affinity-clusterip-9rbbs
    Mar  1 12:24:18.253: INFO: Received response from host: affinity-clusterip-9rbbs
    Mar  1 12:24:18.253: INFO: Received response from host: affinity-clusterip-9rbbs
    Mar  1 12:24:18.253: INFO: Received response from host: affinity-clusterip-9rbbs
    Mar  1 12:24:18.253: INFO: Received response from host: affinity-clusterip-9rbbs
    Mar  1 12:24:18.253: INFO: Received response from host: affinity-clusterip-9rbbs
    Mar  1 12:24:18.253: INFO: Received response from host: affinity-clusterip-9rbbs
    Mar  1 12:24:18.253: INFO: Received response from host: affinity-clusterip-9rbbs
    Mar  1 12:24:18.253: INFO: Received response from host: affinity-clusterip-9rbbs
    Mar  1 12:24:18.253: INFO: Received response from host: affinity-clusterip-9rbbs
    Mar  1 12:24:18.253: INFO: Received response from host: affinity-clusterip-9rbbs
    Mar  1 12:24:18.253: INFO: Received response from host: affinity-clusterip-9rbbs
    Mar  1 12:24:18.253: INFO: Received response from host: affinity-clusterip-9rbbs
    Mar  1 12:24:18.253: INFO: Received response from host: affinity-clusterip-9rbbs
    Mar  1 12:24:18.253: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-6084, will wait for the garbage collector to delete the pods 03/01/23 12:24:18.269
    Mar  1 12:24:18.335: INFO: Deleting ReplicationController affinity-clusterip took: 9.712485ms
    Mar  1 12:24:18.436: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.687088ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar  1 12:24:20.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-6084" for this suite. 03/01/23 12:24:20.869
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:24:20.881
Mar  1 12:24:20.881: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename emptydir 03/01/23 12:24:20.882
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:24:20.905
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:24:20.909
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
STEP: Creating a pod to test emptydir 0777 on tmpfs 03/01/23 12:24:20.914
Mar  1 12:24:20.924: INFO: Waiting up to 5m0s for pod "pod-e39dbaca-37c5-4334-a085-105399b8eeac" in namespace "emptydir-6908" to be "Succeeded or Failed"
Mar  1 12:24:20.931: INFO: Pod "pod-e39dbaca-37c5-4334-a085-105399b8eeac": Phase="Pending", Reason="", readiness=false. Elapsed: 7.281452ms
Mar  1 12:24:22.937: INFO: Pod "pod-e39dbaca-37c5-4334-a085-105399b8eeac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012916397s
Mar  1 12:24:24.937: INFO: Pod "pod-e39dbaca-37c5-4334-a085-105399b8eeac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013259215s
STEP: Saw pod success 03/01/23 12:24:24.937
Mar  1 12:24:24.937: INFO: Pod "pod-e39dbaca-37c5-4334-a085-105399b8eeac" satisfied condition "Succeeded or Failed"
Mar  1 12:24:24.941: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-e39dbaca-37c5-4334-a085-105399b8eeac container test-container: <nil>
STEP: delete the pod 03/01/23 12:24:24.951
Mar  1 12:24:24.972: INFO: Waiting for pod pod-e39dbaca-37c5-4334-a085-105399b8eeac to disappear
Mar  1 12:24:24.977: INFO: Pod pod-e39dbaca-37c5-4334-a085-105399b8eeac no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar  1 12:24:24.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6908" for this suite. 03/01/23 12:24:24.986
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":158,"skipped":2893,"failed":0}
------------------------------
â€¢ [4.115 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:24:20.881
    Mar  1 12:24:20.881: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename emptydir 03/01/23 12:24:20.882
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:24:20.905
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:24:20.909
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:116
    STEP: Creating a pod to test emptydir 0777 on tmpfs 03/01/23 12:24:20.914
    Mar  1 12:24:20.924: INFO: Waiting up to 5m0s for pod "pod-e39dbaca-37c5-4334-a085-105399b8eeac" in namespace "emptydir-6908" to be "Succeeded or Failed"
    Mar  1 12:24:20.931: INFO: Pod "pod-e39dbaca-37c5-4334-a085-105399b8eeac": Phase="Pending", Reason="", readiness=false. Elapsed: 7.281452ms
    Mar  1 12:24:22.937: INFO: Pod "pod-e39dbaca-37c5-4334-a085-105399b8eeac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012916397s
    Mar  1 12:24:24.937: INFO: Pod "pod-e39dbaca-37c5-4334-a085-105399b8eeac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013259215s
    STEP: Saw pod success 03/01/23 12:24:24.937
    Mar  1 12:24:24.937: INFO: Pod "pod-e39dbaca-37c5-4334-a085-105399b8eeac" satisfied condition "Succeeded or Failed"
    Mar  1 12:24:24.941: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-e39dbaca-37c5-4334-a085-105399b8eeac container test-container: <nil>
    STEP: delete the pod 03/01/23 12:24:24.951
    Mar  1 12:24:24.972: INFO: Waiting for pod pod-e39dbaca-37c5-4334-a085-105399b8eeac to disappear
    Mar  1 12:24:24.977: INFO: Pod pod-e39dbaca-37c5-4334-a085-105399b8eeac no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar  1 12:24:24.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-6908" for this suite. 03/01/23 12:24:24.986
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:24:24.998
Mar  1 12:24:24.998: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename namespaces 03/01/23 12:24:24.999
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:24:25.023
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:24:25.026
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
STEP: Creating a test namespace 03/01/23 12:24:25.029
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:24:25.054
STEP: Creating a pod in the namespace 03/01/23 12:24:25.058
STEP: Waiting for the pod to have running status 03/01/23 12:24:25.07
Mar  1 12:24:25.070: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-2445" to be "running"
Mar  1 12:24:25.079: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.375346ms
Mar  1 12:24:27.086: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.015396673s
Mar  1 12:24:27.086: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 03/01/23 12:24:27.086
STEP: Waiting for the namespace to be removed. 03/01/23 12:24:27.096
STEP: Recreating the namespace 03/01/23 12:24:38.101
STEP: Verifying there are no pods in the namespace 03/01/23 12:24:38.123
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Mar  1 12:24:38.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-4325" for this suite. 03/01/23 12:24:38.133
STEP: Destroying namespace "nsdeletetest-2445" for this suite. 03/01/23 12:24:38.141
Mar  1 12:24:38.147: INFO: Namespace nsdeletetest-2445 was already deleted
STEP: Destroying namespace "nsdeletetest-8040" for this suite. 03/01/23 12:24:38.147
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","completed":159,"skipped":2894,"failed":0}
------------------------------
â€¢ [SLOW TEST] [13.159 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:24:24.998
    Mar  1 12:24:24.998: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename namespaces 03/01/23 12:24:24.999
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:24:25.023
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:24:25.026
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:242
    STEP: Creating a test namespace 03/01/23 12:24:25.029
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:24:25.054
    STEP: Creating a pod in the namespace 03/01/23 12:24:25.058
    STEP: Waiting for the pod to have running status 03/01/23 12:24:25.07
    Mar  1 12:24:25.070: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-2445" to be "running"
    Mar  1 12:24:25.079: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.375346ms
    Mar  1 12:24:27.086: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.015396673s
    Mar  1 12:24:27.086: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 03/01/23 12:24:27.086
    STEP: Waiting for the namespace to be removed. 03/01/23 12:24:27.096
    STEP: Recreating the namespace 03/01/23 12:24:38.101
    STEP: Verifying there are no pods in the namespace 03/01/23 12:24:38.123
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Mar  1 12:24:38.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-4325" for this suite. 03/01/23 12:24:38.133
    STEP: Destroying namespace "nsdeletetest-2445" for this suite. 03/01/23 12:24:38.141
    Mar  1 12:24:38.147: INFO: Namespace nsdeletetest-2445 was already deleted
    STEP: Destroying namespace "nsdeletetest-8040" for this suite. 03/01/23 12:24:38.147
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:24:38.158
Mar  1 12:24:38.158: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename watch 03/01/23 12:24:38.159
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:24:38.185
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:24:38.189
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 03/01/23 12:24:38.192
STEP: creating a new configmap 03/01/23 12:24:38.193
STEP: modifying the configmap once 03/01/23 12:24:38.2
STEP: closing the watch once it receives two notifications 03/01/23 12:24:38.212
Mar  1 12:24:38.212: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6409  8104dc71-7339-4c78-b0a2-405ba2008283 26860 0 2023-03-01 12:24:38 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-01 12:24:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  1 12:24:38.216: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6409  8104dc71-7339-4c78-b0a2-405ba2008283 26861 0 2023-03-01 12:24:38 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-01 12:24:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 03/01/23 12:24:38.216
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 03/01/23 12:24:38.226
STEP: deleting the configmap 03/01/23 12:24:38.229
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 03/01/23 12:24:38.237
Mar  1 12:24:38.237: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6409  8104dc71-7339-4c78-b0a2-405ba2008283 26862 0 2023-03-01 12:24:38 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-01 12:24:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  1 12:24:38.238: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6409  8104dc71-7339-4c78-b0a2-405ba2008283 26863 0 2023-03-01 12:24:38 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-01 12:24:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Mar  1 12:24:38.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6409" for this suite. 03/01/23 12:24:38.246
{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","completed":160,"skipped":2897,"failed":0}
------------------------------
â€¢ [0.096 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:24:38.158
    Mar  1 12:24:38.158: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename watch 03/01/23 12:24:38.159
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:24:38.185
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:24:38.189
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 03/01/23 12:24:38.192
    STEP: creating a new configmap 03/01/23 12:24:38.193
    STEP: modifying the configmap once 03/01/23 12:24:38.2
    STEP: closing the watch once it receives two notifications 03/01/23 12:24:38.212
    Mar  1 12:24:38.212: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6409  8104dc71-7339-4c78-b0a2-405ba2008283 26860 0 2023-03-01 12:24:38 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-01 12:24:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar  1 12:24:38.216: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6409  8104dc71-7339-4c78-b0a2-405ba2008283 26861 0 2023-03-01 12:24:38 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-01 12:24:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 03/01/23 12:24:38.216
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 03/01/23 12:24:38.226
    STEP: deleting the configmap 03/01/23 12:24:38.229
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 03/01/23 12:24:38.237
    Mar  1 12:24:38.237: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6409  8104dc71-7339-4c78-b0a2-405ba2008283 26862 0 2023-03-01 12:24:38 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-01 12:24:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar  1 12:24:38.238: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6409  8104dc71-7339-4c78-b0a2-405ba2008283 26863 0 2023-03-01 12:24:38 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-01 12:24:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Mar  1 12:24:38.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-6409" for this suite. 03/01/23 12:24:38.246
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:24:38.255
Mar  1 12:24:38.256: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename daemonsets 03/01/23 12:24:38.257
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:24:38.281
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:24:38.285
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
STEP: Creating simple DaemonSet "daemon-set" 03/01/23 12:24:38.313
STEP: Check that daemon pods launch on every node of the cluster. 03/01/23 12:24:38.319
Mar  1 12:24:38.324: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 12:24:38.324: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 12:24:38.324: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 12:24:38.328: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  1 12:24:38.328: INFO: Node lab1-k8s-node-1 is running 0 daemon pod, expected 1
Mar  1 12:24:39.336: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 12:24:39.336: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 12:24:39.336: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 12:24:39.342: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  1 12:24:39.342: INFO: Node lab1-k8s-node-1 is running 0 daemon pod, expected 1
Mar  1 12:24:40.337: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 12:24:40.337: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 12:24:40.337: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 12:24:40.342: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar  1 12:24:40.342: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: listing all DeamonSets 03/01/23 12:24:40.346
STEP: DeleteCollection of the DaemonSets 03/01/23 12:24:40.352
STEP: Verify that ReplicaSets have been deleted 03/01/23 12:24:40.364
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
Mar  1 12:24:40.378: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"26920"},"items":null}

Mar  1 12:24:40.383: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"26920"},"items":[{"metadata":{"name":"daemon-set-6vndn","generateName":"daemon-set-","namespace":"daemonsets-7001","uid":"a6f573f3-e7e3-4698-b05b-41eb0461830a","resourceVersion":"26910","creationTimestamp":"2023-03-01T12:24:38Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"84eaa11704aa00af82db405e7279f7f4c68fe92269f4d297f10be1339de94623","cni.projectcalico.org/podIP":"10.233.95.140/32","cni.projectcalico.org/podIPs":"10.233.95.140/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"11104df2-ee2a-4df7-a22d-91edd6743f3e","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-03-01T12:24:38Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-03-01T12:24:38Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"11104df2-ee2a-4df7-a22d-91edd6743f3e\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-03-01T12:24:39Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.95.140\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-q97f2","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-q97f2","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"lab1-k8s-node-1","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["lab1-k8s-node-1"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-01T12:24:38Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-01T12:24:39Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-01T12:24:39Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-01T12:24:38Z"}],"hostIP":"10.128.0.178","podIP":"10.233.95.140","podIPs":[{"ip":"10.233.95.140"}],"startTime":"2023-03-01T12:24:38Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-03-01T12:24:39Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://ce64e6eb3f18b2c9dbb3f9df2a07188aefe0fc0dfbd1c81f7dd3dc941f3e59ca","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-c82ng","generateName":"daemon-set-","namespace":"daemonsets-7001","uid":"17c3a691-5842-4cce-84fe-70282547060f","resourceVersion":"26915","creationTimestamp":"2023-03-01T12:24:38Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"eea9fdb5e776ead2657b364f8ea2af4a5a739e15100b0b6bb6ccac5ce7a8c84c","cni.projectcalico.org/podIP":"10.233.74.227/32","cni.projectcalico.org/podIPs":"10.233.74.227/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"11104df2-ee2a-4df7-a22d-91edd6743f3e","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-03-01T12:24:38Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-03-01T12:24:38Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"11104df2-ee2a-4df7-a22d-91edd6743f3e\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-03-01T12:24:40Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.74.227\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-mr2jv","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-mr2jv","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"lab1-k8s-node-3","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["lab1-k8s-node-3"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-01T12:24:38Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-01T12:24:39Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-01T12:24:39Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-01T12:24:38Z"}],"hostIP":"10.128.2.241","podIP":"10.233.74.227","podIPs":[{"ip":"10.233.74.227"}],"startTime":"2023-03-01T12:24:38Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-03-01T12:24:39Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://f5edc1cc00430ce530d11a412d9ab15b9409461a1899e2f544c0f66d8fcb1d74","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-wbvsh","generateName":"daemon-set-","namespace":"daemonsets-7001","uid":"c394f1f1-0ceb-439f-9fbb-b044902f2513","resourceVersion":"26913","creationTimestamp":"2023-03-01T12:24:38Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"bc5faa23a42e9ac08ff937266a276bb3aac28986cec47b6a6d6f6fa9eb8f83c7","cni.projectcalico.org/podIP":"10.233.64.116/32","cni.projectcalico.org/podIPs":"10.233.64.116/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"11104df2-ee2a-4df7-a22d-91edd6743f3e","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-03-01T12:24:38Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-03-01T12:24:38Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"11104df2-ee2a-4df7-a22d-91edd6743f3e\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-03-01T12:24:39Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.116\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-mfxww","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-mfxww","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"lab1-k8s-node-2","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["lab1-k8s-node-2"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-01T12:24:38Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-01T12:24:39Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-01T12:24:39Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-01T12:24:38Z"}],"hostIP":"10.128.0.76","podIP":"10.233.64.116","podIPs":[{"ip":"10.233.64.116"}],"startTime":"2023-03-01T12:24:38Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-03-01T12:24:39Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://72b02b3de03f49871cc060cc1b43022c9a3331651b7f5c65db1c7d195e5510eb","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Mar  1 12:24:40.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7001" for this suite. 03/01/23 12:24:40.416
{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","completed":161,"skipped":2917,"failed":0}
------------------------------
â€¢ [2.169 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:24:38.255
    Mar  1 12:24:38.256: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename daemonsets 03/01/23 12:24:38.257
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:24:38.281
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:24:38.285
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:822
    STEP: Creating simple DaemonSet "daemon-set" 03/01/23 12:24:38.313
    STEP: Check that daemon pods launch on every node of the cluster. 03/01/23 12:24:38.319
    Mar  1 12:24:38.324: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 12:24:38.324: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 12:24:38.324: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 12:24:38.328: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  1 12:24:38.328: INFO: Node lab1-k8s-node-1 is running 0 daemon pod, expected 1
    Mar  1 12:24:39.336: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 12:24:39.336: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 12:24:39.336: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 12:24:39.342: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  1 12:24:39.342: INFO: Node lab1-k8s-node-1 is running 0 daemon pod, expected 1
    Mar  1 12:24:40.337: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 12:24:40.337: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 12:24:40.337: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 12:24:40.342: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar  1 12:24:40.342: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: listing all DeamonSets 03/01/23 12:24:40.346
    STEP: DeleteCollection of the DaemonSets 03/01/23 12:24:40.352
    STEP: Verify that ReplicaSets have been deleted 03/01/23 12:24:40.364
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    Mar  1 12:24:40.378: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"26920"},"items":null}

    Mar  1 12:24:40.383: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"26920"},"items":[{"metadata":{"name":"daemon-set-6vndn","generateName":"daemon-set-","namespace":"daemonsets-7001","uid":"a6f573f3-e7e3-4698-b05b-41eb0461830a","resourceVersion":"26910","creationTimestamp":"2023-03-01T12:24:38Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"84eaa11704aa00af82db405e7279f7f4c68fe92269f4d297f10be1339de94623","cni.projectcalico.org/podIP":"10.233.95.140/32","cni.projectcalico.org/podIPs":"10.233.95.140/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"11104df2-ee2a-4df7-a22d-91edd6743f3e","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-03-01T12:24:38Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-03-01T12:24:38Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"11104df2-ee2a-4df7-a22d-91edd6743f3e\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-03-01T12:24:39Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.95.140\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-q97f2","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-q97f2","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"lab1-k8s-node-1","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["lab1-k8s-node-1"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-01T12:24:38Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-01T12:24:39Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-01T12:24:39Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-01T12:24:38Z"}],"hostIP":"10.128.0.178","podIP":"10.233.95.140","podIPs":[{"ip":"10.233.95.140"}],"startTime":"2023-03-01T12:24:38Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-03-01T12:24:39Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://ce64e6eb3f18b2c9dbb3f9df2a07188aefe0fc0dfbd1c81f7dd3dc941f3e59ca","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-c82ng","generateName":"daemon-set-","namespace":"daemonsets-7001","uid":"17c3a691-5842-4cce-84fe-70282547060f","resourceVersion":"26915","creationTimestamp":"2023-03-01T12:24:38Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"eea9fdb5e776ead2657b364f8ea2af4a5a739e15100b0b6bb6ccac5ce7a8c84c","cni.projectcalico.org/podIP":"10.233.74.227/32","cni.projectcalico.org/podIPs":"10.233.74.227/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"11104df2-ee2a-4df7-a22d-91edd6743f3e","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-03-01T12:24:38Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-03-01T12:24:38Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"11104df2-ee2a-4df7-a22d-91edd6743f3e\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-03-01T12:24:40Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.74.227\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-mr2jv","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-mr2jv","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"lab1-k8s-node-3","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["lab1-k8s-node-3"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-01T12:24:38Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-01T12:24:39Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-01T12:24:39Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-01T12:24:38Z"}],"hostIP":"10.128.2.241","podIP":"10.233.74.227","podIPs":[{"ip":"10.233.74.227"}],"startTime":"2023-03-01T12:24:38Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-03-01T12:24:39Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://f5edc1cc00430ce530d11a412d9ab15b9409461a1899e2f544c0f66d8fcb1d74","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-wbvsh","generateName":"daemon-set-","namespace":"daemonsets-7001","uid":"c394f1f1-0ceb-439f-9fbb-b044902f2513","resourceVersion":"26913","creationTimestamp":"2023-03-01T12:24:38Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"bc5faa23a42e9ac08ff937266a276bb3aac28986cec47b6a6d6f6fa9eb8f83c7","cni.projectcalico.org/podIP":"10.233.64.116/32","cni.projectcalico.org/podIPs":"10.233.64.116/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"11104df2-ee2a-4df7-a22d-91edd6743f3e","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-03-01T12:24:38Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-03-01T12:24:38Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"11104df2-ee2a-4df7-a22d-91edd6743f3e\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-03-01T12:24:39Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.116\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-mfxww","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-mfxww","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"lab1-k8s-node-2","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["lab1-k8s-node-2"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-01T12:24:38Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-01T12:24:39Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-01T12:24:39Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-01T12:24:38Z"}],"hostIP":"10.128.0.76","podIP":"10.233.64.116","podIPs":[{"ip":"10.233.64.116"}],"startTime":"2023-03-01T12:24:38Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-03-01T12:24:39Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://72b02b3de03f49871cc060cc1b43022c9a3331651b7f5c65db1c7d195e5510eb","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Mar  1 12:24:40.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-7001" for this suite. 03/01/23 12:24:40.416
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:24:40.427
Mar  1 12:24:40.427: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename dns 03/01/23 12:24:40.428
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:24:40.446
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:24:40.45
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 03/01/23 12:24:40.453
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9204.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-9204.svc.cluster.local; sleep 1; done
 03/01/23 12:24:40.459
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9204.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-9204.svc.cluster.local; sleep 1; done
 03/01/23 12:24:40.459
STEP: creating a pod to probe DNS 03/01/23 12:24:40.459
STEP: submitting the pod to kubernetes 03/01/23 12:24:40.459
Mar  1 12:24:40.472: INFO: Waiting up to 15m0s for pod "dns-test-509870ba-de4f-4744-b338-3ba3bbb6d30e" in namespace "dns-9204" to be "running"
Mar  1 12:24:40.478: INFO: Pod "dns-test-509870ba-de4f-4744-b338-3ba3bbb6d30e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.609945ms
Mar  1 12:24:42.484: INFO: Pod "dns-test-509870ba-de4f-4744-b338-3ba3bbb6d30e": Phase="Running", Reason="", readiness=true. Elapsed: 2.012319423s
Mar  1 12:24:42.485: INFO: Pod "dns-test-509870ba-de4f-4744-b338-3ba3bbb6d30e" satisfied condition "running"
STEP: retrieving the pod 03/01/23 12:24:42.485
STEP: looking for the results for each expected name from probers 03/01/23 12:24:42.49
Mar  1 12:24:42.501: INFO: DNS probes using dns-test-509870ba-de4f-4744-b338-3ba3bbb6d30e succeeded

STEP: deleting the pod 03/01/23 12:24:42.501
STEP: changing the externalName to bar.example.com 03/01/23 12:24:42.52
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9204.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-9204.svc.cluster.local; sleep 1; done
 03/01/23 12:24:42.53
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9204.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-9204.svc.cluster.local; sleep 1; done
 03/01/23 12:24:42.531
STEP: creating a second pod to probe DNS 03/01/23 12:24:42.531
STEP: submitting the pod to kubernetes 03/01/23 12:24:42.531
Mar  1 12:24:42.540: INFO: Waiting up to 15m0s for pod "dns-test-f023bf38-6936-4f38-8e87-742dddee04af" in namespace "dns-9204" to be "running"
Mar  1 12:24:42.548: INFO: Pod "dns-test-f023bf38-6936-4f38-8e87-742dddee04af": Phase="Pending", Reason="", readiness=false. Elapsed: 7.523733ms
Mar  1 12:24:44.554: INFO: Pod "dns-test-f023bf38-6936-4f38-8e87-742dddee04af": Phase="Running", Reason="", readiness=true. Elapsed: 2.013445758s
Mar  1 12:24:44.554: INFO: Pod "dns-test-f023bf38-6936-4f38-8e87-742dddee04af" satisfied condition "running"
STEP: retrieving the pod 03/01/23 12:24:44.554
STEP: looking for the results for each expected name from probers 03/01/23 12:24:44.558
Mar  1 12:24:44.565: INFO: File wheezy_udp@dns-test-service-3.dns-9204.svc.cluster.local from pod  dns-9204/dns-test-f023bf38-6936-4f38-8e87-742dddee04af contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  1 12:24:44.570: INFO: File jessie_udp@dns-test-service-3.dns-9204.svc.cluster.local from pod  dns-9204/dns-test-f023bf38-6936-4f38-8e87-742dddee04af contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  1 12:24:44.570: INFO: Lookups using dns-9204/dns-test-f023bf38-6936-4f38-8e87-742dddee04af failed for: [wheezy_udp@dns-test-service-3.dns-9204.svc.cluster.local jessie_udp@dns-test-service-3.dns-9204.svc.cluster.local]

Mar  1 12:24:49.589: INFO: DNS probes using dns-test-f023bf38-6936-4f38-8e87-742dddee04af succeeded

STEP: deleting the pod 03/01/23 12:24:49.589
STEP: changing the service to type=ClusterIP 03/01/23 12:24:49.605
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9204.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-9204.svc.cluster.local; sleep 1; done
 03/01/23 12:24:49.625
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9204.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-9204.svc.cluster.local; sleep 1; done
 03/01/23 12:24:49.626
STEP: creating a third pod to probe DNS 03/01/23 12:24:49.626
STEP: submitting the pod to kubernetes 03/01/23 12:24:49.64
Mar  1 12:24:49.646: INFO: Waiting up to 15m0s for pod "dns-test-4dc725d0-fcf1-41a6-b0e6-c653caf2d380" in namespace "dns-9204" to be "running"
Mar  1 12:24:49.654: INFO: Pod "dns-test-4dc725d0-fcf1-41a6-b0e6-c653caf2d380": Phase="Pending", Reason="", readiness=false. Elapsed: 7.574538ms
Mar  1 12:24:51.660: INFO: Pod "dns-test-4dc725d0-fcf1-41a6-b0e6-c653caf2d380": Phase="Running", Reason="", readiness=true. Elapsed: 2.013572807s
Mar  1 12:24:51.660: INFO: Pod "dns-test-4dc725d0-fcf1-41a6-b0e6-c653caf2d380" satisfied condition "running"
STEP: retrieving the pod 03/01/23 12:24:51.66
STEP: looking for the results for each expected name from probers 03/01/23 12:24:51.666
Mar  1 12:24:51.680: INFO: DNS probes using dns-test-4dc725d0-fcf1-41a6-b0e6-c653caf2d380 succeeded

STEP: deleting the pod 03/01/23 12:24:51.68
STEP: deleting the test externalName service 03/01/23 12:24:51.698
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Mar  1 12:24:51.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9204" for this suite. 03/01/23 12:24:51.726
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","completed":162,"skipped":2927,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.308 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:24:40.427
    Mar  1 12:24:40.427: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename dns 03/01/23 12:24:40.428
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:24:40.446
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:24:40.45
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 03/01/23 12:24:40.453
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9204.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-9204.svc.cluster.local; sleep 1; done
     03/01/23 12:24:40.459
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9204.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-9204.svc.cluster.local; sleep 1; done
     03/01/23 12:24:40.459
    STEP: creating a pod to probe DNS 03/01/23 12:24:40.459
    STEP: submitting the pod to kubernetes 03/01/23 12:24:40.459
    Mar  1 12:24:40.472: INFO: Waiting up to 15m0s for pod "dns-test-509870ba-de4f-4744-b338-3ba3bbb6d30e" in namespace "dns-9204" to be "running"
    Mar  1 12:24:40.478: INFO: Pod "dns-test-509870ba-de4f-4744-b338-3ba3bbb6d30e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.609945ms
    Mar  1 12:24:42.484: INFO: Pod "dns-test-509870ba-de4f-4744-b338-3ba3bbb6d30e": Phase="Running", Reason="", readiness=true. Elapsed: 2.012319423s
    Mar  1 12:24:42.485: INFO: Pod "dns-test-509870ba-de4f-4744-b338-3ba3bbb6d30e" satisfied condition "running"
    STEP: retrieving the pod 03/01/23 12:24:42.485
    STEP: looking for the results for each expected name from probers 03/01/23 12:24:42.49
    Mar  1 12:24:42.501: INFO: DNS probes using dns-test-509870ba-de4f-4744-b338-3ba3bbb6d30e succeeded

    STEP: deleting the pod 03/01/23 12:24:42.501
    STEP: changing the externalName to bar.example.com 03/01/23 12:24:42.52
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9204.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-9204.svc.cluster.local; sleep 1; done
     03/01/23 12:24:42.53
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9204.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-9204.svc.cluster.local; sleep 1; done
     03/01/23 12:24:42.531
    STEP: creating a second pod to probe DNS 03/01/23 12:24:42.531
    STEP: submitting the pod to kubernetes 03/01/23 12:24:42.531
    Mar  1 12:24:42.540: INFO: Waiting up to 15m0s for pod "dns-test-f023bf38-6936-4f38-8e87-742dddee04af" in namespace "dns-9204" to be "running"
    Mar  1 12:24:42.548: INFO: Pod "dns-test-f023bf38-6936-4f38-8e87-742dddee04af": Phase="Pending", Reason="", readiness=false. Elapsed: 7.523733ms
    Mar  1 12:24:44.554: INFO: Pod "dns-test-f023bf38-6936-4f38-8e87-742dddee04af": Phase="Running", Reason="", readiness=true. Elapsed: 2.013445758s
    Mar  1 12:24:44.554: INFO: Pod "dns-test-f023bf38-6936-4f38-8e87-742dddee04af" satisfied condition "running"
    STEP: retrieving the pod 03/01/23 12:24:44.554
    STEP: looking for the results for each expected name from probers 03/01/23 12:24:44.558
    Mar  1 12:24:44.565: INFO: File wheezy_udp@dns-test-service-3.dns-9204.svc.cluster.local from pod  dns-9204/dns-test-f023bf38-6936-4f38-8e87-742dddee04af contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Mar  1 12:24:44.570: INFO: File jessie_udp@dns-test-service-3.dns-9204.svc.cluster.local from pod  dns-9204/dns-test-f023bf38-6936-4f38-8e87-742dddee04af contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Mar  1 12:24:44.570: INFO: Lookups using dns-9204/dns-test-f023bf38-6936-4f38-8e87-742dddee04af failed for: [wheezy_udp@dns-test-service-3.dns-9204.svc.cluster.local jessie_udp@dns-test-service-3.dns-9204.svc.cluster.local]

    Mar  1 12:24:49.589: INFO: DNS probes using dns-test-f023bf38-6936-4f38-8e87-742dddee04af succeeded

    STEP: deleting the pod 03/01/23 12:24:49.589
    STEP: changing the service to type=ClusterIP 03/01/23 12:24:49.605
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9204.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-9204.svc.cluster.local; sleep 1; done
     03/01/23 12:24:49.625
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9204.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-9204.svc.cluster.local; sleep 1; done
     03/01/23 12:24:49.626
    STEP: creating a third pod to probe DNS 03/01/23 12:24:49.626
    STEP: submitting the pod to kubernetes 03/01/23 12:24:49.64
    Mar  1 12:24:49.646: INFO: Waiting up to 15m0s for pod "dns-test-4dc725d0-fcf1-41a6-b0e6-c653caf2d380" in namespace "dns-9204" to be "running"
    Mar  1 12:24:49.654: INFO: Pod "dns-test-4dc725d0-fcf1-41a6-b0e6-c653caf2d380": Phase="Pending", Reason="", readiness=false. Elapsed: 7.574538ms
    Mar  1 12:24:51.660: INFO: Pod "dns-test-4dc725d0-fcf1-41a6-b0e6-c653caf2d380": Phase="Running", Reason="", readiness=true. Elapsed: 2.013572807s
    Mar  1 12:24:51.660: INFO: Pod "dns-test-4dc725d0-fcf1-41a6-b0e6-c653caf2d380" satisfied condition "running"
    STEP: retrieving the pod 03/01/23 12:24:51.66
    STEP: looking for the results for each expected name from probers 03/01/23 12:24:51.666
    Mar  1 12:24:51.680: INFO: DNS probes using dns-test-4dc725d0-fcf1-41a6-b0e6-c653caf2d380 succeeded

    STEP: deleting the pod 03/01/23 12:24:51.68
    STEP: deleting the test externalName service 03/01/23 12:24:51.698
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Mar  1 12:24:51.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-9204" for this suite. 03/01/23 12:24:51.726
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:24:51.739
Mar  1 12:24:51.739: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename dns 03/01/23 12:24:51.74
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:24:51.76
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:24:51.763
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 03/01/23 12:24:51.766
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3893.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-3893.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3893.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3893.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3893.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-3893.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3893.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-3893.svc.cluster.local;sleep 1; done
 03/01/23 12:24:51.773
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3893.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-3893.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3893.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-3893.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3893.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-3893.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3893.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-3893.svc.cluster.local;sleep 1; done
 03/01/23 12:24:51.773
STEP: creating a pod to probe DNS 03/01/23 12:24:51.773
STEP: submitting the pod to kubernetes 03/01/23 12:24:51.773
Mar  1 12:24:51.788: INFO: Waiting up to 15m0s for pod "dns-test-43a5b5b7-84b4-4d57-81c5-e5deec88e24f" in namespace "dns-3893" to be "running"
Mar  1 12:24:51.796: INFO: Pod "dns-test-43a5b5b7-84b4-4d57-81c5-e5deec88e24f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.482298ms
Mar  1 12:24:53.802: INFO: Pod "dns-test-43a5b5b7-84b4-4d57-81c5-e5deec88e24f": Phase="Running", Reason="", readiness=true. Elapsed: 2.013881102s
Mar  1 12:24:53.802: INFO: Pod "dns-test-43a5b5b7-84b4-4d57-81c5-e5deec88e24f" satisfied condition "running"
STEP: retrieving the pod 03/01/23 12:24:53.803
STEP: looking for the results for each expected name from probers 03/01/23 12:24:53.807
Mar  1 12:24:53.814: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3893.svc.cluster.local from pod dns-3893/dns-test-43a5b5b7-84b4-4d57-81c5-e5deec88e24f: the server could not find the requested resource (get pods dns-test-43a5b5b7-84b4-4d57-81c5-e5deec88e24f)
Mar  1 12:24:53.823: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3893.svc.cluster.local from pod dns-3893/dns-test-43a5b5b7-84b4-4d57-81c5-e5deec88e24f: the server could not find the requested resource (get pods dns-test-43a5b5b7-84b4-4d57-81c5-e5deec88e24f)
Mar  1 12:24:53.829: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3893.svc.cluster.local from pod dns-3893/dns-test-43a5b5b7-84b4-4d57-81c5-e5deec88e24f: the server could not find the requested resource (get pods dns-test-43a5b5b7-84b4-4d57-81c5-e5deec88e24f)
Mar  1 12:24:53.835: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3893.svc.cluster.local from pod dns-3893/dns-test-43a5b5b7-84b4-4d57-81c5-e5deec88e24f: the server could not find the requested resource (get pods dns-test-43a5b5b7-84b4-4d57-81c5-e5deec88e24f)
Mar  1 12:24:53.843: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3893.svc.cluster.local from pod dns-3893/dns-test-43a5b5b7-84b4-4d57-81c5-e5deec88e24f: the server could not find the requested resource (get pods dns-test-43a5b5b7-84b4-4d57-81c5-e5deec88e24f)
Mar  1 12:24:53.848: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3893.svc.cluster.local from pod dns-3893/dns-test-43a5b5b7-84b4-4d57-81c5-e5deec88e24f: the server could not find the requested resource (get pods dns-test-43a5b5b7-84b4-4d57-81c5-e5deec88e24f)
Mar  1 12:24:53.854: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3893.svc.cluster.local from pod dns-3893/dns-test-43a5b5b7-84b4-4d57-81c5-e5deec88e24f: the server could not find the requested resource (get pods dns-test-43a5b5b7-84b4-4d57-81c5-e5deec88e24f)
Mar  1 12:24:53.859: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3893.svc.cluster.local from pod dns-3893/dns-test-43a5b5b7-84b4-4d57-81c5-e5deec88e24f: the server could not find the requested resource (get pods dns-test-43a5b5b7-84b4-4d57-81c5-e5deec88e24f)
Mar  1 12:24:53.859: INFO: Lookups using dns-3893/dns-test-43a5b5b7-84b4-4d57-81c5-e5deec88e24f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3893.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3893.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3893.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3893.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3893.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3893.svc.cluster.local jessie_udp@dns-test-service-2.dns-3893.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3893.svc.cluster.local]

Mar  1 12:24:58.904: INFO: DNS probes using dns-3893/dns-test-43a5b5b7-84b4-4d57-81c5-e5deec88e24f succeeded

STEP: deleting the pod 03/01/23 12:24:58.904
STEP: deleting the test headless service 03/01/23 12:24:58.921
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Mar  1 12:24:58.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3893" for this suite. 03/01/23 12:24:58.955
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","completed":163,"skipped":2948,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.225 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:24:51.739
    Mar  1 12:24:51.739: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename dns 03/01/23 12:24:51.74
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:24:51.76
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:24:51.763
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 03/01/23 12:24:51.766
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3893.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-3893.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3893.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3893.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3893.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-3893.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3893.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-3893.svc.cluster.local;sleep 1; done
     03/01/23 12:24:51.773
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3893.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-3893.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3893.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-3893.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3893.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-3893.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3893.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-3893.svc.cluster.local;sleep 1; done
     03/01/23 12:24:51.773
    STEP: creating a pod to probe DNS 03/01/23 12:24:51.773
    STEP: submitting the pod to kubernetes 03/01/23 12:24:51.773
    Mar  1 12:24:51.788: INFO: Waiting up to 15m0s for pod "dns-test-43a5b5b7-84b4-4d57-81c5-e5deec88e24f" in namespace "dns-3893" to be "running"
    Mar  1 12:24:51.796: INFO: Pod "dns-test-43a5b5b7-84b4-4d57-81c5-e5deec88e24f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.482298ms
    Mar  1 12:24:53.802: INFO: Pod "dns-test-43a5b5b7-84b4-4d57-81c5-e5deec88e24f": Phase="Running", Reason="", readiness=true. Elapsed: 2.013881102s
    Mar  1 12:24:53.802: INFO: Pod "dns-test-43a5b5b7-84b4-4d57-81c5-e5deec88e24f" satisfied condition "running"
    STEP: retrieving the pod 03/01/23 12:24:53.803
    STEP: looking for the results for each expected name from probers 03/01/23 12:24:53.807
    Mar  1 12:24:53.814: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3893.svc.cluster.local from pod dns-3893/dns-test-43a5b5b7-84b4-4d57-81c5-e5deec88e24f: the server could not find the requested resource (get pods dns-test-43a5b5b7-84b4-4d57-81c5-e5deec88e24f)
    Mar  1 12:24:53.823: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3893.svc.cluster.local from pod dns-3893/dns-test-43a5b5b7-84b4-4d57-81c5-e5deec88e24f: the server could not find the requested resource (get pods dns-test-43a5b5b7-84b4-4d57-81c5-e5deec88e24f)
    Mar  1 12:24:53.829: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3893.svc.cluster.local from pod dns-3893/dns-test-43a5b5b7-84b4-4d57-81c5-e5deec88e24f: the server could not find the requested resource (get pods dns-test-43a5b5b7-84b4-4d57-81c5-e5deec88e24f)
    Mar  1 12:24:53.835: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3893.svc.cluster.local from pod dns-3893/dns-test-43a5b5b7-84b4-4d57-81c5-e5deec88e24f: the server could not find the requested resource (get pods dns-test-43a5b5b7-84b4-4d57-81c5-e5deec88e24f)
    Mar  1 12:24:53.843: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3893.svc.cluster.local from pod dns-3893/dns-test-43a5b5b7-84b4-4d57-81c5-e5deec88e24f: the server could not find the requested resource (get pods dns-test-43a5b5b7-84b4-4d57-81c5-e5deec88e24f)
    Mar  1 12:24:53.848: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3893.svc.cluster.local from pod dns-3893/dns-test-43a5b5b7-84b4-4d57-81c5-e5deec88e24f: the server could not find the requested resource (get pods dns-test-43a5b5b7-84b4-4d57-81c5-e5deec88e24f)
    Mar  1 12:24:53.854: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3893.svc.cluster.local from pod dns-3893/dns-test-43a5b5b7-84b4-4d57-81c5-e5deec88e24f: the server could not find the requested resource (get pods dns-test-43a5b5b7-84b4-4d57-81c5-e5deec88e24f)
    Mar  1 12:24:53.859: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3893.svc.cluster.local from pod dns-3893/dns-test-43a5b5b7-84b4-4d57-81c5-e5deec88e24f: the server could not find the requested resource (get pods dns-test-43a5b5b7-84b4-4d57-81c5-e5deec88e24f)
    Mar  1 12:24:53.859: INFO: Lookups using dns-3893/dns-test-43a5b5b7-84b4-4d57-81c5-e5deec88e24f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3893.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3893.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3893.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3893.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3893.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3893.svc.cluster.local jessie_udp@dns-test-service-2.dns-3893.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3893.svc.cluster.local]

    Mar  1 12:24:58.904: INFO: DNS probes using dns-3893/dns-test-43a5b5b7-84b4-4d57-81c5-e5deec88e24f succeeded

    STEP: deleting the pod 03/01/23 12:24:58.904
    STEP: deleting the test headless service 03/01/23 12:24:58.921
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Mar  1 12:24:58.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-3893" for this suite. 03/01/23 12:24:58.955
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:24:58.967
Mar  1 12:24:58.967: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename kubectl 03/01/23 12:24:58.968
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:24:58.989
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:24:58.992
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
Mar  1 12:24:58.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-1714 version'
Mar  1 12:24:59.049: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Mar  1 12:24:59.049: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.6\", GitCommit:\"ff2c119726cc1f8926fb0585c74b25921e866a28\", GitTreeState:\"clean\", BuildDate:\"2023-01-18T19:22:09Z\", GoVersion:\"go1.19.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.6\", GitCommit:\"ff2c119726cc1f8926fb0585c74b25921e866a28\", GitTreeState:\"clean\", BuildDate:\"2023-01-18T19:15:26Z\", GoVersion:\"go1.19.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar  1 12:24:59.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1714" for this suite. 03/01/23 12:24:59.055
{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","completed":164,"skipped":2958,"failed":0}
------------------------------
â€¢ [0.097 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1677
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1683

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:24:58.967
    Mar  1 12:24:58.967: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename kubectl 03/01/23 12:24:58.968
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:24:58.989
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:24:58.992
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1683
    Mar  1 12:24:58.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-1714 version'
    Mar  1 12:24:59.049: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Mar  1 12:24:59.049: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.6\", GitCommit:\"ff2c119726cc1f8926fb0585c74b25921e866a28\", GitTreeState:\"clean\", BuildDate:\"2023-01-18T19:22:09Z\", GoVersion:\"go1.19.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.6\", GitCommit:\"ff2c119726cc1f8926fb0585c74b25921e866a28\", GitTreeState:\"clean\", BuildDate:\"2023-01-18T19:15:26Z\", GoVersion:\"go1.19.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar  1 12:24:59.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-1714" for this suite. 03/01/23 12:24:59.055
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:24:59.064
Mar  1 12:24:59.064: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename replicaset 03/01/23 12:24:59.065
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:24:59.086
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:24:59.089
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 03/01/23 12:24:59.092
Mar  1 12:24:59.102: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar  1 12:25:04.108: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 03/01/23 12:25:04.108
STEP: getting scale subresource 03/01/23 12:25:04.109
STEP: updating a scale subresource 03/01/23 12:25:04.114
STEP: verifying the replicaset Spec.Replicas was modified 03/01/23 12:25:04.123
STEP: Patch a scale subresource 03/01/23 12:25:04.13
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Mar  1 12:25:04.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4196" for this suite. 03/01/23 12:25:04.162
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","completed":165,"skipped":2963,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.112 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:24:59.064
    Mar  1 12:24:59.064: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename replicaset 03/01/23 12:24:59.065
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:24:59.086
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:24:59.089
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 03/01/23 12:24:59.092
    Mar  1 12:24:59.102: INFO: Pod name sample-pod: Found 0 pods out of 1
    Mar  1 12:25:04.108: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 03/01/23 12:25:04.108
    STEP: getting scale subresource 03/01/23 12:25:04.109
    STEP: updating a scale subresource 03/01/23 12:25:04.114
    STEP: verifying the replicaset Spec.Replicas was modified 03/01/23 12:25:04.123
    STEP: Patch a scale subresource 03/01/23 12:25:04.13
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Mar  1 12:25:04.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-4196" for this suite. 03/01/23 12:25:04.162
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:25:04.177
Mar  1 12:25:04.177: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename controllerrevisions 03/01/23 12:25:04.178
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:25:04.214
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:25:04.217
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-d9cts-daemon-set" 03/01/23 12:25:04.248
STEP: Check that daemon pods launch on every node of the cluster. 03/01/23 12:25:04.255
Mar  1 12:25:04.262: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 12:25:04.262: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 12:25:04.263: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 12:25:04.268: INFO: Number of nodes with available pods controlled by daemonset e2e-d9cts-daemon-set: 0
Mar  1 12:25:04.268: INFO: Node lab1-k8s-node-1 is running 0 daemon pod, expected 1
Mar  1 12:25:05.278: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 12:25:05.278: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 12:25:05.278: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 12:25:05.283: INFO: Number of nodes with available pods controlled by daemonset e2e-d9cts-daemon-set: 1
Mar  1 12:25:05.283: INFO: Node lab1-k8s-node-1 is running 0 daemon pod, expected 1
Mar  1 12:25:06.276: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 12:25:06.276: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 12:25:06.276: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 12:25:06.281: INFO: Number of nodes with available pods controlled by daemonset e2e-d9cts-daemon-set: 3
Mar  1 12:25:06.281: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-d9cts-daemon-set
STEP: Confirm DaemonSet "e2e-d9cts-daemon-set" successfully created with "daemonset-name=e2e-d9cts-daemon-set" label 03/01/23 12:25:06.286
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-d9cts-daemon-set" 03/01/23 12:25:06.297
Mar  1 12:25:06.302: INFO: Located ControllerRevision: "e2e-d9cts-daemon-set-86dcfd6ffd"
STEP: Patching ControllerRevision "e2e-d9cts-daemon-set-86dcfd6ffd" 03/01/23 12:25:06.307
Mar  1 12:25:06.315: INFO: e2e-d9cts-daemon-set-86dcfd6ffd has been patched
STEP: Create a new ControllerRevision 03/01/23 12:25:06.315
Mar  1 12:25:06.325: INFO: Created ControllerRevision: e2e-d9cts-daemon-set-57bcc9658d
STEP: Confirm that there are two ControllerRevisions 03/01/23 12:25:06.325
Mar  1 12:25:06.325: INFO: Requesting list of ControllerRevisions to confirm quantity
Mar  1 12:25:06.331: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-d9cts-daemon-set-86dcfd6ffd" 03/01/23 12:25:06.331
STEP: Confirm that there is only one ControllerRevision 03/01/23 12:25:06.339
Mar  1 12:25:06.339: INFO: Requesting list of ControllerRevisions to confirm quantity
Mar  1 12:25:06.344: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-d9cts-daemon-set-57bcc9658d" 03/01/23 12:25:06.348
Mar  1 12:25:06.359: INFO: e2e-d9cts-daemon-set-57bcc9658d has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 03/01/23 12:25:06.359
W0301 12:25:06.368392      19 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 03/01/23 12:25:06.368
Mar  1 12:25:06.368: INFO: Requesting list of ControllerRevisions to confirm quantity
Mar  1 12:25:07.373: INFO: Requesting list of ControllerRevisions to confirm quantity
Mar  1 12:25:07.379: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-d9cts-daemon-set-57bcc9658d=updated" 03/01/23 12:25:07.379
STEP: Confirm that there is only one ControllerRevision 03/01/23 12:25:07.392
Mar  1 12:25:07.392: INFO: Requesting list of ControllerRevisions to confirm quantity
Mar  1 12:25:07.398: INFO: Found 1 ControllerRevisions
Mar  1 12:25:07.402: INFO: ControllerRevision "e2e-d9cts-daemon-set-9bfc49777" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-d9cts-daemon-set" 03/01/23 12:25:07.407
STEP: deleting DaemonSet.extensions e2e-d9cts-daemon-set in namespace controllerrevisions-6001, will wait for the garbage collector to delete the pods 03/01/23 12:25:07.407
Mar  1 12:25:07.471: INFO: Deleting DaemonSet.extensions e2e-d9cts-daemon-set took: 9.630367ms
Mar  1 12:25:07.571: INFO: Terminating DaemonSet.extensions e2e-d9cts-daemon-set pods took: 100.402429ms
Mar  1 12:25:08.977: INFO: Number of nodes with available pods controlled by daemonset e2e-d9cts-daemon-set: 0
Mar  1 12:25:08.978: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-d9cts-daemon-set
Mar  1 12:25:08.982: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"27372"},"items":null}

Mar  1 12:25:08.986: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"27372"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:187
Mar  1 12:25:09.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "controllerrevisions-6001" for this suite. 03/01/23 12:25:09.013
{"msg":"PASSED [sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]","completed":166,"skipped":2972,"failed":0}
------------------------------
â€¢ [4.846 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:25:04.177
    Mar  1 12:25:04.177: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename controllerrevisions 03/01/23 12:25:04.178
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:25:04.214
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:25:04.217
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-d9cts-daemon-set" 03/01/23 12:25:04.248
    STEP: Check that daemon pods launch on every node of the cluster. 03/01/23 12:25:04.255
    Mar  1 12:25:04.262: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 12:25:04.262: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 12:25:04.263: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 12:25:04.268: INFO: Number of nodes with available pods controlled by daemonset e2e-d9cts-daemon-set: 0
    Mar  1 12:25:04.268: INFO: Node lab1-k8s-node-1 is running 0 daemon pod, expected 1
    Mar  1 12:25:05.278: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 12:25:05.278: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 12:25:05.278: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 12:25:05.283: INFO: Number of nodes with available pods controlled by daemonset e2e-d9cts-daemon-set: 1
    Mar  1 12:25:05.283: INFO: Node lab1-k8s-node-1 is running 0 daemon pod, expected 1
    Mar  1 12:25:06.276: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 12:25:06.276: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 12:25:06.276: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 12:25:06.281: INFO: Number of nodes with available pods controlled by daemonset e2e-d9cts-daemon-set: 3
    Mar  1 12:25:06.281: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-d9cts-daemon-set
    STEP: Confirm DaemonSet "e2e-d9cts-daemon-set" successfully created with "daemonset-name=e2e-d9cts-daemon-set" label 03/01/23 12:25:06.286
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-d9cts-daemon-set" 03/01/23 12:25:06.297
    Mar  1 12:25:06.302: INFO: Located ControllerRevision: "e2e-d9cts-daemon-set-86dcfd6ffd"
    STEP: Patching ControllerRevision "e2e-d9cts-daemon-set-86dcfd6ffd" 03/01/23 12:25:06.307
    Mar  1 12:25:06.315: INFO: e2e-d9cts-daemon-set-86dcfd6ffd has been patched
    STEP: Create a new ControllerRevision 03/01/23 12:25:06.315
    Mar  1 12:25:06.325: INFO: Created ControllerRevision: e2e-d9cts-daemon-set-57bcc9658d
    STEP: Confirm that there are two ControllerRevisions 03/01/23 12:25:06.325
    Mar  1 12:25:06.325: INFO: Requesting list of ControllerRevisions to confirm quantity
    Mar  1 12:25:06.331: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-d9cts-daemon-set-86dcfd6ffd" 03/01/23 12:25:06.331
    STEP: Confirm that there is only one ControllerRevision 03/01/23 12:25:06.339
    Mar  1 12:25:06.339: INFO: Requesting list of ControllerRevisions to confirm quantity
    Mar  1 12:25:06.344: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-d9cts-daemon-set-57bcc9658d" 03/01/23 12:25:06.348
    Mar  1 12:25:06.359: INFO: e2e-d9cts-daemon-set-57bcc9658d has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 03/01/23 12:25:06.359
    W0301 12:25:06.368392      19 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 03/01/23 12:25:06.368
    Mar  1 12:25:06.368: INFO: Requesting list of ControllerRevisions to confirm quantity
    Mar  1 12:25:07.373: INFO: Requesting list of ControllerRevisions to confirm quantity
    Mar  1 12:25:07.379: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-d9cts-daemon-set-57bcc9658d=updated" 03/01/23 12:25:07.379
    STEP: Confirm that there is only one ControllerRevision 03/01/23 12:25:07.392
    Mar  1 12:25:07.392: INFO: Requesting list of ControllerRevisions to confirm quantity
    Mar  1 12:25:07.398: INFO: Found 1 ControllerRevisions
    Mar  1 12:25:07.402: INFO: ControllerRevision "e2e-d9cts-daemon-set-9bfc49777" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-d9cts-daemon-set" 03/01/23 12:25:07.407
    STEP: deleting DaemonSet.extensions e2e-d9cts-daemon-set in namespace controllerrevisions-6001, will wait for the garbage collector to delete the pods 03/01/23 12:25:07.407
    Mar  1 12:25:07.471: INFO: Deleting DaemonSet.extensions e2e-d9cts-daemon-set took: 9.630367ms
    Mar  1 12:25:07.571: INFO: Terminating DaemonSet.extensions e2e-d9cts-daemon-set pods took: 100.402429ms
    Mar  1 12:25:08.977: INFO: Number of nodes with available pods controlled by daemonset e2e-d9cts-daemon-set: 0
    Mar  1 12:25:08.978: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-d9cts-daemon-set
    Mar  1 12:25:08.982: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"27372"},"items":null}

    Mar  1 12:25:08.986: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"27372"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:187
    Mar  1 12:25:09.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "controllerrevisions-6001" for this suite. 03/01/23 12:25:09.013
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:25:09.028
Mar  1 12:25:09.028: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename webhook 03/01/23 12:25:09.029
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:25:09.059
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:25:09.063
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/01/23 12:25:09.085
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/01/23 12:25:09.417
STEP: Deploying the webhook pod 03/01/23 12:25:09.43
STEP: Wait for the deployment to be ready 03/01/23 12:25:09.449
Mar  1 12:25:09.470: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/01/23 12:25:11.487
STEP: Verifying the service has paired with the endpoint 03/01/23 12:25:11.504
Mar  1 12:25:12.505: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
STEP: Creating a validating webhook configuration 03/01/23 12:25:12.511
STEP: Creating a configMap that does not comply to the validation webhook rules 03/01/23 12:25:12.53
STEP: Updating a validating webhook configuration's rules to not include the create operation 03/01/23 12:25:12.539
STEP: Creating a configMap that does not comply to the validation webhook rules 03/01/23 12:25:12.557
STEP: Patching a validating webhook configuration's rules to include the create operation 03/01/23 12:25:12.574
STEP: Creating a configMap that does not comply to the validation webhook rules 03/01/23 12:25:12.581
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  1 12:25:12.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5498" for this suite. 03/01/23 12:25:12.603
STEP: Destroying namespace "webhook-5498-markers" for this suite. 03/01/23 12:25:12.612
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","completed":167,"skipped":3009,"failed":0}
------------------------------
â€¢ [3.646 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:25:09.028
    Mar  1 12:25:09.028: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename webhook 03/01/23 12:25:09.029
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:25:09.059
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:25:09.063
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/01/23 12:25:09.085
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/01/23 12:25:09.417
    STEP: Deploying the webhook pod 03/01/23 12:25:09.43
    STEP: Wait for the deployment to be ready 03/01/23 12:25:09.449
    Mar  1 12:25:09.470: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/01/23 12:25:11.487
    STEP: Verifying the service has paired with the endpoint 03/01/23 12:25:11.504
    Mar  1 12:25:12.505: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:412
    STEP: Creating a validating webhook configuration 03/01/23 12:25:12.511
    STEP: Creating a configMap that does not comply to the validation webhook rules 03/01/23 12:25:12.53
    STEP: Updating a validating webhook configuration's rules to not include the create operation 03/01/23 12:25:12.539
    STEP: Creating a configMap that does not comply to the validation webhook rules 03/01/23 12:25:12.557
    STEP: Patching a validating webhook configuration's rules to include the create operation 03/01/23 12:25:12.574
    STEP: Creating a configMap that does not comply to the validation webhook rules 03/01/23 12:25:12.581
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  1 12:25:12.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-5498" for this suite. 03/01/23 12:25:12.603
    STEP: Destroying namespace "webhook-5498-markers" for this suite. 03/01/23 12:25:12.612
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:25:12.676
Mar  1 12:25:12.676: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename projected 03/01/23 12:25:12.676
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:25:12.701
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:25:12.705
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
STEP: Creating a pod to test downward API volume plugin 03/01/23 12:25:12.709
Mar  1 12:25:12.718: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bdc4f0c7-b0c8-4209-9b5e-ffcdf47536a3" in namespace "projected-920" to be "Succeeded or Failed"
Mar  1 12:25:12.723: INFO: Pod "downwardapi-volume-bdc4f0c7-b0c8-4209-9b5e-ffcdf47536a3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.912585ms
Mar  1 12:25:14.730: INFO: Pod "downwardapi-volume-bdc4f0c7-b0c8-4209-9b5e-ffcdf47536a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011051819s
Mar  1 12:25:16.732: INFO: Pod "downwardapi-volume-bdc4f0c7-b0c8-4209-9b5e-ffcdf47536a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013592251s
STEP: Saw pod success 03/01/23 12:25:16.732
Mar  1 12:25:16.732: INFO: Pod "downwardapi-volume-bdc4f0c7-b0c8-4209-9b5e-ffcdf47536a3" satisfied condition "Succeeded or Failed"
Mar  1 12:25:16.738: INFO: Trying to get logs from node lab1-k8s-node-3 pod downwardapi-volume-bdc4f0c7-b0c8-4209-9b5e-ffcdf47536a3 container client-container: <nil>
STEP: delete the pod 03/01/23 12:25:16.748
Mar  1 12:25:16.764: INFO: Waiting for pod downwardapi-volume-bdc4f0c7-b0c8-4209-9b5e-ffcdf47536a3 to disappear
Mar  1 12:25:16.770: INFO: Pod downwardapi-volume-bdc4f0c7-b0c8-4209-9b5e-ffcdf47536a3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Mar  1 12:25:16.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-920" for this suite. 03/01/23 12:25:16.779
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","completed":168,"skipped":3028,"failed":0}
------------------------------
â€¢ [4.112 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:25:12.676
    Mar  1 12:25:12.676: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename projected 03/01/23 12:25:12.676
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:25:12.701
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:25:12.705
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:220
    STEP: Creating a pod to test downward API volume plugin 03/01/23 12:25:12.709
    Mar  1 12:25:12.718: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bdc4f0c7-b0c8-4209-9b5e-ffcdf47536a3" in namespace "projected-920" to be "Succeeded or Failed"
    Mar  1 12:25:12.723: INFO: Pod "downwardapi-volume-bdc4f0c7-b0c8-4209-9b5e-ffcdf47536a3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.912585ms
    Mar  1 12:25:14.730: INFO: Pod "downwardapi-volume-bdc4f0c7-b0c8-4209-9b5e-ffcdf47536a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011051819s
    Mar  1 12:25:16.732: INFO: Pod "downwardapi-volume-bdc4f0c7-b0c8-4209-9b5e-ffcdf47536a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013592251s
    STEP: Saw pod success 03/01/23 12:25:16.732
    Mar  1 12:25:16.732: INFO: Pod "downwardapi-volume-bdc4f0c7-b0c8-4209-9b5e-ffcdf47536a3" satisfied condition "Succeeded or Failed"
    Mar  1 12:25:16.738: INFO: Trying to get logs from node lab1-k8s-node-3 pod downwardapi-volume-bdc4f0c7-b0c8-4209-9b5e-ffcdf47536a3 container client-container: <nil>
    STEP: delete the pod 03/01/23 12:25:16.748
    Mar  1 12:25:16.764: INFO: Waiting for pod downwardapi-volume-bdc4f0c7-b0c8-4209-9b5e-ffcdf47536a3 to disappear
    Mar  1 12:25:16.770: INFO: Pod downwardapi-volume-bdc4f0c7-b0c8-4209-9b5e-ffcdf47536a3 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Mar  1 12:25:16.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-920" for this suite. 03/01/23 12:25:16.779
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:25:16.789
Mar  1 12:25:16.789: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename kubectl 03/01/23 12:25:16.79
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:25:16.816
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:25:16.82
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
Mar  1 12:25:16.823: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-2293 create -f -'
Mar  1 12:25:17.673: INFO: stderr: ""
Mar  1 12:25:17.673: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Mar  1 12:25:17.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-2293 create -f -'
Mar  1 12:25:18.511: INFO: stderr: ""
Mar  1 12:25:18.511: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 03/01/23 12:25:18.511
Mar  1 12:25:19.517: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  1 12:25:19.517: INFO: Found 1 / 1
Mar  1 12:25:19.517: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar  1 12:25:19.522: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  1 12:25:19.522: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar  1 12:25:19.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-2293 describe pod agnhost-primary-zdg6c'
Mar  1 12:25:19.601: INFO: stderr: ""
Mar  1 12:25:19.601: INFO: stdout: "Name:             agnhost-primary-zdg6c\nNamespace:        kubectl-2293\nPriority:         0\nService Account:  default\nNode:             lab1-k8s-node-3/10.128.2.241\nStart Time:       Wed, 01 Mar 2023 12:25:17 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: ac3d17615ad5b9a92fd3a724c46185b24abaf29184235195a95ee1a239291db4\n                  cni.projectcalico.org/podIP: 10.233.74.236/32\n                  cni.projectcalico.org/podIPs: 10.233.74.236/32\nStatus:           Running\nIP:               10.233.74.236\nIPs:\n  IP:           10.233.74.236\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://e0a04a378ac0af7ef1c133d66c5e31ef6dcf2f085e5674323701271a49230301\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.40\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 01 Mar 2023 12:25:18 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-t4mgg (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-t4mgg:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  1s    default-scheduler  Successfully assigned kubectl-2293/agnhost-primary-zdg6c to lab1-k8s-node-3\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.40\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
Mar  1 12:25:19.601: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-2293 describe rc agnhost-primary'
Mar  1 12:25:19.700: INFO: stderr: ""
Mar  1 12:25:19.700: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-2293\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-zdg6c\n"
Mar  1 12:25:19.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-2293 describe service agnhost-primary'
Mar  1 12:25:19.773: INFO: stderr: ""
Mar  1 12:25:19.773: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-2293\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.233.34.50\nIPs:               10.233.34.50\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.233.74.236:6379\nSession Affinity:  None\nEvents:            <none>\n"
Mar  1 12:25:19.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-2293 describe node lab1-k8s-master-1'
Mar  1 12:25:19.924: INFO: stderr: ""
Mar  1 12:25:19.924: INFO: stdout: "Name:               lab1-k8s-master-1\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=v1-c2-m8-d80\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=se-sto\n                    failure-domain.beta.kubernetes.io/zone=sto1\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=lab1-k8s-master-1\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\n                    node.kubernetes.io/instance-type=v1-c2-m8-d80\n                    topology.cinder.csi.openstack.org/zone=sto1\n                    topology.kubernetes.io/region=se-sto\n                    topology.kubernetes.io/zone=sto1\nAnnotations:        alpha.kubernetes.io/provided-node-ip: 10.128.0.115\n                    csi.volume.kubernetes.io/nodeid: {\"cinder.csi.openstack.org\":\"d5aeed0e-35ef-47b7-ac91-0bd6f2465088\"}\n                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.128.0.115/22\n                    projectcalico.org/IPv4IPIPTunnelAddr: 10.233.99.0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 01 Mar 2023 11:31:25 +0000\nTaints:             node-role.kubernetes.io/control-plane:NoSchedule\n                    node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  lab1-k8s-master-1\n  AcquireTime:     <unset>\n  RenewTime:       Wed, 01 Mar 2023 12:25:10 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 01 Mar 2023 11:36:55 +0000   Wed, 01 Mar 2023 11:36:55 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Wed, 01 Mar 2023 12:25:15 +0000   Wed, 01 Mar 2023 11:31:22 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Wed, 01 Mar 2023 12:25:15 +0000   Wed, 01 Mar 2023 11:31:22 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Wed, 01 Mar 2023 12:25:15 +0000   Wed, 01 Mar 2023 11:31:22 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Wed, 01 Mar 2023 12:25:15 +0000   Wed, 01 Mar 2023 11:38:58 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.128.0.115\n  ExternalIP:  91.197.42.235\n  Hostname:    lab1-k8s-master-1\nCapacity:\n  cpu:                2\n  ephemeral-storage:  40470732Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             8148660Ki\n  pods:               110\nAllocatable:\n  cpu:                1750m\n  ephemeral-storage:  37297826550\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             7784116Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 d5aeed0e35ef47b7ac910bd6f2465088\n  System UUID:                d5aeed0e-35ef-47b7-ac91-0bd6f2465088\n  Boot ID:                    8e80c318-2220-4937-b10c-eb37afaaddc3\n  Kernel Version:             5.4.0-113-generic\n  OS Image:                   Ubuntu 20.04.4 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.15\n  Kubelet Version:            v1.25.6\n  Kube-Proxy Version:         v1.25.6\nPodCIDR:                      10.233.64.0/24\nPodCIDRs:                     10.233.64.0/24\nProviderID:                   openstack:///d5aeed0e-35ef-47b7-ac91-0bd6f2465088\nNon-terminated Pods:          (9 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-node-9kd4x                                          150m (8%)     2 (114%)    64M (0%)         500M (6%)      49m\n  kube-system                 csi-cinder-nodeplugin-rwtv5                                0 (0%)        0 (0%)      0 (0%)           0 (0%)         46m\n  kube-system                 kube-apiserver-lab1-k8s-master-1                           250m (14%)    0 (0%)      0 (0%)           0 (0%)         53m\n  kube-system                 kube-controller-manager-lab1-k8s-master-1                  200m (11%)    0 (0%)      0 (0%)           0 (0%)         53m\n  kube-system                 kube-proxy-c8kwj                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         49m\n  kube-system                 kube-scheduler-lab1-k8s-master-1                           100m (5%)     0 (0%)      0 (0%)           0 (0%)         53m\n  kube-system                 nodelocaldns-m9pfl                                         100m (5%)     0 (0%)      70Mi (0%)        200Mi (2%)     47m\n  kube-system                 openstack-cloud-controller-manager-d6lcx                   200m (11%)    0 (0%)      0 (0%)           0 (0%)         48m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-1321f7b040854c53-fhnpr    0 (0%)        0 (0%)      0 (0%)           0 (0%)         43m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests        Limits\n  --------           --------        ------\n  cpu                1 (57%)         2 (114%)\n  memory             137400320 (1%)  709715200 (8%)\n  ephemeral-storage  0 (0%)          0 (0%)\n  hugepages-1Gi      0 (0%)          0 (0%)\n  hugepages-2Mi      0 (0%)          0 (0%)\nEvents:\n  Type     Reason                   Age                From                   Message\n  ----     ------                   ----               ----                   -------\n  Normal   Starting                 52m                kube-proxy             \n  Normal   Starting                 49m                kube-proxy             \n  Normal   NodeHasNoDiskPressure    53m (x5 over 53m)  kubelet                Node lab1-k8s-master-1 status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientMemory  53m (x6 over 53m)  kubelet                Node lab1-k8s-master-1 status is now: NodeHasSufficientMemory\n  Normal   NodeHasSufficientPID     53m (x5 over 53m)  kubelet                Node lab1-k8s-master-1 status is now: NodeHasSufficientPID\n  Warning  InvalidDiskCapacity      53m                kubelet                invalid capacity 0 on image filesystem\n  Normal   NodeAllocatableEnforced  53m                kubelet                Updated Node Allocatable limit across pods\n  Normal   NodeHasSufficientPID     53m                kubelet                Node lab1-k8s-master-1 status is now: NodeHasSufficientPID\n  Normal   NodeHasNoDiskPressure    53m                kubelet                Node lab1-k8s-master-1 status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientMemory  53m                kubelet                Node lab1-k8s-master-1 status is now: NodeHasSufficientMemory\n  Normal   Starting                 53m                kubelet                Starting kubelet.\n  Normal   RegisteredNode           52m                node-controller        Node lab1-k8s-master-1 event: Registered Node lab1-k8s-master-1 in Controller\n  Normal   RegisteredNode           51m                node-controller        Node lab1-k8s-master-1 event: Registered Node lab1-k8s-master-1 in Controller\n  Normal   NodeHasNoDiskPressure    50m                kubelet                Node lab1-k8s-master-1 status is now: NodeHasNoDiskPressure\n  Normal   Starting                 50m                kubelet                Starting kubelet.\n  Warning  InvalidDiskCapacity      50m                kubelet                invalid capacity 0 on image filesystem\n  Normal   NodeAllocatableEnforced  50m                kubelet                Updated Node Allocatable limit across pods\n  Normal   NodeHasSufficientPID     50m                kubelet                Node lab1-k8s-master-1 status is now: NodeHasSufficientPID\n  Normal   NodeHasSufficientMemory  50m                kubelet                Node lab1-k8s-master-1 status is now: NodeHasSufficientMemory\n  Normal   RegisteredNode           50m                node-controller        Node lab1-k8s-master-1 event: Registered Node lab1-k8s-master-1 in Controller\n  Normal   NodeReady                48m                kubelet                Node lab1-k8s-master-1 status is now: NodeReady\n  Normal   RegisteredNode           48m                node-controller        Node lab1-k8s-master-1 event: Registered Node lab1-k8s-master-1 in Controller\n  Normal   Synced                   48m                cloud-node-controller  Node synced successfully\n  Normal   RegisteredNode           46m                node-controller        Node lab1-k8s-master-1 event: Registered Node lab1-k8s-master-1 in Controller\n  Normal   Starting                 46m                kubelet                Starting kubelet.\n  Warning  InvalidDiskCapacity      46m                kubelet                invalid capacity 0 on image filesystem\n  Normal   NodeReady                46m                kubelet                Node lab1-k8s-master-1 status is now: NodeReady\n  Normal   NodeHasSufficientMemory  46m                kubelet                Node lab1-k8s-master-1 status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure    46m                kubelet                Node lab1-k8s-master-1 status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     46m                kubelet                Node lab1-k8s-master-1 status is now: NodeHasSufficientPID\n  Normal   NodeNotReady             46m                kubelet                Node lab1-k8s-master-1 status is now: NodeNotReady\n  Normal   NodeAllocatableEnforced  46m                kubelet                Updated Node Allocatable limit across pods\n  Normal   RegisteredNode           45m                node-controller        Node lab1-k8s-master-1 event: Registered Node lab1-k8s-master-1 in Controller\n"
Mar  1 12:25:19.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-2293 describe namespace kubectl-2293'
Mar  1 12:25:20.005: INFO: stderr: ""
Mar  1 12:25:20.005: INFO: stdout: "Name:         kubectl-2293\nLabels:       e2e-framework=kubectl\n              e2e-run=8dccadbc-6480-4735-8e50-691d6b653465\n              kubernetes.io/metadata.name=kubectl-2293\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar  1 12:25:20.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2293" for this suite. 03/01/23 12:25:20.012
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","completed":169,"skipped":3037,"failed":0}
------------------------------
â€¢ [3.234 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1268
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1274

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:25:16.789
    Mar  1 12:25:16.789: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename kubectl 03/01/23 12:25:16.79
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:25:16.816
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:25:16.82
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1274
    Mar  1 12:25:16.823: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-2293 create -f -'
    Mar  1 12:25:17.673: INFO: stderr: ""
    Mar  1 12:25:17.673: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Mar  1 12:25:17.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-2293 create -f -'
    Mar  1 12:25:18.511: INFO: stderr: ""
    Mar  1 12:25:18.511: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 03/01/23 12:25:18.511
    Mar  1 12:25:19.517: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar  1 12:25:19.517: INFO: Found 1 / 1
    Mar  1 12:25:19.517: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Mar  1 12:25:19.522: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar  1 12:25:19.522: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Mar  1 12:25:19.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-2293 describe pod agnhost-primary-zdg6c'
    Mar  1 12:25:19.601: INFO: stderr: ""
    Mar  1 12:25:19.601: INFO: stdout: "Name:             agnhost-primary-zdg6c\nNamespace:        kubectl-2293\nPriority:         0\nService Account:  default\nNode:             lab1-k8s-node-3/10.128.2.241\nStart Time:       Wed, 01 Mar 2023 12:25:17 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: ac3d17615ad5b9a92fd3a724c46185b24abaf29184235195a95ee1a239291db4\n                  cni.projectcalico.org/podIP: 10.233.74.236/32\n                  cni.projectcalico.org/podIPs: 10.233.74.236/32\nStatus:           Running\nIP:               10.233.74.236\nIPs:\n  IP:           10.233.74.236\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://e0a04a378ac0af7ef1c133d66c5e31ef6dcf2f085e5674323701271a49230301\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.40\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 01 Mar 2023 12:25:18 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-t4mgg (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-t4mgg:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  1s    default-scheduler  Successfully assigned kubectl-2293/agnhost-primary-zdg6c to lab1-k8s-node-3\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.40\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
    Mar  1 12:25:19.601: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-2293 describe rc agnhost-primary'
    Mar  1 12:25:19.700: INFO: stderr: ""
    Mar  1 12:25:19.700: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-2293\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-zdg6c\n"
    Mar  1 12:25:19.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-2293 describe service agnhost-primary'
    Mar  1 12:25:19.773: INFO: stderr: ""
    Mar  1 12:25:19.773: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-2293\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.233.34.50\nIPs:               10.233.34.50\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.233.74.236:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Mar  1 12:25:19.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-2293 describe node lab1-k8s-master-1'
    Mar  1 12:25:19.924: INFO: stderr: ""
    Mar  1 12:25:19.924: INFO: stdout: "Name:               lab1-k8s-master-1\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=v1-c2-m8-d80\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=se-sto\n                    failure-domain.beta.kubernetes.io/zone=sto1\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=lab1-k8s-master-1\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\n                    node.kubernetes.io/instance-type=v1-c2-m8-d80\n                    topology.cinder.csi.openstack.org/zone=sto1\n                    topology.kubernetes.io/region=se-sto\n                    topology.kubernetes.io/zone=sto1\nAnnotations:        alpha.kubernetes.io/provided-node-ip: 10.128.0.115\n                    csi.volume.kubernetes.io/nodeid: {\"cinder.csi.openstack.org\":\"d5aeed0e-35ef-47b7-ac91-0bd6f2465088\"}\n                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.128.0.115/22\n                    projectcalico.org/IPv4IPIPTunnelAddr: 10.233.99.0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 01 Mar 2023 11:31:25 +0000\nTaints:             node-role.kubernetes.io/control-plane:NoSchedule\n                    node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  lab1-k8s-master-1\n  AcquireTime:     <unset>\n  RenewTime:       Wed, 01 Mar 2023 12:25:10 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 01 Mar 2023 11:36:55 +0000   Wed, 01 Mar 2023 11:36:55 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Wed, 01 Mar 2023 12:25:15 +0000   Wed, 01 Mar 2023 11:31:22 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Wed, 01 Mar 2023 12:25:15 +0000   Wed, 01 Mar 2023 11:31:22 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Wed, 01 Mar 2023 12:25:15 +0000   Wed, 01 Mar 2023 11:31:22 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Wed, 01 Mar 2023 12:25:15 +0000   Wed, 01 Mar 2023 11:38:58 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.128.0.115\n  ExternalIP:  91.197.42.235\n  Hostname:    lab1-k8s-master-1\nCapacity:\n  cpu:                2\n  ephemeral-storage:  40470732Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             8148660Ki\n  pods:               110\nAllocatable:\n  cpu:                1750m\n  ephemeral-storage:  37297826550\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             7784116Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 d5aeed0e35ef47b7ac910bd6f2465088\n  System UUID:                d5aeed0e-35ef-47b7-ac91-0bd6f2465088\n  Boot ID:                    8e80c318-2220-4937-b10c-eb37afaaddc3\n  Kernel Version:             5.4.0-113-generic\n  OS Image:                   Ubuntu 20.04.4 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.15\n  Kubelet Version:            v1.25.6\n  Kube-Proxy Version:         v1.25.6\nPodCIDR:                      10.233.64.0/24\nPodCIDRs:                     10.233.64.0/24\nProviderID:                   openstack:///d5aeed0e-35ef-47b7-ac91-0bd6f2465088\nNon-terminated Pods:          (9 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-node-9kd4x                                          150m (8%)     2 (114%)    64M (0%)         500M (6%)      49m\n  kube-system                 csi-cinder-nodeplugin-rwtv5                                0 (0%)        0 (0%)      0 (0%)           0 (0%)         46m\n  kube-system                 kube-apiserver-lab1-k8s-master-1                           250m (14%)    0 (0%)      0 (0%)           0 (0%)         53m\n  kube-system                 kube-controller-manager-lab1-k8s-master-1                  200m (11%)    0 (0%)      0 (0%)           0 (0%)         53m\n  kube-system                 kube-proxy-c8kwj                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         49m\n  kube-system                 kube-scheduler-lab1-k8s-master-1                           100m (5%)     0 (0%)      0 (0%)           0 (0%)         53m\n  kube-system                 nodelocaldns-m9pfl                                         100m (5%)     0 (0%)      70Mi (0%)        200Mi (2%)     47m\n  kube-system                 openstack-cloud-controller-manager-d6lcx                   200m (11%)    0 (0%)      0 (0%)           0 (0%)         48m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-1321f7b040854c53-fhnpr    0 (0%)        0 (0%)      0 (0%)           0 (0%)         43m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests        Limits\n  --------           --------        ------\n  cpu                1 (57%)         2 (114%)\n  memory             137400320 (1%)  709715200 (8%)\n  ephemeral-storage  0 (0%)          0 (0%)\n  hugepages-1Gi      0 (0%)          0 (0%)\n  hugepages-2Mi      0 (0%)          0 (0%)\nEvents:\n  Type     Reason                   Age                From                   Message\n  ----     ------                   ----               ----                   -------\n  Normal   Starting                 52m                kube-proxy             \n  Normal   Starting                 49m                kube-proxy             \n  Normal   NodeHasNoDiskPressure    53m (x5 over 53m)  kubelet                Node lab1-k8s-master-1 status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientMemory  53m (x6 over 53m)  kubelet                Node lab1-k8s-master-1 status is now: NodeHasSufficientMemory\n  Normal   NodeHasSufficientPID     53m (x5 over 53m)  kubelet                Node lab1-k8s-master-1 status is now: NodeHasSufficientPID\n  Warning  InvalidDiskCapacity      53m                kubelet                invalid capacity 0 on image filesystem\n  Normal   NodeAllocatableEnforced  53m                kubelet                Updated Node Allocatable limit across pods\n  Normal   NodeHasSufficientPID     53m                kubelet                Node lab1-k8s-master-1 status is now: NodeHasSufficientPID\n  Normal   NodeHasNoDiskPressure    53m                kubelet                Node lab1-k8s-master-1 status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientMemory  53m                kubelet                Node lab1-k8s-master-1 status is now: NodeHasSufficientMemory\n  Normal   Starting                 53m                kubelet                Starting kubelet.\n  Normal   RegisteredNode           52m                node-controller        Node lab1-k8s-master-1 event: Registered Node lab1-k8s-master-1 in Controller\n  Normal   RegisteredNode           51m                node-controller        Node lab1-k8s-master-1 event: Registered Node lab1-k8s-master-1 in Controller\n  Normal   NodeHasNoDiskPressure    50m                kubelet                Node lab1-k8s-master-1 status is now: NodeHasNoDiskPressure\n  Normal   Starting                 50m                kubelet                Starting kubelet.\n  Warning  InvalidDiskCapacity      50m                kubelet                invalid capacity 0 on image filesystem\n  Normal   NodeAllocatableEnforced  50m                kubelet                Updated Node Allocatable limit across pods\n  Normal   NodeHasSufficientPID     50m                kubelet                Node lab1-k8s-master-1 status is now: NodeHasSufficientPID\n  Normal   NodeHasSufficientMemory  50m                kubelet                Node lab1-k8s-master-1 status is now: NodeHasSufficientMemory\n  Normal   RegisteredNode           50m                node-controller        Node lab1-k8s-master-1 event: Registered Node lab1-k8s-master-1 in Controller\n  Normal   NodeReady                48m                kubelet                Node lab1-k8s-master-1 status is now: NodeReady\n  Normal   RegisteredNode           48m                node-controller        Node lab1-k8s-master-1 event: Registered Node lab1-k8s-master-1 in Controller\n  Normal   Synced                   48m                cloud-node-controller  Node synced successfully\n  Normal   RegisteredNode           46m                node-controller        Node lab1-k8s-master-1 event: Registered Node lab1-k8s-master-1 in Controller\n  Normal   Starting                 46m                kubelet                Starting kubelet.\n  Warning  InvalidDiskCapacity      46m                kubelet                invalid capacity 0 on image filesystem\n  Normal   NodeReady                46m                kubelet                Node lab1-k8s-master-1 status is now: NodeReady\n  Normal   NodeHasSufficientMemory  46m                kubelet                Node lab1-k8s-master-1 status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure    46m                kubelet                Node lab1-k8s-master-1 status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     46m                kubelet                Node lab1-k8s-master-1 status is now: NodeHasSufficientPID\n  Normal   NodeNotReady             46m                kubelet                Node lab1-k8s-master-1 status is now: NodeNotReady\n  Normal   NodeAllocatableEnforced  46m                kubelet                Updated Node Allocatable limit across pods\n  Normal   RegisteredNode           45m                node-controller        Node lab1-k8s-master-1 event: Registered Node lab1-k8s-master-1 in Controller\n"
    Mar  1 12:25:19.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-2293 describe namespace kubectl-2293'
    Mar  1 12:25:20.005: INFO: stderr: ""
    Mar  1 12:25:20.005: INFO: stdout: "Name:         kubectl-2293\nLabels:       e2e-framework=kubectl\n              e2e-run=8dccadbc-6480-4735-8e50-691d6b653465\n              kubernetes.io/metadata.name=kubectl-2293\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar  1 12:25:20.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-2293" for this suite. 03/01/23 12:25:20.012
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:25:20.023
Mar  1 12:25:20.023: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename svcaccounts 03/01/23 12:25:20.024
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:25:20.05
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:25:20.053
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
STEP: creating a ServiceAccount 03/01/23 12:25:20.055
STEP: watching for the ServiceAccount to be added 03/01/23 12:25:20.069
STEP: patching the ServiceAccount 03/01/23 12:25:20.071
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 03/01/23 12:25:20.08
STEP: deleting the ServiceAccount 03/01/23 12:25:20.086
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Mar  1 12:25:20.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2252" for this suite. 03/01/23 12:25:20.111
{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","completed":170,"skipped":3039,"failed":0}
------------------------------
â€¢ [0.097 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:25:20.023
    Mar  1 12:25:20.023: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename svcaccounts 03/01/23 12:25:20.024
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:25:20.05
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:25:20.053
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:646
    STEP: creating a ServiceAccount 03/01/23 12:25:20.055
    STEP: watching for the ServiceAccount to be added 03/01/23 12:25:20.069
    STEP: patching the ServiceAccount 03/01/23 12:25:20.071
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 03/01/23 12:25:20.08
    STEP: deleting the ServiceAccount 03/01/23 12:25:20.086
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Mar  1 12:25:20.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-2252" for this suite. 03/01/23 12:25:20.111
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:25:20.12
Mar  1 12:25:20.120: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename custom-resource-definition 03/01/23 12:25:20.121
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:25:20.146
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:25:20.15
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Mar  1 12:25:20.154: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  1 12:25:25.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3198" for this suite. 03/01/23 12:25:25.714
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","completed":171,"skipped":3050,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.607 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:25:20.12
    Mar  1 12:25:20.120: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename custom-resource-definition 03/01/23 12:25:20.121
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:25:20.146
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:25:20.15
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Mar  1 12:25:20.154: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  1 12:25:25.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-3198" for this suite. 03/01/23 12:25:25.714
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:25:25.73
Mar  1 12:25:25.730: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename kubectl 03/01/23 12:25:25.731
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:25:25.764
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:25:25.773
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1492
STEP: creating the pod 03/01/23 12:25:25.777
Mar  1 12:25:25.777: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-6166 create -f -'
Mar  1 12:25:26.589: INFO: stderr: ""
Mar  1 12:25:26.589: INFO: stdout: "pod/pause created\n"
Mar  1 12:25:26.589: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Mar  1 12:25:26.589: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-6166" to be "running and ready"
Mar  1 12:25:26.594: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 4.882018ms
Mar  1 12:25:26.594: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'lab1-k8s-node-3' to be 'Running' but was 'Pending'
Mar  1 12:25:28.598: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.009433282s
Mar  1 12:25:28.598: INFO: Pod "pause" satisfied condition "running and ready"
Mar  1 12:25:28.598: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
STEP: adding the label testing-label with value testing-label-value to a pod 03/01/23 12:25:28.598
Mar  1 12:25:28.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-6166 label pods pause testing-label=testing-label-value'
Mar  1 12:25:28.675: INFO: stderr: ""
Mar  1 12:25:28.675: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 03/01/23 12:25:28.675
Mar  1 12:25:28.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-6166 get pod pause -L testing-label'
Mar  1 12:25:28.739: INFO: stderr: ""
Mar  1 12:25:28.739: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod 03/01/23 12:25:28.739
Mar  1 12:25:28.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-6166 label pods pause testing-label-'
Mar  1 12:25:28.817: INFO: stderr: ""
Mar  1 12:25:28.817: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 03/01/23 12:25:28.817
Mar  1 12:25:28.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-6166 get pod pause -L testing-label'
Mar  1 12:25:28.883: INFO: stderr: ""
Mar  1 12:25:28.883: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1498
STEP: using delete to clean up resources 03/01/23 12:25:28.883
Mar  1 12:25:28.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-6166 delete --grace-period=0 --force -f -'
Mar  1 12:25:28.958: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  1 12:25:28.958: INFO: stdout: "pod \"pause\" force deleted\n"
Mar  1 12:25:28.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-6166 get rc,svc -l name=pause --no-headers'
Mar  1 12:25:29.028: INFO: stderr: "No resources found in kubectl-6166 namespace.\n"
Mar  1 12:25:29.028: INFO: stdout: ""
Mar  1 12:25:29.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-6166 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  1 12:25:29.088: INFO: stderr: ""
Mar  1 12:25:29.088: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar  1 12:25:29.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6166" for this suite. 03/01/23 12:25:29.097
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","completed":172,"skipped":3068,"failed":0}
------------------------------
â€¢ [3.376 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1490
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:25:25.73
    Mar  1 12:25:25.730: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename kubectl 03/01/23 12:25:25.731
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:25:25.764
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:25:25.773
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1492
    STEP: creating the pod 03/01/23 12:25:25.777
    Mar  1 12:25:25.777: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-6166 create -f -'
    Mar  1 12:25:26.589: INFO: stderr: ""
    Mar  1 12:25:26.589: INFO: stdout: "pod/pause created\n"
    Mar  1 12:25:26.589: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Mar  1 12:25:26.589: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-6166" to be "running and ready"
    Mar  1 12:25:26.594: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 4.882018ms
    Mar  1 12:25:26.594: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'lab1-k8s-node-3' to be 'Running' but was 'Pending'
    Mar  1 12:25:28.598: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.009433282s
    Mar  1 12:25:28.598: INFO: Pod "pause" satisfied condition "running and ready"
    Mar  1 12:25:28.598: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1507
    STEP: adding the label testing-label with value testing-label-value to a pod 03/01/23 12:25:28.598
    Mar  1 12:25:28.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-6166 label pods pause testing-label=testing-label-value'
    Mar  1 12:25:28.675: INFO: stderr: ""
    Mar  1 12:25:28.675: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 03/01/23 12:25:28.675
    Mar  1 12:25:28.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-6166 get pod pause -L testing-label'
    Mar  1 12:25:28.739: INFO: stderr: ""
    Mar  1 12:25:28.739: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 03/01/23 12:25:28.739
    Mar  1 12:25:28.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-6166 label pods pause testing-label-'
    Mar  1 12:25:28.817: INFO: stderr: ""
    Mar  1 12:25:28.817: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 03/01/23 12:25:28.817
    Mar  1 12:25:28.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-6166 get pod pause -L testing-label'
    Mar  1 12:25:28.883: INFO: stderr: ""
    Mar  1 12:25:28.883: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1498
    STEP: using delete to clean up resources 03/01/23 12:25:28.883
    Mar  1 12:25:28.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-6166 delete --grace-period=0 --force -f -'
    Mar  1 12:25:28.958: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar  1 12:25:28.958: INFO: stdout: "pod \"pause\" force deleted\n"
    Mar  1 12:25:28.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-6166 get rc,svc -l name=pause --no-headers'
    Mar  1 12:25:29.028: INFO: stderr: "No resources found in kubectl-6166 namespace.\n"
    Mar  1 12:25:29.028: INFO: stdout: ""
    Mar  1 12:25:29.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-6166 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Mar  1 12:25:29.088: INFO: stderr: ""
    Mar  1 12:25:29.088: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar  1 12:25:29.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-6166" for this suite. 03/01/23 12:25:29.097
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:25:29.108
Mar  1 12:25:29.108: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename podtemplate 03/01/23 12:25:29.108
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:25:29.137
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:25:29.141
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 03/01/23 12:25:29.144
Mar  1 12:25:29.152: INFO: created test-podtemplate-1
Mar  1 12:25:29.158: INFO: created test-podtemplate-2
Mar  1 12:25:29.165: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 03/01/23 12:25:29.165
STEP: delete collection of pod templates 03/01/23 12:25:29.171
Mar  1 12:25:29.171: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 03/01/23 12:25:29.196
Mar  1 12:25:29.196: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Mar  1 12:25:29.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-1102" for this suite. 03/01/23 12:25:29.207
{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","completed":173,"skipped":3071,"failed":0}
------------------------------
â€¢ [0.107 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:25:29.108
    Mar  1 12:25:29.108: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename podtemplate 03/01/23 12:25:29.108
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:25:29.137
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:25:29.141
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 03/01/23 12:25:29.144
    Mar  1 12:25:29.152: INFO: created test-podtemplate-1
    Mar  1 12:25:29.158: INFO: created test-podtemplate-2
    Mar  1 12:25:29.165: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 03/01/23 12:25:29.165
    STEP: delete collection of pod templates 03/01/23 12:25:29.171
    Mar  1 12:25:29.171: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 03/01/23 12:25:29.196
    Mar  1 12:25:29.196: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Mar  1 12:25:29.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-1102" for this suite. 03/01/23 12:25:29.207
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:25:29.218
Mar  1 12:25:29.218: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename secrets 03/01/23 12:25:29.219
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:25:29.248
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:25:29.252
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
STEP: Creating secret with name secret-test-map-e39a2447-c0b7-4fc6-9829-4df1c622b260 03/01/23 12:25:29.255
STEP: Creating a pod to test consume secrets 03/01/23 12:25:29.266
Mar  1 12:25:29.285: INFO: Waiting up to 5m0s for pod "pod-secrets-469e9d2c-d469-40ce-b079-bee2ebf364b6" in namespace "secrets-9204" to be "Succeeded or Failed"
Mar  1 12:25:29.292: INFO: Pod "pod-secrets-469e9d2c-d469-40ce-b079-bee2ebf364b6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.97424ms
Mar  1 12:25:31.297: INFO: Pod "pod-secrets-469e9d2c-d469-40ce-b079-bee2ebf364b6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012418592s
Mar  1 12:25:33.297: INFO: Pod "pod-secrets-469e9d2c-d469-40ce-b079-bee2ebf364b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012139927s
STEP: Saw pod success 03/01/23 12:25:33.297
Mar  1 12:25:33.297: INFO: Pod "pod-secrets-469e9d2c-d469-40ce-b079-bee2ebf364b6" satisfied condition "Succeeded or Failed"
Mar  1 12:25:33.301: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-secrets-469e9d2c-d469-40ce-b079-bee2ebf364b6 container secret-volume-test: <nil>
STEP: delete the pod 03/01/23 12:25:33.312
Mar  1 12:25:33.327: INFO: Waiting for pod pod-secrets-469e9d2c-d469-40ce-b079-bee2ebf364b6 to disappear
Mar  1 12:25:33.335: INFO: Pod pod-secrets-469e9d2c-d469-40ce-b079-bee2ebf364b6 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Mar  1 12:25:33.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9204" for this suite. 03/01/23 12:25:33.343
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":174,"skipped":3087,"failed":0}
------------------------------
â€¢ [4.133 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:25:29.218
    Mar  1 12:25:29.218: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename secrets 03/01/23 12:25:29.219
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:25:29.248
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:25:29.252
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:88
    STEP: Creating secret with name secret-test-map-e39a2447-c0b7-4fc6-9829-4df1c622b260 03/01/23 12:25:29.255
    STEP: Creating a pod to test consume secrets 03/01/23 12:25:29.266
    Mar  1 12:25:29.285: INFO: Waiting up to 5m0s for pod "pod-secrets-469e9d2c-d469-40ce-b079-bee2ebf364b6" in namespace "secrets-9204" to be "Succeeded or Failed"
    Mar  1 12:25:29.292: INFO: Pod "pod-secrets-469e9d2c-d469-40ce-b079-bee2ebf364b6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.97424ms
    Mar  1 12:25:31.297: INFO: Pod "pod-secrets-469e9d2c-d469-40ce-b079-bee2ebf364b6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012418592s
    Mar  1 12:25:33.297: INFO: Pod "pod-secrets-469e9d2c-d469-40ce-b079-bee2ebf364b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012139927s
    STEP: Saw pod success 03/01/23 12:25:33.297
    Mar  1 12:25:33.297: INFO: Pod "pod-secrets-469e9d2c-d469-40ce-b079-bee2ebf364b6" satisfied condition "Succeeded or Failed"
    Mar  1 12:25:33.301: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-secrets-469e9d2c-d469-40ce-b079-bee2ebf364b6 container secret-volume-test: <nil>
    STEP: delete the pod 03/01/23 12:25:33.312
    Mar  1 12:25:33.327: INFO: Waiting for pod pod-secrets-469e9d2c-d469-40ce-b079-bee2ebf364b6 to disappear
    Mar  1 12:25:33.335: INFO: Pod pod-secrets-469e9d2c-d469-40ce-b079-bee2ebf364b6 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Mar  1 12:25:33.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-9204" for this suite. 03/01/23 12:25:33.343
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:25:33.353
Mar  1 12:25:33.353: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename runtimeclass 03/01/23 12:25:33.354
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:25:33.376
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:25:33.382
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Mar  1 12:25:33.413: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-9306 to be scheduled
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Mar  1 12:25:33.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-9306" for this suite. 03/01/23 12:25:33.44
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]","completed":175,"skipped":3113,"failed":0}
------------------------------
â€¢ [0.096 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:25:33.353
    Mar  1 12:25:33.353: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename runtimeclass 03/01/23 12:25:33.354
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:25:33.376
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:25:33.382
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Mar  1 12:25:33.413: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-9306 to be scheduled
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Mar  1 12:25:33.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-9306" for this suite. 03/01/23 12:25:33.44
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:25:33.456
Mar  1 12:25:33.456: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename pod-network-test 03/01/23 12:25:33.456
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:25:33.479
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:25:33.483
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-8181 03/01/23 12:25:33.487
STEP: creating a selector 03/01/23 12:25:33.487
STEP: Creating the service pods in kubernetes 03/01/23 12:25:33.487
Mar  1 12:25:33.487: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar  1 12:25:33.532: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-8181" to be "running and ready"
Mar  1 12:25:33.537: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.756571ms
Mar  1 12:25:33.537: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar  1 12:25:35.545: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.012463213s
Mar  1 12:25:35.545: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  1 12:25:37.543: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.010197578s
Mar  1 12:25:37.543: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  1 12:25:39.543: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.010598769s
Mar  1 12:25:39.544: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  1 12:25:41.547: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.013893036s
Mar  1 12:25:41.547: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  1 12:25:43.543: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.010152371s
Mar  1 12:25:43.543: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  1 12:25:45.543: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.010174483s
Mar  1 12:25:45.543: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  1 12:25:47.544: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.011606358s
Mar  1 12:25:47.545: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  1 12:25:49.544: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.011542608s
Mar  1 12:25:49.544: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  1 12:25:51.542: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.009753658s
Mar  1 12:25:51.542: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  1 12:25:53.544: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.011164467s
Mar  1 12:25:53.544: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  1 12:25:55.543: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.010241878s
Mar  1 12:25:55.543: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Mar  1 12:25:55.543: INFO: Pod "netserver-0" satisfied condition "running and ready"
Mar  1 12:25:55.548: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-8181" to be "running and ready"
Mar  1 12:25:55.553: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.945348ms
Mar  1 12:25:55.553: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Mar  1 12:25:55.553: INFO: Pod "netserver-1" satisfied condition "running and ready"
Mar  1 12:25:55.557: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-8181" to be "running and ready"
Mar  1 12:25:55.561: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 4.0353ms
Mar  1 12:25:55.561: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Mar  1 12:25:55.561: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 03/01/23 12:25:55.566
Mar  1 12:25:55.572: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-8181" to be "running"
Mar  1 12:25:55.576: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.150553ms
Mar  1 12:25:57.582: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009305191s
Mar  1 12:25:57.582: INFO: Pod "test-container-pod" satisfied condition "running"
Mar  1 12:25:57.586: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Mar  1 12:25:57.586: INFO: Breadth first check of 10.233.95.142 on host 10.128.0.178...
Mar  1 12:25:57.591: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.74.240:9080/dial?request=hostname&protocol=http&host=10.233.95.142&port=8083&tries=1'] Namespace:pod-network-test-8181 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  1 12:25:57.591: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
Mar  1 12:25:57.592: INFO: ExecWithOptions: Clientset creation
Mar  1 12:25:57.592: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-8181/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.74.240%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.95.142%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar  1 12:25:57.670: INFO: Waiting for responses: map[]
Mar  1 12:25:57.670: INFO: reached 10.233.95.142 after 0/1 tries
Mar  1 12:25:57.670: INFO: Breadth first check of 10.233.64.118 on host 10.128.0.76...
Mar  1 12:25:57.676: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.74.240:9080/dial?request=hostname&protocol=http&host=10.233.64.118&port=8083&tries=1'] Namespace:pod-network-test-8181 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  1 12:25:57.676: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
Mar  1 12:25:57.676: INFO: ExecWithOptions: Clientset creation
Mar  1 12:25:57.676: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-8181/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.74.240%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.64.118%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar  1 12:25:57.749: INFO: Waiting for responses: map[]
Mar  1 12:25:57.749: INFO: reached 10.233.64.118 after 0/1 tries
Mar  1 12:25:57.750: INFO: Breadth first check of 10.233.74.239 on host 10.128.2.241...
Mar  1 12:25:57.755: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.74.240:9080/dial?request=hostname&protocol=http&host=10.233.74.239&port=8083&tries=1'] Namespace:pod-network-test-8181 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  1 12:25:57.755: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
Mar  1 12:25:57.756: INFO: ExecWithOptions: Clientset creation
Mar  1 12:25:57.756: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-8181/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.74.240%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.74.239%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar  1 12:25:57.836: INFO: Waiting for responses: map[]
Mar  1 12:25:57.836: INFO: reached 10.233.74.239 after 0/1 tries
Mar  1 12:25:57.836: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Mar  1 12:25:57.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8181" for this suite. 03/01/23 12:25:57.843
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","completed":176,"skipped":3142,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.398 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:25:33.456
    Mar  1 12:25:33.456: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename pod-network-test 03/01/23 12:25:33.456
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:25:33.479
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:25:33.483
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-8181 03/01/23 12:25:33.487
    STEP: creating a selector 03/01/23 12:25:33.487
    STEP: Creating the service pods in kubernetes 03/01/23 12:25:33.487
    Mar  1 12:25:33.487: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Mar  1 12:25:33.532: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-8181" to be "running and ready"
    Mar  1 12:25:33.537: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.756571ms
    Mar  1 12:25:33.537: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Mar  1 12:25:35.545: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.012463213s
    Mar  1 12:25:35.545: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  1 12:25:37.543: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.010197578s
    Mar  1 12:25:37.543: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  1 12:25:39.543: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.010598769s
    Mar  1 12:25:39.544: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  1 12:25:41.547: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.013893036s
    Mar  1 12:25:41.547: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  1 12:25:43.543: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.010152371s
    Mar  1 12:25:43.543: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  1 12:25:45.543: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.010174483s
    Mar  1 12:25:45.543: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  1 12:25:47.544: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.011606358s
    Mar  1 12:25:47.545: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  1 12:25:49.544: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.011542608s
    Mar  1 12:25:49.544: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  1 12:25:51.542: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.009753658s
    Mar  1 12:25:51.542: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  1 12:25:53.544: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.011164467s
    Mar  1 12:25:53.544: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  1 12:25:55.543: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.010241878s
    Mar  1 12:25:55.543: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Mar  1 12:25:55.543: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Mar  1 12:25:55.548: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-8181" to be "running and ready"
    Mar  1 12:25:55.553: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.945348ms
    Mar  1 12:25:55.553: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Mar  1 12:25:55.553: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Mar  1 12:25:55.557: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-8181" to be "running and ready"
    Mar  1 12:25:55.561: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 4.0353ms
    Mar  1 12:25:55.561: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Mar  1 12:25:55.561: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 03/01/23 12:25:55.566
    Mar  1 12:25:55.572: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-8181" to be "running"
    Mar  1 12:25:55.576: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.150553ms
    Mar  1 12:25:57.582: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009305191s
    Mar  1 12:25:57.582: INFO: Pod "test-container-pod" satisfied condition "running"
    Mar  1 12:25:57.586: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Mar  1 12:25:57.586: INFO: Breadth first check of 10.233.95.142 on host 10.128.0.178...
    Mar  1 12:25:57.591: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.74.240:9080/dial?request=hostname&protocol=http&host=10.233.95.142&port=8083&tries=1'] Namespace:pod-network-test-8181 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  1 12:25:57.591: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    Mar  1 12:25:57.592: INFO: ExecWithOptions: Clientset creation
    Mar  1 12:25:57.592: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-8181/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.74.240%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.95.142%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Mar  1 12:25:57.670: INFO: Waiting for responses: map[]
    Mar  1 12:25:57.670: INFO: reached 10.233.95.142 after 0/1 tries
    Mar  1 12:25:57.670: INFO: Breadth first check of 10.233.64.118 on host 10.128.0.76...
    Mar  1 12:25:57.676: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.74.240:9080/dial?request=hostname&protocol=http&host=10.233.64.118&port=8083&tries=1'] Namespace:pod-network-test-8181 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  1 12:25:57.676: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    Mar  1 12:25:57.676: INFO: ExecWithOptions: Clientset creation
    Mar  1 12:25:57.676: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-8181/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.74.240%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.64.118%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Mar  1 12:25:57.749: INFO: Waiting for responses: map[]
    Mar  1 12:25:57.749: INFO: reached 10.233.64.118 after 0/1 tries
    Mar  1 12:25:57.750: INFO: Breadth first check of 10.233.74.239 on host 10.128.2.241...
    Mar  1 12:25:57.755: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.74.240:9080/dial?request=hostname&protocol=http&host=10.233.74.239&port=8083&tries=1'] Namespace:pod-network-test-8181 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  1 12:25:57.755: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    Mar  1 12:25:57.756: INFO: ExecWithOptions: Clientset creation
    Mar  1 12:25:57.756: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-8181/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.74.240%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.74.239%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Mar  1 12:25:57.836: INFO: Waiting for responses: map[]
    Mar  1 12:25:57.836: INFO: reached 10.233.74.239 after 0/1 tries
    Mar  1 12:25:57.836: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Mar  1 12:25:57.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-8181" for this suite. 03/01/23 12:25:57.843
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:25:57.871
Mar  1 12:25:57.872: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename hostport 03/01/23 12:25:57.873
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:25:57.905
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:25:57.91
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 03/01/23 12:25:57.931
Mar  1 12:25:57.944: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-1696" to be "running and ready"
Mar  1 12:25:57.949: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.190155ms
Mar  1 12:25:57.949: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Mar  1 12:25:59.955: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.011315422s
Mar  1 12:25:59.955: INFO: The phase of Pod pod1 is Running (Ready = true)
Mar  1 12:25:59.955: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.128.0.178 on the node which pod1 resides and expect scheduled 03/01/23 12:25:59.956
Mar  1 12:25:59.966: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-1696" to be "running and ready"
Mar  1 12:25:59.971: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.358849ms
Mar  1 12:25:59.971: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Mar  1 12:26:01.979: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.013169004s
Mar  1 12:26:01.979: INFO: The phase of Pod pod2 is Running (Ready = true)
Mar  1 12:26:01.979: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.128.0.178 but use UDP protocol on the node which pod2 resides 03/01/23 12:26:01.979
Mar  1 12:26:01.988: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-1696" to be "running and ready"
Mar  1 12:26:01.999: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 10.51636ms
Mar  1 12:26:01.999: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Mar  1 12:26:04.005: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.016802836s
Mar  1 12:26:04.005: INFO: The phase of Pod pod3 is Running (Ready = true)
Mar  1 12:26:04.005: INFO: Pod "pod3" satisfied condition "running and ready"
Mar  1 12:26:04.015: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-1696" to be "running and ready"
Mar  1 12:26:04.024: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 9.465224ms
Mar  1 12:26:04.025: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Mar  1 12:26:06.030: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.015446393s
Mar  1 12:26:06.030: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Mar  1 12:26:06.031: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 03/01/23 12:26:06.035
Mar  1 12:26:06.035: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.128.0.178 http://127.0.0.1:54323/hostname] Namespace:hostport-1696 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  1 12:26:06.035: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
Mar  1 12:26:06.036: INFO: ExecWithOptions: Clientset creation
Mar  1 12:26:06.036: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-1696/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.128.0.178+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.128.0.178, port: 54323 03/01/23 12:26:06.114
Mar  1 12:26:06.114: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.128.0.178:54323/hostname] Namespace:hostport-1696 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  1 12:26:06.114: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
Mar  1 12:26:06.114: INFO: ExecWithOptions: Clientset creation
Mar  1 12:26:06.115: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-1696/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.128.0.178%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.128.0.178, port: 54323 UDP 03/01/23 12:26:06.191
Mar  1 12:26:06.191: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.128.0.178 54323] Namespace:hostport-1696 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  1 12:26:06.191: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
Mar  1 12:26:06.192: INFO: ExecWithOptions: Clientset creation
Mar  1 12:26:06.192: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-1696/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.128.0.178+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/framework.go:187
Mar  1 12:26:11.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-1696" for this suite. 03/01/23 12:26:11.285
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","completed":177,"skipped":3266,"failed":0}
------------------------------
â€¢ [SLOW TEST] [13.423 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:25:57.871
    Mar  1 12:25:57.872: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename hostport 03/01/23 12:25:57.873
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:25:57.905
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:25:57.91
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 03/01/23 12:25:57.931
    Mar  1 12:25:57.944: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-1696" to be "running and ready"
    Mar  1 12:25:57.949: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.190155ms
    Mar  1 12:25:57.949: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Mar  1 12:25:59.955: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.011315422s
    Mar  1 12:25:59.955: INFO: The phase of Pod pod1 is Running (Ready = true)
    Mar  1 12:25:59.955: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.128.0.178 on the node which pod1 resides and expect scheduled 03/01/23 12:25:59.956
    Mar  1 12:25:59.966: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-1696" to be "running and ready"
    Mar  1 12:25:59.971: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.358849ms
    Mar  1 12:25:59.971: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Mar  1 12:26:01.979: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.013169004s
    Mar  1 12:26:01.979: INFO: The phase of Pod pod2 is Running (Ready = true)
    Mar  1 12:26:01.979: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.128.0.178 but use UDP protocol on the node which pod2 resides 03/01/23 12:26:01.979
    Mar  1 12:26:01.988: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-1696" to be "running and ready"
    Mar  1 12:26:01.999: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 10.51636ms
    Mar  1 12:26:01.999: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Mar  1 12:26:04.005: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.016802836s
    Mar  1 12:26:04.005: INFO: The phase of Pod pod3 is Running (Ready = true)
    Mar  1 12:26:04.005: INFO: Pod "pod3" satisfied condition "running and ready"
    Mar  1 12:26:04.015: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-1696" to be "running and ready"
    Mar  1 12:26:04.024: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 9.465224ms
    Mar  1 12:26:04.025: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Mar  1 12:26:06.030: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.015446393s
    Mar  1 12:26:06.030: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Mar  1 12:26:06.031: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 03/01/23 12:26:06.035
    Mar  1 12:26:06.035: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.128.0.178 http://127.0.0.1:54323/hostname] Namespace:hostport-1696 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  1 12:26:06.035: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    Mar  1 12:26:06.036: INFO: ExecWithOptions: Clientset creation
    Mar  1 12:26:06.036: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-1696/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.128.0.178+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.128.0.178, port: 54323 03/01/23 12:26:06.114
    Mar  1 12:26:06.114: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.128.0.178:54323/hostname] Namespace:hostport-1696 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  1 12:26:06.114: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    Mar  1 12:26:06.114: INFO: ExecWithOptions: Clientset creation
    Mar  1 12:26:06.115: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-1696/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.128.0.178%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.128.0.178, port: 54323 UDP 03/01/23 12:26:06.191
    Mar  1 12:26:06.191: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.128.0.178 54323] Namespace:hostport-1696 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  1 12:26:06.191: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    Mar  1 12:26:06.192: INFO: ExecWithOptions: Clientset creation
    Mar  1 12:26:06.192: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-1696/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.128.0.178+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/framework.go:187
    Mar  1 12:26:11.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "hostport-1696" for this suite. 03/01/23 12:26:11.285
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:26:11.297
Mar  1 12:26:11.297: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename gc 03/01/23 12:26:11.298
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:26:11.321
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:26:11.325
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 03/01/23 12:26:11.329
STEP: Wait for the Deployment to create new ReplicaSet 03/01/23 12:26:11.335
STEP: delete the deployment 03/01/23 12:26:11.846
STEP: wait for all rs to be garbage collected 03/01/23 12:26:11.855
STEP: expected 0 rs, got 1 rs 03/01/23 12:26:11.873
STEP: expected 0 pods, got 2 pods 03/01/23 12:26:11.879
STEP: Gathering metrics 03/01/23 12:26:12.395
Mar  1 12:26:12.426: INFO: Waiting up to 5m0s for pod "kube-controller-manager-lab1-k8s-master-3" in namespace "kube-system" to be "running and ready"
Mar  1 12:26:12.432: INFO: Pod "kube-controller-manager-lab1-k8s-master-3": Phase="Running", Reason="", readiness=true. Elapsed: 5.862779ms
Mar  1 12:26:12.432: INFO: The phase of Pod kube-controller-manager-lab1-k8s-master-3 is Running (Ready = true)
Mar  1 12:26:12.432: INFO: Pod "kube-controller-manager-lab1-k8s-master-3" satisfied condition "running and ready"
Mar  1 12:26:12.479: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Mar  1 12:26:12.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5330" for this suite. 03/01/23 12:26:12.486
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","completed":178,"skipped":3272,"failed":0}
------------------------------
â€¢ [1.198 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:26:11.297
    Mar  1 12:26:11.297: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename gc 03/01/23 12:26:11.298
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:26:11.321
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:26:11.325
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 03/01/23 12:26:11.329
    STEP: Wait for the Deployment to create new ReplicaSet 03/01/23 12:26:11.335
    STEP: delete the deployment 03/01/23 12:26:11.846
    STEP: wait for all rs to be garbage collected 03/01/23 12:26:11.855
    STEP: expected 0 rs, got 1 rs 03/01/23 12:26:11.873
    STEP: expected 0 pods, got 2 pods 03/01/23 12:26:11.879
    STEP: Gathering metrics 03/01/23 12:26:12.395
    Mar  1 12:26:12.426: INFO: Waiting up to 5m0s for pod "kube-controller-manager-lab1-k8s-master-3" in namespace "kube-system" to be "running and ready"
    Mar  1 12:26:12.432: INFO: Pod "kube-controller-manager-lab1-k8s-master-3": Phase="Running", Reason="", readiness=true. Elapsed: 5.862779ms
    Mar  1 12:26:12.432: INFO: The phase of Pod kube-controller-manager-lab1-k8s-master-3 is Running (Ready = true)
    Mar  1 12:26:12.432: INFO: Pod "kube-controller-manager-lab1-k8s-master-3" satisfied condition "running and ready"
    Mar  1 12:26:12.479: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Mar  1 12:26:12.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-5330" for this suite. 03/01/23 12:26:12.486
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:26:12.495
Mar  1 12:26:12.495: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename gc 03/01/23 12:26:12.496
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:26:12.52
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:26:12.523
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 03/01/23 12:26:12.526
STEP: Wait for the Deployment to create new ReplicaSet 03/01/23 12:26:12.532
STEP: delete the deployment 03/01/23 12:26:13.059
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 03/01/23 12:26:13.071
STEP: Gathering metrics 03/01/23 12:26:13.606
Mar  1 12:26:13.630: INFO: Waiting up to 5m0s for pod "kube-controller-manager-lab1-k8s-master-3" in namespace "kube-system" to be "running and ready"
Mar  1 12:26:13.635: INFO: Pod "kube-controller-manager-lab1-k8s-master-3": Phase="Running", Reason="", readiness=true. Elapsed: 4.391745ms
Mar  1 12:26:13.635: INFO: The phase of Pod kube-controller-manager-lab1-k8s-master-3 is Running (Ready = true)
Mar  1 12:26:13.635: INFO: Pod "kube-controller-manager-lab1-k8s-master-3" satisfied condition "running and ready"
Mar  1 12:26:13.686: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Mar  1 12:26:13.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9599" for this suite. 03/01/23 12:26:13.694
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","completed":179,"skipped":3276,"failed":0}
------------------------------
â€¢ [1.210 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:26:12.495
    Mar  1 12:26:12.495: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename gc 03/01/23 12:26:12.496
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:26:12.52
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:26:12.523
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 03/01/23 12:26:12.526
    STEP: Wait for the Deployment to create new ReplicaSet 03/01/23 12:26:12.532
    STEP: delete the deployment 03/01/23 12:26:13.059
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 03/01/23 12:26:13.071
    STEP: Gathering metrics 03/01/23 12:26:13.606
    Mar  1 12:26:13.630: INFO: Waiting up to 5m0s for pod "kube-controller-manager-lab1-k8s-master-3" in namespace "kube-system" to be "running and ready"
    Mar  1 12:26:13.635: INFO: Pod "kube-controller-manager-lab1-k8s-master-3": Phase="Running", Reason="", readiness=true. Elapsed: 4.391745ms
    Mar  1 12:26:13.635: INFO: The phase of Pod kube-controller-manager-lab1-k8s-master-3 is Running (Ready = true)
    Mar  1 12:26:13.635: INFO: Pod "kube-controller-manager-lab1-k8s-master-3" satisfied condition "running and ready"
    Mar  1 12:26:13.686: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Mar  1 12:26:13.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-9599" for this suite. 03/01/23 12:26:13.694
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:26:13.706
Mar  1 12:26:13.707: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename downward-api 03/01/23 12:26:13.708
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:26:13.731
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:26:13.735
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
STEP: Creating a pod to test downward api env vars 03/01/23 12:26:13.738
Mar  1 12:26:13.749: INFO: Waiting up to 5m0s for pod "downward-api-3cae6c9d-7683-4d02-88a6-779bb2e0429b" in namespace "downward-api-3203" to be "Succeeded or Failed"
Mar  1 12:26:13.755: INFO: Pod "downward-api-3cae6c9d-7683-4d02-88a6-779bb2e0429b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.167589ms
Mar  1 12:26:15.760: INFO: Pod "downward-api-3cae6c9d-7683-4d02-88a6-779bb2e0429b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010776273s
Mar  1 12:26:17.760: INFO: Pod "downward-api-3cae6c9d-7683-4d02-88a6-779bb2e0429b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010760001s
STEP: Saw pod success 03/01/23 12:26:17.76
Mar  1 12:26:17.761: INFO: Pod "downward-api-3cae6c9d-7683-4d02-88a6-779bb2e0429b" satisfied condition "Succeeded or Failed"
Mar  1 12:26:17.771: INFO: Trying to get logs from node lab1-k8s-node-3 pod downward-api-3cae6c9d-7683-4d02-88a6-779bb2e0429b container dapi-container: <nil>
STEP: delete the pod 03/01/23 12:26:17.78
Mar  1 12:26:17.804: INFO: Waiting for pod downward-api-3cae6c9d-7683-4d02-88a6-779bb2e0429b to disappear
Mar  1 12:26:17.810: INFO: Pod downward-api-3cae6c9d-7683-4d02-88a6-779bb2e0429b no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Mar  1 12:26:17.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3203" for this suite. 03/01/23 12:26:17.823
{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","completed":180,"skipped":3283,"failed":0}
------------------------------
â€¢ [4.129 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:26:13.706
    Mar  1 12:26:13.707: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename downward-api 03/01/23 12:26:13.708
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:26:13.731
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:26:13.735
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:266
    STEP: Creating a pod to test downward api env vars 03/01/23 12:26:13.738
    Mar  1 12:26:13.749: INFO: Waiting up to 5m0s for pod "downward-api-3cae6c9d-7683-4d02-88a6-779bb2e0429b" in namespace "downward-api-3203" to be "Succeeded or Failed"
    Mar  1 12:26:13.755: INFO: Pod "downward-api-3cae6c9d-7683-4d02-88a6-779bb2e0429b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.167589ms
    Mar  1 12:26:15.760: INFO: Pod "downward-api-3cae6c9d-7683-4d02-88a6-779bb2e0429b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010776273s
    Mar  1 12:26:17.760: INFO: Pod "downward-api-3cae6c9d-7683-4d02-88a6-779bb2e0429b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010760001s
    STEP: Saw pod success 03/01/23 12:26:17.76
    Mar  1 12:26:17.761: INFO: Pod "downward-api-3cae6c9d-7683-4d02-88a6-779bb2e0429b" satisfied condition "Succeeded or Failed"
    Mar  1 12:26:17.771: INFO: Trying to get logs from node lab1-k8s-node-3 pod downward-api-3cae6c9d-7683-4d02-88a6-779bb2e0429b container dapi-container: <nil>
    STEP: delete the pod 03/01/23 12:26:17.78
    Mar  1 12:26:17.804: INFO: Waiting for pod downward-api-3cae6c9d-7683-4d02-88a6-779bb2e0429b to disappear
    Mar  1 12:26:17.810: INFO: Pod downward-api-3cae6c9d-7683-4d02-88a6-779bb2e0429b no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Mar  1 12:26:17.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-3203" for this suite. 03/01/23 12:26:17.823
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:26:17.84
Mar  1 12:26:17.841: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename projected 03/01/23 12:26:17.841
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:26:17.879
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:26:17.883
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
STEP: Creating configMap with name projected-configmap-test-volume-map-5650ad0c-02e4-441c-bd00-4cc989192743 03/01/23 12:26:17.886
STEP: Creating a pod to test consume configMaps 03/01/23 12:26:17.894
Mar  1 12:26:17.916: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-43275fc4-e764-491c-bfcb-45b02bc73005" in namespace "projected-1949" to be "Succeeded or Failed"
Mar  1 12:26:17.925: INFO: Pod "pod-projected-configmaps-43275fc4-e764-491c-bfcb-45b02bc73005": Phase="Pending", Reason="", readiness=false. Elapsed: 9.318476ms
Mar  1 12:26:19.930: INFO: Pod "pod-projected-configmaps-43275fc4-e764-491c-bfcb-45b02bc73005": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014781008s
Mar  1 12:26:21.932: INFO: Pod "pod-projected-configmaps-43275fc4-e764-491c-bfcb-45b02bc73005": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016453303s
STEP: Saw pod success 03/01/23 12:26:21.932
Mar  1 12:26:21.932: INFO: Pod "pod-projected-configmaps-43275fc4-e764-491c-bfcb-45b02bc73005" satisfied condition "Succeeded or Failed"
Mar  1 12:26:21.938: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-projected-configmaps-43275fc4-e764-491c-bfcb-45b02bc73005 container agnhost-container: <nil>
STEP: delete the pod 03/01/23 12:26:21.947
Mar  1 12:26:21.961: INFO: Waiting for pod pod-projected-configmaps-43275fc4-e764-491c-bfcb-45b02bc73005 to disappear
Mar  1 12:26:21.965: INFO: Pod pod-projected-configmaps-43275fc4-e764-491c-bfcb-45b02bc73005 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Mar  1 12:26:21.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1949" for this suite. 03/01/23 12:26:21.973
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":181,"skipped":3310,"failed":0}
------------------------------
â€¢ [4.142 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:26:17.84
    Mar  1 12:26:17.841: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename projected 03/01/23 12:26:17.841
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:26:17.879
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:26:17.883
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:108
    STEP: Creating configMap with name projected-configmap-test-volume-map-5650ad0c-02e4-441c-bd00-4cc989192743 03/01/23 12:26:17.886
    STEP: Creating a pod to test consume configMaps 03/01/23 12:26:17.894
    Mar  1 12:26:17.916: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-43275fc4-e764-491c-bfcb-45b02bc73005" in namespace "projected-1949" to be "Succeeded or Failed"
    Mar  1 12:26:17.925: INFO: Pod "pod-projected-configmaps-43275fc4-e764-491c-bfcb-45b02bc73005": Phase="Pending", Reason="", readiness=false. Elapsed: 9.318476ms
    Mar  1 12:26:19.930: INFO: Pod "pod-projected-configmaps-43275fc4-e764-491c-bfcb-45b02bc73005": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014781008s
    Mar  1 12:26:21.932: INFO: Pod "pod-projected-configmaps-43275fc4-e764-491c-bfcb-45b02bc73005": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016453303s
    STEP: Saw pod success 03/01/23 12:26:21.932
    Mar  1 12:26:21.932: INFO: Pod "pod-projected-configmaps-43275fc4-e764-491c-bfcb-45b02bc73005" satisfied condition "Succeeded or Failed"
    Mar  1 12:26:21.938: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-projected-configmaps-43275fc4-e764-491c-bfcb-45b02bc73005 container agnhost-container: <nil>
    STEP: delete the pod 03/01/23 12:26:21.947
    Mar  1 12:26:21.961: INFO: Waiting for pod pod-projected-configmaps-43275fc4-e764-491c-bfcb-45b02bc73005 to disappear
    Mar  1 12:26:21.965: INFO: Pod pod-projected-configmaps-43275fc4-e764-491c-bfcb-45b02bc73005 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Mar  1 12:26:21.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1949" for this suite. 03/01/23 12:26:21.973
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:26:21.985
Mar  1 12:26:21.985: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename deployment 03/01/23 12:26:21.985
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:26:22.007
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:26:22.011
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Mar  1 12:26:22.025: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Mar  1 12:26:27.032: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 03/01/23 12:26:27.033
Mar  1 12:26:27.033: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 03/01/23 12:26:27.048
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar  1 12:26:27.064: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-7242  40abff5b-1c5f-45db-abb1-4816d59ddb76 28336 1 2023-03-01 12:26:27 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-03-01 12:26:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00402ebb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Mar  1 12:26:27.070: INFO: New ReplicaSet "test-cleanup-deployment-69cb9c5497" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-69cb9c5497  deployment-7242  5e0f1f45-1926-4f0d-bf69-6a98af79eed9 28339 1 2023-03-01 12:26:27 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 40abff5b-1c5f-45db-abb1-4816d59ddb76 0xc00402f1b7 0xc00402f1b8}] [] [{kube-controller-manager Update apps/v1 2023-03-01 12:26:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"40abff5b-1c5f-45db-abb1-4816d59ddb76\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 69cb9c5497,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00402f248 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  1 12:26:27.070: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Mar  1 12:26:27.071: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-7242  0b032549-a7f6-4de0-b9b9-051e37a5f12c 28338 1 2023-03-01 12:26:22 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 40abff5b-1c5f-45db-abb1-4816d59ddb76 0xc00402f067 0xc00402f068}] [] [{e2e.test Update apps/v1 2023-03-01 12:26:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-01 12:26:23 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-03-01 12:26:27 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"40abff5b-1c5f-45db-abb1-4816d59ddb76\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00402f148 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar  1 12:26:27.079: INFO: Pod "test-cleanup-controller-v69rl" is available:
&Pod{ObjectMeta:{test-cleanup-controller-v69rl test-cleanup-controller- deployment-7242  dfa06efa-c742-456c-b0a3-63cb646915be 28320 0 2023-03-01 12:26:22 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:651a0ca78de61c64c6cca44a0da57a435bec99b4e6e7e05085d5cef5ba828502 cni.projectcalico.org/podIP:10.233.74.245/32 cni.projectcalico.org/podIPs:10.233.74.245/32] [{apps/v1 ReplicaSet test-cleanup-controller 0b032549-a7f6-4de0-b9b9-051e37a5f12c 0xc00402f867 0xc00402f868}] [] [{calico Update v1 2023-03-01 12:26:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-01 12:26:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0b032549-a7f6-4de0-b9b9-051e37a5f12c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-01 12:26:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.74.245\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9z9zv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9z9zv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 12:26:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 12:26:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 12:26:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 12:26:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.2.241,PodIP:10.233.74.245,StartTime:2023-03-01 12:26:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-01 12:26:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://ec67e5ab95e0f94800c243e2273aa2f76c68a746df68caab07b902fe8f5962b0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.74.245,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 12:26:27.079: INFO: Pod "test-cleanup-deployment-69cb9c5497-bt6nj" is not available:
&Pod{ObjectMeta:{test-cleanup-deployment-69cb9c5497-bt6nj test-cleanup-deployment-69cb9c5497- deployment-7242  8ba5648e-3e27-418d-a725-fe5d0c2e313b 28340 0 2023-03-01 12:26:27 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-69cb9c5497 5e0f1f45-1926-4f0d-bf69-6a98af79eed9 0xc00402fa77 0xc00402fa78}] [] [{kube-controller-manager Update v1 2023-03-01 12:26:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5e0f1f45-1926-4f0d-bf69-6a98af79eed9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qn54l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qn54l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Mar  1 12:26:27.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7242" for this suite. 03/01/23 12:26:27.092
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","completed":182,"skipped":3319,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.123 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:26:21.985
    Mar  1 12:26:21.985: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename deployment 03/01/23 12:26:21.985
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:26:22.007
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:26:22.011
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Mar  1 12:26:22.025: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Mar  1 12:26:27.032: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 03/01/23 12:26:27.033
    Mar  1 12:26:27.033: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 03/01/23 12:26:27.048
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar  1 12:26:27.064: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-7242  40abff5b-1c5f-45db-abb1-4816d59ddb76 28336 1 2023-03-01 12:26:27 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-03-01 12:26:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00402ebb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

    Mar  1 12:26:27.070: INFO: New ReplicaSet "test-cleanup-deployment-69cb9c5497" of Deployment "test-cleanup-deployment":
    &ReplicaSet{ObjectMeta:{test-cleanup-deployment-69cb9c5497  deployment-7242  5e0f1f45-1926-4f0d-bf69-6a98af79eed9 28339 1 2023-03-01 12:26:27 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 40abff5b-1c5f-45db-abb1-4816d59ddb76 0xc00402f1b7 0xc00402f1b8}] [] [{kube-controller-manager Update apps/v1 2023-03-01 12:26:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"40abff5b-1c5f-45db-abb1-4816d59ddb76\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 69cb9c5497,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00402f248 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Mar  1 12:26:27.070: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
    Mar  1 12:26:27.071: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-7242  0b032549-a7f6-4de0-b9b9-051e37a5f12c 28338 1 2023-03-01 12:26:22 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 40abff5b-1c5f-45db-abb1-4816d59ddb76 0xc00402f067 0xc00402f068}] [] [{e2e.test Update apps/v1 2023-03-01 12:26:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-01 12:26:23 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-03-01 12:26:27 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"40abff5b-1c5f-45db-abb1-4816d59ddb76\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00402f148 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Mar  1 12:26:27.079: INFO: Pod "test-cleanup-controller-v69rl" is available:
    &Pod{ObjectMeta:{test-cleanup-controller-v69rl test-cleanup-controller- deployment-7242  dfa06efa-c742-456c-b0a3-63cb646915be 28320 0 2023-03-01 12:26:22 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:651a0ca78de61c64c6cca44a0da57a435bec99b4e6e7e05085d5cef5ba828502 cni.projectcalico.org/podIP:10.233.74.245/32 cni.projectcalico.org/podIPs:10.233.74.245/32] [{apps/v1 ReplicaSet test-cleanup-controller 0b032549-a7f6-4de0-b9b9-051e37a5f12c 0xc00402f867 0xc00402f868}] [] [{calico Update v1 2023-03-01 12:26:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-01 12:26:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0b032549-a7f6-4de0-b9b9-051e37a5f12c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-01 12:26:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.74.245\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9z9zv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9z9zv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 12:26:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 12:26:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 12:26:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 12:26:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.2.241,PodIP:10.233.74.245,StartTime:2023-03-01 12:26:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-01 12:26:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://ec67e5ab95e0f94800c243e2273aa2f76c68a746df68caab07b902fe8f5962b0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.74.245,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  1 12:26:27.079: INFO: Pod "test-cleanup-deployment-69cb9c5497-bt6nj" is not available:
    &Pod{ObjectMeta:{test-cleanup-deployment-69cb9c5497-bt6nj test-cleanup-deployment-69cb9c5497- deployment-7242  8ba5648e-3e27-418d-a725-fe5d0c2e313b 28340 0 2023-03-01 12:26:27 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-69cb9c5497 5e0f1f45-1926-4f0d-bf69-6a98af79eed9 0xc00402fa77 0xc00402fa78}] [] [{kube-controller-manager Update v1 2023-03-01 12:26:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5e0f1f45-1926-4f0d-bf69-6a98af79eed9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qn54l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qn54l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Mar  1 12:26:27.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-7242" for this suite. 03/01/23 12:26:27.092
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:26:27.11
Mar  1 12:26:27.110: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename statefulset 03/01/23 12:26:27.111
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:26:27.133
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:26:27.137
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-1387 03/01/23 12:26:27.14
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
STEP: Creating statefulset ss in namespace statefulset-1387 03/01/23 12:26:27.149
Mar  1 12:26:27.168: INFO: Found 0 stateful pods, waiting for 1
Mar  1 12:26:37.175: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 03/01/23 12:26:37.184
STEP: updating a scale subresource 03/01/23 12:26:37.189
STEP: verifying the statefulset Spec.Replicas was modified 03/01/23 12:26:37.196
STEP: Patch a scale subresource 03/01/23 12:26:37.204
STEP: verifying the statefulset Spec.Replicas was modified 03/01/23 12:26:37.217
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Mar  1 12:26:37.229: INFO: Deleting all statefulset in ns statefulset-1387
Mar  1 12:26:37.233: INFO: Scaling statefulset ss to 0
Mar  1 12:26:47.269: INFO: Waiting for statefulset status.replicas updated to 0
Mar  1 12:26:47.274: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Mar  1 12:26:47.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1387" for this suite. 03/01/23 12:26:47.3
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","completed":183,"skipped":3328,"failed":0}
------------------------------
â€¢ [SLOW TEST] [20.199 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:846

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:26:27.11
    Mar  1 12:26:27.110: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename statefulset 03/01/23 12:26:27.111
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:26:27.133
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:26:27.137
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-1387 03/01/23 12:26:27.14
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:846
    STEP: Creating statefulset ss in namespace statefulset-1387 03/01/23 12:26:27.149
    Mar  1 12:26:27.168: INFO: Found 0 stateful pods, waiting for 1
    Mar  1 12:26:37.175: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 03/01/23 12:26:37.184
    STEP: updating a scale subresource 03/01/23 12:26:37.189
    STEP: verifying the statefulset Spec.Replicas was modified 03/01/23 12:26:37.196
    STEP: Patch a scale subresource 03/01/23 12:26:37.204
    STEP: verifying the statefulset Spec.Replicas was modified 03/01/23 12:26:37.217
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Mar  1 12:26:37.229: INFO: Deleting all statefulset in ns statefulset-1387
    Mar  1 12:26:37.233: INFO: Scaling statefulset ss to 0
    Mar  1 12:26:47.269: INFO: Waiting for statefulset status.replicas updated to 0
    Mar  1 12:26:47.274: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Mar  1 12:26:47.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-1387" for this suite. 03/01/23 12:26:47.3
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:26:47.311
Mar  1 12:26:47.312: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename taint-single-pod 03/01/23 12:26:47.312
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:26:47.348
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:26:47.352
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:166
Mar  1 12:26:47.355: INFO: Waiting up to 1m0s for all nodes to be ready
Mar  1 12:27:47.399: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
Mar  1 12:27:47.404: INFO: Starting informer...
STEP: Starting pod... 03/01/23 12:27:47.404
Mar  1 12:27:47.627: INFO: Pod is running on lab1-k8s-node-3. Tainting Node
STEP: Trying to apply a taint on the Node 03/01/23 12:27:47.627
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/01/23 12:27:47.646
STEP: Waiting short time to make sure Pod is queued for deletion 03/01/23 12:27:47.654
Mar  1 12:27:47.655: INFO: Pod wasn't evicted. Proceeding
Mar  1 12:27:47.655: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/01/23 12:27:47.673
STEP: Waiting some time to make sure that toleration time passed. 03/01/23 12:27:47.684
Mar  1 12:29:02.685: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:187
Mar  1 12:29:02.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-2528" for this suite. 03/01/23 12:29:02.695
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","completed":184,"skipped":3350,"failed":0}
------------------------------
â€¢ [SLOW TEST] [135.394 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:26:47.311
    Mar  1 12:26:47.312: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename taint-single-pod 03/01/23 12:26:47.312
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:26:47.348
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:26:47.352
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:166
    Mar  1 12:26:47.355: INFO: Waiting up to 1m0s for all nodes to be ready
    Mar  1 12:27:47.399: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:289
    Mar  1 12:27:47.404: INFO: Starting informer...
    STEP: Starting pod... 03/01/23 12:27:47.404
    Mar  1 12:27:47.627: INFO: Pod is running on lab1-k8s-node-3. Tainting Node
    STEP: Trying to apply a taint on the Node 03/01/23 12:27:47.627
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/01/23 12:27:47.646
    STEP: Waiting short time to make sure Pod is queued for deletion 03/01/23 12:27:47.654
    Mar  1 12:27:47.655: INFO: Pod wasn't evicted. Proceeding
    Mar  1 12:27:47.655: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/01/23 12:27:47.673
    STEP: Waiting some time to make sure that toleration time passed. 03/01/23 12:27:47.684
    Mar  1 12:29:02.685: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:187
    Mar  1 12:29:02.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "taint-single-pod-2528" for this suite. 03/01/23 12:29:02.695
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:29:02.706
Mar  1 12:29:02.706: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename crd-publish-openapi 03/01/23 12:29:02.706
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:29:02.732
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:29:02.735
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
Mar  1 12:29:02.738: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 03/01/23 12:29:10.494
Mar  1 12:29:10.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=crd-publish-openapi-9890 --namespace=crd-publish-openapi-9890 create -f -'
Mar  1 12:29:11.131: INFO: stderr: ""
Mar  1 12:29:11.131: INFO: stdout: "e2e-test-crd-publish-openapi-5095-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Mar  1 12:29:11.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=crd-publish-openapi-9890 --namespace=crd-publish-openapi-9890 delete e2e-test-crd-publish-openapi-5095-crds test-cr'
Mar  1 12:29:11.235: INFO: stderr: ""
Mar  1 12:29:11.235: INFO: stdout: "e2e-test-crd-publish-openapi-5095-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Mar  1 12:29:11.235: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=crd-publish-openapi-9890 --namespace=crd-publish-openapi-9890 apply -f -'
Mar  1 12:29:11.444: INFO: stderr: ""
Mar  1 12:29:11.444: INFO: stdout: "e2e-test-crd-publish-openapi-5095-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Mar  1 12:29:11.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=crd-publish-openapi-9890 --namespace=crd-publish-openapi-9890 delete e2e-test-crd-publish-openapi-5095-crds test-cr'
Mar  1 12:29:11.530: INFO: stderr: ""
Mar  1 12:29:11.530: INFO: stdout: "e2e-test-crd-publish-openapi-5095-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 03/01/23 12:29:11.53
Mar  1 12:29:11.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=crd-publish-openapi-9890 explain e2e-test-crd-publish-openapi-5095-crds'
Mar  1 12:29:12.070: INFO: stderr: ""
Mar  1 12:29:12.070: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5095-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  1 12:29:14.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9890" for this suite. 03/01/23 12:29:14.782
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","completed":185,"skipped":3350,"failed":0}
------------------------------
â€¢ [SLOW TEST] [12.085 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:29:02.706
    Mar  1 12:29:02.706: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename crd-publish-openapi 03/01/23 12:29:02.706
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:29:02.732
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:29:02.735
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:152
    Mar  1 12:29:02.738: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 03/01/23 12:29:10.494
    Mar  1 12:29:10.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=crd-publish-openapi-9890 --namespace=crd-publish-openapi-9890 create -f -'
    Mar  1 12:29:11.131: INFO: stderr: ""
    Mar  1 12:29:11.131: INFO: stdout: "e2e-test-crd-publish-openapi-5095-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Mar  1 12:29:11.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=crd-publish-openapi-9890 --namespace=crd-publish-openapi-9890 delete e2e-test-crd-publish-openapi-5095-crds test-cr'
    Mar  1 12:29:11.235: INFO: stderr: ""
    Mar  1 12:29:11.235: INFO: stdout: "e2e-test-crd-publish-openapi-5095-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Mar  1 12:29:11.235: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=crd-publish-openapi-9890 --namespace=crd-publish-openapi-9890 apply -f -'
    Mar  1 12:29:11.444: INFO: stderr: ""
    Mar  1 12:29:11.444: INFO: stdout: "e2e-test-crd-publish-openapi-5095-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Mar  1 12:29:11.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=crd-publish-openapi-9890 --namespace=crd-publish-openapi-9890 delete e2e-test-crd-publish-openapi-5095-crds test-cr'
    Mar  1 12:29:11.530: INFO: stderr: ""
    Mar  1 12:29:11.530: INFO: stdout: "e2e-test-crd-publish-openapi-5095-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 03/01/23 12:29:11.53
    Mar  1 12:29:11.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=crd-publish-openapi-9890 explain e2e-test-crd-publish-openapi-5095-crds'
    Mar  1 12:29:12.070: INFO: stderr: ""
    Mar  1 12:29:12.070: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5095-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  1 12:29:14.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-9890" for this suite. 03/01/23 12:29:14.782
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:29:14.793
Mar  1 12:29:14.793: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename downward-api 03/01/23 12:29:14.794
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:29:14.818
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:29:14.823
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
STEP: Creating a pod to test downward API volume plugin 03/01/23 12:29:14.825
Mar  1 12:29:14.837: INFO: Waiting up to 5m0s for pod "downwardapi-volume-de3d04af-6376-45bb-93fb-871a0fdcd16c" in namespace "downward-api-373" to be "Succeeded or Failed"
Mar  1 12:29:14.844: INFO: Pod "downwardapi-volume-de3d04af-6376-45bb-93fb-871a0fdcd16c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.91327ms
Mar  1 12:29:16.850: INFO: Pod "downwardapi-volume-de3d04af-6376-45bb-93fb-871a0fdcd16c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012587843s
Mar  1 12:29:18.850: INFO: Pod "downwardapi-volume-de3d04af-6376-45bb-93fb-871a0fdcd16c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012710182s
STEP: Saw pod success 03/01/23 12:29:18.85
Mar  1 12:29:18.850: INFO: Pod "downwardapi-volume-de3d04af-6376-45bb-93fb-871a0fdcd16c" satisfied condition "Succeeded or Failed"
Mar  1 12:29:18.855: INFO: Trying to get logs from node lab1-k8s-node-3 pod downwardapi-volume-de3d04af-6376-45bb-93fb-871a0fdcd16c container client-container: <nil>
STEP: delete the pod 03/01/23 12:29:18.876
Mar  1 12:29:18.891: INFO: Waiting for pod downwardapi-volume-de3d04af-6376-45bb-93fb-871a0fdcd16c to disappear
Mar  1 12:29:18.895: INFO: Pod downwardapi-volume-de3d04af-6376-45bb-93fb-871a0fdcd16c no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Mar  1 12:29:18.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-373" for this suite. 03/01/23 12:29:18.903
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","completed":186,"skipped":3370,"failed":0}
------------------------------
â€¢ [4.119 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:29:14.793
    Mar  1 12:29:14.793: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename downward-api 03/01/23 12:29:14.794
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:29:14.818
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:29:14.823
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:234
    STEP: Creating a pod to test downward API volume plugin 03/01/23 12:29:14.825
    Mar  1 12:29:14.837: INFO: Waiting up to 5m0s for pod "downwardapi-volume-de3d04af-6376-45bb-93fb-871a0fdcd16c" in namespace "downward-api-373" to be "Succeeded or Failed"
    Mar  1 12:29:14.844: INFO: Pod "downwardapi-volume-de3d04af-6376-45bb-93fb-871a0fdcd16c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.91327ms
    Mar  1 12:29:16.850: INFO: Pod "downwardapi-volume-de3d04af-6376-45bb-93fb-871a0fdcd16c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012587843s
    Mar  1 12:29:18.850: INFO: Pod "downwardapi-volume-de3d04af-6376-45bb-93fb-871a0fdcd16c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012710182s
    STEP: Saw pod success 03/01/23 12:29:18.85
    Mar  1 12:29:18.850: INFO: Pod "downwardapi-volume-de3d04af-6376-45bb-93fb-871a0fdcd16c" satisfied condition "Succeeded or Failed"
    Mar  1 12:29:18.855: INFO: Trying to get logs from node lab1-k8s-node-3 pod downwardapi-volume-de3d04af-6376-45bb-93fb-871a0fdcd16c container client-container: <nil>
    STEP: delete the pod 03/01/23 12:29:18.876
    Mar  1 12:29:18.891: INFO: Waiting for pod downwardapi-volume-de3d04af-6376-45bb-93fb-871a0fdcd16c to disappear
    Mar  1 12:29:18.895: INFO: Pod downwardapi-volume-de3d04af-6376-45bb-93fb-871a0fdcd16c no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Mar  1 12:29:18.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-373" for this suite. 03/01/23 12:29:18.903
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:29:18.914
Mar  1 12:29:18.914: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename job 03/01/23 12:29:18.915
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:29:18.936
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:29:18.939
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
STEP: Creating a job 03/01/23 12:29:18.941
STEP: Ensure pods equal to paralellism count is attached to the job 03/01/23 12:29:18.948
STEP: patching /status 03/01/23 12:29:20.954
STEP: updating /status 03/01/23 12:29:20.963
STEP: get /status 03/01/23 12:29:20.997
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Mar  1 12:29:21.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7124" for this suite. 03/01/23 12:29:21.009
{"msg":"PASSED [sig-apps] Job should apply changes to a job status [Conformance]","completed":187,"skipped":3376,"failed":0}
------------------------------
â€¢ [2.107 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:29:18.914
    Mar  1 12:29:18.914: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename job 03/01/23 12:29:18.915
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:29:18.936
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:29:18.939
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:464
    STEP: Creating a job 03/01/23 12:29:18.941
    STEP: Ensure pods equal to paralellism count is attached to the job 03/01/23 12:29:18.948
    STEP: patching /status 03/01/23 12:29:20.954
    STEP: updating /status 03/01/23 12:29:20.963
    STEP: get /status 03/01/23 12:29:20.997
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Mar  1 12:29:21.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-7124" for this suite. 03/01/23 12:29:21.009
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:29:21.024
Mar  1 12:29:21.024: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename statefulset 03/01/23 12:29:21.025
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:29:21.048
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:29:21.051
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-1401 03/01/23 12:29:21.057
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
STEP: Creating a new StatefulSet 03/01/23 12:29:21.064
Mar  1 12:29:21.078: INFO: Found 0 stateful pods, waiting for 3
Mar  1 12:29:31.083: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  1 12:29:31.084: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  1 12:29:31.084: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Mar  1 12:29:31.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=statefulset-1401 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  1 12:29:31.238: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  1 12:29:31.238: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  1 12:29:31.238: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 03/01/23 12:29:41.259
Mar  1 12:29:41.281: INFO: Updating stateful set ss2
STEP: Creating a new revision 03/01/23 12:29:41.281
STEP: Updating Pods in reverse ordinal order 03/01/23 12:29:51.298
Mar  1 12:29:51.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=statefulset-1401 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 12:29:51.439: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  1 12:29:51.439: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  1 12:29:51.439: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision 03/01/23 12:30:11.467
Mar  1 12:30:11.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=statefulset-1401 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  1 12:30:11.608: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  1 12:30:11.608: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  1 12:30:11.608: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  1 12:30:21.656: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 03/01/23 12:30:31.69
Mar  1 12:30:31.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=statefulset-1401 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 12:30:31.830: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  1 12:30:31.830: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  1 12:30:31.830: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Mar  1 12:30:41.860: INFO: Deleting all statefulset in ns statefulset-1401
Mar  1 12:30:41.864: INFO: Scaling statefulset ss2 to 0
Mar  1 12:30:51.885: INFO: Waiting for statefulset status.replicas updated to 0
Mar  1 12:30:51.891: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Mar  1 12:30:51.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1401" for this suite. 03/01/23 12:30:51.922
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","completed":188,"skipped":3395,"failed":0}
------------------------------
â€¢ [SLOW TEST] [90.908 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:304

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:29:21.024
    Mar  1 12:29:21.024: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename statefulset 03/01/23 12:29:21.025
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:29:21.048
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:29:21.051
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-1401 03/01/23 12:29:21.057
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:304
    STEP: Creating a new StatefulSet 03/01/23 12:29:21.064
    Mar  1 12:29:21.078: INFO: Found 0 stateful pods, waiting for 3
    Mar  1 12:29:31.083: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Mar  1 12:29:31.084: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Mar  1 12:29:31.084: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Mar  1 12:29:31.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=statefulset-1401 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar  1 12:29:31.238: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar  1 12:29:31.238: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar  1 12:29:31.238: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 03/01/23 12:29:41.259
    Mar  1 12:29:41.281: INFO: Updating stateful set ss2
    STEP: Creating a new revision 03/01/23 12:29:41.281
    STEP: Updating Pods in reverse ordinal order 03/01/23 12:29:51.298
    Mar  1 12:29:51.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=statefulset-1401 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar  1 12:29:51.439: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar  1 12:29:51.439: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar  1 12:29:51.439: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    STEP: Rolling back to a previous revision 03/01/23 12:30:11.467
    Mar  1 12:30:11.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=statefulset-1401 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar  1 12:30:11.608: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar  1 12:30:11.608: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar  1 12:30:11.608: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar  1 12:30:21.656: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 03/01/23 12:30:31.69
    Mar  1 12:30:31.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=statefulset-1401 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar  1 12:30:31.830: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar  1 12:30:31.830: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar  1 12:30:31.830: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Mar  1 12:30:41.860: INFO: Deleting all statefulset in ns statefulset-1401
    Mar  1 12:30:41.864: INFO: Scaling statefulset ss2 to 0
    Mar  1 12:30:51.885: INFO: Waiting for statefulset status.replicas updated to 0
    Mar  1 12:30:51.891: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Mar  1 12:30:51.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-1401" for this suite. 03/01/23 12:30:51.922
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:30:51.936
Mar  1 12:30:51.936: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename projected 03/01/23 12:30:51.937
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:30:51.967
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:30:51.97
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
STEP: Creating a pod to test downward API volume plugin 03/01/23 12:30:51.972
Mar  1 12:30:51.984: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0025c384-9742-42db-a63a-feda78cf65e0" in namespace "projected-8782" to be "Succeeded or Failed"
Mar  1 12:30:51.990: INFO: Pod "downwardapi-volume-0025c384-9742-42db-a63a-feda78cf65e0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.711087ms
Mar  1 12:30:53.998: INFO: Pod "downwardapi-volume-0025c384-9742-42db-a63a-feda78cf65e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014209707s
Mar  1 12:30:55.997: INFO: Pod "downwardapi-volume-0025c384-9742-42db-a63a-feda78cf65e0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013073716s
STEP: Saw pod success 03/01/23 12:30:55.997
Mar  1 12:30:55.997: INFO: Pod "downwardapi-volume-0025c384-9742-42db-a63a-feda78cf65e0" satisfied condition "Succeeded or Failed"
Mar  1 12:30:56.001: INFO: Trying to get logs from node lab1-k8s-node-3 pod downwardapi-volume-0025c384-9742-42db-a63a-feda78cf65e0 container client-container: <nil>
STEP: delete the pod 03/01/23 12:30:56.018
Mar  1 12:30:56.036: INFO: Waiting for pod downwardapi-volume-0025c384-9742-42db-a63a-feda78cf65e0 to disappear
Mar  1 12:30:56.040: INFO: Pod downwardapi-volume-0025c384-9742-42db-a63a-feda78cf65e0 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Mar  1 12:30:56.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8782" for this suite. 03/01/23 12:30:56.046
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","completed":189,"skipped":3442,"failed":0}
------------------------------
â€¢ [4.120 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:30:51.936
    Mar  1 12:30:51.936: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename projected 03/01/23 12:30:51.937
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:30:51.967
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:30:51.97
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:234
    STEP: Creating a pod to test downward API volume plugin 03/01/23 12:30:51.972
    Mar  1 12:30:51.984: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0025c384-9742-42db-a63a-feda78cf65e0" in namespace "projected-8782" to be "Succeeded or Failed"
    Mar  1 12:30:51.990: INFO: Pod "downwardapi-volume-0025c384-9742-42db-a63a-feda78cf65e0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.711087ms
    Mar  1 12:30:53.998: INFO: Pod "downwardapi-volume-0025c384-9742-42db-a63a-feda78cf65e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014209707s
    Mar  1 12:30:55.997: INFO: Pod "downwardapi-volume-0025c384-9742-42db-a63a-feda78cf65e0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013073716s
    STEP: Saw pod success 03/01/23 12:30:55.997
    Mar  1 12:30:55.997: INFO: Pod "downwardapi-volume-0025c384-9742-42db-a63a-feda78cf65e0" satisfied condition "Succeeded or Failed"
    Mar  1 12:30:56.001: INFO: Trying to get logs from node lab1-k8s-node-3 pod downwardapi-volume-0025c384-9742-42db-a63a-feda78cf65e0 container client-container: <nil>
    STEP: delete the pod 03/01/23 12:30:56.018
    Mar  1 12:30:56.036: INFO: Waiting for pod downwardapi-volume-0025c384-9742-42db-a63a-feda78cf65e0 to disappear
    Mar  1 12:30:56.040: INFO: Pod downwardapi-volume-0025c384-9742-42db-a63a-feda78cf65e0 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Mar  1 12:30:56.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8782" for this suite. 03/01/23 12:30:56.046
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:30:56.057
Mar  1 12:30:56.058: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename pods 03/01/23 12:30:56.058
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:30:56.082
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:30:56.085
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
STEP: Create a pod 03/01/23 12:30:56.087
Mar  1 12:30:56.099: INFO: Waiting up to 5m0s for pod "pod-2tlmv" in namespace "pods-8234" to be "running"
Mar  1 12:30:56.105: INFO: Pod "pod-2tlmv": Phase="Pending", Reason="", readiness=false. Elapsed: 6.680865ms
Mar  1 12:30:58.112: INFO: Pod "pod-2tlmv": Phase="Running", Reason="", readiness=true. Elapsed: 2.012908173s
Mar  1 12:30:58.112: INFO: Pod "pod-2tlmv" satisfied condition "running"
STEP: patching /status 03/01/23 12:30:58.112
Mar  1 12:30:58.122: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Mar  1 12:30:58.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8234" for this suite. 03/01/23 12:30:58.13
{"msg":"PASSED [sig-node] Pods should patch a pod status [Conformance]","completed":190,"skipped":3449,"failed":0}
------------------------------
â€¢ [2.083 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:30:56.057
    Mar  1 12:30:56.058: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename pods 03/01/23 12:30:56.058
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:30:56.082
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:30:56.085
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1082
    STEP: Create a pod 03/01/23 12:30:56.087
    Mar  1 12:30:56.099: INFO: Waiting up to 5m0s for pod "pod-2tlmv" in namespace "pods-8234" to be "running"
    Mar  1 12:30:56.105: INFO: Pod "pod-2tlmv": Phase="Pending", Reason="", readiness=false. Elapsed: 6.680865ms
    Mar  1 12:30:58.112: INFO: Pod "pod-2tlmv": Phase="Running", Reason="", readiness=true. Elapsed: 2.012908173s
    Mar  1 12:30:58.112: INFO: Pod "pod-2tlmv" satisfied condition "running"
    STEP: patching /status 03/01/23 12:30:58.112
    Mar  1 12:30:58.122: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Mar  1 12:30:58.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-8234" for this suite. 03/01/23 12:30:58.13
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:30:58.141
Mar  1 12:30:58.141: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename downward-api 03/01/23 12:30:58.142
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:30:58.167
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:30:58.17
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
STEP: Creating the pod 03/01/23 12:30:58.172
Mar  1 12:30:58.183: INFO: Waiting up to 5m0s for pod "annotationupdate8dabdd76-7c1c-4543-a9fa-23fdc5f9389d" in namespace "downward-api-3337" to be "running and ready"
Mar  1 12:30:58.188: INFO: Pod "annotationupdate8dabdd76-7c1c-4543-a9fa-23fdc5f9389d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.334366ms
Mar  1 12:30:58.188: INFO: The phase of Pod annotationupdate8dabdd76-7c1c-4543-a9fa-23fdc5f9389d is Pending, waiting for it to be Running (with Ready = true)
Mar  1 12:31:00.194: INFO: Pod "annotationupdate8dabdd76-7c1c-4543-a9fa-23fdc5f9389d": Phase="Running", Reason="", readiness=true. Elapsed: 2.010336988s
Mar  1 12:31:00.194: INFO: The phase of Pod annotationupdate8dabdd76-7c1c-4543-a9fa-23fdc5f9389d is Running (Ready = true)
Mar  1 12:31:00.194: INFO: Pod "annotationupdate8dabdd76-7c1c-4543-a9fa-23fdc5f9389d" satisfied condition "running and ready"
Mar  1 12:31:00.726: INFO: Successfully updated pod "annotationupdate8dabdd76-7c1c-4543-a9fa-23fdc5f9389d"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Mar  1 12:31:04.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3337" for this suite. 03/01/23 12:31:04.767
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","completed":191,"skipped":3491,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.636 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:30:58.141
    Mar  1 12:30:58.141: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename downward-api 03/01/23 12:30:58.142
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:30:58.167
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:30:58.17
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:161
    STEP: Creating the pod 03/01/23 12:30:58.172
    Mar  1 12:30:58.183: INFO: Waiting up to 5m0s for pod "annotationupdate8dabdd76-7c1c-4543-a9fa-23fdc5f9389d" in namespace "downward-api-3337" to be "running and ready"
    Mar  1 12:30:58.188: INFO: Pod "annotationupdate8dabdd76-7c1c-4543-a9fa-23fdc5f9389d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.334366ms
    Mar  1 12:30:58.188: INFO: The phase of Pod annotationupdate8dabdd76-7c1c-4543-a9fa-23fdc5f9389d is Pending, waiting for it to be Running (with Ready = true)
    Mar  1 12:31:00.194: INFO: Pod "annotationupdate8dabdd76-7c1c-4543-a9fa-23fdc5f9389d": Phase="Running", Reason="", readiness=true. Elapsed: 2.010336988s
    Mar  1 12:31:00.194: INFO: The phase of Pod annotationupdate8dabdd76-7c1c-4543-a9fa-23fdc5f9389d is Running (Ready = true)
    Mar  1 12:31:00.194: INFO: Pod "annotationupdate8dabdd76-7c1c-4543-a9fa-23fdc5f9389d" satisfied condition "running and ready"
    Mar  1 12:31:00.726: INFO: Successfully updated pod "annotationupdate8dabdd76-7c1c-4543-a9fa-23fdc5f9389d"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Mar  1 12:31:04.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-3337" for this suite. 03/01/23 12:31:04.767
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:31:04.78
Mar  1 12:31:04.780: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename services 03/01/23 12:31:04.781
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:31:04.8
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:31:04.802
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415
STEP: creating a Service 03/01/23 12:31:04.808
STEP: watching for the Service to be added 03/01/23 12:31:04.826
Mar  1 12:31:04.827: INFO: Found Service test-service-b8l6z in namespace services-180 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Mar  1 12:31:04.827: INFO: Service test-service-b8l6z created
STEP: Getting /status 03/01/23 12:31:04.827
Mar  1 12:31:04.832: INFO: Service test-service-b8l6z has LoadBalancer: {[]}
STEP: patching the ServiceStatus 03/01/23 12:31:04.832
STEP: watching for the Service to be patched 03/01/23 12:31:04.839
Mar  1 12:31:04.842: INFO: observed Service test-service-b8l6z in namespace services-180 with annotations: map[] & LoadBalancer: {[]}
Mar  1 12:31:04.842: INFO: Found Service test-service-b8l6z in namespace services-180 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Mar  1 12:31:04.842: INFO: Service test-service-b8l6z has service status patched
STEP: updating the ServiceStatus 03/01/23 12:31:04.842
Mar  1 12:31:04.854: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 03/01/23 12:31:04.854
Mar  1 12:31:04.861: INFO: Observed Service test-service-b8l6z in namespace services-180 with annotations: map[] & Conditions: {[]}
Mar  1 12:31:04.862: INFO: Observed event: &Service{ObjectMeta:{test-service-b8l6z  services-180  226cab0d-574d-4830-9a30-bb757798828c 30047 0 2023-03-01 12:31:04 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-03-01 12:31:04 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-03-01 12:31:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.233.54.138,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.233.54.138],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Mar  1 12:31:04.862: INFO: Found Service test-service-b8l6z in namespace services-180 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Mar  1 12:31:04.862: INFO: Service test-service-b8l6z has service status updated
STEP: patching the service 03/01/23 12:31:04.862
STEP: watching for the Service to be patched 03/01/23 12:31:04.873
Mar  1 12:31:04.875: INFO: observed Service test-service-b8l6z in namespace services-180 with labels: map[test-service-static:true]
Mar  1 12:31:04.875: INFO: observed Service test-service-b8l6z in namespace services-180 with labels: map[test-service-static:true]
Mar  1 12:31:04.875: INFO: observed Service test-service-b8l6z in namespace services-180 with labels: map[test-service-static:true]
Mar  1 12:31:04.875: INFO: Found Service test-service-b8l6z in namespace services-180 with labels: map[test-service:patched test-service-static:true]
Mar  1 12:31:04.875: INFO: Service test-service-b8l6z patched
STEP: deleting the service 03/01/23 12:31:04.875
STEP: watching for the Service to be deleted 03/01/23 12:31:04.895
Mar  1 12:31:04.896: INFO: Observed event: ADDED
Mar  1 12:31:04.896: INFO: Observed event: MODIFIED
Mar  1 12:31:04.897: INFO: Observed event: MODIFIED
Mar  1 12:31:04.897: INFO: Observed event: MODIFIED
Mar  1 12:31:04.897: INFO: Found Service test-service-b8l6z in namespace services-180 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Mar  1 12:31:04.897: INFO: Service test-service-b8l6z deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar  1 12:31:04.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-180" for this suite. 03/01/23 12:31:04.903
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","completed":192,"skipped":3502,"failed":0}
------------------------------
â€¢ [0.133 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:31:04.78
    Mar  1 12:31:04.780: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename services 03/01/23 12:31:04.781
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:31:04.8
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:31:04.802
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3415
    STEP: creating a Service 03/01/23 12:31:04.808
    STEP: watching for the Service to be added 03/01/23 12:31:04.826
    Mar  1 12:31:04.827: INFO: Found Service test-service-b8l6z in namespace services-180 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Mar  1 12:31:04.827: INFO: Service test-service-b8l6z created
    STEP: Getting /status 03/01/23 12:31:04.827
    Mar  1 12:31:04.832: INFO: Service test-service-b8l6z has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 03/01/23 12:31:04.832
    STEP: watching for the Service to be patched 03/01/23 12:31:04.839
    Mar  1 12:31:04.842: INFO: observed Service test-service-b8l6z in namespace services-180 with annotations: map[] & LoadBalancer: {[]}
    Mar  1 12:31:04.842: INFO: Found Service test-service-b8l6z in namespace services-180 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Mar  1 12:31:04.842: INFO: Service test-service-b8l6z has service status patched
    STEP: updating the ServiceStatus 03/01/23 12:31:04.842
    Mar  1 12:31:04.854: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 03/01/23 12:31:04.854
    Mar  1 12:31:04.861: INFO: Observed Service test-service-b8l6z in namespace services-180 with annotations: map[] & Conditions: {[]}
    Mar  1 12:31:04.862: INFO: Observed event: &Service{ObjectMeta:{test-service-b8l6z  services-180  226cab0d-574d-4830-9a30-bb757798828c 30047 0 2023-03-01 12:31:04 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-03-01 12:31:04 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-03-01 12:31:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.233.54.138,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.233.54.138],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Mar  1 12:31:04.862: INFO: Found Service test-service-b8l6z in namespace services-180 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Mar  1 12:31:04.862: INFO: Service test-service-b8l6z has service status updated
    STEP: patching the service 03/01/23 12:31:04.862
    STEP: watching for the Service to be patched 03/01/23 12:31:04.873
    Mar  1 12:31:04.875: INFO: observed Service test-service-b8l6z in namespace services-180 with labels: map[test-service-static:true]
    Mar  1 12:31:04.875: INFO: observed Service test-service-b8l6z in namespace services-180 with labels: map[test-service-static:true]
    Mar  1 12:31:04.875: INFO: observed Service test-service-b8l6z in namespace services-180 with labels: map[test-service-static:true]
    Mar  1 12:31:04.875: INFO: Found Service test-service-b8l6z in namespace services-180 with labels: map[test-service:patched test-service-static:true]
    Mar  1 12:31:04.875: INFO: Service test-service-b8l6z patched
    STEP: deleting the service 03/01/23 12:31:04.875
    STEP: watching for the Service to be deleted 03/01/23 12:31:04.895
    Mar  1 12:31:04.896: INFO: Observed event: ADDED
    Mar  1 12:31:04.896: INFO: Observed event: MODIFIED
    Mar  1 12:31:04.897: INFO: Observed event: MODIFIED
    Mar  1 12:31:04.897: INFO: Observed event: MODIFIED
    Mar  1 12:31:04.897: INFO: Found Service test-service-b8l6z in namespace services-180 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Mar  1 12:31:04.897: INFO: Service test-service-b8l6z deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar  1 12:31:04.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-180" for this suite. 03/01/23 12:31:04.903
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:31:04.913
Mar  1 12:31:04.913: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename container-probe 03/01/23 12:31:04.914
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:31:04.935
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:31:04.939
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
STEP: Creating pod busybox-b7248599-d7e4-4713-8e3c-9daf8da1826c in namespace container-probe-391 03/01/23 12:31:04.941
Mar  1 12:31:04.951: INFO: Waiting up to 5m0s for pod "busybox-b7248599-d7e4-4713-8e3c-9daf8da1826c" in namespace "container-probe-391" to be "not pending"
Mar  1 12:31:04.957: INFO: Pod "busybox-b7248599-d7e4-4713-8e3c-9daf8da1826c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.969382ms
Mar  1 12:31:06.961: INFO: Pod "busybox-b7248599-d7e4-4713-8e3c-9daf8da1826c": Phase="Running", Reason="", readiness=true. Elapsed: 2.010776635s
Mar  1 12:31:06.962: INFO: Pod "busybox-b7248599-d7e4-4713-8e3c-9daf8da1826c" satisfied condition "not pending"
Mar  1 12:31:06.962: INFO: Started pod busybox-b7248599-d7e4-4713-8e3c-9daf8da1826c in namespace container-probe-391
STEP: checking the pod's current state and verifying that restartCount is present 03/01/23 12:31:06.962
Mar  1 12:31:06.967: INFO: Initial restart count of pod busybox-b7248599-d7e4-4713-8e3c-9daf8da1826c is 0
STEP: deleting the pod 03/01/23 12:35:07.66
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Mar  1 12:35:07.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-391" for this suite. 03/01/23 12:35:07.683
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":193,"skipped":3503,"failed":0}
------------------------------
â€¢ [SLOW TEST] [242.778 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:31:04.913
    Mar  1 12:31:04.913: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename container-probe 03/01/23 12:31:04.914
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:31:04.935
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:31:04.939
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:148
    STEP: Creating pod busybox-b7248599-d7e4-4713-8e3c-9daf8da1826c in namespace container-probe-391 03/01/23 12:31:04.941
    Mar  1 12:31:04.951: INFO: Waiting up to 5m0s for pod "busybox-b7248599-d7e4-4713-8e3c-9daf8da1826c" in namespace "container-probe-391" to be "not pending"
    Mar  1 12:31:04.957: INFO: Pod "busybox-b7248599-d7e4-4713-8e3c-9daf8da1826c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.969382ms
    Mar  1 12:31:06.961: INFO: Pod "busybox-b7248599-d7e4-4713-8e3c-9daf8da1826c": Phase="Running", Reason="", readiness=true. Elapsed: 2.010776635s
    Mar  1 12:31:06.962: INFO: Pod "busybox-b7248599-d7e4-4713-8e3c-9daf8da1826c" satisfied condition "not pending"
    Mar  1 12:31:06.962: INFO: Started pod busybox-b7248599-d7e4-4713-8e3c-9daf8da1826c in namespace container-probe-391
    STEP: checking the pod's current state and verifying that restartCount is present 03/01/23 12:31:06.962
    Mar  1 12:31:06.967: INFO: Initial restart count of pod busybox-b7248599-d7e4-4713-8e3c-9daf8da1826c is 0
    STEP: deleting the pod 03/01/23 12:35:07.66
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Mar  1 12:35:07.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-391" for this suite. 03/01/23 12:35:07.683
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:35:07.693
Mar  1 12:35:07.693: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename containers 03/01/23 12:35:07.693
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:35:07.714
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:35:07.716
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
STEP: Creating a pod to test override arguments 03/01/23 12:35:07.719
Mar  1 12:35:07.728: INFO: Waiting up to 5m0s for pod "client-containers-f085ee61-67f1-4683-9a42-5bf451a52e11" in namespace "containers-7887" to be "Succeeded or Failed"
Mar  1 12:35:07.734: INFO: Pod "client-containers-f085ee61-67f1-4683-9a42-5bf451a52e11": Phase="Pending", Reason="", readiness=false. Elapsed: 6.398973ms
Mar  1 12:35:09.741: INFO: Pod "client-containers-f085ee61-67f1-4683-9a42-5bf451a52e11": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012603505s
Mar  1 12:35:11.741: INFO: Pod "client-containers-f085ee61-67f1-4683-9a42-5bf451a52e11": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012877104s
STEP: Saw pod success 03/01/23 12:35:11.741
Mar  1 12:35:11.741: INFO: Pod "client-containers-f085ee61-67f1-4683-9a42-5bf451a52e11" satisfied condition "Succeeded or Failed"
Mar  1 12:35:11.745: INFO: Trying to get logs from node lab1-k8s-node-3 pod client-containers-f085ee61-67f1-4683-9a42-5bf451a52e11 container agnhost-container: <nil>
STEP: delete the pod 03/01/23 12:35:11.765
Mar  1 12:35:11.780: INFO: Waiting for pod client-containers-f085ee61-67f1-4683-9a42-5bf451a52e11 to disappear
Mar  1 12:35:11.784: INFO: Pod client-containers-f085ee61-67f1-4683-9a42-5bf451a52e11 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Mar  1 12:35:11.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7887" for this suite. 03/01/23 12:35:11.789
{"msg":"PASSED [sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]","completed":194,"skipped":3514,"failed":0}
------------------------------
â€¢ [4.105 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:35:07.693
    Mar  1 12:35:07.693: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename containers 03/01/23 12:35:07.693
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:35:07.714
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:35:07.716
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:58
    STEP: Creating a pod to test override arguments 03/01/23 12:35:07.719
    Mar  1 12:35:07.728: INFO: Waiting up to 5m0s for pod "client-containers-f085ee61-67f1-4683-9a42-5bf451a52e11" in namespace "containers-7887" to be "Succeeded or Failed"
    Mar  1 12:35:07.734: INFO: Pod "client-containers-f085ee61-67f1-4683-9a42-5bf451a52e11": Phase="Pending", Reason="", readiness=false. Elapsed: 6.398973ms
    Mar  1 12:35:09.741: INFO: Pod "client-containers-f085ee61-67f1-4683-9a42-5bf451a52e11": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012603505s
    Mar  1 12:35:11.741: INFO: Pod "client-containers-f085ee61-67f1-4683-9a42-5bf451a52e11": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012877104s
    STEP: Saw pod success 03/01/23 12:35:11.741
    Mar  1 12:35:11.741: INFO: Pod "client-containers-f085ee61-67f1-4683-9a42-5bf451a52e11" satisfied condition "Succeeded or Failed"
    Mar  1 12:35:11.745: INFO: Trying to get logs from node lab1-k8s-node-3 pod client-containers-f085ee61-67f1-4683-9a42-5bf451a52e11 container agnhost-container: <nil>
    STEP: delete the pod 03/01/23 12:35:11.765
    Mar  1 12:35:11.780: INFO: Waiting for pod client-containers-f085ee61-67f1-4683-9a42-5bf451a52e11 to disappear
    Mar  1 12:35:11.784: INFO: Pod client-containers-f085ee61-67f1-4683-9a42-5bf451a52e11 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Mar  1 12:35:11.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-7887" for this suite. 03/01/23 12:35:11.789
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:35:11.8
Mar  1 12:35:11.801: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename dns 03/01/23 12:35:11.802
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:35:11.832
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:35:11.836
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 03/01/23 12:35:11.838
Mar  1 12:35:11.848: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-494  e8c654ef-b7f5-4103-859a-986e60b58816 30856 0 2023-03-01 12:35:11 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-03-01 12:35:11 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-969bd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-969bd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 12:35:11.849: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-494" to be "running and ready"
Mar  1 12:35:11.855: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 6.60744ms
Mar  1 12:35:11.855: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Mar  1 12:35:13.860: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.011123801s
Mar  1 12:35:13.860: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Mar  1 12:35:13.860: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 03/01/23 12:35:13.86
Mar  1 12:35:13.860: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-494 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  1 12:35:13.860: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
Mar  1 12:35:13.861: INFO: ExecWithOptions: Clientset creation
Mar  1 12:35:13.861: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-494/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 03/01/23 12:35:13.96
Mar  1 12:35:13.960: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-494 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  1 12:35:13.960: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
Mar  1 12:35:13.961: INFO: ExecWithOptions: Clientset creation
Mar  1 12:35:13.961: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-494/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar  1 12:35:14.055: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Mar  1 12:35:14.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-494" for this suite. 03/01/23 12:35:14.079
{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","completed":195,"skipped":3529,"failed":0}
------------------------------
â€¢ [2.287 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:35:11.8
    Mar  1 12:35:11.801: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename dns 03/01/23 12:35:11.802
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:35:11.832
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:35:11.836
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 03/01/23 12:35:11.838
    Mar  1 12:35:11.848: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-494  e8c654ef-b7f5-4103-859a-986e60b58816 30856 0 2023-03-01 12:35:11 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-03-01 12:35:11 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-969bd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-969bd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  1 12:35:11.849: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-494" to be "running and ready"
    Mar  1 12:35:11.855: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 6.60744ms
    Mar  1 12:35:11.855: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Mar  1 12:35:13.860: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.011123801s
    Mar  1 12:35:13.860: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Mar  1 12:35:13.860: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 03/01/23 12:35:13.86
    Mar  1 12:35:13.860: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-494 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  1 12:35:13.860: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    Mar  1 12:35:13.861: INFO: ExecWithOptions: Clientset creation
    Mar  1 12:35:13.861: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-494/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 03/01/23 12:35:13.96
    Mar  1 12:35:13.960: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-494 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  1 12:35:13.960: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    Mar  1 12:35:13.961: INFO: ExecWithOptions: Clientset creation
    Mar  1 12:35:13.961: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-494/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar  1 12:35:14.055: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Mar  1 12:35:14.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-494" for this suite. 03/01/23 12:35:14.079
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:35:14.089
Mar  1 12:35:14.089: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename secrets 03/01/23 12:35:14.09
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:35:14.11
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:35:14.112
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
STEP: Creating secret with name secret-test-20d450d5-c30e-4a9b-aaed-782a90956b68 03/01/23 12:35:14.138
STEP: Creating a pod to test consume secrets 03/01/23 12:35:14.149
Mar  1 12:35:14.159: INFO: Waiting up to 5m0s for pod "pod-secrets-71a1ac81-d1d1-4b0b-9c21-0ac5e5a6bfae" in namespace "secrets-3136" to be "Succeeded or Failed"
Mar  1 12:35:14.166: INFO: Pod "pod-secrets-71a1ac81-d1d1-4b0b-9c21-0ac5e5a6bfae": Phase="Pending", Reason="", readiness=false. Elapsed: 7.236638ms
Mar  1 12:35:16.173: INFO: Pod "pod-secrets-71a1ac81-d1d1-4b0b-9c21-0ac5e5a6bfae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0141196s
Mar  1 12:35:18.172: INFO: Pod "pod-secrets-71a1ac81-d1d1-4b0b-9c21-0ac5e5a6bfae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01357412s
STEP: Saw pod success 03/01/23 12:35:18.172
Mar  1 12:35:18.173: INFO: Pod "pod-secrets-71a1ac81-d1d1-4b0b-9c21-0ac5e5a6bfae" satisfied condition "Succeeded or Failed"
Mar  1 12:35:18.177: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-secrets-71a1ac81-d1d1-4b0b-9c21-0ac5e5a6bfae container secret-volume-test: <nil>
STEP: delete the pod 03/01/23 12:35:18.191
Mar  1 12:35:18.207: INFO: Waiting for pod pod-secrets-71a1ac81-d1d1-4b0b-9c21-0ac5e5a6bfae to disappear
Mar  1 12:35:18.211: INFO: Pod pod-secrets-71a1ac81-d1d1-4b0b-9c21-0ac5e5a6bfae no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Mar  1 12:35:18.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3136" for this suite. 03/01/23 12:35:18.219
STEP: Destroying namespace "secret-namespace-7467" for this suite. 03/01/23 12:35:18.229
{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","completed":196,"skipped":3537,"failed":0}
------------------------------
â€¢ [4.149 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:35:14.089
    Mar  1 12:35:14.089: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename secrets 03/01/23 12:35:14.09
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:35:14.11
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:35:14.112
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:98
    STEP: Creating secret with name secret-test-20d450d5-c30e-4a9b-aaed-782a90956b68 03/01/23 12:35:14.138
    STEP: Creating a pod to test consume secrets 03/01/23 12:35:14.149
    Mar  1 12:35:14.159: INFO: Waiting up to 5m0s for pod "pod-secrets-71a1ac81-d1d1-4b0b-9c21-0ac5e5a6bfae" in namespace "secrets-3136" to be "Succeeded or Failed"
    Mar  1 12:35:14.166: INFO: Pod "pod-secrets-71a1ac81-d1d1-4b0b-9c21-0ac5e5a6bfae": Phase="Pending", Reason="", readiness=false. Elapsed: 7.236638ms
    Mar  1 12:35:16.173: INFO: Pod "pod-secrets-71a1ac81-d1d1-4b0b-9c21-0ac5e5a6bfae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0141196s
    Mar  1 12:35:18.172: INFO: Pod "pod-secrets-71a1ac81-d1d1-4b0b-9c21-0ac5e5a6bfae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01357412s
    STEP: Saw pod success 03/01/23 12:35:18.172
    Mar  1 12:35:18.173: INFO: Pod "pod-secrets-71a1ac81-d1d1-4b0b-9c21-0ac5e5a6bfae" satisfied condition "Succeeded or Failed"
    Mar  1 12:35:18.177: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-secrets-71a1ac81-d1d1-4b0b-9c21-0ac5e5a6bfae container secret-volume-test: <nil>
    STEP: delete the pod 03/01/23 12:35:18.191
    Mar  1 12:35:18.207: INFO: Waiting for pod pod-secrets-71a1ac81-d1d1-4b0b-9c21-0ac5e5a6bfae to disappear
    Mar  1 12:35:18.211: INFO: Pod pod-secrets-71a1ac81-d1d1-4b0b-9c21-0ac5e5a6bfae no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Mar  1 12:35:18.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-3136" for this suite. 03/01/23 12:35:18.219
    STEP: Destroying namespace "secret-namespace-7467" for this suite. 03/01/23 12:35:18.229
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:35:18.242
Mar  1 12:35:18.242: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename deployment 03/01/23 12:35:18.243
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:35:18.26
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:35:18.263
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Mar  1 12:35:18.266: INFO: Creating simple deployment test-new-deployment
Mar  1 12:35:18.281: INFO: new replicaset for deployment "test-new-deployment" is yet to be created
STEP: getting scale subresource 03/01/23 12:35:20.299
STEP: updating a scale subresource 03/01/23 12:35:20.303
STEP: verifying the deployment Spec.Replicas was modified 03/01/23 12:35:20.311
STEP: Patch a scale subresource 03/01/23 12:35:20.316
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar  1 12:35:20.356: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-1211  ff2dbd6b-4d30-4a06-a8b3-064be17c1553 30985 3 2023-03-01 12:35:18 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-03-01 12:35:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-01 12:35:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003588fe8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-845c8977d9" has successfully progressed.,LastUpdateTime:2023-03-01 12:35:19 +0000 UTC,LastTransitionTime:2023-03-01 12:35:18 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-03-01 12:35:20 +0000 UTC,LastTransitionTime:2023-03-01 12:35:20 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar  1 12:35:20.362: INFO: New ReplicaSet "test-new-deployment-845c8977d9" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-845c8977d9  deployment-1211  591e070d-e506-4d65-848b-845717ddc884 30991 3 2023-03-01 12:35:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment ff2dbd6b-4d30-4a06-a8b3-064be17c1553 0xc003589447 0xc003589448}] [] [{kube-controller-manager Update apps/v1 2023-03-01 12:35:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ff2dbd6b-4d30-4a06-a8b3-064be17c1553\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-01 12:35:20 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0035894d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar  1 12:35:20.372: INFO: Pod "test-new-deployment-845c8977d9-7ssc5" is available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-7ssc5 test-new-deployment-845c8977d9- deployment-1211  77bc369a-4c01-495e-8bbe-88088b01441a 30972 0 2023-03-01 12:35:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:e594e10037709fca4e39d711938a0dc9a1b9b8c5e35ab19f86ad207ed8f988b1 cni.projectcalico.org/podIP:10.233.74.8/32 cni.projectcalico.org/podIPs:10.233.74.8/32] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 591e070d-e506-4d65-848b-845717ddc884 0xc0035898e7 0xc0035898e8}] [] [{calico Update v1 2023-03-01 12:35:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-01 12:35:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"591e070d-e506-4d65-848b-845717ddc884\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-01 12:35:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.74.8\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z5pc6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z5pc6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 12:35:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 12:35:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 12:35:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 12:35:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.2.241,PodIP:10.233.74.8,StartTime:2023-03-01 12:35:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-01 12:35:19 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://66c2aa868189bdb5e25c62ab37ca3c23bfd4f05466feca431ca8751e88242122,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.74.8,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 12:35:20.372: INFO: Pod "test-new-deployment-845c8977d9-w4v8b" is not available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-w4v8b test-new-deployment-845c8977d9- deployment-1211  f91639c6-ab0e-407e-aa10-91b5ba083e01 30992 0 2023-03-01 12:35:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 591e070d-e506-4d65-848b-845717ddc884 0xc003589ae0 0xc003589ae1}] [] [{kube-controller-manager Update v1 2023-03-01 12:35:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"591e070d-e506-4d65-848b-845717ddc884\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-01 12:35:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-52zfl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-52zfl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 12:35:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 12:35:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 12:35:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 12:35:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.178,PodIP:,StartTime:2023-03-01 12:35:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Mar  1 12:35:20.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1211" for this suite. 03/01/23 12:35:20.379
{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","completed":197,"skipped":3617,"failed":0}
------------------------------
â€¢ [2.162 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:35:18.242
    Mar  1 12:35:18.242: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename deployment 03/01/23 12:35:18.243
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:35:18.26
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:35:18.263
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Mar  1 12:35:18.266: INFO: Creating simple deployment test-new-deployment
    Mar  1 12:35:18.281: INFO: new replicaset for deployment "test-new-deployment" is yet to be created
    STEP: getting scale subresource 03/01/23 12:35:20.299
    STEP: updating a scale subresource 03/01/23 12:35:20.303
    STEP: verifying the deployment Spec.Replicas was modified 03/01/23 12:35:20.311
    STEP: Patch a scale subresource 03/01/23 12:35:20.316
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar  1 12:35:20.356: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-1211  ff2dbd6b-4d30-4a06-a8b3-064be17c1553 30985 3 2023-03-01 12:35:18 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-03-01 12:35:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-01 12:35:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003588fe8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-845c8977d9" has successfully progressed.,LastUpdateTime:2023-03-01 12:35:19 +0000 UTC,LastTransitionTime:2023-03-01 12:35:18 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-03-01 12:35:20 +0000 UTC,LastTransitionTime:2023-03-01 12:35:20 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Mar  1 12:35:20.362: INFO: New ReplicaSet "test-new-deployment-845c8977d9" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-845c8977d9  deployment-1211  591e070d-e506-4d65-848b-845717ddc884 30991 3 2023-03-01 12:35:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment ff2dbd6b-4d30-4a06-a8b3-064be17c1553 0xc003589447 0xc003589448}] [] [{kube-controller-manager Update apps/v1 2023-03-01 12:35:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ff2dbd6b-4d30-4a06-a8b3-064be17c1553\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-01 12:35:20 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0035894d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Mar  1 12:35:20.372: INFO: Pod "test-new-deployment-845c8977d9-7ssc5" is available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-7ssc5 test-new-deployment-845c8977d9- deployment-1211  77bc369a-4c01-495e-8bbe-88088b01441a 30972 0 2023-03-01 12:35:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:e594e10037709fca4e39d711938a0dc9a1b9b8c5e35ab19f86ad207ed8f988b1 cni.projectcalico.org/podIP:10.233.74.8/32 cni.projectcalico.org/podIPs:10.233.74.8/32] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 591e070d-e506-4d65-848b-845717ddc884 0xc0035898e7 0xc0035898e8}] [] [{calico Update v1 2023-03-01 12:35:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-01 12:35:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"591e070d-e506-4d65-848b-845717ddc884\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-01 12:35:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.74.8\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z5pc6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z5pc6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 12:35:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 12:35:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 12:35:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 12:35:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.2.241,PodIP:10.233.74.8,StartTime:2023-03-01 12:35:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-01 12:35:19 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://66c2aa868189bdb5e25c62ab37ca3c23bfd4f05466feca431ca8751e88242122,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.74.8,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  1 12:35:20.372: INFO: Pod "test-new-deployment-845c8977d9-w4v8b" is not available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-w4v8b test-new-deployment-845c8977d9- deployment-1211  f91639c6-ab0e-407e-aa10-91b5ba083e01 30992 0 2023-03-01 12:35:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 591e070d-e506-4d65-848b-845717ddc884 0xc003589ae0 0xc003589ae1}] [] [{kube-controller-manager Update v1 2023-03-01 12:35:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"591e070d-e506-4d65-848b-845717ddc884\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-01 12:35:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-52zfl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-52zfl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 12:35:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 12:35:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 12:35:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 12:35:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.178,PodIP:,StartTime:2023-03-01 12:35:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Mar  1 12:35:20.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-1211" for this suite. 03/01/23 12:35:20.379
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:35:20.407
Mar  1 12:35:20.407: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename sched-pred 03/01/23 12:35:20.408
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:35:20.433
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:35:20.436
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Mar  1 12:35:20.438: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar  1 12:35:20.451: INFO: Waiting for terminating namespaces to be deleted...
Mar  1 12:35:20.455: INFO: 
Logging pods the apiserver thinks is on node lab1-k8s-node-1 before test
Mar  1 12:35:20.465: INFO: test-new-deployment-845c8977d9-w4v8b from deployment-1211 started at 2023-03-01 12:35:20 +0000 UTC (1 container statuses recorded)
Mar  1 12:35:20.465: INFO: 	Container httpd ready: false, restart count 0
Mar  1 12:35:20.465: INFO: calico-node-kjj57 from kube-system started at 2023-03-01 11:36:06 +0000 UTC (1 container statuses recorded)
Mar  1 12:35:20.465: INFO: 	Container calico-node ready: true, restart count 1
Mar  1 12:35:20.465: INFO: csi-cinder-nodeplugin-fjt6c from kube-system started at 2023-03-01 11:38:24 +0000 UTC (3 container statuses recorded)
Mar  1 12:35:20.465: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Mar  1 12:35:20.465: INFO: 	Container liveness-probe ready: true, restart count 0
Mar  1 12:35:20.465: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar  1 12:35:20.465: INFO: kube-proxy-xmdzj from kube-system started at 2023-03-01 11:35:32 +0000 UTC (1 container statuses recorded)
Mar  1 12:35:20.465: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  1 12:35:20.465: INFO: metrics-server-6bd8d699c5-pwxfp from kube-system started at 2023-03-01 11:38:24 +0000 UTC (1 container statuses recorded)
Mar  1 12:35:20.465: INFO: 	Container metrics-server ready: true, restart count 0
Mar  1 12:35:20.465: INFO: nginx-proxy-lab1-k8s-node-1 from kube-system started at 2023-03-01 11:36:22 +0000 UTC (1 container statuses recorded)
Mar  1 12:35:20.465: INFO: 	Container nginx-proxy ready: true, restart count 0
Mar  1 12:35:20.465: INFO: nodelocaldns-mclb6 from kube-system started at 2023-03-01 11:37:23 +0000 UTC (1 container statuses recorded)
Mar  1 12:35:20.465: INFO: 	Container node-cache ready: true, restart count 0
Mar  1 12:35:20.465: INFO: snapshot-controller-7d445c66c9-m2qf9 from kube-system started at 2023-03-01 12:18:37 +0000 UTC (1 container statuses recorded)
Mar  1 12:35:20.465: INFO: 	Container snapshot-controller ready: true, restart count 0
Mar  1 12:35:20.465: INFO: sonobuoy-e2e-job-5f1d4571b8c24260 from sonobuoy started at 2023-03-01 11:42:09 +0000 UTC (2 container statuses recorded)
Mar  1 12:35:20.465: INFO: 	Container e2e ready: true, restart count 0
Mar  1 12:35:20.465: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  1 12:35:20.465: INFO: sonobuoy-systemd-logs-daemon-set-1321f7b040854c53-9ckr7 from sonobuoy started at 2023-03-01 11:42:09 +0000 UTC (2 container statuses recorded)
Mar  1 12:35:20.465: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  1 12:35:20.465: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  1 12:35:20.465: INFO: 
Logging pods the apiserver thinks is on node lab1-k8s-node-2 before test
Mar  1 12:35:20.479: INFO: test-new-deployment-845c8977d9-8b69v from deployment-1211 started at 2023-03-01 12:35:20 +0000 UTC (1 container statuses recorded)
Mar  1 12:35:20.479: INFO: 	Container httpd ready: false, restart count 0
Mar  1 12:35:20.479: INFO: calico-kube-controllers-ff45567bb-9k2q7 from kube-system started at 2023-03-01 11:37:06 +0000 UTC (1 container statuses recorded)
Mar  1 12:35:20.479: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Mar  1 12:35:20.479: INFO: calico-node-5vzf7 from kube-system started at 2023-03-01 11:36:06 +0000 UTC (1 container statuses recorded)
Mar  1 12:35:20.479: INFO: 	Container calico-node ready: true, restart count 1
Mar  1 12:35:20.479: INFO: csi-cinder-controllerplugin-6f68fbd578-krvcc from kube-system started at 2023-03-01 11:38:24 +0000 UTC (6 container statuses recorded)
Mar  1 12:35:20.479: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Mar  1 12:35:20.479: INFO: 	Container csi-attacher ready: true, restart count 0
Mar  1 12:35:20.479: INFO: 	Container csi-provisioner ready: true, restart count 0
Mar  1 12:35:20.479: INFO: 	Container csi-resizer ready: true, restart count 0
Mar  1 12:35:20.479: INFO: 	Container csi-snapshotter ready: true, restart count 0
Mar  1 12:35:20.479: INFO: 	Container liveness-probe ready: true, restart count 0
Mar  1 12:35:20.479: INFO: csi-cinder-nodeplugin-zl564 from kube-system started at 2023-03-01 11:38:24 +0000 UTC (3 container statuses recorded)
Mar  1 12:35:20.479: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Mar  1 12:35:20.479: INFO: 	Container liveness-probe ready: true, restart count 0
Mar  1 12:35:20.479: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar  1 12:35:20.479: INFO: kube-proxy-jllc6 from kube-system started at 2023-03-01 11:35:32 +0000 UTC (1 container statuses recorded)
Mar  1 12:35:20.479: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  1 12:35:20.479: INFO: nginx-proxy-lab1-k8s-node-2 from kube-system started at 2023-03-01 11:36:22 +0000 UTC (1 container statuses recorded)
Mar  1 12:35:20.479: INFO: 	Container nginx-proxy ready: true, restart count 0
Mar  1 12:35:20.479: INFO: nodelocaldns-mhm2s from kube-system started at 2023-03-01 11:37:23 +0000 UTC (1 container statuses recorded)
Mar  1 12:35:20.479: INFO: 	Container node-cache ready: true, restart count 0
Mar  1 12:35:20.479: INFO: sonobuoy-systemd-logs-daemon-set-1321f7b040854c53-nklmj from sonobuoy started at 2023-03-01 11:42:09 +0000 UTC (2 container statuses recorded)
Mar  1 12:35:20.479: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  1 12:35:20.479: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  1 12:35:20.479: INFO: 
Logging pods the apiserver thinks is on node lab1-k8s-node-3 before test
Mar  1 12:35:20.492: INFO: test-new-deployment-845c8977d9-7ssc5 from deployment-1211 started at 2023-03-01 12:35:18 +0000 UTC (1 container statuses recorded)
Mar  1 12:35:20.492: INFO: 	Container httpd ready: true, restart count 0
Mar  1 12:35:20.492: INFO: test-new-deployment-845c8977d9-w9m7p from deployment-1211 started at 2023-03-01 12:35:20 +0000 UTC (1 container statuses recorded)
Mar  1 12:35:20.492: INFO: 	Container httpd ready: false, restart count 0
Mar  1 12:35:20.492: INFO: calico-node-zjksl from kube-system started at 2023-03-01 11:36:06 +0000 UTC (1 container statuses recorded)
Mar  1 12:35:20.492: INFO: 	Container calico-node ready: true, restart count 1
Mar  1 12:35:20.492: INFO: csi-cinder-nodeplugin-xh6tw from kube-system started at 2023-03-01 12:27:48 +0000 UTC (3 container statuses recorded)
Mar  1 12:35:20.492: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Mar  1 12:35:20.492: INFO: 	Container liveness-probe ready: true, restart count 0
Mar  1 12:35:20.492: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar  1 12:35:20.492: INFO: kube-proxy-2jcfl from kube-system started at 2023-03-01 11:35:29 +0000 UTC (1 container statuses recorded)
Mar  1 12:35:20.492: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  1 12:35:20.492: INFO: nginx-proxy-lab1-k8s-node-3 from kube-system started at 2023-03-01 11:38:59 +0000 UTC (1 container statuses recorded)
Mar  1 12:35:20.492: INFO: 	Container nginx-proxy ready: true, restart count 0
Mar  1 12:35:20.492: INFO: nodelocaldns-5tw4l from kube-system started at 2023-03-01 11:37:23 +0000 UTC (1 container statuses recorded)
Mar  1 12:35:20.492: INFO: 	Container node-cache ready: true, restart count 0
Mar  1 12:35:20.492: INFO: sonobuoy from sonobuoy started at 2023-03-01 11:42:06 +0000 UTC (1 container statuses recorded)
Mar  1 12:35:20.492: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar  1 12:35:20.492: INFO: sonobuoy-systemd-logs-daemon-set-1321f7b040854c53-9b52c from sonobuoy started at 2023-03-01 11:42:09 +0000 UTC (2 container statuses recorded)
Mar  1 12:35:20.492: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  1 12:35:20.492: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
STEP: Trying to launch a pod without a label to get a node which can launch it. 03/01/23 12:35:20.492
Mar  1 12:35:20.502: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-5869" to be "running"
Mar  1 12:35:20.509: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 6.436926ms
Mar  1 12:35:22.514: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.011923202s
Mar  1 12:35:22.515: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 03/01/23 12:35:22.519
STEP: Trying to apply a random label on the found node. 03/01/23 12:35:22.541
STEP: verifying the node has the label kubernetes.io/e2e-9ee709d1-d4d7-42be-8b3b-17098cd5a5aa 95 03/01/23 12:35:22.554
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 03/01/23 12:35:22.56
Mar  1 12:35:22.567: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-5869" to be "not pending"
Mar  1 12:35:22.571: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.670061ms
Mar  1 12:35:24.576: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.00877311s
Mar  1 12:35:24.576: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.128.2.241 on the node which pod4 resides and expect not scheduled 03/01/23 12:35:24.576
Mar  1 12:35:24.584: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-5869" to be "not pending"
Mar  1 12:35:24.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.16046ms
Mar  1 12:35:26.595: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010421638s
Mar  1 12:35:28.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009768487s
Mar  1 12:35:30.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.00929251s
Mar  1 12:35:32.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.010071333s
Mar  1 12:35:34.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.009537902s
Mar  1 12:35:36.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.008968517s
Mar  1 12:35:38.595: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.01103425s
Mar  1 12:35:40.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.010267931s
Mar  1 12:35:42.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.008588715s
Mar  1 12:35:44.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.008599984s
Mar  1 12:35:46.595: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.01078526s
Mar  1 12:35:48.596: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.01206324s
Mar  1 12:35:50.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.010091967s
Mar  1 12:35:52.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.009889702s
Mar  1 12:35:54.595: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.011109164s
Mar  1 12:35:56.595: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.010481757s
Mar  1 12:35:58.595: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.010480891s
Mar  1 12:36:00.597: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.012740442s
Mar  1 12:36:02.595: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.010501818s
Mar  1 12:36:04.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.009545647s
Mar  1 12:36:06.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.008914953s
Mar  1 12:36:08.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.0095762s
Mar  1 12:36:10.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.009016788s
Mar  1 12:36:12.596: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.01225591s
Mar  1 12:36:14.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.00909276s
Mar  1 12:36:16.595: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.01123861s
Mar  1 12:36:18.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.009898583s
Mar  1 12:36:20.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.008809814s
Mar  1 12:36:22.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.009919862s
Mar  1 12:36:24.595: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.011207433s
Mar  1 12:36:26.595: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.011278707s
Mar  1 12:36:28.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.009053678s
Mar  1 12:36:30.595: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.010847599s
Mar  1 12:36:32.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.010018531s
Mar  1 12:36:34.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.009487789s
Mar  1 12:36:36.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.009373586s
Mar  1 12:36:38.595: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.010513428s
Mar  1 12:36:40.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.010008464s
Mar  1 12:36:42.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.008955303s
Mar  1 12:36:44.595: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.010381222s
Mar  1 12:36:46.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.01005799s
Mar  1 12:36:48.595: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.011007714s
Mar  1 12:36:50.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.010059442s
Mar  1 12:36:52.596: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.011677216s
Mar  1 12:36:54.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.009046106s
Mar  1 12:36:56.595: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.011011143s
Mar  1 12:36:58.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.009620328s
Mar  1 12:37:00.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.01005245s
Mar  1 12:37:02.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.009647963s
Mar  1 12:37:04.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.009405911s
Mar  1 12:37:06.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.009453874s
Mar  1 12:37:08.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.009752625s
Mar  1 12:37:10.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.009247739s
Mar  1 12:37:12.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.009206324s
Mar  1 12:37:14.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.009208316s
Mar  1 12:37:16.596: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.011725321s
Mar  1 12:37:18.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.009891873s
Mar  1 12:37:20.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.009237609s
Mar  1 12:37:22.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.008655553s
Mar  1 12:37:24.595: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.010422943s
Mar  1 12:37:26.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.009088628s
Mar  1 12:37:28.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.009446133s
Mar  1 12:37:30.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.008869162s
Mar  1 12:37:32.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.010332012s
Mar  1 12:37:34.596: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.011856622s
Mar  1 12:37:36.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.009279206s
Mar  1 12:37:38.595: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.010713736s
Mar  1 12:37:40.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.008703051s
Mar  1 12:37:42.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.009806687s
Mar  1 12:37:44.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.009799212s
Mar  1 12:37:46.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.009481828s
Mar  1 12:37:48.595: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.011003946s
Mar  1 12:37:50.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.009970951s
Mar  1 12:37:52.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.009146687s
Mar  1 12:37:54.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.009820703s
Mar  1 12:37:56.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.009499134s
Mar  1 12:37:58.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.008878872s
Mar  1 12:38:00.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.008745441s
Mar  1 12:38:02.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.009435964s
Mar  1 12:38:04.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.008903704s
Mar  1 12:38:06.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.009293166s
Mar  1 12:38:08.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.009804189s
Mar  1 12:38:10.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.009581931s
Mar  1 12:38:12.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.008886211s
Mar  1 12:38:14.597: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.012486035s
Mar  1 12:38:16.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.010210967s
Mar  1 12:38:18.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.00963566s
Mar  1 12:38:20.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.009863591s
Mar  1 12:38:22.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.009775843s
Mar  1 12:38:24.595: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.01121394s
Mar  1 12:38:26.595: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.010612511s
Mar  1 12:38:28.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.009634357s
Mar  1 12:38:30.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.009320517s
Mar  1 12:38:32.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.010179216s
Mar  1 12:38:34.595: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.010960946s
Mar  1 12:38:36.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.009414192s
Mar  1 12:38:38.595: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.010414622s
Mar  1 12:38:40.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.009342438s
Mar  1 12:38:42.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.008768792s
Mar  1 12:38:44.596: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.011972198s
Mar  1 12:38:46.595: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.01037494s
Mar  1 12:38:48.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.009145028s
Mar  1 12:38:50.595: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.010626937s
Mar  1 12:38:52.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.009453659s
Mar  1 12:38:54.596: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.012044433s
Mar  1 12:38:56.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.009167811s
Mar  1 12:38:58.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.009953824s
Mar  1 12:39:00.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.009340816s
Mar  1 12:39:02.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.009239519s
Mar  1 12:39:04.595: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.010849979s
Mar  1 12:39:06.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.009914061s
Mar  1 12:39:08.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.009320661s
Mar  1 12:39:10.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.010083069s
Mar  1 12:39:12.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.009148351s
Mar  1 12:39:14.595: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.011072427s
Mar  1 12:39:16.595: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.011041157s
Mar  1 12:39:18.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.009837809s
Mar  1 12:39:20.599: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.014768442s
Mar  1 12:39:22.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.009401434s
Mar  1 12:39:24.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.010213073s
Mar  1 12:39:26.595: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.010786999s
Mar  1 12:39:28.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.009972735s
Mar  1 12:39:30.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.009750999s
Mar  1 12:39:32.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.009505712s
Mar  1 12:39:34.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.009212014s
Mar  1 12:39:36.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.009267951s
Mar  1 12:39:38.595: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.010839428s
Mar  1 12:39:40.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.008997013s
Mar  1 12:39:42.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.009539423s
Mar  1 12:39:44.595: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.01051248s
Mar  1 12:39:46.596: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.011468037s
Mar  1 12:39:48.595: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.010599688s
Mar  1 12:39:50.595: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.010382717s
Mar  1 12:39:52.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.009145596s
Mar  1 12:39:54.596: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.011521634s
Mar  1 12:39:56.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.009590702s
Mar  1 12:39:58.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.009386409s
Mar  1 12:40:00.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.008598812s
Mar  1 12:40:02.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.009913879s
Mar  1 12:40:04.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.010236188s
Mar  1 12:40:06.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.009244586s
Mar  1 12:40:08.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.009911883s
Mar  1 12:40:10.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.009039394s
Mar  1 12:40:12.595: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.011117779s
Mar  1 12:40:14.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.018735541s
Mar  1 12:40:16.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.010167113s
Mar  1 12:40:18.595: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.010458781s
Mar  1 12:40:20.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.009642382s
Mar  1 12:40:22.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.009924825s
Mar  1 12:40:24.595: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.010899149s
Mar  1 12:40:24.600: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.015352129s
STEP: removing the label kubernetes.io/e2e-9ee709d1-d4d7-42be-8b3b-17098cd5a5aa off the node lab1-k8s-node-3 03/01/23 12:40:24.6
STEP: verifying the node doesn't have the label kubernetes.io/e2e-9ee709d1-d4d7-42be-8b3b-17098cd5a5aa 03/01/23 12:40:24.616
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Mar  1 12:40:24.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5869" for this suite. 03/01/23 12:40:24.631
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","completed":198,"skipped":3665,"failed":0}
------------------------------
â€¢ [SLOW TEST] [304.234 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:35:20.407
    Mar  1 12:35:20.407: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename sched-pred 03/01/23 12:35:20.408
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:35:20.433
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:35:20.436
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Mar  1 12:35:20.438: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Mar  1 12:35:20.451: INFO: Waiting for terminating namespaces to be deleted...
    Mar  1 12:35:20.455: INFO: 
    Logging pods the apiserver thinks is on node lab1-k8s-node-1 before test
    Mar  1 12:35:20.465: INFO: test-new-deployment-845c8977d9-w4v8b from deployment-1211 started at 2023-03-01 12:35:20 +0000 UTC (1 container statuses recorded)
    Mar  1 12:35:20.465: INFO: 	Container httpd ready: false, restart count 0
    Mar  1 12:35:20.465: INFO: calico-node-kjj57 from kube-system started at 2023-03-01 11:36:06 +0000 UTC (1 container statuses recorded)
    Mar  1 12:35:20.465: INFO: 	Container calico-node ready: true, restart count 1
    Mar  1 12:35:20.465: INFO: csi-cinder-nodeplugin-fjt6c from kube-system started at 2023-03-01 11:38:24 +0000 UTC (3 container statuses recorded)
    Mar  1 12:35:20.465: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Mar  1 12:35:20.465: INFO: 	Container liveness-probe ready: true, restart count 0
    Mar  1 12:35:20.465: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Mar  1 12:35:20.465: INFO: kube-proxy-xmdzj from kube-system started at 2023-03-01 11:35:32 +0000 UTC (1 container statuses recorded)
    Mar  1 12:35:20.465: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar  1 12:35:20.465: INFO: metrics-server-6bd8d699c5-pwxfp from kube-system started at 2023-03-01 11:38:24 +0000 UTC (1 container statuses recorded)
    Mar  1 12:35:20.465: INFO: 	Container metrics-server ready: true, restart count 0
    Mar  1 12:35:20.465: INFO: nginx-proxy-lab1-k8s-node-1 from kube-system started at 2023-03-01 11:36:22 +0000 UTC (1 container statuses recorded)
    Mar  1 12:35:20.465: INFO: 	Container nginx-proxy ready: true, restart count 0
    Mar  1 12:35:20.465: INFO: nodelocaldns-mclb6 from kube-system started at 2023-03-01 11:37:23 +0000 UTC (1 container statuses recorded)
    Mar  1 12:35:20.465: INFO: 	Container node-cache ready: true, restart count 0
    Mar  1 12:35:20.465: INFO: snapshot-controller-7d445c66c9-m2qf9 from kube-system started at 2023-03-01 12:18:37 +0000 UTC (1 container statuses recorded)
    Mar  1 12:35:20.465: INFO: 	Container snapshot-controller ready: true, restart count 0
    Mar  1 12:35:20.465: INFO: sonobuoy-e2e-job-5f1d4571b8c24260 from sonobuoy started at 2023-03-01 11:42:09 +0000 UTC (2 container statuses recorded)
    Mar  1 12:35:20.465: INFO: 	Container e2e ready: true, restart count 0
    Mar  1 12:35:20.465: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar  1 12:35:20.465: INFO: sonobuoy-systemd-logs-daemon-set-1321f7b040854c53-9ckr7 from sonobuoy started at 2023-03-01 11:42:09 +0000 UTC (2 container statuses recorded)
    Mar  1 12:35:20.465: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar  1 12:35:20.465: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar  1 12:35:20.465: INFO: 
    Logging pods the apiserver thinks is on node lab1-k8s-node-2 before test
    Mar  1 12:35:20.479: INFO: test-new-deployment-845c8977d9-8b69v from deployment-1211 started at 2023-03-01 12:35:20 +0000 UTC (1 container statuses recorded)
    Mar  1 12:35:20.479: INFO: 	Container httpd ready: false, restart count 0
    Mar  1 12:35:20.479: INFO: calico-kube-controllers-ff45567bb-9k2q7 from kube-system started at 2023-03-01 11:37:06 +0000 UTC (1 container statuses recorded)
    Mar  1 12:35:20.479: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Mar  1 12:35:20.479: INFO: calico-node-5vzf7 from kube-system started at 2023-03-01 11:36:06 +0000 UTC (1 container statuses recorded)
    Mar  1 12:35:20.479: INFO: 	Container calico-node ready: true, restart count 1
    Mar  1 12:35:20.479: INFO: csi-cinder-controllerplugin-6f68fbd578-krvcc from kube-system started at 2023-03-01 11:38:24 +0000 UTC (6 container statuses recorded)
    Mar  1 12:35:20.479: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Mar  1 12:35:20.479: INFO: 	Container csi-attacher ready: true, restart count 0
    Mar  1 12:35:20.479: INFO: 	Container csi-provisioner ready: true, restart count 0
    Mar  1 12:35:20.479: INFO: 	Container csi-resizer ready: true, restart count 0
    Mar  1 12:35:20.479: INFO: 	Container csi-snapshotter ready: true, restart count 0
    Mar  1 12:35:20.479: INFO: 	Container liveness-probe ready: true, restart count 0
    Mar  1 12:35:20.479: INFO: csi-cinder-nodeplugin-zl564 from kube-system started at 2023-03-01 11:38:24 +0000 UTC (3 container statuses recorded)
    Mar  1 12:35:20.479: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Mar  1 12:35:20.479: INFO: 	Container liveness-probe ready: true, restart count 0
    Mar  1 12:35:20.479: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Mar  1 12:35:20.479: INFO: kube-proxy-jllc6 from kube-system started at 2023-03-01 11:35:32 +0000 UTC (1 container statuses recorded)
    Mar  1 12:35:20.479: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar  1 12:35:20.479: INFO: nginx-proxy-lab1-k8s-node-2 from kube-system started at 2023-03-01 11:36:22 +0000 UTC (1 container statuses recorded)
    Mar  1 12:35:20.479: INFO: 	Container nginx-proxy ready: true, restart count 0
    Mar  1 12:35:20.479: INFO: nodelocaldns-mhm2s from kube-system started at 2023-03-01 11:37:23 +0000 UTC (1 container statuses recorded)
    Mar  1 12:35:20.479: INFO: 	Container node-cache ready: true, restart count 0
    Mar  1 12:35:20.479: INFO: sonobuoy-systemd-logs-daemon-set-1321f7b040854c53-nklmj from sonobuoy started at 2023-03-01 11:42:09 +0000 UTC (2 container statuses recorded)
    Mar  1 12:35:20.479: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar  1 12:35:20.479: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar  1 12:35:20.479: INFO: 
    Logging pods the apiserver thinks is on node lab1-k8s-node-3 before test
    Mar  1 12:35:20.492: INFO: test-new-deployment-845c8977d9-7ssc5 from deployment-1211 started at 2023-03-01 12:35:18 +0000 UTC (1 container statuses recorded)
    Mar  1 12:35:20.492: INFO: 	Container httpd ready: true, restart count 0
    Mar  1 12:35:20.492: INFO: test-new-deployment-845c8977d9-w9m7p from deployment-1211 started at 2023-03-01 12:35:20 +0000 UTC (1 container statuses recorded)
    Mar  1 12:35:20.492: INFO: 	Container httpd ready: false, restart count 0
    Mar  1 12:35:20.492: INFO: calico-node-zjksl from kube-system started at 2023-03-01 11:36:06 +0000 UTC (1 container statuses recorded)
    Mar  1 12:35:20.492: INFO: 	Container calico-node ready: true, restart count 1
    Mar  1 12:35:20.492: INFO: csi-cinder-nodeplugin-xh6tw from kube-system started at 2023-03-01 12:27:48 +0000 UTC (3 container statuses recorded)
    Mar  1 12:35:20.492: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Mar  1 12:35:20.492: INFO: 	Container liveness-probe ready: true, restart count 0
    Mar  1 12:35:20.492: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Mar  1 12:35:20.492: INFO: kube-proxy-2jcfl from kube-system started at 2023-03-01 11:35:29 +0000 UTC (1 container statuses recorded)
    Mar  1 12:35:20.492: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar  1 12:35:20.492: INFO: nginx-proxy-lab1-k8s-node-3 from kube-system started at 2023-03-01 11:38:59 +0000 UTC (1 container statuses recorded)
    Mar  1 12:35:20.492: INFO: 	Container nginx-proxy ready: true, restart count 0
    Mar  1 12:35:20.492: INFO: nodelocaldns-5tw4l from kube-system started at 2023-03-01 11:37:23 +0000 UTC (1 container statuses recorded)
    Mar  1 12:35:20.492: INFO: 	Container node-cache ready: true, restart count 0
    Mar  1 12:35:20.492: INFO: sonobuoy from sonobuoy started at 2023-03-01 11:42:06 +0000 UTC (1 container statuses recorded)
    Mar  1 12:35:20.492: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Mar  1 12:35:20.492: INFO: sonobuoy-systemd-logs-daemon-set-1321f7b040854c53-9b52c from sonobuoy started at 2023-03-01 11:42:09 +0000 UTC (2 container statuses recorded)
    Mar  1 12:35:20.492: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar  1 12:35:20.492: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:699
    STEP: Trying to launch a pod without a label to get a node which can launch it. 03/01/23 12:35:20.492
    Mar  1 12:35:20.502: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-5869" to be "running"
    Mar  1 12:35:20.509: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 6.436926ms
    Mar  1 12:35:22.514: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.011923202s
    Mar  1 12:35:22.515: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 03/01/23 12:35:22.519
    STEP: Trying to apply a random label on the found node. 03/01/23 12:35:22.541
    STEP: verifying the node has the label kubernetes.io/e2e-9ee709d1-d4d7-42be-8b3b-17098cd5a5aa 95 03/01/23 12:35:22.554
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 03/01/23 12:35:22.56
    Mar  1 12:35:22.567: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-5869" to be "not pending"
    Mar  1 12:35:22.571: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.670061ms
    Mar  1 12:35:24.576: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.00877311s
    Mar  1 12:35:24.576: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.128.2.241 on the node which pod4 resides and expect not scheduled 03/01/23 12:35:24.576
    Mar  1 12:35:24.584: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-5869" to be "not pending"
    Mar  1 12:35:24.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.16046ms
    Mar  1 12:35:26.595: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010421638s
    Mar  1 12:35:28.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009768487s
    Mar  1 12:35:30.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.00929251s
    Mar  1 12:35:32.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.010071333s
    Mar  1 12:35:34.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.009537902s
    Mar  1 12:35:36.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.008968517s
    Mar  1 12:35:38.595: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.01103425s
    Mar  1 12:35:40.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.010267931s
    Mar  1 12:35:42.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.008588715s
    Mar  1 12:35:44.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.008599984s
    Mar  1 12:35:46.595: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.01078526s
    Mar  1 12:35:48.596: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.01206324s
    Mar  1 12:35:50.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.010091967s
    Mar  1 12:35:52.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.009889702s
    Mar  1 12:35:54.595: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.011109164s
    Mar  1 12:35:56.595: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.010481757s
    Mar  1 12:35:58.595: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.010480891s
    Mar  1 12:36:00.597: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.012740442s
    Mar  1 12:36:02.595: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.010501818s
    Mar  1 12:36:04.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.009545647s
    Mar  1 12:36:06.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.008914953s
    Mar  1 12:36:08.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.0095762s
    Mar  1 12:36:10.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.009016788s
    Mar  1 12:36:12.596: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.01225591s
    Mar  1 12:36:14.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.00909276s
    Mar  1 12:36:16.595: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.01123861s
    Mar  1 12:36:18.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.009898583s
    Mar  1 12:36:20.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.008809814s
    Mar  1 12:36:22.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.009919862s
    Mar  1 12:36:24.595: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.011207433s
    Mar  1 12:36:26.595: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.011278707s
    Mar  1 12:36:28.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.009053678s
    Mar  1 12:36:30.595: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.010847599s
    Mar  1 12:36:32.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.010018531s
    Mar  1 12:36:34.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.009487789s
    Mar  1 12:36:36.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.009373586s
    Mar  1 12:36:38.595: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.010513428s
    Mar  1 12:36:40.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.010008464s
    Mar  1 12:36:42.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.008955303s
    Mar  1 12:36:44.595: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.010381222s
    Mar  1 12:36:46.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.01005799s
    Mar  1 12:36:48.595: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.011007714s
    Mar  1 12:36:50.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.010059442s
    Mar  1 12:36:52.596: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.011677216s
    Mar  1 12:36:54.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.009046106s
    Mar  1 12:36:56.595: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.011011143s
    Mar  1 12:36:58.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.009620328s
    Mar  1 12:37:00.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.01005245s
    Mar  1 12:37:02.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.009647963s
    Mar  1 12:37:04.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.009405911s
    Mar  1 12:37:06.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.009453874s
    Mar  1 12:37:08.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.009752625s
    Mar  1 12:37:10.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.009247739s
    Mar  1 12:37:12.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.009206324s
    Mar  1 12:37:14.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.009208316s
    Mar  1 12:37:16.596: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.011725321s
    Mar  1 12:37:18.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.009891873s
    Mar  1 12:37:20.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.009237609s
    Mar  1 12:37:22.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.008655553s
    Mar  1 12:37:24.595: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.010422943s
    Mar  1 12:37:26.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.009088628s
    Mar  1 12:37:28.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.009446133s
    Mar  1 12:37:30.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.008869162s
    Mar  1 12:37:32.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.010332012s
    Mar  1 12:37:34.596: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.011856622s
    Mar  1 12:37:36.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.009279206s
    Mar  1 12:37:38.595: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.010713736s
    Mar  1 12:37:40.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.008703051s
    Mar  1 12:37:42.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.009806687s
    Mar  1 12:37:44.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.009799212s
    Mar  1 12:37:46.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.009481828s
    Mar  1 12:37:48.595: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.011003946s
    Mar  1 12:37:50.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.009970951s
    Mar  1 12:37:52.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.009146687s
    Mar  1 12:37:54.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.009820703s
    Mar  1 12:37:56.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.009499134s
    Mar  1 12:37:58.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.008878872s
    Mar  1 12:38:00.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.008745441s
    Mar  1 12:38:02.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.009435964s
    Mar  1 12:38:04.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.008903704s
    Mar  1 12:38:06.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.009293166s
    Mar  1 12:38:08.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.009804189s
    Mar  1 12:38:10.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.009581931s
    Mar  1 12:38:12.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.008886211s
    Mar  1 12:38:14.597: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.012486035s
    Mar  1 12:38:16.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.010210967s
    Mar  1 12:38:18.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.00963566s
    Mar  1 12:38:20.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.009863591s
    Mar  1 12:38:22.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.009775843s
    Mar  1 12:38:24.595: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.01121394s
    Mar  1 12:38:26.595: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.010612511s
    Mar  1 12:38:28.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.009634357s
    Mar  1 12:38:30.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.009320517s
    Mar  1 12:38:32.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.010179216s
    Mar  1 12:38:34.595: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.010960946s
    Mar  1 12:38:36.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.009414192s
    Mar  1 12:38:38.595: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.010414622s
    Mar  1 12:38:40.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.009342438s
    Mar  1 12:38:42.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.008768792s
    Mar  1 12:38:44.596: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.011972198s
    Mar  1 12:38:46.595: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.01037494s
    Mar  1 12:38:48.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.009145028s
    Mar  1 12:38:50.595: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.010626937s
    Mar  1 12:38:52.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.009453659s
    Mar  1 12:38:54.596: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.012044433s
    Mar  1 12:38:56.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.009167811s
    Mar  1 12:38:58.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.009953824s
    Mar  1 12:39:00.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.009340816s
    Mar  1 12:39:02.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.009239519s
    Mar  1 12:39:04.595: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.010849979s
    Mar  1 12:39:06.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.009914061s
    Mar  1 12:39:08.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.009320661s
    Mar  1 12:39:10.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.010083069s
    Mar  1 12:39:12.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.009148351s
    Mar  1 12:39:14.595: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.011072427s
    Mar  1 12:39:16.595: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.011041157s
    Mar  1 12:39:18.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.009837809s
    Mar  1 12:39:20.599: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.014768442s
    Mar  1 12:39:22.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.009401434s
    Mar  1 12:39:24.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.010213073s
    Mar  1 12:39:26.595: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.010786999s
    Mar  1 12:39:28.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.009972735s
    Mar  1 12:39:30.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.009750999s
    Mar  1 12:39:32.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.009505712s
    Mar  1 12:39:34.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.009212014s
    Mar  1 12:39:36.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.009267951s
    Mar  1 12:39:38.595: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.010839428s
    Mar  1 12:39:40.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.008997013s
    Mar  1 12:39:42.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.009539423s
    Mar  1 12:39:44.595: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.01051248s
    Mar  1 12:39:46.596: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.011468037s
    Mar  1 12:39:48.595: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.010599688s
    Mar  1 12:39:50.595: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.010382717s
    Mar  1 12:39:52.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.009145596s
    Mar  1 12:39:54.596: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.011521634s
    Mar  1 12:39:56.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.009590702s
    Mar  1 12:39:58.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.009386409s
    Mar  1 12:40:00.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.008598812s
    Mar  1 12:40:02.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.009913879s
    Mar  1 12:40:04.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.010236188s
    Mar  1 12:40:06.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.009244586s
    Mar  1 12:40:08.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.009911883s
    Mar  1 12:40:10.593: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.009039394s
    Mar  1 12:40:12.595: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.011117779s
    Mar  1 12:40:14.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.018735541s
    Mar  1 12:40:16.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.010167113s
    Mar  1 12:40:18.595: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.010458781s
    Mar  1 12:40:20.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.009642382s
    Mar  1 12:40:22.594: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.009924825s
    Mar  1 12:40:24.595: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.010899149s
    Mar  1 12:40:24.600: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.015352129s
    STEP: removing the label kubernetes.io/e2e-9ee709d1-d4d7-42be-8b3b-17098cd5a5aa off the node lab1-k8s-node-3 03/01/23 12:40:24.6
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-9ee709d1-d4d7-42be-8b3b-17098cd5a5aa 03/01/23 12:40:24.616
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Mar  1 12:40:24.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-5869" for this suite. 03/01/23 12:40:24.631
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:40:24.646
Mar  1 12:40:24.646: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename podtemplate 03/01/23 12:40:24.647
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:40:24.669
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:40:24.673
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 03/01/23 12:40:24.676
STEP: Replace a pod template 03/01/23 12:40:24.683
Mar  1 12:40:24.695: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Mar  1 12:40:24.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-4732" for this suite. 03/01/23 12:40:24.708
{"msg":"PASSED [sig-node] PodTemplates should replace a pod template [Conformance]","completed":199,"skipped":3679,"failed":0}
------------------------------
â€¢ [0.071 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:40:24.646
    Mar  1 12:40:24.646: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename podtemplate 03/01/23 12:40:24.647
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:40:24.669
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:40:24.673
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 03/01/23 12:40:24.676
    STEP: Replace a pod template 03/01/23 12:40:24.683
    Mar  1 12:40:24.695: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Mar  1 12:40:24.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-4732" for this suite. 03/01/23 12:40:24.708
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:40:24.723
Mar  1 12:40:24.723: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename webhook 03/01/23 12:40:24.724
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:40:24.75
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:40:24.753
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/01/23 12:40:24.782
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/01/23 12:40:25.109
STEP: Deploying the webhook pod 03/01/23 12:40:25.12
STEP: Wait for the deployment to be ready 03/01/23 12:40:25.137
Mar  1 12:40:25.162: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/01/23 12:40:27.176
STEP: Verifying the service has paired with the endpoint 03/01/23 12:40:27.189
Mar  1 12:40:28.189: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
STEP: Registering the webhook via the AdmissionRegistration API 03/01/23 12:40:28.194
STEP: create a pod 03/01/23 12:40:28.213
Mar  1 12:40:28.224: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-9673" to be "running"
Mar  1 12:40:28.228: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.8724ms
Mar  1 12:40:30.233: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009186602s
Mar  1 12:40:30.233: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 03/01/23 12:40:30.233
Mar  1 12:40:30.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=webhook-9673 attach --namespace=webhook-9673 to-be-attached-pod -i -c=container1'
Mar  1 12:40:30.311: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  1 12:40:30.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9673" for this suite. 03/01/23 12:40:30.334
STEP: Destroying namespace "webhook-9673-markers" for this suite. 03/01/23 12:40:30.346
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","completed":200,"skipped":3681,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.701 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:40:24.723
    Mar  1 12:40:24.723: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename webhook 03/01/23 12:40:24.724
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:40:24.75
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:40:24.753
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/01/23 12:40:24.782
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/01/23 12:40:25.109
    STEP: Deploying the webhook pod 03/01/23 12:40:25.12
    STEP: Wait for the deployment to be ready 03/01/23 12:40:25.137
    Mar  1 12:40:25.162: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/01/23 12:40:27.176
    STEP: Verifying the service has paired with the endpoint 03/01/23 12:40:27.189
    Mar  1 12:40:28.189: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:208
    STEP: Registering the webhook via the AdmissionRegistration API 03/01/23 12:40:28.194
    STEP: create a pod 03/01/23 12:40:28.213
    Mar  1 12:40:28.224: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-9673" to be "running"
    Mar  1 12:40:28.228: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.8724ms
    Mar  1 12:40:30.233: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009186602s
    Mar  1 12:40:30.233: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 03/01/23 12:40:30.233
    Mar  1 12:40:30.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=webhook-9673 attach --namespace=webhook-9673 to-be-attached-pod -i -c=container1'
    Mar  1 12:40:30.311: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  1 12:40:30.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-9673" for this suite. 03/01/23 12:40:30.334
    STEP: Destroying namespace "webhook-9673-markers" for this suite. 03/01/23 12:40:30.346
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:40:30.425
Mar  1 12:40:30.425: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename sched-pred 03/01/23 12:40:30.426
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:40:30.501
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:40:30.504
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Mar  1 12:40:30.507: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar  1 12:40:30.518: INFO: Waiting for terminating namespaces to be deleted...
Mar  1 12:40:30.523: INFO: 
Logging pods the apiserver thinks is on node lab1-k8s-node-1 before test
Mar  1 12:40:30.531: INFO: calico-node-kjj57 from kube-system started at 2023-03-01 11:36:06 +0000 UTC (1 container statuses recorded)
Mar  1 12:40:30.532: INFO: 	Container calico-node ready: true, restart count 1
Mar  1 12:40:30.532: INFO: csi-cinder-nodeplugin-fjt6c from kube-system started at 2023-03-01 11:38:24 +0000 UTC (3 container statuses recorded)
Mar  1 12:40:30.532: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Mar  1 12:40:30.532: INFO: 	Container liveness-probe ready: true, restart count 0
Mar  1 12:40:30.532: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar  1 12:40:30.532: INFO: kube-proxy-xmdzj from kube-system started at 2023-03-01 11:35:32 +0000 UTC (1 container statuses recorded)
Mar  1 12:40:30.532: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  1 12:40:30.532: INFO: metrics-server-6bd8d699c5-pwxfp from kube-system started at 2023-03-01 11:38:24 +0000 UTC (1 container statuses recorded)
Mar  1 12:40:30.532: INFO: 	Container metrics-server ready: true, restart count 0
Mar  1 12:40:30.532: INFO: nginx-proxy-lab1-k8s-node-1 from kube-system started at 2023-03-01 11:36:22 +0000 UTC (1 container statuses recorded)
Mar  1 12:40:30.532: INFO: 	Container nginx-proxy ready: true, restart count 0
Mar  1 12:40:30.532: INFO: nodelocaldns-mclb6 from kube-system started at 2023-03-01 11:37:23 +0000 UTC (1 container statuses recorded)
Mar  1 12:40:30.532: INFO: 	Container node-cache ready: true, restart count 0
Mar  1 12:40:30.532: INFO: snapshot-controller-7d445c66c9-m2qf9 from kube-system started at 2023-03-01 12:18:37 +0000 UTC (1 container statuses recorded)
Mar  1 12:40:30.532: INFO: 	Container snapshot-controller ready: true, restart count 0
Mar  1 12:40:30.532: INFO: sonobuoy-e2e-job-5f1d4571b8c24260 from sonobuoy started at 2023-03-01 11:42:09 +0000 UTC (2 container statuses recorded)
Mar  1 12:40:30.532: INFO: 	Container e2e ready: true, restart count 0
Mar  1 12:40:30.532: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  1 12:40:30.532: INFO: sonobuoy-systemd-logs-daemon-set-1321f7b040854c53-9ckr7 from sonobuoy started at 2023-03-01 11:42:09 +0000 UTC (2 container statuses recorded)
Mar  1 12:40:30.533: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  1 12:40:30.533: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  1 12:40:30.533: INFO: 
Logging pods the apiserver thinks is on node lab1-k8s-node-2 before test
Mar  1 12:40:30.545: INFO: calico-kube-controllers-ff45567bb-9k2q7 from kube-system started at 2023-03-01 11:37:06 +0000 UTC (1 container statuses recorded)
Mar  1 12:40:30.545: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Mar  1 12:40:30.545: INFO: calico-node-5vzf7 from kube-system started at 2023-03-01 11:36:06 +0000 UTC (1 container statuses recorded)
Mar  1 12:40:30.545: INFO: 	Container calico-node ready: true, restart count 1
Mar  1 12:40:30.545: INFO: csi-cinder-controllerplugin-6f68fbd578-krvcc from kube-system started at 2023-03-01 11:38:24 +0000 UTC (6 container statuses recorded)
Mar  1 12:40:30.545: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Mar  1 12:40:30.545: INFO: 	Container csi-attacher ready: true, restart count 0
Mar  1 12:40:30.545: INFO: 	Container csi-provisioner ready: true, restart count 0
Mar  1 12:40:30.545: INFO: 	Container csi-resizer ready: true, restart count 0
Mar  1 12:40:30.545: INFO: 	Container csi-snapshotter ready: true, restart count 0
Mar  1 12:40:30.545: INFO: 	Container liveness-probe ready: true, restart count 0
Mar  1 12:40:30.545: INFO: csi-cinder-nodeplugin-zl564 from kube-system started at 2023-03-01 11:38:24 +0000 UTC (3 container statuses recorded)
Mar  1 12:40:30.546: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Mar  1 12:40:30.546: INFO: 	Container liveness-probe ready: true, restart count 0
Mar  1 12:40:30.546: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar  1 12:40:30.546: INFO: kube-proxy-jllc6 from kube-system started at 2023-03-01 11:35:32 +0000 UTC (1 container statuses recorded)
Mar  1 12:40:30.546: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  1 12:40:30.546: INFO: nginx-proxy-lab1-k8s-node-2 from kube-system started at 2023-03-01 11:36:22 +0000 UTC (1 container statuses recorded)
Mar  1 12:40:30.546: INFO: 	Container nginx-proxy ready: true, restart count 0
Mar  1 12:40:30.546: INFO: nodelocaldns-mhm2s from kube-system started at 2023-03-01 11:37:23 +0000 UTC (1 container statuses recorded)
Mar  1 12:40:30.546: INFO: 	Container node-cache ready: true, restart count 0
Mar  1 12:40:30.546: INFO: sonobuoy-systemd-logs-daemon-set-1321f7b040854c53-nklmj from sonobuoy started at 2023-03-01 11:42:09 +0000 UTC (2 container statuses recorded)
Mar  1 12:40:30.546: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  1 12:40:30.546: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  1 12:40:30.546: INFO: 
Logging pods the apiserver thinks is on node lab1-k8s-node-3 before test
Mar  1 12:40:30.557: INFO: calico-node-zjksl from kube-system started at 2023-03-01 11:36:06 +0000 UTC (1 container statuses recorded)
Mar  1 12:40:30.557: INFO: 	Container calico-node ready: true, restart count 1
Mar  1 12:40:30.558: INFO: csi-cinder-nodeplugin-xh6tw from kube-system started at 2023-03-01 12:27:48 +0000 UTC (3 container statuses recorded)
Mar  1 12:40:30.558: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Mar  1 12:40:30.558: INFO: 	Container liveness-probe ready: true, restart count 0
Mar  1 12:40:30.558: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar  1 12:40:30.558: INFO: kube-proxy-2jcfl from kube-system started at 2023-03-01 11:35:29 +0000 UTC (1 container statuses recorded)
Mar  1 12:40:30.558: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  1 12:40:30.558: INFO: nginx-proxy-lab1-k8s-node-3 from kube-system started at 2023-03-01 11:38:59 +0000 UTC (1 container statuses recorded)
Mar  1 12:40:30.558: INFO: 	Container nginx-proxy ready: true, restart count 0
Mar  1 12:40:30.558: INFO: nodelocaldns-5tw4l from kube-system started at 2023-03-01 11:37:23 +0000 UTC (1 container statuses recorded)
Mar  1 12:40:30.558: INFO: 	Container node-cache ready: true, restart count 0
Mar  1 12:40:30.558: INFO: pod4 from sched-pred-5869 started at 2023-03-01 12:35:22 +0000 UTC (1 container statuses recorded)
Mar  1 12:40:30.558: INFO: 	Container agnhost ready: true, restart count 0
Mar  1 12:40:30.558: INFO: sonobuoy from sonobuoy started at 2023-03-01 11:42:06 +0000 UTC (1 container statuses recorded)
Mar  1 12:40:30.558: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar  1 12:40:30.558: INFO: sonobuoy-systemd-logs-daemon-set-1321f7b040854c53-9b52c from sonobuoy started at 2023-03-01 11:42:09 +0000 UTC (2 container statuses recorded)
Mar  1 12:40:30.558: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  1 12:40:30.558: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  1 12:40:30.558: INFO: to-be-attached-pod from webhook-9673 started at 2023-03-01 12:40:28 +0000 UTC (1 container statuses recorded)
Mar  1 12:40:30.558: INFO: 	Container container1 ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
STEP: Trying to launch a pod without a label to get a node which can launch it. 03/01/23 12:40:30.559
Mar  1 12:40:30.570: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-6713" to be "running"
Mar  1 12:40:30.574: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.99324ms
Mar  1 12:40:32.580: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.009690691s
Mar  1 12:40:32.580: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 03/01/23 12:40:32.584
STEP: Trying to apply a random label on the found node. 03/01/23 12:40:32.604
STEP: verifying the node has the label kubernetes.io/e2e-6f1af4c2-f7df-4eab-842e-07e5c3c8faef 42 03/01/23 12:40:32.617
STEP: Trying to relaunch the pod, now with labels. 03/01/23 12:40:32.623
Mar  1 12:40:32.629: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-6713" to be "not pending"
Mar  1 12:40:32.634: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 4.370861ms
Mar  1 12:40:34.640: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.010062013s
Mar  1 12:40:34.640: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-6f1af4c2-f7df-4eab-842e-07e5c3c8faef off the node lab1-k8s-node-3 03/01/23 12:40:34.644
STEP: verifying the node doesn't have the label kubernetes.io/e2e-6f1af4c2-f7df-4eab-842e-07e5c3c8faef 03/01/23 12:40:34.662
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Mar  1 12:40:34.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6713" for this suite. 03/01/23 12:40:34.674
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","completed":201,"skipped":3682,"failed":0}
------------------------------
â€¢ [4.258 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:40:30.425
    Mar  1 12:40:30.425: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename sched-pred 03/01/23 12:40:30.426
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:40:30.501
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:40:30.504
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Mar  1 12:40:30.507: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Mar  1 12:40:30.518: INFO: Waiting for terminating namespaces to be deleted...
    Mar  1 12:40:30.523: INFO: 
    Logging pods the apiserver thinks is on node lab1-k8s-node-1 before test
    Mar  1 12:40:30.531: INFO: calico-node-kjj57 from kube-system started at 2023-03-01 11:36:06 +0000 UTC (1 container statuses recorded)
    Mar  1 12:40:30.532: INFO: 	Container calico-node ready: true, restart count 1
    Mar  1 12:40:30.532: INFO: csi-cinder-nodeplugin-fjt6c from kube-system started at 2023-03-01 11:38:24 +0000 UTC (3 container statuses recorded)
    Mar  1 12:40:30.532: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Mar  1 12:40:30.532: INFO: 	Container liveness-probe ready: true, restart count 0
    Mar  1 12:40:30.532: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Mar  1 12:40:30.532: INFO: kube-proxy-xmdzj from kube-system started at 2023-03-01 11:35:32 +0000 UTC (1 container statuses recorded)
    Mar  1 12:40:30.532: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar  1 12:40:30.532: INFO: metrics-server-6bd8d699c5-pwxfp from kube-system started at 2023-03-01 11:38:24 +0000 UTC (1 container statuses recorded)
    Mar  1 12:40:30.532: INFO: 	Container metrics-server ready: true, restart count 0
    Mar  1 12:40:30.532: INFO: nginx-proxy-lab1-k8s-node-1 from kube-system started at 2023-03-01 11:36:22 +0000 UTC (1 container statuses recorded)
    Mar  1 12:40:30.532: INFO: 	Container nginx-proxy ready: true, restart count 0
    Mar  1 12:40:30.532: INFO: nodelocaldns-mclb6 from kube-system started at 2023-03-01 11:37:23 +0000 UTC (1 container statuses recorded)
    Mar  1 12:40:30.532: INFO: 	Container node-cache ready: true, restart count 0
    Mar  1 12:40:30.532: INFO: snapshot-controller-7d445c66c9-m2qf9 from kube-system started at 2023-03-01 12:18:37 +0000 UTC (1 container statuses recorded)
    Mar  1 12:40:30.532: INFO: 	Container snapshot-controller ready: true, restart count 0
    Mar  1 12:40:30.532: INFO: sonobuoy-e2e-job-5f1d4571b8c24260 from sonobuoy started at 2023-03-01 11:42:09 +0000 UTC (2 container statuses recorded)
    Mar  1 12:40:30.532: INFO: 	Container e2e ready: true, restart count 0
    Mar  1 12:40:30.532: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar  1 12:40:30.532: INFO: sonobuoy-systemd-logs-daemon-set-1321f7b040854c53-9ckr7 from sonobuoy started at 2023-03-01 11:42:09 +0000 UTC (2 container statuses recorded)
    Mar  1 12:40:30.533: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar  1 12:40:30.533: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar  1 12:40:30.533: INFO: 
    Logging pods the apiserver thinks is on node lab1-k8s-node-2 before test
    Mar  1 12:40:30.545: INFO: calico-kube-controllers-ff45567bb-9k2q7 from kube-system started at 2023-03-01 11:37:06 +0000 UTC (1 container statuses recorded)
    Mar  1 12:40:30.545: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Mar  1 12:40:30.545: INFO: calico-node-5vzf7 from kube-system started at 2023-03-01 11:36:06 +0000 UTC (1 container statuses recorded)
    Mar  1 12:40:30.545: INFO: 	Container calico-node ready: true, restart count 1
    Mar  1 12:40:30.545: INFO: csi-cinder-controllerplugin-6f68fbd578-krvcc from kube-system started at 2023-03-01 11:38:24 +0000 UTC (6 container statuses recorded)
    Mar  1 12:40:30.545: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Mar  1 12:40:30.545: INFO: 	Container csi-attacher ready: true, restart count 0
    Mar  1 12:40:30.545: INFO: 	Container csi-provisioner ready: true, restart count 0
    Mar  1 12:40:30.545: INFO: 	Container csi-resizer ready: true, restart count 0
    Mar  1 12:40:30.545: INFO: 	Container csi-snapshotter ready: true, restart count 0
    Mar  1 12:40:30.545: INFO: 	Container liveness-probe ready: true, restart count 0
    Mar  1 12:40:30.545: INFO: csi-cinder-nodeplugin-zl564 from kube-system started at 2023-03-01 11:38:24 +0000 UTC (3 container statuses recorded)
    Mar  1 12:40:30.546: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Mar  1 12:40:30.546: INFO: 	Container liveness-probe ready: true, restart count 0
    Mar  1 12:40:30.546: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Mar  1 12:40:30.546: INFO: kube-proxy-jllc6 from kube-system started at 2023-03-01 11:35:32 +0000 UTC (1 container statuses recorded)
    Mar  1 12:40:30.546: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar  1 12:40:30.546: INFO: nginx-proxy-lab1-k8s-node-2 from kube-system started at 2023-03-01 11:36:22 +0000 UTC (1 container statuses recorded)
    Mar  1 12:40:30.546: INFO: 	Container nginx-proxy ready: true, restart count 0
    Mar  1 12:40:30.546: INFO: nodelocaldns-mhm2s from kube-system started at 2023-03-01 11:37:23 +0000 UTC (1 container statuses recorded)
    Mar  1 12:40:30.546: INFO: 	Container node-cache ready: true, restart count 0
    Mar  1 12:40:30.546: INFO: sonobuoy-systemd-logs-daemon-set-1321f7b040854c53-nklmj from sonobuoy started at 2023-03-01 11:42:09 +0000 UTC (2 container statuses recorded)
    Mar  1 12:40:30.546: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar  1 12:40:30.546: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar  1 12:40:30.546: INFO: 
    Logging pods the apiserver thinks is on node lab1-k8s-node-3 before test
    Mar  1 12:40:30.557: INFO: calico-node-zjksl from kube-system started at 2023-03-01 11:36:06 +0000 UTC (1 container statuses recorded)
    Mar  1 12:40:30.557: INFO: 	Container calico-node ready: true, restart count 1
    Mar  1 12:40:30.558: INFO: csi-cinder-nodeplugin-xh6tw from kube-system started at 2023-03-01 12:27:48 +0000 UTC (3 container statuses recorded)
    Mar  1 12:40:30.558: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Mar  1 12:40:30.558: INFO: 	Container liveness-probe ready: true, restart count 0
    Mar  1 12:40:30.558: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Mar  1 12:40:30.558: INFO: kube-proxy-2jcfl from kube-system started at 2023-03-01 11:35:29 +0000 UTC (1 container statuses recorded)
    Mar  1 12:40:30.558: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar  1 12:40:30.558: INFO: nginx-proxy-lab1-k8s-node-3 from kube-system started at 2023-03-01 11:38:59 +0000 UTC (1 container statuses recorded)
    Mar  1 12:40:30.558: INFO: 	Container nginx-proxy ready: true, restart count 0
    Mar  1 12:40:30.558: INFO: nodelocaldns-5tw4l from kube-system started at 2023-03-01 11:37:23 +0000 UTC (1 container statuses recorded)
    Mar  1 12:40:30.558: INFO: 	Container node-cache ready: true, restart count 0
    Mar  1 12:40:30.558: INFO: pod4 from sched-pred-5869 started at 2023-03-01 12:35:22 +0000 UTC (1 container statuses recorded)
    Mar  1 12:40:30.558: INFO: 	Container agnhost ready: true, restart count 0
    Mar  1 12:40:30.558: INFO: sonobuoy from sonobuoy started at 2023-03-01 11:42:06 +0000 UTC (1 container statuses recorded)
    Mar  1 12:40:30.558: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Mar  1 12:40:30.558: INFO: sonobuoy-systemd-logs-daemon-set-1321f7b040854c53-9b52c from sonobuoy started at 2023-03-01 11:42:09 +0000 UTC (2 container statuses recorded)
    Mar  1 12:40:30.558: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar  1 12:40:30.558: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar  1 12:40:30.558: INFO: to-be-attached-pod from webhook-9673 started at 2023-03-01 12:40:28 +0000 UTC (1 container statuses recorded)
    Mar  1 12:40:30.558: INFO: 	Container container1 ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:461
    STEP: Trying to launch a pod without a label to get a node which can launch it. 03/01/23 12:40:30.559
    Mar  1 12:40:30.570: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-6713" to be "running"
    Mar  1 12:40:30.574: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.99324ms
    Mar  1 12:40:32.580: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.009690691s
    Mar  1 12:40:32.580: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 03/01/23 12:40:32.584
    STEP: Trying to apply a random label on the found node. 03/01/23 12:40:32.604
    STEP: verifying the node has the label kubernetes.io/e2e-6f1af4c2-f7df-4eab-842e-07e5c3c8faef 42 03/01/23 12:40:32.617
    STEP: Trying to relaunch the pod, now with labels. 03/01/23 12:40:32.623
    Mar  1 12:40:32.629: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-6713" to be "not pending"
    Mar  1 12:40:32.634: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 4.370861ms
    Mar  1 12:40:34.640: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.010062013s
    Mar  1 12:40:34.640: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-6f1af4c2-f7df-4eab-842e-07e5c3c8faef off the node lab1-k8s-node-3 03/01/23 12:40:34.644
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-6f1af4c2-f7df-4eab-842e-07e5c3c8faef 03/01/23 12:40:34.662
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Mar  1 12:40:34.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-6713" for this suite. 03/01/23 12:40:34.674
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:40:34.687
Mar  1 12:40:34.687: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename cronjob 03/01/23 12:40:34.688
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:40:34.709
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:40:34.712
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 03/01/23 12:40:34.715
STEP: Ensuring a job is scheduled 03/01/23 12:40:34.723
STEP: Ensuring exactly one is scheduled 03/01/23 12:41:00.728
STEP: Ensuring exactly one running job exists by listing jobs explicitly 03/01/23 12:41:00.734
STEP: Ensuring no more jobs are scheduled 03/01/23 12:41:00.738
STEP: Removing cronjob 03/01/23 12:46:00.749
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Mar  1 12:46:00.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-3420" for this suite. 03/01/23 12:46:00.764
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","completed":202,"skipped":3727,"failed":0}
------------------------------
â€¢ [SLOW TEST] [326.086 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:40:34.687
    Mar  1 12:40:34.687: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename cronjob 03/01/23 12:40:34.688
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:40:34.709
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:40:34.712
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 03/01/23 12:40:34.715
    STEP: Ensuring a job is scheduled 03/01/23 12:40:34.723
    STEP: Ensuring exactly one is scheduled 03/01/23 12:41:00.728
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 03/01/23 12:41:00.734
    STEP: Ensuring no more jobs are scheduled 03/01/23 12:41:00.738
    STEP: Removing cronjob 03/01/23 12:46:00.749
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Mar  1 12:46:00.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-3420" for this suite. 03/01/23 12:46:00.764
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:46:00.773
Mar  1 12:46:00.774: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename crd-publish-openapi 03/01/23 12:46:00.774
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:46:00.803
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:46:00.807
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 03/01/23 12:46:00.809
Mar  1 12:46:00.810: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
Mar  1 12:46:08.496: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  1 12:46:24.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-971" for this suite. 03/01/23 12:46:24.465
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","completed":203,"skipped":3730,"failed":0}
------------------------------
â€¢ [SLOW TEST] [23.701 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:46:00.773
    Mar  1 12:46:00.774: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename crd-publish-openapi 03/01/23 12:46:00.774
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:46:00.803
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:46:00.807
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:275
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 03/01/23 12:46:00.809
    Mar  1 12:46:00.810: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    Mar  1 12:46:08.496: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  1 12:46:24.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-971" for this suite. 03/01/23 12:46:24.465
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:46:24.479
Mar  1 12:46:24.480: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename job 03/01/23 12:46:24.48
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:46:24.503
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:46:24.506
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
STEP: Creating a job 03/01/23 12:46:24.51
STEP: Ensuring active pods == parallelism 03/01/23 12:46:24.516
STEP: Orphaning one of the Job's Pods 03/01/23 12:46:26.523
Mar  1 12:46:27.051: INFO: Successfully updated pod "adopt-release-jh628"
STEP: Checking that the Job readopts the Pod 03/01/23 12:46:27.051
Mar  1 12:46:27.051: INFO: Waiting up to 15m0s for pod "adopt-release-jh628" in namespace "job-2464" to be "adopted"
Mar  1 12:46:27.057: INFO: Pod "adopt-release-jh628": Phase="Running", Reason="", readiness=true. Elapsed: 5.732633ms
Mar  1 12:46:29.075: INFO: Pod "adopt-release-jh628": Phase="Running", Reason="", readiness=true. Elapsed: 2.023907652s
Mar  1 12:46:29.075: INFO: Pod "adopt-release-jh628" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 03/01/23 12:46:29.075
Mar  1 12:46:29.598: INFO: Successfully updated pod "adopt-release-jh628"
STEP: Checking that the Job releases the Pod 03/01/23 12:46:29.598
Mar  1 12:46:29.598: INFO: Waiting up to 15m0s for pod "adopt-release-jh628" in namespace "job-2464" to be "released"
Mar  1 12:46:29.603: INFO: Pod "adopt-release-jh628": Phase="Running", Reason="", readiness=true. Elapsed: 5.122162ms
Mar  1 12:46:31.610: INFO: Pod "adopt-release-jh628": Phase="Running", Reason="", readiness=true. Elapsed: 2.011642195s
Mar  1 12:46:31.610: INFO: Pod "adopt-release-jh628" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Mar  1 12:46:31.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-2464" for this suite. 03/01/23 12:46:31.62
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","completed":204,"skipped":3784,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.153 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:46:24.479
    Mar  1 12:46:24.480: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename job 03/01/23 12:46:24.48
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:46:24.503
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:46:24.506
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:335
    STEP: Creating a job 03/01/23 12:46:24.51
    STEP: Ensuring active pods == parallelism 03/01/23 12:46:24.516
    STEP: Orphaning one of the Job's Pods 03/01/23 12:46:26.523
    Mar  1 12:46:27.051: INFO: Successfully updated pod "adopt-release-jh628"
    STEP: Checking that the Job readopts the Pod 03/01/23 12:46:27.051
    Mar  1 12:46:27.051: INFO: Waiting up to 15m0s for pod "adopt-release-jh628" in namespace "job-2464" to be "adopted"
    Mar  1 12:46:27.057: INFO: Pod "adopt-release-jh628": Phase="Running", Reason="", readiness=true. Elapsed: 5.732633ms
    Mar  1 12:46:29.075: INFO: Pod "adopt-release-jh628": Phase="Running", Reason="", readiness=true. Elapsed: 2.023907652s
    Mar  1 12:46:29.075: INFO: Pod "adopt-release-jh628" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 03/01/23 12:46:29.075
    Mar  1 12:46:29.598: INFO: Successfully updated pod "adopt-release-jh628"
    STEP: Checking that the Job releases the Pod 03/01/23 12:46:29.598
    Mar  1 12:46:29.598: INFO: Waiting up to 15m0s for pod "adopt-release-jh628" in namespace "job-2464" to be "released"
    Mar  1 12:46:29.603: INFO: Pod "adopt-release-jh628": Phase="Running", Reason="", readiness=true. Elapsed: 5.122162ms
    Mar  1 12:46:31.610: INFO: Pod "adopt-release-jh628": Phase="Running", Reason="", readiness=true. Elapsed: 2.011642195s
    Mar  1 12:46:31.610: INFO: Pod "adopt-release-jh628" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Mar  1 12:46:31.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-2464" for this suite. 03/01/23 12:46:31.62
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:46:31.641
Mar  1 12:46:31.641: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename svcaccounts 03/01/23 12:46:31.641
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:46:31.662
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:46:31.666
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
Mar  1 12:46:31.674: INFO: Got root ca configmap in namespace "svcaccounts-1458"
Mar  1 12:46:31.682: INFO: Deleted root ca configmap in namespace "svcaccounts-1458"
STEP: waiting for a new root ca configmap created 03/01/23 12:46:32.183
Mar  1 12:46:32.190: INFO: Recreated root ca configmap in namespace "svcaccounts-1458"
Mar  1 12:46:32.196: INFO: Updated root ca configmap in namespace "svcaccounts-1458"
STEP: waiting for the root ca configmap reconciled 03/01/23 12:46:32.697
Mar  1 12:46:32.703: INFO: Reconciled root ca configmap in namespace "svcaccounts-1458"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Mar  1 12:46:32.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1458" for this suite. 03/01/23 12:46:32.712
{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","completed":205,"skipped":3900,"failed":0}
------------------------------
â€¢ [1.084 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:46:31.641
    Mar  1 12:46:31.641: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename svcaccounts 03/01/23 12:46:31.641
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:46:31.662
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:46:31.666
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:739
    Mar  1 12:46:31.674: INFO: Got root ca configmap in namespace "svcaccounts-1458"
    Mar  1 12:46:31.682: INFO: Deleted root ca configmap in namespace "svcaccounts-1458"
    STEP: waiting for a new root ca configmap created 03/01/23 12:46:32.183
    Mar  1 12:46:32.190: INFO: Recreated root ca configmap in namespace "svcaccounts-1458"
    Mar  1 12:46:32.196: INFO: Updated root ca configmap in namespace "svcaccounts-1458"
    STEP: waiting for the root ca configmap reconciled 03/01/23 12:46:32.697
    Mar  1 12:46:32.703: INFO: Reconciled root ca configmap in namespace "svcaccounts-1458"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Mar  1 12:46:32.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-1458" for this suite. 03/01/23 12:46:32.712
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:46:32.726
Mar  1 12:46:32.726: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename podtemplate 03/01/23 12:46:32.727
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:46:32.741
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:46:32.745
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Mar  1 12:46:32.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-8092" for this suite. 03/01/23 12:46:32.798
{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","completed":206,"skipped":3946,"failed":0}
------------------------------
â€¢ [0.082 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:46:32.726
    Mar  1 12:46:32.726: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename podtemplate 03/01/23 12:46:32.727
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:46:32.741
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:46:32.745
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Mar  1 12:46:32.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-8092" for this suite. 03/01/23 12:46:32.798
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:46:32.813
Mar  1 12:46:32.813: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename kubelet-test 03/01/23 12:46:32.814
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:46:32.832
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:46:32.836
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Mar  1 12:46:32.891: INFO: Waiting up to 5m0s for pod "busybox-scheduling-ea75941a-9311-4c87-98b1-dc30280953d5" in namespace "kubelet-test-2199" to be "running and ready"
Mar  1 12:46:32.899: INFO: Pod "busybox-scheduling-ea75941a-9311-4c87-98b1-dc30280953d5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.8474ms
Mar  1 12:46:32.899: INFO: The phase of Pod busybox-scheduling-ea75941a-9311-4c87-98b1-dc30280953d5 is Pending, waiting for it to be Running (with Ready = true)
Mar  1 12:46:34.906: INFO: Pod "busybox-scheduling-ea75941a-9311-4c87-98b1-dc30280953d5": Phase="Running", Reason="", readiness=true. Elapsed: 2.014549221s
Mar  1 12:46:34.906: INFO: The phase of Pod busybox-scheduling-ea75941a-9311-4c87-98b1-dc30280953d5 is Running (Ready = true)
Mar  1 12:46:34.906: INFO: Pod "busybox-scheduling-ea75941a-9311-4c87-98b1-dc30280953d5" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Mar  1 12:46:34.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2199" for this suite. 03/01/23 12:46:34.939
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","completed":207,"skipped":3969,"failed":0}
------------------------------
â€¢ [2.139 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:46:32.813
    Mar  1 12:46:32.813: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename kubelet-test 03/01/23 12:46:32.814
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:46:32.832
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:46:32.836
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Mar  1 12:46:32.891: INFO: Waiting up to 5m0s for pod "busybox-scheduling-ea75941a-9311-4c87-98b1-dc30280953d5" in namespace "kubelet-test-2199" to be "running and ready"
    Mar  1 12:46:32.899: INFO: Pod "busybox-scheduling-ea75941a-9311-4c87-98b1-dc30280953d5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.8474ms
    Mar  1 12:46:32.899: INFO: The phase of Pod busybox-scheduling-ea75941a-9311-4c87-98b1-dc30280953d5 is Pending, waiting for it to be Running (with Ready = true)
    Mar  1 12:46:34.906: INFO: Pod "busybox-scheduling-ea75941a-9311-4c87-98b1-dc30280953d5": Phase="Running", Reason="", readiness=true. Elapsed: 2.014549221s
    Mar  1 12:46:34.906: INFO: The phase of Pod busybox-scheduling-ea75941a-9311-4c87-98b1-dc30280953d5 is Running (Ready = true)
    Mar  1 12:46:34.906: INFO: Pod "busybox-scheduling-ea75941a-9311-4c87-98b1-dc30280953d5" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Mar  1 12:46:34.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-2199" for this suite. 03/01/23 12:46:34.939
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:46:34.955
Mar  1 12:46:34.955: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename events 03/01/23 12:46:34.956
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:46:34.979
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:46:34.983
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 03/01/23 12:46:34.986
STEP: listing all events in all namespaces 03/01/23 12:46:34.993
STEP: patching the test event 03/01/23 12:46:34.997
STEP: fetching the test event 03/01/23 12:46:35.009
STEP: updating the test event 03/01/23 12:46:35.013
STEP: getting the test event 03/01/23 12:46:35.025
STEP: deleting the test event 03/01/23 12:46:35.03
STEP: listing all events in all namespaces 03/01/23 12:46:35.04
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
Mar  1 12:46:35.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-7017" for this suite. 03/01/23 12:46:35.055
{"msg":"PASSED [sig-instrumentation] Events should manage the lifecycle of an event [Conformance]","completed":208,"skipped":3993,"failed":0}
------------------------------
â€¢ [0.112 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:46:34.955
    Mar  1 12:46:34.955: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename events 03/01/23 12:46:34.956
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:46:34.979
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:46:34.983
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 03/01/23 12:46:34.986
    STEP: listing all events in all namespaces 03/01/23 12:46:34.993
    STEP: patching the test event 03/01/23 12:46:34.997
    STEP: fetching the test event 03/01/23 12:46:35.009
    STEP: updating the test event 03/01/23 12:46:35.013
    STEP: getting the test event 03/01/23 12:46:35.025
    STEP: deleting the test event 03/01/23 12:46:35.03
    STEP: listing all events in all namespaces 03/01/23 12:46:35.04
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    Mar  1 12:46:35.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-7017" for this suite. 03/01/23 12:46:35.055
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:46:35.068
Mar  1 12:46:35.068: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename init-container 03/01/23 12:46:35.069
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:46:35.089
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:46:35.093
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
STEP: creating the pod 03/01/23 12:46:35.096
Mar  1 12:46:35.096: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Mar  1 12:46:40.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2851" for this suite. 03/01/23 12:46:40.141
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","completed":209,"skipped":3998,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.083 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:46:35.068
    Mar  1 12:46:35.068: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename init-container 03/01/23 12:46:35.069
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:46:35.089
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:46:35.093
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:176
    STEP: creating the pod 03/01/23 12:46:35.096
    Mar  1 12:46:35.096: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Mar  1 12:46:40.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-2851" for this suite. 03/01/23 12:46:40.141
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:46:40.153
Mar  1 12:46:40.153: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename disruption 03/01/23 12:46:40.153
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:46:40.177
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:46:40.181
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
STEP: Waiting for the pdb to be processed 03/01/23 12:46:40.191
STEP: Updating PodDisruptionBudget status 03/01/23 12:46:42.203
STEP: Waiting for all pods to be running 03/01/23 12:46:42.213
Mar  1 12:46:42.222: INFO: running pods: 0 < 1
STEP: locating a running pod 03/01/23 12:46:44.229
STEP: Waiting for the pdb to be processed 03/01/23 12:46:44.249
STEP: Patching PodDisruptionBudget status 03/01/23 12:46:44.261
STEP: Waiting for the pdb to be processed 03/01/23 12:46:44.275
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Mar  1 12:46:44.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-5214" for this suite. 03/01/23 12:46:44.289
{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","completed":210,"skipped":4001,"failed":0}
------------------------------
â€¢ [4.146 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:46:40.153
    Mar  1 12:46:40.153: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename disruption 03/01/23 12:46:40.153
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:46:40.177
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:46:40.181
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:163
    STEP: Waiting for the pdb to be processed 03/01/23 12:46:40.191
    STEP: Updating PodDisruptionBudget status 03/01/23 12:46:42.203
    STEP: Waiting for all pods to be running 03/01/23 12:46:42.213
    Mar  1 12:46:42.222: INFO: running pods: 0 < 1
    STEP: locating a running pod 03/01/23 12:46:44.229
    STEP: Waiting for the pdb to be processed 03/01/23 12:46:44.249
    STEP: Patching PodDisruptionBudget status 03/01/23 12:46:44.261
    STEP: Waiting for the pdb to be processed 03/01/23 12:46:44.275
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Mar  1 12:46:44.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-5214" for this suite. 03/01/23 12:46:44.289
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:46:44.301
Mar  1 12:46:44.301: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename configmap 03/01/23 12:46:44.302
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:46:44.321
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:46:44.328
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
STEP: Creating configMap with name configmap-test-volume-map-38f7517e-6a71-4e7f-801d-af3f7f746f08 03/01/23 12:46:44.331
STEP: Creating a pod to test consume configMaps 03/01/23 12:46:44.338
Mar  1 12:46:44.346: INFO: Waiting up to 5m0s for pod "pod-configmaps-72bdc268-29c4-497d-a7d6-1aebd75ded0e" in namespace "configmap-7212" to be "Succeeded or Failed"
Mar  1 12:46:44.354: INFO: Pod "pod-configmaps-72bdc268-29c4-497d-a7d6-1aebd75ded0e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.585435ms
Mar  1 12:46:46.359: INFO: Pod "pod-configmaps-72bdc268-29c4-497d-a7d6-1aebd75ded0e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012563797s
Mar  1 12:46:48.360: INFO: Pod "pod-configmaps-72bdc268-29c4-497d-a7d6-1aebd75ded0e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013025442s
STEP: Saw pod success 03/01/23 12:46:48.36
Mar  1 12:46:48.360: INFO: Pod "pod-configmaps-72bdc268-29c4-497d-a7d6-1aebd75ded0e" satisfied condition "Succeeded or Failed"
Mar  1 12:46:48.366: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-configmaps-72bdc268-29c4-497d-a7d6-1aebd75ded0e container agnhost-container: <nil>
STEP: delete the pod 03/01/23 12:46:48.377
Mar  1 12:46:48.396: INFO: Waiting for pod pod-configmaps-72bdc268-29c4-497d-a7d6-1aebd75ded0e to disappear
Mar  1 12:46:48.399: INFO: Pod pod-configmaps-72bdc268-29c4-497d-a7d6-1aebd75ded0e no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Mar  1 12:46:48.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7212" for this suite. 03/01/23 12:46:48.405
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":211,"skipped":4016,"failed":0}
------------------------------
â€¢ [4.115 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:46:44.301
    Mar  1 12:46:44.301: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename configmap 03/01/23 12:46:44.302
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:46:44.321
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:46:44.328
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:98
    STEP: Creating configMap with name configmap-test-volume-map-38f7517e-6a71-4e7f-801d-af3f7f746f08 03/01/23 12:46:44.331
    STEP: Creating a pod to test consume configMaps 03/01/23 12:46:44.338
    Mar  1 12:46:44.346: INFO: Waiting up to 5m0s for pod "pod-configmaps-72bdc268-29c4-497d-a7d6-1aebd75ded0e" in namespace "configmap-7212" to be "Succeeded or Failed"
    Mar  1 12:46:44.354: INFO: Pod "pod-configmaps-72bdc268-29c4-497d-a7d6-1aebd75ded0e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.585435ms
    Mar  1 12:46:46.359: INFO: Pod "pod-configmaps-72bdc268-29c4-497d-a7d6-1aebd75ded0e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012563797s
    Mar  1 12:46:48.360: INFO: Pod "pod-configmaps-72bdc268-29c4-497d-a7d6-1aebd75ded0e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013025442s
    STEP: Saw pod success 03/01/23 12:46:48.36
    Mar  1 12:46:48.360: INFO: Pod "pod-configmaps-72bdc268-29c4-497d-a7d6-1aebd75ded0e" satisfied condition "Succeeded or Failed"
    Mar  1 12:46:48.366: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-configmaps-72bdc268-29c4-497d-a7d6-1aebd75ded0e container agnhost-container: <nil>
    STEP: delete the pod 03/01/23 12:46:48.377
    Mar  1 12:46:48.396: INFO: Waiting for pod pod-configmaps-72bdc268-29c4-497d-a7d6-1aebd75ded0e to disappear
    Mar  1 12:46:48.399: INFO: Pod pod-configmaps-72bdc268-29c4-497d-a7d6-1aebd75ded0e no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Mar  1 12:46:48.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-7212" for this suite. 03/01/23 12:46:48.405
  << End Captured GinkgoWriter Output
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:46:48.416
Mar  1 12:46:48.416: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename events 03/01/23 12:46:48.417
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:46:48.437
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:46:48.441
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 03/01/23 12:46:48.444
STEP: get a list of Events with a label in the current namespace 03/01/23 12:46:48.468
STEP: delete a list of events 03/01/23 12:46:48.473
Mar  1 12:46:48.473: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 03/01/23 12:46:48.504
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
Mar  1 12:46:48.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6712" for this suite. 03/01/23 12:46:48.52
{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","completed":212,"skipped":4016,"failed":0}
------------------------------
â€¢ [0.115 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:46:48.416
    Mar  1 12:46:48.416: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename events 03/01/23 12:46:48.417
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:46:48.437
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:46:48.441
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 03/01/23 12:46:48.444
    STEP: get a list of Events with a label in the current namespace 03/01/23 12:46:48.468
    STEP: delete a list of events 03/01/23 12:46:48.473
    Mar  1 12:46:48.473: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 03/01/23 12:46:48.504
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    Mar  1 12:46:48.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-6712" for this suite. 03/01/23 12:46:48.52
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:46:48.533
Mar  1 12:46:48.533: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename projected 03/01/23 12:46:48.534
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:46:48.555
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:46:48.559
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
STEP: Creating configMap with name projected-configmap-test-volume-a7a79d58-6266-441b-a3e9-9888d04af0ab 03/01/23 12:46:48.563
STEP: Creating a pod to test consume configMaps 03/01/23 12:46:48.57
Mar  1 12:46:48.583: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-39b6102c-a327-4928-8268-0e68a3375860" in namespace "projected-2906" to be "Succeeded or Failed"
Mar  1 12:46:48.592: INFO: Pod "pod-projected-configmaps-39b6102c-a327-4928-8268-0e68a3375860": Phase="Pending", Reason="", readiness=false. Elapsed: 8.617879ms
Mar  1 12:46:50.598: INFO: Pod "pod-projected-configmaps-39b6102c-a327-4928-8268-0e68a3375860": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014369509s
Mar  1 12:46:52.597: INFO: Pod "pod-projected-configmaps-39b6102c-a327-4928-8268-0e68a3375860": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013865299s
STEP: Saw pod success 03/01/23 12:46:52.597
Mar  1 12:46:52.598: INFO: Pod "pod-projected-configmaps-39b6102c-a327-4928-8268-0e68a3375860" satisfied condition "Succeeded or Failed"
Mar  1 12:46:52.603: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-projected-configmaps-39b6102c-a327-4928-8268-0e68a3375860 container agnhost-container: <nil>
STEP: delete the pod 03/01/23 12:46:52.614
Mar  1 12:46:52.635: INFO: Waiting for pod pod-projected-configmaps-39b6102c-a327-4928-8268-0e68a3375860 to disappear
Mar  1 12:46:52.640: INFO: Pod pod-projected-configmaps-39b6102c-a327-4928-8268-0e68a3375860 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Mar  1 12:46:52.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2906" for this suite. 03/01/23 12:46:52.65
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":213,"skipped":4061,"failed":0}
------------------------------
â€¢ [4.126 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:46:48.533
    Mar  1 12:46:48.533: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename projected 03/01/23 12:46:48.534
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:46:48.555
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:46:48.559
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:56
    STEP: Creating configMap with name projected-configmap-test-volume-a7a79d58-6266-441b-a3e9-9888d04af0ab 03/01/23 12:46:48.563
    STEP: Creating a pod to test consume configMaps 03/01/23 12:46:48.57
    Mar  1 12:46:48.583: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-39b6102c-a327-4928-8268-0e68a3375860" in namespace "projected-2906" to be "Succeeded or Failed"
    Mar  1 12:46:48.592: INFO: Pod "pod-projected-configmaps-39b6102c-a327-4928-8268-0e68a3375860": Phase="Pending", Reason="", readiness=false. Elapsed: 8.617879ms
    Mar  1 12:46:50.598: INFO: Pod "pod-projected-configmaps-39b6102c-a327-4928-8268-0e68a3375860": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014369509s
    Mar  1 12:46:52.597: INFO: Pod "pod-projected-configmaps-39b6102c-a327-4928-8268-0e68a3375860": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013865299s
    STEP: Saw pod success 03/01/23 12:46:52.597
    Mar  1 12:46:52.598: INFO: Pod "pod-projected-configmaps-39b6102c-a327-4928-8268-0e68a3375860" satisfied condition "Succeeded or Failed"
    Mar  1 12:46:52.603: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-projected-configmaps-39b6102c-a327-4928-8268-0e68a3375860 container agnhost-container: <nil>
    STEP: delete the pod 03/01/23 12:46:52.614
    Mar  1 12:46:52.635: INFO: Waiting for pod pod-projected-configmaps-39b6102c-a327-4928-8268-0e68a3375860 to disappear
    Mar  1 12:46:52.640: INFO: Pod pod-projected-configmaps-39b6102c-a327-4928-8268-0e68a3375860 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Mar  1 12:46:52.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2906" for this suite. 03/01/23 12:46:52.65
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:46:52.66
Mar  1 12:46:52.660: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename configmap 03/01/23 12:46:52.661
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:46:52.686
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:46:52.69
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
STEP: Creating configMap configmap-7430/configmap-test-8bc0424a-e347-44ce-bfc5-d59a8505a0a0 03/01/23 12:46:52.693
STEP: Creating a pod to test consume configMaps 03/01/23 12:46:52.7
Mar  1 12:46:52.713: INFO: Waiting up to 5m0s for pod "pod-configmaps-a0a5a09c-4958-4830-8a3b-63097c7f575e" in namespace "configmap-7430" to be "Succeeded or Failed"
Mar  1 12:46:52.719: INFO: Pod "pod-configmaps-a0a5a09c-4958-4830-8a3b-63097c7f575e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.277499ms
Mar  1 12:46:54.726: INFO: Pod "pod-configmaps-a0a5a09c-4958-4830-8a3b-63097c7f575e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012975494s
Mar  1 12:46:56.726: INFO: Pod "pod-configmaps-a0a5a09c-4958-4830-8a3b-63097c7f575e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012895509s
STEP: Saw pod success 03/01/23 12:46:56.726
Mar  1 12:46:56.726: INFO: Pod "pod-configmaps-a0a5a09c-4958-4830-8a3b-63097c7f575e" satisfied condition "Succeeded or Failed"
Mar  1 12:46:56.730: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-configmaps-a0a5a09c-4958-4830-8a3b-63097c7f575e container env-test: <nil>
STEP: delete the pod 03/01/23 12:46:56.754
Mar  1 12:46:56.775: INFO: Waiting for pod pod-configmaps-a0a5a09c-4958-4830-8a3b-63097c7f575e to disappear
Mar  1 12:46:56.779: INFO: Pod pod-configmaps-a0a5a09c-4958-4830-8a3b-63097c7f575e no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Mar  1 12:46:56.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7430" for this suite. 03/01/23 12:46:56.785
{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","completed":214,"skipped":4064,"failed":0}
------------------------------
â€¢ [4.137 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:46:52.66
    Mar  1 12:46:52.660: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename configmap 03/01/23 12:46:52.661
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:46:52.686
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:46:52.69
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:44
    STEP: Creating configMap configmap-7430/configmap-test-8bc0424a-e347-44ce-bfc5-d59a8505a0a0 03/01/23 12:46:52.693
    STEP: Creating a pod to test consume configMaps 03/01/23 12:46:52.7
    Mar  1 12:46:52.713: INFO: Waiting up to 5m0s for pod "pod-configmaps-a0a5a09c-4958-4830-8a3b-63097c7f575e" in namespace "configmap-7430" to be "Succeeded or Failed"
    Mar  1 12:46:52.719: INFO: Pod "pod-configmaps-a0a5a09c-4958-4830-8a3b-63097c7f575e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.277499ms
    Mar  1 12:46:54.726: INFO: Pod "pod-configmaps-a0a5a09c-4958-4830-8a3b-63097c7f575e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012975494s
    Mar  1 12:46:56.726: INFO: Pod "pod-configmaps-a0a5a09c-4958-4830-8a3b-63097c7f575e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012895509s
    STEP: Saw pod success 03/01/23 12:46:56.726
    Mar  1 12:46:56.726: INFO: Pod "pod-configmaps-a0a5a09c-4958-4830-8a3b-63097c7f575e" satisfied condition "Succeeded or Failed"
    Mar  1 12:46:56.730: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-configmaps-a0a5a09c-4958-4830-8a3b-63097c7f575e container env-test: <nil>
    STEP: delete the pod 03/01/23 12:46:56.754
    Mar  1 12:46:56.775: INFO: Waiting for pod pod-configmaps-a0a5a09c-4958-4830-8a3b-63097c7f575e to disappear
    Mar  1 12:46:56.779: INFO: Pod pod-configmaps-a0a5a09c-4958-4830-8a3b-63097c7f575e no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Mar  1 12:46:56.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-7430" for this suite. 03/01/23 12:46:56.785
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:46:56.8
Mar  1 12:46:56.800: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename crd-publish-openapi 03/01/23 12:46:56.801
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:46:56.821
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:46:56.825
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
STEP: set up a multi version CRD 03/01/23 12:46:56.828
Mar  1 12:46:56.828: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: mark a version not serverd 03/01/23 12:47:09.868
STEP: check the unserved version gets removed 03/01/23 12:47:09.903
STEP: check the other version is not changed 03/01/23 12:47:12.975
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  1 12:47:18.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9701" for this suite. 03/01/23 12:47:18.35
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","completed":215,"skipped":4071,"failed":0}
------------------------------
â€¢ [SLOW TEST] [21.560 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:46:56.8
    Mar  1 12:46:56.800: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename crd-publish-openapi 03/01/23 12:46:56.801
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:46:56.821
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:46:56.825
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:441
    STEP: set up a multi version CRD 03/01/23 12:46:56.828
    Mar  1 12:46:56.828: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: mark a version not serverd 03/01/23 12:47:09.868
    STEP: check the unserved version gets removed 03/01/23 12:47:09.903
    STEP: check the other version is not changed 03/01/23 12:47:12.975
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  1 12:47:18.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-9701" for this suite. 03/01/23 12:47:18.35
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:47:18.363
Mar  1 12:47:18.363: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename dns 03/01/23 12:47:18.363
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:47:18.383
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:47:18.386
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4626.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-4626.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 03/01/23 12:47:18.389
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4626.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-4626.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 03/01/23 12:47:18.389
STEP: creating a pod to probe /etc/hosts 03/01/23 12:47:18.389
STEP: submitting the pod to kubernetes 03/01/23 12:47:18.389
Mar  1 12:47:18.401: INFO: Waiting up to 15m0s for pod "dns-test-5ff39614-4a57-45e6-84f0-92dc857bc4f6" in namespace "dns-4626" to be "running"
Mar  1 12:47:18.406: INFO: Pod "dns-test-5ff39614-4a57-45e6-84f0-92dc857bc4f6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.343717ms
Mar  1 12:47:20.413: INFO: Pod "dns-test-5ff39614-4a57-45e6-84f0-92dc857bc4f6": Phase="Running", Reason="", readiness=true. Elapsed: 2.012218993s
Mar  1 12:47:20.413: INFO: Pod "dns-test-5ff39614-4a57-45e6-84f0-92dc857bc4f6" satisfied condition "running"
STEP: retrieving the pod 03/01/23 12:47:20.413
STEP: looking for the results for each expected name from probers 03/01/23 12:47:20.419
Mar  1 12:47:20.443: INFO: DNS probes using dns-4626/dns-test-5ff39614-4a57-45e6-84f0-92dc857bc4f6 succeeded

STEP: deleting the pod 03/01/23 12:47:20.443
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Mar  1 12:47:20.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4626" for this suite. 03/01/23 12:47:20.468
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]","completed":216,"skipped":4101,"failed":0}
------------------------------
â€¢ [2.118 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:47:18.363
    Mar  1 12:47:18.363: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename dns 03/01/23 12:47:18.363
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:47:18.383
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:47:18.386
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4626.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-4626.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     03/01/23 12:47:18.389
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4626.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-4626.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     03/01/23 12:47:18.389
    STEP: creating a pod to probe /etc/hosts 03/01/23 12:47:18.389
    STEP: submitting the pod to kubernetes 03/01/23 12:47:18.389
    Mar  1 12:47:18.401: INFO: Waiting up to 15m0s for pod "dns-test-5ff39614-4a57-45e6-84f0-92dc857bc4f6" in namespace "dns-4626" to be "running"
    Mar  1 12:47:18.406: INFO: Pod "dns-test-5ff39614-4a57-45e6-84f0-92dc857bc4f6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.343717ms
    Mar  1 12:47:20.413: INFO: Pod "dns-test-5ff39614-4a57-45e6-84f0-92dc857bc4f6": Phase="Running", Reason="", readiness=true. Elapsed: 2.012218993s
    Mar  1 12:47:20.413: INFO: Pod "dns-test-5ff39614-4a57-45e6-84f0-92dc857bc4f6" satisfied condition "running"
    STEP: retrieving the pod 03/01/23 12:47:20.413
    STEP: looking for the results for each expected name from probers 03/01/23 12:47:20.419
    Mar  1 12:47:20.443: INFO: DNS probes using dns-4626/dns-test-5ff39614-4a57-45e6-84f0-92dc857bc4f6 succeeded

    STEP: deleting the pod 03/01/23 12:47:20.443
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Mar  1 12:47:20.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-4626" for this suite. 03/01/23 12:47:20.468
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:47:20.481
Mar  1 12:47:20.482: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename services 03/01/23 12:47:20.482
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:47:20.5
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:47:20.503
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar  1 12:47:20.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1999" for this suite. 03/01/23 12:47:20.518
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","completed":217,"skipped":4109,"failed":0}
------------------------------
â€¢ [0.048 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:47:20.481
    Mar  1 12:47:20.482: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename services 03/01/23 12:47:20.482
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:47:20.5
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:47:20.503
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:781
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar  1 12:47:20.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-1999" for this suite. 03/01/23 12:47:20.518
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:47:20.533
Mar  1 12:47:20.533: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename webhook 03/01/23 12:47:20.534
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:47:20.553
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:47:20.557
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/01/23 12:47:20.579
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/01/23 12:47:21.245
STEP: Deploying the webhook pod 03/01/23 12:47:21.259
STEP: Wait for the deployment to be ready 03/01/23 12:47:21.279
Mar  1 12:47:21.296: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/01/23 12:47:23.313
STEP: Verifying the service has paired with the endpoint 03/01/23 12:47:23.333
Mar  1 12:47:24.333: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
Mar  1 12:47:24.339: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Registering the custom resource webhook via the AdmissionRegistration API 03/01/23 12:47:29.855
STEP: Creating a custom resource that should be denied by the webhook 03/01/23 12:47:29.875
STEP: Creating a custom resource whose deletion would be denied by the webhook 03/01/23 12:47:31.914
STEP: Updating the custom resource with disallowed data should be denied 03/01/23 12:47:31.93
STEP: Deleting the custom resource should be denied 03/01/23 12:47:31.943
STEP: Remove the offending key and value from the custom resource data 03/01/23 12:47:31.953
STEP: Deleting the updated custom resource should be successful 03/01/23 12:47:31.965
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  1 12:47:32.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5414" for this suite. 03/01/23 12:47:32.51
STEP: Destroying namespace "webhook-5414-markers" for this suite. 03/01/23 12:47:32.52
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","completed":218,"skipped":4121,"failed":0}
------------------------------
â€¢ [SLOW TEST] [12.077 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:47:20.533
    Mar  1 12:47:20.533: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename webhook 03/01/23 12:47:20.534
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:47:20.553
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:47:20.557
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/01/23 12:47:20.579
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/01/23 12:47:21.245
    STEP: Deploying the webhook pod 03/01/23 12:47:21.259
    STEP: Wait for the deployment to be ready 03/01/23 12:47:21.279
    Mar  1 12:47:21.296: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/01/23 12:47:23.313
    STEP: Verifying the service has paired with the endpoint 03/01/23 12:47:23.333
    Mar  1 12:47:24.333: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:220
    Mar  1 12:47:24.339: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 03/01/23 12:47:29.855
    STEP: Creating a custom resource that should be denied by the webhook 03/01/23 12:47:29.875
    STEP: Creating a custom resource whose deletion would be denied by the webhook 03/01/23 12:47:31.914
    STEP: Updating the custom resource with disallowed data should be denied 03/01/23 12:47:31.93
    STEP: Deleting the custom resource should be denied 03/01/23 12:47:31.943
    STEP: Remove the offending key and value from the custom resource data 03/01/23 12:47:31.953
    STEP: Deleting the updated custom resource should be successful 03/01/23 12:47:31.965
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  1 12:47:32.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-5414" for this suite. 03/01/23 12:47:32.51
    STEP: Destroying namespace "webhook-5414-markers" for this suite. 03/01/23 12:47:32.52
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:47:32.612
Mar  1 12:47:32.612: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename job 03/01/23 12:47:32.612
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:47:32.635
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:47:32.638
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
STEP: Creating a suspended job 03/01/23 12:47:32.645
STEP: Patching the Job 03/01/23 12:47:32.653
STEP: Watching for Job to be patched 03/01/23 12:47:32.675
Mar  1 12:47:32.677: INFO: Event ADDED observed for Job e2e-88nkx in namespace job-8523 with labels: map[e2e-job-label:e2e-88nkx] and annotations: map[batch.kubernetes.io/job-tracking:]
Mar  1 12:47:32.677: INFO: Event MODIFIED observed for Job e2e-88nkx in namespace job-8523 with labels: map[e2e-job-label:e2e-88nkx] and annotations: map[batch.kubernetes.io/job-tracking:]
Mar  1 12:47:32.677: INFO: Event MODIFIED found for Job e2e-88nkx in namespace job-8523 with labels: map[e2e-88nkx:patched e2e-job-label:e2e-88nkx] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 03/01/23 12:47:32.677
STEP: Watching for Job to be updated 03/01/23 12:47:32.689
Mar  1 12:47:32.693: INFO: Event MODIFIED found for Job e2e-88nkx in namespace job-8523 with labels: map[e2e-88nkx:patched e2e-job-label:e2e-88nkx] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar  1 12:47:32.693: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 03/01/23 12:47:32.693
Mar  1 12:47:32.701: INFO: Job: e2e-88nkx as labels: map[e2e-88nkx:patched e2e-job-label:e2e-88nkx]
STEP: Waiting for job to complete 03/01/23 12:47:32.701
STEP: Delete a job collection with a labelselector 03/01/23 12:47:40.712
STEP: Watching for Job to be deleted 03/01/23 12:47:40.724
Mar  1 12:47:40.729: INFO: Event MODIFIED observed for Job e2e-88nkx in namespace job-8523 with labels: map[e2e-88nkx:patched e2e-job-label:e2e-88nkx] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar  1 12:47:40.729: INFO: Event MODIFIED observed for Job e2e-88nkx in namespace job-8523 with labels: map[e2e-88nkx:patched e2e-job-label:e2e-88nkx] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar  1 12:47:40.729: INFO: Event MODIFIED observed for Job e2e-88nkx in namespace job-8523 with labels: map[e2e-88nkx:patched e2e-job-label:e2e-88nkx] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar  1 12:47:40.729: INFO: Event MODIFIED observed for Job e2e-88nkx in namespace job-8523 with labels: map[e2e-88nkx:patched e2e-job-label:e2e-88nkx] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar  1 12:47:40.730: INFO: Event MODIFIED observed for Job e2e-88nkx in namespace job-8523 with labels: map[e2e-88nkx:patched e2e-job-label:e2e-88nkx] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar  1 12:47:40.730: INFO: Event MODIFIED observed for Job e2e-88nkx in namespace job-8523 with labels: map[e2e-88nkx:patched e2e-job-label:e2e-88nkx] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar  1 12:47:40.730: INFO: Event MODIFIED observed for Job e2e-88nkx in namespace job-8523 with labels: map[e2e-88nkx:patched e2e-job-label:e2e-88nkx] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar  1 12:47:40.730: INFO: Event DELETED found for Job e2e-88nkx in namespace job-8523 with labels: map[e2e-88nkx:patched e2e-job-label:e2e-88nkx] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 03/01/23 12:47:40.73
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Mar  1 12:47:40.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-8523" for this suite. 03/01/23 12:47:40.754
{"msg":"PASSED [sig-apps] Job should manage the lifecycle of a job [Conformance]","completed":219,"skipped":4136,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.159 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:47:32.612
    Mar  1 12:47:32.612: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename job 03/01/23 12:47:32.612
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:47:32.635
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:47:32.638
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:531
    STEP: Creating a suspended job 03/01/23 12:47:32.645
    STEP: Patching the Job 03/01/23 12:47:32.653
    STEP: Watching for Job to be patched 03/01/23 12:47:32.675
    Mar  1 12:47:32.677: INFO: Event ADDED observed for Job e2e-88nkx in namespace job-8523 with labels: map[e2e-job-label:e2e-88nkx] and annotations: map[batch.kubernetes.io/job-tracking:]
    Mar  1 12:47:32.677: INFO: Event MODIFIED observed for Job e2e-88nkx in namespace job-8523 with labels: map[e2e-job-label:e2e-88nkx] and annotations: map[batch.kubernetes.io/job-tracking:]
    Mar  1 12:47:32.677: INFO: Event MODIFIED found for Job e2e-88nkx in namespace job-8523 with labels: map[e2e-88nkx:patched e2e-job-label:e2e-88nkx] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 03/01/23 12:47:32.677
    STEP: Watching for Job to be updated 03/01/23 12:47:32.689
    Mar  1 12:47:32.693: INFO: Event MODIFIED found for Job e2e-88nkx in namespace job-8523 with labels: map[e2e-88nkx:patched e2e-job-label:e2e-88nkx] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar  1 12:47:32.693: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 03/01/23 12:47:32.693
    Mar  1 12:47:32.701: INFO: Job: e2e-88nkx as labels: map[e2e-88nkx:patched e2e-job-label:e2e-88nkx]
    STEP: Waiting for job to complete 03/01/23 12:47:32.701
    STEP: Delete a job collection with a labelselector 03/01/23 12:47:40.712
    STEP: Watching for Job to be deleted 03/01/23 12:47:40.724
    Mar  1 12:47:40.729: INFO: Event MODIFIED observed for Job e2e-88nkx in namespace job-8523 with labels: map[e2e-88nkx:patched e2e-job-label:e2e-88nkx] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar  1 12:47:40.729: INFO: Event MODIFIED observed for Job e2e-88nkx in namespace job-8523 with labels: map[e2e-88nkx:patched e2e-job-label:e2e-88nkx] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar  1 12:47:40.729: INFO: Event MODIFIED observed for Job e2e-88nkx in namespace job-8523 with labels: map[e2e-88nkx:patched e2e-job-label:e2e-88nkx] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar  1 12:47:40.729: INFO: Event MODIFIED observed for Job e2e-88nkx in namespace job-8523 with labels: map[e2e-88nkx:patched e2e-job-label:e2e-88nkx] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar  1 12:47:40.730: INFO: Event MODIFIED observed for Job e2e-88nkx in namespace job-8523 with labels: map[e2e-88nkx:patched e2e-job-label:e2e-88nkx] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar  1 12:47:40.730: INFO: Event MODIFIED observed for Job e2e-88nkx in namespace job-8523 with labels: map[e2e-88nkx:patched e2e-job-label:e2e-88nkx] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar  1 12:47:40.730: INFO: Event MODIFIED observed for Job e2e-88nkx in namespace job-8523 with labels: map[e2e-88nkx:patched e2e-job-label:e2e-88nkx] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar  1 12:47:40.730: INFO: Event DELETED found for Job e2e-88nkx in namespace job-8523 with labels: map[e2e-88nkx:patched e2e-job-label:e2e-88nkx] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 03/01/23 12:47:40.73
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Mar  1 12:47:40.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-8523" for this suite. 03/01/23 12:47:40.754
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:47:40.773
Mar  1 12:47:40.773: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename namespaces 03/01/23 12:47:40.773
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:47:40.793
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:47:40.797
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
STEP: creating a Namespace 03/01/23 12:47:40.8
STEP: patching the Namespace 03/01/23 12:47:40.817
STEP: get the Namespace and ensuring it has the label 03/01/23 12:47:40.824
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Mar  1 12:47:40.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-7668" for this suite. 03/01/23 12:47:40.835
STEP: Destroying namespace "nspatchtest-ea5a37b2-19d0-42d0-b0d8-19667ea28687-9326" for this suite. 03/01/23 12:47:40.843
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","completed":220,"skipped":4161,"failed":0}
------------------------------
â€¢ [0.079 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:47:40.773
    Mar  1 12:47:40.773: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename namespaces 03/01/23 12:47:40.773
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:47:40.793
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:47:40.797
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:267
    STEP: creating a Namespace 03/01/23 12:47:40.8
    STEP: patching the Namespace 03/01/23 12:47:40.817
    STEP: get the Namespace and ensuring it has the label 03/01/23 12:47:40.824
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Mar  1 12:47:40.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-7668" for this suite. 03/01/23 12:47:40.835
    STEP: Destroying namespace "nspatchtest-ea5a37b2-19d0-42d0-b0d8-19667ea28687-9326" for this suite. 03/01/23 12:47:40.843
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:47:40.852
Mar  1 12:47:40.852: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename resourcequota 03/01/23 12:47:40.853
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:47:40.922
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:47:40.927
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
STEP: Counting existing ResourceQuota 03/01/23 12:47:40.931
STEP: Creating a ResourceQuota 03/01/23 12:47:45.936
STEP: Ensuring resource quota status is calculated 03/01/23 12:47:45.947
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Mar  1 12:47:47.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7049" for this suite. 03/01/23 12:47:47.963
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","completed":221,"skipped":4173,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.120 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:47:40.852
    Mar  1 12:47:40.852: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename resourcequota 03/01/23 12:47:40.853
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:47:40.922
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:47:40.927
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:65
    STEP: Counting existing ResourceQuota 03/01/23 12:47:40.931
    STEP: Creating a ResourceQuota 03/01/23 12:47:45.936
    STEP: Ensuring resource quota status is calculated 03/01/23 12:47:45.947
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Mar  1 12:47:47.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-7049" for this suite. 03/01/23 12:47:47.963
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:47:47.983
Mar  1 12:47:47.983: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename services 03/01/23 12:47:47.983
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:47:48.004
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:47:48.009
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189
STEP: creating service in namespace services-2842 03/01/23 12:47:48.013
STEP: creating service affinity-clusterip-transition in namespace services-2842 03/01/23 12:47:48.013
STEP: creating replication controller affinity-clusterip-transition in namespace services-2842 03/01/23 12:47:48.025
I0301 12:47:48.038008      19 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-2842, replica count: 3
I0301 12:47:51.089370      19 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  1 12:47:51.100: INFO: Creating new exec pod
Mar  1 12:47:51.108: INFO: Waiting up to 5m0s for pod "execpod-affinitytv4q2" in namespace "services-2842" to be "running"
Mar  1 12:47:51.112: INFO: Pod "execpod-affinitytv4q2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.534178ms
Mar  1 12:47:53.117: INFO: Pod "execpod-affinitytv4q2": Phase="Running", Reason="", readiness=true. Elapsed: 2.008142213s
Mar  1 12:47:53.117: INFO: Pod "execpod-affinitytv4q2" satisfied condition "running"
Mar  1 12:47:54.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-2842 exec execpod-affinitytv4q2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Mar  1 12:47:54.264: INFO: stderr: "+ nc -v -t -w 2 affinity-clusterip-transition 80\n+ echo hostName\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Mar  1 12:47:54.264: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  1 12:47:54.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-2842 exec execpod-affinitytv4q2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.41.255 80'
Mar  1 12:47:54.394: INFO: stderr: "+ nc -v -t -w 2 10.233.41.255 80\n+ echo hostName\nConnection to 10.233.41.255 80 port [tcp/http] succeeded!\n"
Mar  1 12:47:54.394: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  1 12:47:54.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-2842 exec execpod-affinitytv4q2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.41.255:80/ ; done'
Mar  1 12:47:54.631: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.255:80/\n"
Mar  1 12:47:54.631: INFO: stdout: "\naffinity-clusterip-transition-mzb52\naffinity-clusterip-transition-wfn7h\naffinity-clusterip-transition-wfn7h\naffinity-clusterip-transition-wfn7h\naffinity-clusterip-transition-9wdd6\naffinity-clusterip-transition-wfn7h\naffinity-clusterip-transition-wfn7h\naffinity-clusterip-transition-9wdd6\naffinity-clusterip-transition-9wdd6\naffinity-clusterip-transition-wfn7h\naffinity-clusterip-transition-mzb52\naffinity-clusterip-transition-mzb52\naffinity-clusterip-transition-9wdd6\naffinity-clusterip-transition-wfn7h\naffinity-clusterip-transition-mzb52\naffinity-clusterip-transition-mzb52"
Mar  1 12:47:54.631: INFO: Received response from host: affinity-clusterip-transition-mzb52
Mar  1 12:47:54.631: INFO: Received response from host: affinity-clusterip-transition-wfn7h
Mar  1 12:47:54.631: INFO: Received response from host: affinity-clusterip-transition-wfn7h
Mar  1 12:47:54.631: INFO: Received response from host: affinity-clusterip-transition-wfn7h
Mar  1 12:47:54.631: INFO: Received response from host: affinity-clusterip-transition-9wdd6
Mar  1 12:47:54.631: INFO: Received response from host: affinity-clusterip-transition-wfn7h
Mar  1 12:47:54.631: INFO: Received response from host: affinity-clusterip-transition-wfn7h
Mar  1 12:47:54.631: INFO: Received response from host: affinity-clusterip-transition-9wdd6
Mar  1 12:47:54.631: INFO: Received response from host: affinity-clusterip-transition-9wdd6
Mar  1 12:47:54.631: INFO: Received response from host: affinity-clusterip-transition-wfn7h
Mar  1 12:47:54.631: INFO: Received response from host: affinity-clusterip-transition-mzb52
Mar  1 12:47:54.631: INFO: Received response from host: affinity-clusterip-transition-mzb52
Mar  1 12:47:54.631: INFO: Received response from host: affinity-clusterip-transition-9wdd6
Mar  1 12:47:54.631: INFO: Received response from host: affinity-clusterip-transition-wfn7h
Mar  1 12:47:54.631: INFO: Received response from host: affinity-clusterip-transition-mzb52
Mar  1 12:47:54.631: INFO: Received response from host: affinity-clusterip-transition-mzb52
Mar  1 12:47:54.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-2842 exec execpod-affinitytv4q2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.41.255:80/ ; done'
Mar  1 12:47:54.873: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.255:80/\n"
Mar  1 12:47:54.873: INFO: stdout: "\naffinity-clusterip-transition-mzb52\naffinity-clusterip-transition-mzb52\naffinity-clusterip-transition-mzb52\naffinity-clusterip-transition-mzb52\naffinity-clusterip-transition-mzb52\naffinity-clusterip-transition-mzb52\naffinity-clusterip-transition-mzb52\naffinity-clusterip-transition-mzb52\naffinity-clusterip-transition-mzb52\naffinity-clusterip-transition-mzb52\naffinity-clusterip-transition-mzb52\naffinity-clusterip-transition-mzb52\naffinity-clusterip-transition-mzb52\naffinity-clusterip-transition-mzb52\naffinity-clusterip-transition-mzb52\naffinity-clusterip-transition-mzb52"
Mar  1 12:47:54.873: INFO: Received response from host: affinity-clusterip-transition-mzb52
Mar  1 12:47:54.873: INFO: Received response from host: affinity-clusterip-transition-mzb52
Mar  1 12:47:54.873: INFO: Received response from host: affinity-clusterip-transition-mzb52
Mar  1 12:47:54.873: INFO: Received response from host: affinity-clusterip-transition-mzb52
Mar  1 12:47:54.873: INFO: Received response from host: affinity-clusterip-transition-mzb52
Mar  1 12:47:54.873: INFO: Received response from host: affinity-clusterip-transition-mzb52
Mar  1 12:47:54.873: INFO: Received response from host: affinity-clusterip-transition-mzb52
Mar  1 12:47:54.873: INFO: Received response from host: affinity-clusterip-transition-mzb52
Mar  1 12:47:54.873: INFO: Received response from host: affinity-clusterip-transition-mzb52
Mar  1 12:47:54.873: INFO: Received response from host: affinity-clusterip-transition-mzb52
Mar  1 12:47:54.873: INFO: Received response from host: affinity-clusterip-transition-mzb52
Mar  1 12:47:54.873: INFO: Received response from host: affinity-clusterip-transition-mzb52
Mar  1 12:47:54.873: INFO: Received response from host: affinity-clusterip-transition-mzb52
Mar  1 12:47:54.873: INFO: Received response from host: affinity-clusterip-transition-mzb52
Mar  1 12:47:54.873: INFO: Received response from host: affinity-clusterip-transition-mzb52
Mar  1 12:47:54.873: INFO: Received response from host: affinity-clusterip-transition-mzb52
Mar  1 12:47:54.873: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-2842, will wait for the garbage collector to delete the pods 03/01/23 12:47:54.892
Mar  1 12:47:54.964: INFO: Deleting ReplicationController affinity-clusterip-transition took: 13.239246ms
Mar  1 12:47:55.065: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.699734ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar  1 12:47:57.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2842" for this suite. 03/01/23 12:47:57.403
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","completed":222,"skipped":4269,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.429 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:47:47.983
    Mar  1 12:47:47.983: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename services 03/01/23 12:47:47.983
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:47:48.004
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:47:48.009
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2189
    STEP: creating service in namespace services-2842 03/01/23 12:47:48.013
    STEP: creating service affinity-clusterip-transition in namespace services-2842 03/01/23 12:47:48.013
    STEP: creating replication controller affinity-clusterip-transition in namespace services-2842 03/01/23 12:47:48.025
    I0301 12:47:48.038008      19 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-2842, replica count: 3
    I0301 12:47:51.089370      19 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar  1 12:47:51.100: INFO: Creating new exec pod
    Mar  1 12:47:51.108: INFO: Waiting up to 5m0s for pod "execpod-affinitytv4q2" in namespace "services-2842" to be "running"
    Mar  1 12:47:51.112: INFO: Pod "execpod-affinitytv4q2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.534178ms
    Mar  1 12:47:53.117: INFO: Pod "execpod-affinitytv4q2": Phase="Running", Reason="", readiness=true. Elapsed: 2.008142213s
    Mar  1 12:47:53.117: INFO: Pod "execpod-affinitytv4q2" satisfied condition "running"
    Mar  1 12:47:54.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-2842 exec execpod-affinitytv4q2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
    Mar  1 12:47:54.264: INFO: stderr: "+ nc -v -t -w 2 affinity-clusterip-transition 80\n+ echo hostName\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Mar  1 12:47:54.264: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  1 12:47:54.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-2842 exec execpod-affinitytv4q2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.41.255 80'
    Mar  1 12:47:54.394: INFO: stderr: "+ nc -v -t -w 2 10.233.41.255 80\n+ echo hostName\nConnection to 10.233.41.255 80 port [tcp/http] succeeded!\n"
    Mar  1 12:47:54.394: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  1 12:47:54.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-2842 exec execpod-affinitytv4q2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.41.255:80/ ; done'
    Mar  1 12:47:54.631: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.255:80/\n"
    Mar  1 12:47:54.631: INFO: stdout: "\naffinity-clusterip-transition-mzb52\naffinity-clusterip-transition-wfn7h\naffinity-clusterip-transition-wfn7h\naffinity-clusterip-transition-wfn7h\naffinity-clusterip-transition-9wdd6\naffinity-clusterip-transition-wfn7h\naffinity-clusterip-transition-wfn7h\naffinity-clusterip-transition-9wdd6\naffinity-clusterip-transition-9wdd6\naffinity-clusterip-transition-wfn7h\naffinity-clusterip-transition-mzb52\naffinity-clusterip-transition-mzb52\naffinity-clusterip-transition-9wdd6\naffinity-clusterip-transition-wfn7h\naffinity-clusterip-transition-mzb52\naffinity-clusterip-transition-mzb52"
    Mar  1 12:47:54.631: INFO: Received response from host: affinity-clusterip-transition-mzb52
    Mar  1 12:47:54.631: INFO: Received response from host: affinity-clusterip-transition-wfn7h
    Mar  1 12:47:54.631: INFO: Received response from host: affinity-clusterip-transition-wfn7h
    Mar  1 12:47:54.631: INFO: Received response from host: affinity-clusterip-transition-wfn7h
    Mar  1 12:47:54.631: INFO: Received response from host: affinity-clusterip-transition-9wdd6
    Mar  1 12:47:54.631: INFO: Received response from host: affinity-clusterip-transition-wfn7h
    Mar  1 12:47:54.631: INFO: Received response from host: affinity-clusterip-transition-wfn7h
    Mar  1 12:47:54.631: INFO: Received response from host: affinity-clusterip-transition-9wdd6
    Mar  1 12:47:54.631: INFO: Received response from host: affinity-clusterip-transition-9wdd6
    Mar  1 12:47:54.631: INFO: Received response from host: affinity-clusterip-transition-wfn7h
    Mar  1 12:47:54.631: INFO: Received response from host: affinity-clusterip-transition-mzb52
    Mar  1 12:47:54.631: INFO: Received response from host: affinity-clusterip-transition-mzb52
    Mar  1 12:47:54.631: INFO: Received response from host: affinity-clusterip-transition-9wdd6
    Mar  1 12:47:54.631: INFO: Received response from host: affinity-clusterip-transition-wfn7h
    Mar  1 12:47:54.631: INFO: Received response from host: affinity-clusterip-transition-mzb52
    Mar  1 12:47:54.631: INFO: Received response from host: affinity-clusterip-transition-mzb52
    Mar  1 12:47:54.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-2842 exec execpod-affinitytv4q2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.41.255:80/ ; done'
    Mar  1 12:47:54.873: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.41.255:80/\n"
    Mar  1 12:47:54.873: INFO: stdout: "\naffinity-clusterip-transition-mzb52\naffinity-clusterip-transition-mzb52\naffinity-clusterip-transition-mzb52\naffinity-clusterip-transition-mzb52\naffinity-clusterip-transition-mzb52\naffinity-clusterip-transition-mzb52\naffinity-clusterip-transition-mzb52\naffinity-clusterip-transition-mzb52\naffinity-clusterip-transition-mzb52\naffinity-clusterip-transition-mzb52\naffinity-clusterip-transition-mzb52\naffinity-clusterip-transition-mzb52\naffinity-clusterip-transition-mzb52\naffinity-clusterip-transition-mzb52\naffinity-clusterip-transition-mzb52\naffinity-clusterip-transition-mzb52"
    Mar  1 12:47:54.873: INFO: Received response from host: affinity-clusterip-transition-mzb52
    Mar  1 12:47:54.873: INFO: Received response from host: affinity-clusterip-transition-mzb52
    Mar  1 12:47:54.873: INFO: Received response from host: affinity-clusterip-transition-mzb52
    Mar  1 12:47:54.873: INFO: Received response from host: affinity-clusterip-transition-mzb52
    Mar  1 12:47:54.873: INFO: Received response from host: affinity-clusterip-transition-mzb52
    Mar  1 12:47:54.873: INFO: Received response from host: affinity-clusterip-transition-mzb52
    Mar  1 12:47:54.873: INFO: Received response from host: affinity-clusterip-transition-mzb52
    Mar  1 12:47:54.873: INFO: Received response from host: affinity-clusterip-transition-mzb52
    Mar  1 12:47:54.873: INFO: Received response from host: affinity-clusterip-transition-mzb52
    Mar  1 12:47:54.873: INFO: Received response from host: affinity-clusterip-transition-mzb52
    Mar  1 12:47:54.873: INFO: Received response from host: affinity-clusterip-transition-mzb52
    Mar  1 12:47:54.873: INFO: Received response from host: affinity-clusterip-transition-mzb52
    Mar  1 12:47:54.873: INFO: Received response from host: affinity-clusterip-transition-mzb52
    Mar  1 12:47:54.873: INFO: Received response from host: affinity-clusterip-transition-mzb52
    Mar  1 12:47:54.873: INFO: Received response from host: affinity-clusterip-transition-mzb52
    Mar  1 12:47:54.873: INFO: Received response from host: affinity-clusterip-transition-mzb52
    Mar  1 12:47:54.873: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-2842, will wait for the garbage collector to delete the pods 03/01/23 12:47:54.892
    Mar  1 12:47:54.964: INFO: Deleting ReplicationController affinity-clusterip-transition took: 13.239246ms
    Mar  1 12:47:55.065: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.699734ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar  1 12:47:57.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-2842" for this suite. 03/01/23 12:47:57.403
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:47:57.417
Mar  1 12:47:57.418: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename daemonsets 03/01/23 12:47:57.418
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:47:57.438
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:47:57.445
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
STEP: Creating simple DaemonSet "daemon-set" 03/01/23 12:47:57.485
STEP: Check that daemon pods launch on every node of the cluster. 03/01/23 12:47:57.497
Mar  1 12:47:57.506: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 12:47:57.506: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 12:47:57.506: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 12:47:57.514: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  1 12:47:57.514: INFO: Node lab1-k8s-node-1 is running 0 daemon pod, expected 1
Mar  1 12:47:58.528: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 12:47:58.529: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 12:47:58.529: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 12:47:58.535: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar  1 12:47:58.535: INFO: Node lab1-k8s-node-1 is running 0 daemon pod, expected 1
Mar  1 12:47:59.524: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 12:47:59.524: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 12:47:59.524: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 12:47:59.530: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar  1 12:47:59.530: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Getting /status 03/01/23 12:47:59.535
Mar  1 12:47:59.541: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 03/01/23 12:47:59.541
Mar  1 12:47:59.555: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 03/01/23 12:47:59.555
Mar  1 12:47:59.558: INFO: Observed &DaemonSet event: ADDED
Mar  1 12:47:59.558: INFO: Observed &DaemonSet event: MODIFIED
Mar  1 12:47:59.559: INFO: Observed &DaemonSet event: MODIFIED
Mar  1 12:47:59.559: INFO: Observed &DaemonSet event: MODIFIED
Mar  1 12:47:59.559: INFO: Observed &DaemonSet event: MODIFIED
Mar  1 12:47:59.559: INFO: Found daemon set daemon-set in namespace daemonsets-7400 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Mar  1 12:47:59.559: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 03/01/23 12:47:59.559
STEP: watching for the daemon set status to be patched 03/01/23 12:47:59.571
Mar  1 12:47:59.576: INFO: Observed &DaemonSet event: ADDED
Mar  1 12:47:59.576: INFO: Observed &DaemonSet event: MODIFIED
Mar  1 12:47:59.576: INFO: Observed &DaemonSet event: MODIFIED
Mar  1 12:47:59.576: INFO: Observed &DaemonSet event: MODIFIED
Mar  1 12:47:59.576: INFO: Observed &DaemonSet event: MODIFIED
Mar  1 12:47:59.577: INFO: Observed daemon set daemon-set in namespace daemonsets-7400 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Mar  1 12:47:59.577: INFO: Observed &DaemonSet event: MODIFIED
Mar  1 12:47:59.577: INFO: Found daemon set daemon-set in namespace daemonsets-7400 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Mar  1 12:47:59.577: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 03/01/23 12:47:59.582
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7400, will wait for the garbage collector to delete the pods 03/01/23 12:47:59.582
Mar  1 12:47:59.651: INFO: Deleting DaemonSet.extensions daemon-set took: 11.716004ms
Mar  1 12:47:59.752: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.137464ms
Mar  1 12:48:02.358: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  1 12:48:02.358: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar  1 12:48:02.363: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"34533"},"items":null}

Mar  1 12:48:02.368: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"34533"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Mar  1 12:48:02.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7400" for this suite. 03/01/23 12:48:02.396
{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","completed":223,"skipped":4293,"failed":0}
------------------------------
â€¢ [4.987 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:47:57.417
    Mar  1 12:47:57.418: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename daemonsets 03/01/23 12:47:57.418
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:47:57.438
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:47:57.445
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:861
    STEP: Creating simple DaemonSet "daemon-set" 03/01/23 12:47:57.485
    STEP: Check that daemon pods launch on every node of the cluster. 03/01/23 12:47:57.497
    Mar  1 12:47:57.506: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 12:47:57.506: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 12:47:57.506: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 12:47:57.514: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  1 12:47:57.514: INFO: Node lab1-k8s-node-1 is running 0 daemon pod, expected 1
    Mar  1 12:47:58.528: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 12:47:58.529: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 12:47:58.529: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 12:47:58.535: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Mar  1 12:47:58.535: INFO: Node lab1-k8s-node-1 is running 0 daemon pod, expected 1
    Mar  1 12:47:59.524: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 12:47:59.524: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 12:47:59.524: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 12:47:59.530: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar  1 12:47:59.530: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Getting /status 03/01/23 12:47:59.535
    Mar  1 12:47:59.541: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 03/01/23 12:47:59.541
    Mar  1 12:47:59.555: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 03/01/23 12:47:59.555
    Mar  1 12:47:59.558: INFO: Observed &DaemonSet event: ADDED
    Mar  1 12:47:59.558: INFO: Observed &DaemonSet event: MODIFIED
    Mar  1 12:47:59.559: INFO: Observed &DaemonSet event: MODIFIED
    Mar  1 12:47:59.559: INFO: Observed &DaemonSet event: MODIFIED
    Mar  1 12:47:59.559: INFO: Observed &DaemonSet event: MODIFIED
    Mar  1 12:47:59.559: INFO: Found daemon set daemon-set in namespace daemonsets-7400 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Mar  1 12:47:59.559: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 03/01/23 12:47:59.559
    STEP: watching for the daemon set status to be patched 03/01/23 12:47:59.571
    Mar  1 12:47:59.576: INFO: Observed &DaemonSet event: ADDED
    Mar  1 12:47:59.576: INFO: Observed &DaemonSet event: MODIFIED
    Mar  1 12:47:59.576: INFO: Observed &DaemonSet event: MODIFIED
    Mar  1 12:47:59.576: INFO: Observed &DaemonSet event: MODIFIED
    Mar  1 12:47:59.576: INFO: Observed &DaemonSet event: MODIFIED
    Mar  1 12:47:59.577: INFO: Observed daemon set daemon-set in namespace daemonsets-7400 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Mar  1 12:47:59.577: INFO: Observed &DaemonSet event: MODIFIED
    Mar  1 12:47:59.577: INFO: Found daemon set daemon-set in namespace daemonsets-7400 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Mar  1 12:47:59.577: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 03/01/23 12:47:59.582
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7400, will wait for the garbage collector to delete the pods 03/01/23 12:47:59.582
    Mar  1 12:47:59.651: INFO: Deleting DaemonSet.extensions daemon-set took: 11.716004ms
    Mar  1 12:47:59.752: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.137464ms
    Mar  1 12:48:02.358: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  1 12:48:02.358: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Mar  1 12:48:02.363: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"34533"},"items":null}

    Mar  1 12:48:02.368: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"34533"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Mar  1 12:48:02.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-7400" for this suite. 03/01/23 12:48:02.396
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:48:02.407
Mar  1 12:48:02.407: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename services 03/01/23 12:48:02.408
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:48:02.441
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:48:02.446
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
STEP: creating service nodeport-test with type=NodePort in namespace services-444 03/01/23 12:48:02.45
STEP: creating replication controller nodeport-test in namespace services-444 03/01/23 12:48:02.483
I0301 12:48:02.493208      19 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-444, replica count: 2
I0301 12:48:05.544767      19 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  1 12:48:05.545: INFO: Creating new exec pod
Mar  1 12:48:05.555: INFO: Waiting up to 5m0s for pod "execpodnblt6" in namespace "services-444" to be "running"
Mar  1 12:48:05.562: INFO: Pod "execpodnblt6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.374981ms
Mar  1 12:48:07.569: INFO: Pod "execpodnblt6": Phase="Running", Reason="", readiness=true. Elapsed: 2.012935925s
Mar  1 12:48:07.569: INFO: Pod "execpodnblt6" satisfied condition "running"
Mar  1 12:48:08.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-444 exec execpodnblt6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Mar  1 12:48:08.716: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Mar  1 12:48:08.716: INFO: stdout: "nodeport-test-s6twd"
Mar  1 12:48:08.716: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-444 exec execpodnblt6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.43.78 80'
Mar  1 12:48:08.847: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.43.78 80\nConnection to 10.233.43.78 80 port [tcp/http] succeeded!\n"
Mar  1 12:48:08.847: INFO: stdout: "nodeport-test-mrp4f"
Mar  1 12:48:08.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-444 exec execpodnblt6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.128.0.76 30864'
Mar  1 12:48:08.981: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.128.0.76 30864\nConnection to 10.128.0.76 30864 port [tcp/*] succeeded!\n"
Mar  1 12:48:08.981: INFO: stdout: "nodeport-test-mrp4f"
Mar  1 12:48:08.981: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-444 exec execpodnblt6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.128.2.241 30864'
Mar  1 12:48:09.125: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.128.2.241 30864\nConnection to 10.128.2.241 30864 port [tcp/*] succeeded!\n"
Mar  1 12:48:09.125: INFO: stdout: "nodeport-test-mrp4f"
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar  1 12:48:09.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-444" for this suite. 03/01/23 12:48:09.134
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","completed":224,"skipped":4302,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.736 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:48:02.407
    Mar  1 12:48:02.407: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename services 03/01/23 12:48:02.408
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:48:02.441
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:48:02.446
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1268
    STEP: creating service nodeport-test with type=NodePort in namespace services-444 03/01/23 12:48:02.45
    STEP: creating replication controller nodeport-test in namespace services-444 03/01/23 12:48:02.483
    I0301 12:48:02.493208      19 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-444, replica count: 2
    I0301 12:48:05.544767      19 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar  1 12:48:05.545: INFO: Creating new exec pod
    Mar  1 12:48:05.555: INFO: Waiting up to 5m0s for pod "execpodnblt6" in namespace "services-444" to be "running"
    Mar  1 12:48:05.562: INFO: Pod "execpodnblt6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.374981ms
    Mar  1 12:48:07.569: INFO: Pod "execpodnblt6": Phase="Running", Reason="", readiness=true. Elapsed: 2.012935925s
    Mar  1 12:48:07.569: INFO: Pod "execpodnblt6" satisfied condition "running"
    Mar  1 12:48:08.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-444 exec execpodnblt6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Mar  1 12:48:08.716: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Mar  1 12:48:08.716: INFO: stdout: "nodeport-test-s6twd"
    Mar  1 12:48:08.716: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-444 exec execpodnblt6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.43.78 80'
    Mar  1 12:48:08.847: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.43.78 80\nConnection to 10.233.43.78 80 port [tcp/http] succeeded!\n"
    Mar  1 12:48:08.847: INFO: stdout: "nodeport-test-mrp4f"
    Mar  1 12:48:08.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-444 exec execpodnblt6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.128.0.76 30864'
    Mar  1 12:48:08.981: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.128.0.76 30864\nConnection to 10.128.0.76 30864 port [tcp/*] succeeded!\n"
    Mar  1 12:48:08.981: INFO: stdout: "nodeport-test-mrp4f"
    Mar  1 12:48:08.981: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-444 exec execpodnblt6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.128.2.241 30864'
    Mar  1 12:48:09.125: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.128.2.241 30864\nConnection to 10.128.2.241 30864 port [tcp/*] succeeded!\n"
    Mar  1 12:48:09.125: INFO: stdout: "nodeport-test-mrp4f"
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar  1 12:48:09.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-444" for this suite. 03/01/23 12:48:09.134
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:48:09.143
Mar  1 12:48:09.143: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename replication-controller 03/01/23 12:48:09.144
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:48:09.162
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:48:09.166
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
STEP: creating a ReplicationController 03/01/23 12:48:09.176
STEP: waiting for RC to be added 03/01/23 12:48:09.181
STEP: waiting for available Replicas 03/01/23 12:48:09.183
STEP: patching ReplicationController 03/01/23 12:48:10.123
STEP: waiting for RC to be modified 03/01/23 12:48:10.134
STEP: patching ReplicationController status 03/01/23 12:48:10.134
STEP: waiting for RC to be modified 03/01/23 12:48:10.142
STEP: waiting for available Replicas 03/01/23 12:48:10.142
STEP: fetching ReplicationController status 03/01/23 12:48:10.148
STEP: patching ReplicationController scale 03/01/23 12:48:10.153
STEP: waiting for RC to be modified 03/01/23 12:48:10.163
STEP: waiting for ReplicationController's scale to be the max amount 03/01/23 12:48:10.163
STEP: fetching ReplicationController; ensuring that it's patched 03/01/23 12:48:11.304
STEP: updating ReplicationController status 03/01/23 12:48:11.309
STEP: waiting for RC to be modified 03/01/23 12:48:11.317
STEP: listing all ReplicationControllers 03/01/23 12:48:11.317
STEP: checking that ReplicationController has expected values 03/01/23 12:48:11.324
STEP: deleting ReplicationControllers by collection 03/01/23 12:48:11.324
STEP: waiting for ReplicationController to have a DELETED watchEvent 03/01/23 12:48:11.337
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Mar  1 12:48:11.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7307" for this suite. 03/01/23 12:48:11.421
{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","completed":225,"skipped":4310,"failed":0}
------------------------------
â€¢ [2.289 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:48:09.143
    Mar  1 12:48:09.143: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename replication-controller 03/01/23 12:48:09.144
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:48:09.162
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:48:09.166
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:109
    STEP: creating a ReplicationController 03/01/23 12:48:09.176
    STEP: waiting for RC to be added 03/01/23 12:48:09.181
    STEP: waiting for available Replicas 03/01/23 12:48:09.183
    STEP: patching ReplicationController 03/01/23 12:48:10.123
    STEP: waiting for RC to be modified 03/01/23 12:48:10.134
    STEP: patching ReplicationController status 03/01/23 12:48:10.134
    STEP: waiting for RC to be modified 03/01/23 12:48:10.142
    STEP: waiting for available Replicas 03/01/23 12:48:10.142
    STEP: fetching ReplicationController status 03/01/23 12:48:10.148
    STEP: patching ReplicationController scale 03/01/23 12:48:10.153
    STEP: waiting for RC to be modified 03/01/23 12:48:10.163
    STEP: waiting for ReplicationController's scale to be the max amount 03/01/23 12:48:10.163
    STEP: fetching ReplicationController; ensuring that it's patched 03/01/23 12:48:11.304
    STEP: updating ReplicationController status 03/01/23 12:48:11.309
    STEP: waiting for RC to be modified 03/01/23 12:48:11.317
    STEP: listing all ReplicationControllers 03/01/23 12:48:11.317
    STEP: checking that ReplicationController has expected values 03/01/23 12:48:11.324
    STEP: deleting ReplicationControllers by collection 03/01/23 12:48:11.324
    STEP: waiting for ReplicationController to have a DELETED watchEvent 03/01/23 12:48:11.337
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Mar  1 12:48:11.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-7307" for this suite. 03/01/23 12:48:11.421
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:48:11.435
Mar  1 12:48:11.435: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename daemonsets 03/01/23 12:48:11.435
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:48:11.453
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:48:11.456
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
Mar  1 12:48:11.504: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 03/01/23 12:48:11.511
Mar  1 12:48:11.515: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  1 12:48:11.515: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 03/01/23 12:48:11.515
Mar  1 12:48:11.550: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  1 12:48:11.550: INFO: Node lab1-k8s-node-3 is running 0 daemon pod, expected 1
Mar  1 12:48:12.555: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  1 12:48:12.555: INFO: Node lab1-k8s-node-3 is running 0 daemon pod, expected 1
Mar  1 12:48:13.555: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar  1 12:48:13.555: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 03/01/23 12:48:13.56
Mar  1 12:48:13.591: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar  1 12:48:13.591: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Mar  1 12:48:14.597: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  1 12:48:14.597: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 03/01/23 12:48:14.597
Mar  1 12:48:14.616: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  1 12:48:14.616: INFO: Node lab1-k8s-node-3 is running 0 daemon pod, expected 1
Mar  1 12:48:15.621: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  1 12:48:15.621: INFO: Node lab1-k8s-node-3 is running 0 daemon pod, expected 1
Mar  1 12:48:16.623: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  1 12:48:16.623: INFO: Node lab1-k8s-node-3 is running 0 daemon pod, expected 1
Mar  1 12:48:17.623: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar  1 12:48:17.623: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 03/01/23 12:48:17.633
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4235, will wait for the garbage collector to delete the pods 03/01/23 12:48:17.633
Mar  1 12:48:17.698: INFO: Deleting DaemonSet.extensions daemon-set took: 10.358414ms
Mar  1 12:48:17.799: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.026877ms
Mar  1 12:48:20.206: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  1 12:48:20.206: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar  1 12:48:20.211: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"34886"},"items":null}

Mar  1 12:48:20.214: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"34886"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Mar  1 12:48:20.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4235" for this suite. 03/01/23 12:48:20.265
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","completed":226,"skipped":4320,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.839 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:48:11.435
    Mar  1 12:48:11.435: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename daemonsets 03/01/23 12:48:11.435
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:48:11.453
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:48:11.456
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:193
    Mar  1 12:48:11.504: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 03/01/23 12:48:11.511
    Mar  1 12:48:11.515: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  1 12:48:11.515: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 03/01/23 12:48:11.515
    Mar  1 12:48:11.550: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  1 12:48:11.550: INFO: Node lab1-k8s-node-3 is running 0 daemon pod, expected 1
    Mar  1 12:48:12.555: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  1 12:48:12.555: INFO: Node lab1-k8s-node-3 is running 0 daemon pod, expected 1
    Mar  1 12:48:13.555: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Mar  1 12:48:13.555: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 03/01/23 12:48:13.56
    Mar  1 12:48:13.591: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Mar  1 12:48:13.591: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
    Mar  1 12:48:14.597: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  1 12:48:14.597: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 03/01/23 12:48:14.597
    Mar  1 12:48:14.616: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  1 12:48:14.616: INFO: Node lab1-k8s-node-3 is running 0 daemon pod, expected 1
    Mar  1 12:48:15.621: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  1 12:48:15.621: INFO: Node lab1-k8s-node-3 is running 0 daemon pod, expected 1
    Mar  1 12:48:16.623: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  1 12:48:16.623: INFO: Node lab1-k8s-node-3 is running 0 daemon pod, expected 1
    Mar  1 12:48:17.623: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Mar  1 12:48:17.623: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 03/01/23 12:48:17.633
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4235, will wait for the garbage collector to delete the pods 03/01/23 12:48:17.633
    Mar  1 12:48:17.698: INFO: Deleting DaemonSet.extensions daemon-set took: 10.358414ms
    Mar  1 12:48:17.799: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.026877ms
    Mar  1 12:48:20.206: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  1 12:48:20.206: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Mar  1 12:48:20.211: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"34886"},"items":null}

    Mar  1 12:48:20.214: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"34886"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Mar  1 12:48:20.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-4235" for this suite. 03/01/23 12:48:20.265
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:48:20.274
Mar  1 12:48:20.274: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename subpath 03/01/23 12:48:20.276
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:48:20.297
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:48:20.301
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 03/01/23 12:48:20.304
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-4wrk 03/01/23 12:48:20.316
STEP: Creating a pod to test atomic-volume-subpath 03/01/23 12:48:20.316
Mar  1 12:48:20.329: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-4wrk" in namespace "subpath-8290" to be "Succeeded or Failed"
Mar  1 12:48:20.336: INFO: Pod "pod-subpath-test-configmap-4wrk": Phase="Pending", Reason="", readiness=false. Elapsed: 7.086623ms
Mar  1 12:48:22.341: INFO: Pod "pod-subpath-test-configmap-4wrk": Phase="Running", Reason="", readiness=true. Elapsed: 2.012375203s
Mar  1 12:48:24.343: INFO: Pod "pod-subpath-test-configmap-4wrk": Phase="Running", Reason="", readiness=true. Elapsed: 4.014111911s
Mar  1 12:48:26.342: INFO: Pod "pod-subpath-test-configmap-4wrk": Phase="Running", Reason="", readiness=true. Elapsed: 6.01393706s
Mar  1 12:48:28.343: INFO: Pod "pod-subpath-test-configmap-4wrk": Phase="Running", Reason="", readiness=true. Elapsed: 8.013965704s
Mar  1 12:48:30.341: INFO: Pod "pod-subpath-test-configmap-4wrk": Phase="Running", Reason="", readiness=true. Elapsed: 10.01230608s
Mar  1 12:48:32.343: INFO: Pod "pod-subpath-test-configmap-4wrk": Phase="Running", Reason="", readiness=true. Elapsed: 12.014766387s
Mar  1 12:48:34.342: INFO: Pod "pod-subpath-test-configmap-4wrk": Phase="Running", Reason="", readiness=true. Elapsed: 14.013492821s
Mar  1 12:48:36.341: INFO: Pod "pod-subpath-test-configmap-4wrk": Phase="Running", Reason="", readiness=true. Elapsed: 16.012735632s
Mar  1 12:48:38.342: INFO: Pod "pod-subpath-test-configmap-4wrk": Phase="Running", Reason="", readiness=true. Elapsed: 18.013730075s
Mar  1 12:48:40.342: INFO: Pod "pod-subpath-test-configmap-4wrk": Phase="Running", Reason="", readiness=true. Elapsed: 20.013777663s
Mar  1 12:48:42.343: INFO: Pod "pod-subpath-test-configmap-4wrk": Phase="Running", Reason="", readiness=false. Elapsed: 22.014131782s
Mar  1 12:48:44.344: INFO: Pod "pod-subpath-test-configmap-4wrk": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.015294787s
STEP: Saw pod success 03/01/23 12:48:44.344
Mar  1 12:48:44.344: INFO: Pod "pod-subpath-test-configmap-4wrk" satisfied condition "Succeeded or Failed"
Mar  1 12:48:44.350: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-subpath-test-configmap-4wrk container test-container-subpath-configmap-4wrk: <nil>
STEP: delete the pod 03/01/23 12:48:44.366
Mar  1 12:48:44.382: INFO: Waiting for pod pod-subpath-test-configmap-4wrk to disappear
Mar  1 12:48:44.387: INFO: Pod pod-subpath-test-configmap-4wrk no longer exists
STEP: Deleting pod pod-subpath-test-configmap-4wrk 03/01/23 12:48:44.387
Mar  1 12:48:44.387: INFO: Deleting pod "pod-subpath-test-configmap-4wrk" in namespace "subpath-8290"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Mar  1 12:48:44.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8290" for this suite. 03/01/23 12:48:44.399
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]","completed":227,"skipped":4323,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.134 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:48:20.274
    Mar  1 12:48:20.274: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename subpath 03/01/23 12:48:20.276
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:48:20.297
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:48:20.301
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 03/01/23 12:48:20.304
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-4wrk 03/01/23 12:48:20.316
    STEP: Creating a pod to test atomic-volume-subpath 03/01/23 12:48:20.316
    Mar  1 12:48:20.329: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-4wrk" in namespace "subpath-8290" to be "Succeeded or Failed"
    Mar  1 12:48:20.336: INFO: Pod "pod-subpath-test-configmap-4wrk": Phase="Pending", Reason="", readiness=false. Elapsed: 7.086623ms
    Mar  1 12:48:22.341: INFO: Pod "pod-subpath-test-configmap-4wrk": Phase="Running", Reason="", readiness=true. Elapsed: 2.012375203s
    Mar  1 12:48:24.343: INFO: Pod "pod-subpath-test-configmap-4wrk": Phase="Running", Reason="", readiness=true. Elapsed: 4.014111911s
    Mar  1 12:48:26.342: INFO: Pod "pod-subpath-test-configmap-4wrk": Phase="Running", Reason="", readiness=true. Elapsed: 6.01393706s
    Mar  1 12:48:28.343: INFO: Pod "pod-subpath-test-configmap-4wrk": Phase="Running", Reason="", readiness=true. Elapsed: 8.013965704s
    Mar  1 12:48:30.341: INFO: Pod "pod-subpath-test-configmap-4wrk": Phase="Running", Reason="", readiness=true. Elapsed: 10.01230608s
    Mar  1 12:48:32.343: INFO: Pod "pod-subpath-test-configmap-4wrk": Phase="Running", Reason="", readiness=true. Elapsed: 12.014766387s
    Mar  1 12:48:34.342: INFO: Pod "pod-subpath-test-configmap-4wrk": Phase="Running", Reason="", readiness=true. Elapsed: 14.013492821s
    Mar  1 12:48:36.341: INFO: Pod "pod-subpath-test-configmap-4wrk": Phase="Running", Reason="", readiness=true. Elapsed: 16.012735632s
    Mar  1 12:48:38.342: INFO: Pod "pod-subpath-test-configmap-4wrk": Phase="Running", Reason="", readiness=true. Elapsed: 18.013730075s
    Mar  1 12:48:40.342: INFO: Pod "pod-subpath-test-configmap-4wrk": Phase="Running", Reason="", readiness=true. Elapsed: 20.013777663s
    Mar  1 12:48:42.343: INFO: Pod "pod-subpath-test-configmap-4wrk": Phase="Running", Reason="", readiness=false. Elapsed: 22.014131782s
    Mar  1 12:48:44.344: INFO: Pod "pod-subpath-test-configmap-4wrk": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.015294787s
    STEP: Saw pod success 03/01/23 12:48:44.344
    Mar  1 12:48:44.344: INFO: Pod "pod-subpath-test-configmap-4wrk" satisfied condition "Succeeded or Failed"
    Mar  1 12:48:44.350: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-subpath-test-configmap-4wrk container test-container-subpath-configmap-4wrk: <nil>
    STEP: delete the pod 03/01/23 12:48:44.366
    Mar  1 12:48:44.382: INFO: Waiting for pod pod-subpath-test-configmap-4wrk to disappear
    Mar  1 12:48:44.387: INFO: Pod pod-subpath-test-configmap-4wrk no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-4wrk 03/01/23 12:48:44.387
    Mar  1 12:48:44.387: INFO: Deleting pod "pod-subpath-test-configmap-4wrk" in namespace "subpath-8290"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Mar  1 12:48:44.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-8290" for this suite. 03/01/23 12:48:44.399
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:48:44.411
Mar  1 12:48:44.411: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename webhook 03/01/23 12:48:44.412
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:48:44.434
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:48:44.438
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/01/23 12:48:44.462
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/01/23 12:48:45.106
STEP: Deploying the webhook pod 03/01/23 12:48:45.117
STEP: Wait for the deployment to be ready 03/01/23 12:48:45.135
Mar  1 12:48:45.158: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/01/23 12:48:47.174
STEP: Verifying the service has paired with the endpoint 03/01/23 12:48:47.191
Mar  1 12:48:48.192: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
STEP: fetching the /apis discovery document 03/01/23 12:48:48.196
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 03/01/23 12:48:48.198
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 03/01/23 12:48:48.198
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 03/01/23 12:48:48.198
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 03/01/23 12:48:48.2
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 03/01/23 12:48:48.2
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 03/01/23 12:48:48.202
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  1 12:48:48.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8607" for this suite. 03/01/23 12:48:48.211
STEP: Destroying namespace "webhook-8607-markers" for this suite. 03/01/23 12:48:48.225
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","completed":228,"skipped":4326,"failed":0}
------------------------------
â€¢ [3.889 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:48:44.411
    Mar  1 12:48:44.411: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename webhook 03/01/23 12:48:44.412
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:48:44.434
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:48:44.438
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/01/23 12:48:44.462
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/01/23 12:48:45.106
    STEP: Deploying the webhook pod 03/01/23 12:48:45.117
    STEP: Wait for the deployment to be ready 03/01/23 12:48:45.135
    Mar  1 12:48:45.158: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/01/23 12:48:47.174
    STEP: Verifying the service has paired with the endpoint 03/01/23 12:48:47.191
    Mar  1 12:48:48.192: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:116
    STEP: fetching the /apis discovery document 03/01/23 12:48:48.196
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 03/01/23 12:48:48.198
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 03/01/23 12:48:48.198
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 03/01/23 12:48:48.198
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 03/01/23 12:48:48.2
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 03/01/23 12:48:48.2
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 03/01/23 12:48:48.202
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  1 12:48:48.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-8607" for this suite. 03/01/23 12:48:48.211
    STEP: Destroying namespace "webhook-8607-markers" for this suite. 03/01/23 12:48:48.225
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:48:48.301
Mar  1 12:48:48.301: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename projected 03/01/23 12:48:48.302
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:48:48.327
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:48:48.331
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
STEP: Creating configMap with name cm-test-opt-del-42068f74-cdbd-40eb-aa66-d504a0109192 03/01/23 12:48:48.342
STEP: Creating configMap with name cm-test-opt-upd-db6af5de-d737-47e4-ad9f-bf1560d18e9b 03/01/23 12:48:48.35
STEP: Creating the pod 03/01/23 12:48:48.357
Mar  1 12:48:48.371: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d622daf0-1790-4beb-9083-5cd64a9f7ded" in namespace "projected-5642" to be "running and ready"
Mar  1 12:48:48.379: INFO: Pod "pod-projected-configmaps-d622daf0-1790-4beb-9083-5cd64a9f7ded": Phase="Pending", Reason="", readiness=false. Elapsed: 8.4317ms
Mar  1 12:48:48.379: INFO: The phase of Pod pod-projected-configmaps-d622daf0-1790-4beb-9083-5cd64a9f7ded is Pending, waiting for it to be Running (with Ready = true)
Mar  1 12:48:50.385: INFO: Pod "pod-projected-configmaps-d622daf0-1790-4beb-9083-5cd64a9f7ded": Phase="Running", Reason="", readiness=true. Elapsed: 2.014288844s
Mar  1 12:48:50.385: INFO: The phase of Pod pod-projected-configmaps-d622daf0-1790-4beb-9083-5cd64a9f7ded is Running (Ready = true)
Mar  1 12:48:50.385: INFO: Pod "pod-projected-configmaps-d622daf0-1790-4beb-9083-5cd64a9f7ded" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-42068f74-cdbd-40eb-aa66-d504a0109192 03/01/23 12:48:50.42
STEP: Updating configmap cm-test-opt-upd-db6af5de-d737-47e4-ad9f-bf1560d18e9b 03/01/23 12:48:50.428
STEP: Creating configMap with name cm-test-opt-create-3aec2462-010d-41b9-a432-057200a44224 03/01/23 12:48:50.435
STEP: waiting to observe update in volume 03/01/23 12:48:50.44
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Mar  1 12:48:54.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5642" for this suite. 03/01/23 12:48:54.505
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":229,"skipped":4326,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.215 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:48:48.301
    Mar  1 12:48:48.301: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename projected 03/01/23 12:48:48.302
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:48:48.327
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:48:48.331
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:173
    STEP: Creating configMap with name cm-test-opt-del-42068f74-cdbd-40eb-aa66-d504a0109192 03/01/23 12:48:48.342
    STEP: Creating configMap with name cm-test-opt-upd-db6af5de-d737-47e4-ad9f-bf1560d18e9b 03/01/23 12:48:48.35
    STEP: Creating the pod 03/01/23 12:48:48.357
    Mar  1 12:48:48.371: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d622daf0-1790-4beb-9083-5cd64a9f7ded" in namespace "projected-5642" to be "running and ready"
    Mar  1 12:48:48.379: INFO: Pod "pod-projected-configmaps-d622daf0-1790-4beb-9083-5cd64a9f7ded": Phase="Pending", Reason="", readiness=false. Elapsed: 8.4317ms
    Mar  1 12:48:48.379: INFO: The phase of Pod pod-projected-configmaps-d622daf0-1790-4beb-9083-5cd64a9f7ded is Pending, waiting for it to be Running (with Ready = true)
    Mar  1 12:48:50.385: INFO: Pod "pod-projected-configmaps-d622daf0-1790-4beb-9083-5cd64a9f7ded": Phase="Running", Reason="", readiness=true. Elapsed: 2.014288844s
    Mar  1 12:48:50.385: INFO: The phase of Pod pod-projected-configmaps-d622daf0-1790-4beb-9083-5cd64a9f7ded is Running (Ready = true)
    Mar  1 12:48:50.385: INFO: Pod "pod-projected-configmaps-d622daf0-1790-4beb-9083-5cd64a9f7ded" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-42068f74-cdbd-40eb-aa66-d504a0109192 03/01/23 12:48:50.42
    STEP: Updating configmap cm-test-opt-upd-db6af5de-d737-47e4-ad9f-bf1560d18e9b 03/01/23 12:48:50.428
    STEP: Creating configMap with name cm-test-opt-create-3aec2462-010d-41b9-a432-057200a44224 03/01/23 12:48:50.435
    STEP: waiting to observe update in volume 03/01/23 12:48:50.44
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Mar  1 12:48:54.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5642" for this suite. 03/01/23 12:48:54.505
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:48:54.518
Mar  1 12:48:54.519: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename configmap 03/01/23 12:48:54.519
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:48:54.538
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:48:54.542
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
STEP: Creating configMap with name configmap-test-volume-map-c568e20c-14ef-41ba-b52b-4b953797b99f 03/01/23 12:48:54.545
STEP: Creating a pod to test consume configMaps 03/01/23 12:48:54.552
Mar  1 12:48:54.564: INFO: Waiting up to 5m0s for pod "pod-configmaps-2c5d6dd8-b76f-49be-b695-a85862639217" in namespace "configmap-1927" to be "Succeeded or Failed"
Mar  1 12:48:54.572: INFO: Pod "pod-configmaps-2c5d6dd8-b76f-49be-b695-a85862639217": Phase="Pending", Reason="", readiness=false. Elapsed: 7.996682ms
Mar  1 12:48:56.578: INFO: Pod "pod-configmaps-2c5d6dd8-b76f-49be-b695-a85862639217": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014288744s
Mar  1 12:48:58.579: INFO: Pod "pod-configmaps-2c5d6dd8-b76f-49be-b695-a85862639217": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014587832s
STEP: Saw pod success 03/01/23 12:48:58.579
Mar  1 12:48:58.579: INFO: Pod "pod-configmaps-2c5d6dd8-b76f-49be-b695-a85862639217" satisfied condition "Succeeded or Failed"
Mar  1 12:48:58.583: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-configmaps-2c5d6dd8-b76f-49be-b695-a85862639217 container agnhost-container: <nil>
STEP: delete the pod 03/01/23 12:48:58.592
Mar  1 12:48:58.612: INFO: Waiting for pod pod-configmaps-2c5d6dd8-b76f-49be-b695-a85862639217 to disappear
Mar  1 12:48:58.616: INFO: Pod pod-configmaps-2c5d6dd8-b76f-49be-b695-a85862639217 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Mar  1 12:48:58.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1927" for this suite. 03/01/23 12:48:58.625
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":230,"skipped":4348,"failed":0}
------------------------------
â€¢ [4.118 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:48:54.518
    Mar  1 12:48:54.519: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename configmap 03/01/23 12:48:54.519
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:48:54.538
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:48:54.542
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:88
    STEP: Creating configMap with name configmap-test-volume-map-c568e20c-14ef-41ba-b52b-4b953797b99f 03/01/23 12:48:54.545
    STEP: Creating a pod to test consume configMaps 03/01/23 12:48:54.552
    Mar  1 12:48:54.564: INFO: Waiting up to 5m0s for pod "pod-configmaps-2c5d6dd8-b76f-49be-b695-a85862639217" in namespace "configmap-1927" to be "Succeeded or Failed"
    Mar  1 12:48:54.572: INFO: Pod "pod-configmaps-2c5d6dd8-b76f-49be-b695-a85862639217": Phase="Pending", Reason="", readiness=false. Elapsed: 7.996682ms
    Mar  1 12:48:56.578: INFO: Pod "pod-configmaps-2c5d6dd8-b76f-49be-b695-a85862639217": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014288744s
    Mar  1 12:48:58.579: INFO: Pod "pod-configmaps-2c5d6dd8-b76f-49be-b695-a85862639217": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014587832s
    STEP: Saw pod success 03/01/23 12:48:58.579
    Mar  1 12:48:58.579: INFO: Pod "pod-configmaps-2c5d6dd8-b76f-49be-b695-a85862639217" satisfied condition "Succeeded or Failed"
    Mar  1 12:48:58.583: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-configmaps-2c5d6dd8-b76f-49be-b695-a85862639217 container agnhost-container: <nil>
    STEP: delete the pod 03/01/23 12:48:58.592
    Mar  1 12:48:58.612: INFO: Waiting for pod pod-configmaps-2c5d6dd8-b76f-49be-b695-a85862639217 to disappear
    Mar  1 12:48:58.616: INFO: Pod pod-configmaps-2c5d6dd8-b76f-49be-b695-a85862639217 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Mar  1 12:48:58.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-1927" for this suite. 03/01/23 12:48:58.625
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:48:58.637
Mar  1 12:48:58.637: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename emptydir-wrapper 03/01/23 12:48:58.638
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:48:58.659
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:48:58.663
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 03/01/23 12:48:58.666
STEP: Creating RC which spawns configmap-volume pods 03/01/23 12:48:58.972
Mar  1 12:48:58.993: INFO: Pod name wrapped-volume-race-66c1f50a-fbdf-47e0-9dc9-5b4975617861: Found 0 pods out of 5
Mar  1 12:49:04.002: INFO: Pod name wrapped-volume-race-66c1f50a-fbdf-47e0-9dc9-5b4975617861: Found 5 pods out of 5
STEP: Ensuring each pod is running 03/01/23 12:49:04.002
Mar  1 12:49:04.002: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-66c1f50a-fbdf-47e0-9dc9-5b4975617861-2jc8r" in namespace "emptydir-wrapper-1427" to be "running"
Mar  1 12:49:04.008: INFO: Pod "wrapped-volume-race-66c1f50a-fbdf-47e0-9dc9-5b4975617861-2jc8r": Phase="Pending", Reason="", readiness=false. Elapsed: 5.937644ms
Mar  1 12:49:06.016: INFO: Pod "wrapped-volume-race-66c1f50a-fbdf-47e0-9dc9-5b4975617861-2jc8r": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014143267s
Mar  1 12:49:08.017: INFO: Pod "wrapped-volume-race-66c1f50a-fbdf-47e0-9dc9-5b4975617861-2jc8r": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014464082s
Mar  1 12:49:10.015: INFO: Pod "wrapped-volume-race-66c1f50a-fbdf-47e0-9dc9-5b4975617861-2jc8r": Phase="Pending", Reason="", readiness=false. Elapsed: 6.012966361s
Mar  1 12:49:12.020: INFO: Pod "wrapped-volume-race-66c1f50a-fbdf-47e0-9dc9-5b4975617861-2jc8r": Phase="Pending", Reason="", readiness=false. Elapsed: 8.017342459s
Mar  1 12:49:14.014: INFO: Pod "wrapped-volume-race-66c1f50a-fbdf-47e0-9dc9-5b4975617861-2jc8r": Phase="Running", Reason="", readiness=true. Elapsed: 10.011406233s
Mar  1 12:49:14.014: INFO: Pod "wrapped-volume-race-66c1f50a-fbdf-47e0-9dc9-5b4975617861-2jc8r" satisfied condition "running"
Mar  1 12:49:14.014: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-66c1f50a-fbdf-47e0-9dc9-5b4975617861-h6vnf" in namespace "emptydir-wrapper-1427" to be "running"
Mar  1 12:49:14.020: INFO: Pod "wrapped-volume-race-66c1f50a-fbdf-47e0-9dc9-5b4975617861-h6vnf": Phase="Running", Reason="", readiness=true. Elapsed: 6.195155ms
Mar  1 12:49:14.020: INFO: Pod "wrapped-volume-race-66c1f50a-fbdf-47e0-9dc9-5b4975617861-h6vnf" satisfied condition "running"
Mar  1 12:49:14.020: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-66c1f50a-fbdf-47e0-9dc9-5b4975617861-mszfj" in namespace "emptydir-wrapper-1427" to be "running"
Mar  1 12:49:14.027: INFO: Pod "wrapped-volume-race-66c1f50a-fbdf-47e0-9dc9-5b4975617861-mszfj": Phase="Running", Reason="", readiness=true. Elapsed: 6.503182ms
Mar  1 12:49:14.027: INFO: Pod "wrapped-volume-race-66c1f50a-fbdf-47e0-9dc9-5b4975617861-mszfj" satisfied condition "running"
Mar  1 12:49:14.027: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-66c1f50a-fbdf-47e0-9dc9-5b4975617861-rml2c" in namespace "emptydir-wrapper-1427" to be "running"
Mar  1 12:49:14.030: INFO: Pod "wrapped-volume-race-66c1f50a-fbdf-47e0-9dc9-5b4975617861-rml2c": Phase="Running", Reason="", readiness=true. Elapsed: 3.57372ms
Mar  1 12:49:14.030: INFO: Pod "wrapped-volume-race-66c1f50a-fbdf-47e0-9dc9-5b4975617861-rml2c" satisfied condition "running"
Mar  1 12:49:14.030: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-66c1f50a-fbdf-47e0-9dc9-5b4975617861-v25kk" in namespace "emptydir-wrapper-1427" to be "running"
Mar  1 12:49:14.036: INFO: Pod "wrapped-volume-race-66c1f50a-fbdf-47e0-9dc9-5b4975617861-v25kk": Phase="Running", Reason="", readiness=true. Elapsed: 5.678824ms
Mar  1 12:49:14.036: INFO: Pod "wrapped-volume-race-66c1f50a-fbdf-47e0-9dc9-5b4975617861-v25kk" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-66c1f50a-fbdf-47e0-9dc9-5b4975617861 in namespace emptydir-wrapper-1427, will wait for the garbage collector to delete the pods 03/01/23 12:49:14.036
Mar  1 12:49:14.106: INFO: Deleting ReplicationController wrapped-volume-race-66c1f50a-fbdf-47e0-9dc9-5b4975617861 took: 13.275195ms
Mar  1 12:49:14.207: INFO: Terminating ReplicationController wrapped-volume-race-66c1f50a-fbdf-47e0-9dc9-5b4975617861 pods took: 100.949769ms
STEP: Creating RC which spawns configmap-volume pods 03/01/23 12:49:17.516
Mar  1 12:49:17.534: INFO: Pod name wrapped-volume-race-782f665d-dd0d-4277-ba2b-610fe312d9a1: Found 0 pods out of 5
Mar  1 12:49:22.546: INFO: Pod name wrapped-volume-race-782f665d-dd0d-4277-ba2b-610fe312d9a1: Found 5 pods out of 5
STEP: Ensuring each pod is running 03/01/23 12:49:22.546
Mar  1 12:49:22.547: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-782f665d-dd0d-4277-ba2b-610fe312d9a1-8j2s4" in namespace "emptydir-wrapper-1427" to be "running"
Mar  1 12:49:22.551: INFO: Pod "wrapped-volume-race-782f665d-dd0d-4277-ba2b-610fe312d9a1-8j2s4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.14656ms
Mar  1 12:49:24.558: INFO: Pod "wrapped-volume-race-782f665d-dd0d-4277-ba2b-610fe312d9a1-8j2s4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011204303s
Mar  1 12:49:26.559: INFO: Pod "wrapped-volume-race-782f665d-dd0d-4277-ba2b-610fe312d9a1-8j2s4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012142792s
Mar  1 12:49:28.561: INFO: Pod "wrapped-volume-race-782f665d-dd0d-4277-ba2b-610fe312d9a1-8j2s4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.01441499s
Mar  1 12:49:30.561: INFO: Pod "wrapped-volume-race-782f665d-dd0d-4277-ba2b-610fe312d9a1-8j2s4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.014511124s
Mar  1 12:49:32.559: INFO: Pod "wrapped-volume-race-782f665d-dd0d-4277-ba2b-610fe312d9a1-8j2s4": Phase="Running", Reason="", readiness=true. Elapsed: 10.012242422s
Mar  1 12:49:32.559: INFO: Pod "wrapped-volume-race-782f665d-dd0d-4277-ba2b-610fe312d9a1-8j2s4" satisfied condition "running"
Mar  1 12:49:32.559: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-782f665d-dd0d-4277-ba2b-610fe312d9a1-bldkf" in namespace "emptydir-wrapper-1427" to be "running"
Mar  1 12:49:32.566: INFO: Pod "wrapped-volume-race-782f665d-dd0d-4277-ba2b-610fe312d9a1-bldkf": Phase="Running", Reason="", readiness=true. Elapsed: 6.74217ms
Mar  1 12:49:32.566: INFO: Pod "wrapped-volume-race-782f665d-dd0d-4277-ba2b-610fe312d9a1-bldkf" satisfied condition "running"
Mar  1 12:49:32.566: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-782f665d-dd0d-4277-ba2b-610fe312d9a1-kdz8l" in namespace "emptydir-wrapper-1427" to be "running"
Mar  1 12:49:32.570: INFO: Pod "wrapped-volume-race-782f665d-dd0d-4277-ba2b-610fe312d9a1-kdz8l": Phase="Pending", Reason="", readiness=false. Elapsed: 4.225407ms
Mar  1 12:49:34.576: INFO: Pod "wrapped-volume-race-782f665d-dd0d-4277-ba2b-610fe312d9a1-kdz8l": Phase="Running", Reason="", readiness=true. Elapsed: 2.009627556s
Mar  1 12:49:34.576: INFO: Pod "wrapped-volume-race-782f665d-dd0d-4277-ba2b-610fe312d9a1-kdz8l" satisfied condition "running"
Mar  1 12:49:34.576: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-782f665d-dd0d-4277-ba2b-610fe312d9a1-qh2hf" in namespace "emptydir-wrapper-1427" to be "running"
Mar  1 12:49:34.581: INFO: Pod "wrapped-volume-race-782f665d-dd0d-4277-ba2b-610fe312d9a1-qh2hf": Phase="Running", Reason="", readiness=true. Elapsed: 5.676446ms
Mar  1 12:49:34.581: INFO: Pod "wrapped-volume-race-782f665d-dd0d-4277-ba2b-610fe312d9a1-qh2hf" satisfied condition "running"
Mar  1 12:49:34.581: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-782f665d-dd0d-4277-ba2b-610fe312d9a1-t592x" in namespace "emptydir-wrapper-1427" to be "running"
Mar  1 12:49:34.588: INFO: Pod "wrapped-volume-race-782f665d-dd0d-4277-ba2b-610fe312d9a1-t592x": Phase="Running", Reason="", readiness=true. Elapsed: 6.562828ms
Mar  1 12:49:34.588: INFO: Pod "wrapped-volume-race-782f665d-dd0d-4277-ba2b-610fe312d9a1-t592x" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-782f665d-dd0d-4277-ba2b-610fe312d9a1 in namespace emptydir-wrapper-1427, will wait for the garbage collector to delete the pods 03/01/23 12:49:34.588
Mar  1 12:49:34.654: INFO: Deleting ReplicationController wrapped-volume-race-782f665d-dd0d-4277-ba2b-610fe312d9a1 took: 11.581884ms
Mar  1 12:49:34.755: INFO: Terminating ReplicationController wrapped-volume-race-782f665d-dd0d-4277-ba2b-610fe312d9a1 pods took: 100.776755ms
STEP: Creating RC which spawns configmap-volume pods 03/01/23 12:49:37.066
Mar  1 12:49:37.096: INFO: Pod name wrapped-volume-race-74dea83e-d9c6-42b9-8bae-bf8765e5de5e: Found 0 pods out of 5
Mar  1 12:49:42.108: INFO: Pod name wrapped-volume-race-74dea83e-d9c6-42b9-8bae-bf8765e5de5e: Found 5 pods out of 5
STEP: Ensuring each pod is running 03/01/23 12:49:42.108
Mar  1 12:49:42.108: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-74dea83e-d9c6-42b9-8bae-bf8765e5de5e-5pggw" in namespace "emptydir-wrapper-1427" to be "running"
Mar  1 12:49:42.112: INFO: Pod "wrapped-volume-race-74dea83e-d9c6-42b9-8bae-bf8765e5de5e-5pggw": Phase="Pending", Reason="", readiness=false. Elapsed: 4.628608ms
Mar  1 12:49:44.121: INFO: Pod "wrapped-volume-race-74dea83e-d9c6-42b9-8bae-bf8765e5de5e-5pggw": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012886573s
Mar  1 12:49:46.120: INFO: Pod "wrapped-volume-race-74dea83e-d9c6-42b9-8bae-bf8765e5de5e-5pggw": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011915622s
Mar  1 12:49:48.119: INFO: Pod "wrapped-volume-race-74dea83e-d9c6-42b9-8bae-bf8765e5de5e-5pggw": Phase="Pending", Reason="", readiness=false. Elapsed: 6.010890113s
Mar  1 12:49:50.118: INFO: Pod "wrapped-volume-race-74dea83e-d9c6-42b9-8bae-bf8765e5de5e-5pggw": Phase="Pending", Reason="", readiness=false. Elapsed: 8.010574067s
Mar  1 12:49:52.121: INFO: Pod "wrapped-volume-race-74dea83e-d9c6-42b9-8bae-bf8765e5de5e-5pggw": Phase="Running", Reason="", readiness=true. Elapsed: 10.013044405s
Mar  1 12:49:52.121: INFO: Pod "wrapped-volume-race-74dea83e-d9c6-42b9-8bae-bf8765e5de5e-5pggw" satisfied condition "running"
Mar  1 12:49:52.121: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-74dea83e-d9c6-42b9-8bae-bf8765e5de5e-j8b5z" in namespace "emptydir-wrapper-1427" to be "running"
Mar  1 12:49:52.125: INFO: Pod "wrapped-volume-race-74dea83e-d9c6-42b9-8bae-bf8765e5de5e-j8b5z": Phase="Running", Reason="", readiness=true. Elapsed: 4.315313ms
Mar  1 12:49:52.125: INFO: Pod "wrapped-volume-race-74dea83e-d9c6-42b9-8bae-bf8765e5de5e-j8b5z" satisfied condition "running"
Mar  1 12:49:52.125: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-74dea83e-d9c6-42b9-8bae-bf8765e5de5e-kspdw" in namespace "emptydir-wrapper-1427" to be "running"
Mar  1 12:49:52.132: INFO: Pod "wrapped-volume-race-74dea83e-d9c6-42b9-8bae-bf8765e5de5e-kspdw": Phase="Running", Reason="", readiness=true. Elapsed: 6.333467ms
Mar  1 12:49:52.132: INFO: Pod "wrapped-volume-race-74dea83e-d9c6-42b9-8bae-bf8765e5de5e-kspdw" satisfied condition "running"
Mar  1 12:49:52.132: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-74dea83e-d9c6-42b9-8bae-bf8765e5de5e-rnk4k" in namespace "emptydir-wrapper-1427" to be "running"
Mar  1 12:49:52.139: INFO: Pod "wrapped-volume-race-74dea83e-d9c6-42b9-8bae-bf8765e5de5e-rnk4k": Phase="Running", Reason="", readiness=true. Elapsed: 7.146625ms
Mar  1 12:49:52.139: INFO: Pod "wrapped-volume-race-74dea83e-d9c6-42b9-8bae-bf8765e5de5e-rnk4k" satisfied condition "running"
Mar  1 12:49:52.139: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-74dea83e-d9c6-42b9-8bae-bf8765e5de5e-w4zkb" in namespace "emptydir-wrapper-1427" to be "running"
Mar  1 12:49:52.143: INFO: Pod "wrapped-volume-race-74dea83e-d9c6-42b9-8bae-bf8765e5de5e-w4zkb": Phase="Running", Reason="", readiness=true. Elapsed: 3.815897ms
Mar  1 12:49:52.143: INFO: Pod "wrapped-volume-race-74dea83e-d9c6-42b9-8bae-bf8765e5de5e-w4zkb" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-74dea83e-d9c6-42b9-8bae-bf8765e5de5e in namespace emptydir-wrapper-1427, will wait for the garbage collector to delete the pods 03/01/23 12:49:52.143
Mar  1 12:49:52.209: INFO: Deleting ReplicationController wrapped-volume-race-74dea83e-d9c6-42b9-8bae-bf8765e5de5e took: 10.279279ms
Mar  1 12:49:52.310: INFO: Terminating ReplicationController wrapped-volume-race-74dea83e-d9c6-42b9-8bae-bf8765e5de5e pods took: 100.859429ms
STEP: Cleaning up the configMaps 03/01/23 12:49:55.411
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
Mar  1 12:49:55.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-1427" for this suite. 03/01/23 12:49:55.845
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","completed":231,"skipped":4360,"failed":0}
------------------------------
â€¢ [SLOW TEST] [57.215 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:48:58.637
    Mar  1 12:48:58.637: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename emptydir-wrapper 03/01/23 12:48:58.638
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:48:58.659
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:48:58.663
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 03/01/23 12:48:58.666
    STEP: Creating RC which spawns configmap-volume pods 03/01/23 12:48:58.972
    Mar  1 12:48:58.993: INFO: Pod name wrapped-volume-race-66c1f50a-fbdf-47e0-9dc9-5b4975617861: Found 0 pods out of 5
    Mar  1 12:49:04.002: INFO: Pod name wrapped-volume-race-66c1f50a-fbdf-47e0-9dc9-5b4975617861: Found 5 pods out of 5
    STEP: Ensuring each pod is running 03/01/23 12:49:04.002
    Mar  1 12:49:04.002: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-66c1f50a-fbdf-47e0-9dc9-5b4975617861-2jc8r" in namespace "emptydir-wrapper-1427" to be "running"
    Mar  1 12:49:04.008: INFO: Pod "wrapped-volume-race-66c1f50a-fbdf-47e0-9dc9-5b4975617861-2jc8r": Phase="Pending", Reason="", readiness=false. Elapsed: 5.937644ms
    Mar  1 12:49:06.016: INFO: Pod "wrapped-volume-race-66c1f50a-fbdf-47e0-9dc9-5b4975617861-2jc8r": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014143267s
    Mar  1 12:49:08.017: INFO: Pod "wrapped-volume-race-66c1f50a-fbdf-47e0-9dc9-5b4975617861-2jc8r": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014464082s
    Mar  1 12:49:10.015: INFO: Pod "wrapped-volume-race-66c1f50a-fbdf-47e0-9dc9-5b4975617861-2jc8r": Phase="Pending", Reason="", readiness=false. Elapsed: 6.012966361s
    Mar  1 12:49:12.020: INFO: Pod "wrapped-volume-race-66c1f50a-fbdf-47e0-9dc9-5b4975617861-2jc8r": Phase="Pending", Reason="", readiness=false. Elapsed: 8.017342459s
    Mar  1 12:49:14.014: INFO: Pod "wrapped-volume-race-66c1f50a-fbdf-47e0-9dc9-5b4975617861-2jc8r": Phase="Running", Reason="", readiness=true. Elapsed: 10.011406233s
    Mar  1 12:49:14.014: INFO: Pod "wrapped-volume-race-66c1f50a-fbdf-47e0-9dc9-5b4975617861-2jc8r" satisfied condition "running"
    Mar  1 12:49:14.014: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-66c1f50a-fbdf-47e0-9dc9-5b4975617861-h6vnf" in namespace "emptydir-wrapper-1427" to be "running"
    Mar  1 12:49:14.020: INFO: Pod "wrapped-volume-race-66c1f50a-fbdf-47e0-9dc9-5b4975617861-h6vnf": Phase="Running", Reason="", readiness=true. Elapsed: 6.195155ms
    Mar  1 12:49:14.020: INFO: Pod "wrapped-volume-race-66c1f50a-fbdf-47e0-9dc9-5b4975617861-h6vnf" satisfied condition "running"
    Mar  1 12:49:14.020: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-66c1f50a-fbdf-47e0-9dc9-5b4975617861-mszfj" in namespace "emptydir-wrapper-1427" to be "running"
    Mar  1 12:49:14.027: INFO: Pod "wrapped-volume-race-66c1f50a-fbdf-47e0-9dc9-5b4975617861-mszfj": Phase="Running", Reason="", readiness=true. Elapsed: 6.503182ms
    Mar  1 12:49:14.027: INFO: Pod "wrapped-volume-race-66c1f50a-fbdf-47e0-9dc9-5b4975617861-mszfj" satisfied condition "running"
    Mar  1 12:49:14.027: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-66c1f50a-fbdf-47e0-9dc9-5b4975617861-rml2c" in namespace "emptydir-wrapper-1427" to be "running"
    Mar  1 12:49:14.030: INFO: Pod "wrapped-volume-race-66c1f50a-fbdf-47e0-9dc9-5b4975617861-rml2c": Phase="Running", Reason="", readiness=true. Elapsed: 3.57372ms
    Mar  1 12:49:14.030: INFO: Pod "wrapped-volume-race-66c1f50a-fbdf-47e0-9dc9-5b4975617861-rml2c" satisfied condition "running"
    Mar  1 12:49:14.030: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-66c1f50a-fbdf-47e0-9dc9-5b4975617861-v25kk" in namespace "emptydir-wrapper-1427" to be "running"
    Mar  1 12:49:14.036: INFO: Pod "wrapped-volume-race-66c1f50a-fbdf-47e0-9dc9-5b4975617861-v25kk": Phase="Running", Reason="", readiness=true. Elapsed: 5.678824ms
    Mar  1 12:49:14.036: INFO: Pod "wrapped-volume-race-66c1f50a-fbdf-47e0-9dc9-5b4975617861-v25kk" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-66c1f50a-fbdf-47e0-9dc9-5b4975617861 in namespace emptydir-wrapper-1427, will wait for the garbage collector to delete the pods 03/01/23 12:49:14.036
    Mar  1 12:49:14.106: INFO: Deleting ReplicationController wrapped-volume-race-66c1f50a-fbdf-47e0-9dc9-5b4975617861 took: 13.275195ms
    Mar  1 12:49:14.207: INFO: Terminating ReplicationController wrapped-volume-race-66c1f50a-fbdf-47e0-9dc9-5b4975617861 pods took: 100.949769ms
    STEP: Creating RC which spawns configmap-volume pods 03/01/23 12:49:17.516
    Mar  1 12:49:17.534: INFO: Pod name wrapped-volume-race-782f665d-dd0d-4277-ba2b-610fe312d9a1: Found 0 pods out of 5
    Mar  1 12:49:22.546: INFO: Pod name wrapped-volume-race-782f665d-dd0d-4277-ba2b-610fe312d9a1: Found 5 pods out of 5
    STEP: Ensuring each pod is running 03/01/23 12:49:22.546
    Mar  1 12:49:22.547: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-782f665d-dd0d-4277-ba2b-610fe312d9a1-8j2s4" in namespace "emptydir-wrapper-1427" to be "running"
    Mar  1 12:49:22.551: INFO: Pod "wrapped-volume-race-782f665d-dd0d-4277-ba2b-610fe312d9a1-8j2s4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.14656ms
    Mar  1 12:49:24.558: INFO: Pod "wrapped-volume-race-782f665d-dd0d-4277-ba2b-610fe312d9a1-8j2s4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011204303s
    Mar  1 12:49:26.559: INFO: Pod "wrapped-volume-race-782f665d-dd0d-4277-ba2b-610fe312d9a1-8j2s4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012142792s
    Mar  1 12:49:28.561: INFO: Pod "wrapped-volume-race-782f665d-dd0d-4277-ba2b-610fe312d9a1-8j2s4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.01441499s
    Mar  1 12:49:30.561: INFO: Pod "wrapped-volume-race-782f665d-dd0d-4277-ba2b-610fe312d9a1-8j2s4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.014511124s
    Mar  1 12:49:32.559: INFO: Pod "wrapped-volume-race-782f665d-dd0d-4277-ba2b-610fe312d9a1-8j2s4": Phase="Running", Reason="", readiness=true. Elapsed: 10.012242422s
    Mar  1 12:49:32.559: INFO: Pod "wrapped-volume-race-782f665d-dd0d-4277-ba2b-610fe312d9a1-8j2s4" satisfied condition "running"
    Mar  1 12:49:32.559: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-782f665d-dd0d-4277-ba2b-610fe312d9a1-bldkf" in namespace "emptydir-wrapper-1427" to be "running"
    Mar  1 12:49:32.566: INFO: Pod "wrapped-volume-race-782f665d-dd0d-4277-ba2b-610fe312d9a1-bldkf": Phase="Running", Reason="", readiness=true. Elapsed: 6.74217ms
    Mar  1 12:49:32.566: INFO: Pod "wrapped-volume-race-782f665d-dd0d-4277-ba2b-610fe312d9a1-bldkf" satisfied condition "running"
    Mar  1 12:49:32.566: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-782f665d-dd0d-4277-ba2b-610fe312d9a1-kdz8l" in namespace "emptydir-wrapper-1427" to be "running"
    Mar  1 12:49:32.570: INFO: Pod "wrapped-volume-race-782f665d-dd0d-4277-ba2b-610fe312d9a1-kdz8l": Phase="Pending", Reason="", readiness=false. Elapsed: 4.225407ms
    Mar  1 12:49:34.576: INFO: Pod "wrapped-volume-race-782f665d-dd0d-4277-ba2b-610fe312d9a1-kdz8l": Phase="Running", Reason="", readiness=true. Elapsed: 2.009627556s
    Mar  1 12:49:34.576: INFO: Pod "wrapped-volume-race-782f665d-dd0d-4277-ba2b-610fe312d9a1-kdz8l" satisfied condition "running"
    Mar  1 12:49:34.576: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-782f665d-dd0d-4277-ba2b-610fe312d9a1-qh2hf" in namespace "emptydir-wrapper-1427" to be "running"
    Mar  1 12:49:34.581: INFO: Pod "wrapped-volume-race-782f665d-dd0d-4277-ba2b-610fe312d9a1-qh2hf": Phase="Running", Reason="", readiness=true. Elapsed: 5.676446ms
    Mar  1 12:49:34.581: INFO: Pod "wrapped-volume-race-782f665d-dd0d-4277-ba2b-610fe312d9a1-qh2hf" satisfied condition "running"
    Mar  1 12:49:34.581: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-782f665d-dd0d-4277-ba2b-610fe312d9a1-t592x" in namespace "emptydir-wrapper-1427" to be "running"
    Mar  1 12:49:34.588: INFO: Pod "wrapped-volume-race-782f665d-dd0d-4277-ba2b-610fe312d9a1-t592x": Phase="Running", Reason="", readiness=true. Elapsed: 6.562828ms
    Mar  1 12:49:34.588: INFO: Pod "wrapped-volume-race-782f665d-dd0d-4277-ba2b-610fe312d9a1-t592x" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-782f665d-dd0d-4277-ba2b-610fe312d9a1 in namespace emptydir-wrapper-1427, will wait for the garbage collector to delete the pods 03/01/23 12:49:34.588
    Mar  1 12:49:34.654: INFO: Deleting ReplicationController wrapped-volume-race-782f665d-dd0d-4277-ba2b-610fe312d9a1 took: 11.581884ms
    Mar  1 12:49:34.755: INFO: Terminating ReplicationController wrapped-volume-race-782f665d-dd0d-4277-ba2b-610fe312d9a1 pods took: 100.776755ms
    STEP: Creating RC which spawns configmap-volume pods 03/01/23 12:49:37.066
    Mar  1 12:49:37.096: INFO: Pod name wrapped-volume-race-74dea83e-d9c6-42b9-8bae-bf8765e5de5e: Found 0 pods out of 5
    Mar  1 12:49:42.108: INFO: Pod name wrapped-volume-race-74dea83e-d9c6-42b9-8bae-bf8765e5de5e: Found 5 pods out of 5
    STEP: Ensuring each pod is running 03/01/23 12:49:42.108
    Mar  1 12:49:42.108: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-74dea83e-d9c6-42b9-8bae-bf8765e5de5e-5pggw" in namespace "emptydir-wrapper-1427" to be "running"
    Mar  1 12:49:42.112: INFO: Pod "wrapped-volume-race-74dea83e-d9c6-42b9-8bae-bf8765e5de5e-5pggw": Phase="Pending", Reason="", readiness=false. Elapsed: 4.628608ms
    Mar  1 12:49:44.121: INFO: Pod "wrapped-volume-race-74dea83e-d9c6-42b9-8bae-bf8765e5de5e-5pggw": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012886573s
    Mar  1 12:49:46.120: INFO: Pod "wrapped-volume-race-74dea83e-d9c6-42b9-8bae-bf8765e5de5e-5pggw": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011915622s
    Mar  1 12:49:48.119: INFO: Pod "wrapped-volume-race-74dea83e-d9c6-42b9-8bae-bf8765e5de5e-5pggw": Phase="Pending", Reason="", readiness=false. Elapsed: 6.010890113s
    Mar  1 12:49:50.118: INFO: Pod "wrapped-volume-race-74dea83e-d9c6-42b9-8bae-bf8765e5de5e-5pggw": Phase="Pending", Reason="", readiness=false. Elapsed: 8.010574067s
    Mar  1 12:49:52.121: INFO: Pod "wrapped-volume-race-74dea83e-d9c6-42b9-8bae-bf8765e5de5e-5pggw": Phase="Running", Reason="", readiness=true. Elapsed: 10.013044405s
    Mar  1 12:49:52.121: INFO: Pod "wrapped-volume-race-74dea83e-d9c6-42b9-8bae-bf8765e5de5e-5pggw" satisfied condition "running"
    Mar  1 12:49:52.121: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-74dea83e-d9c6-42b9-8bae-bf8765e5de5e-j8b5z" in namespace "emptydir-wrapper-1427" to be "running"
    Mar  1 12:49:52.125: INFO: Pod "wrapped-volume-race-74dea83e-d9c6-42b9-8bae-bf8765e5de5e-j8b5z": Phase="Running", Reason="", readiness=true. Elapsed: 4.315313ms
    Mar  1 12:49:52.125: INFO: Pod "wrapped-volume-race-74dea83e-d9c6-42b9-8bae-bf8765e5de5e-j8b5z" satisfied condition "running"
    Mar  1 12:49:52.125: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-74dea83e-d9c6-42b9-8bae-bf8765e5de5e-kspdw" in namespace "emptydir-wrapper-1427" to be "running"
    Mar  1 12:49:52.132: INFO: Pod "wrapped-volume-race-74dea83e-d9c6-42b9-8bae-bf8765e5de5e-kspdw": Phase="Running", Reason="", readiness=true. Elapsed: 6.333467ms
    Mar  1 12:49:52.132: INFO: Pod "wrapped-volume-race-74dea83e-d9c6-42b9-8bae-bf8765e5de5e-kspdw" satisfied condition "running"
    Mar  1 12:49:52.132: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-74dea83e-d9c6-42b9-8bae-bf8765e5de5e-rnk4k" in namespace "emptydir-wrapper-1427" to be "running"
    Mar  1 12:49:52.139: INFO: Pod "wrapped-volume-race-74dea83e-d9c6-42b9-8bae-bf8765e5de5e-rnk4k": Phase="Running", Reason="", readiness=true. Elapsed: 7.146625ms
    Mar  1 12:49:52.139: INFO: Pod "wrapped-volume-race-74dea83e-d9c6-42b9-8bae-bf8765e5de5e-rnk4k" satisfied condition "running"
    Mar  1 12:49:52.139: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-74dea83e-d9c6-42b9-8bae-bf8765e5de5e-w4zkb" in namespace "emptydir-wrapper-1427" to be "running"
    Mar  1 12:49:52.143: INFO: Pod "wrapped-volume-race-74dea83e-d9c6-42b9-8bae-bf8765e5de5e-w4zkb": Phase="Running", Reason="", readiness=true. Elapsed: 3.815897ms
    Mar  1 12:49:52.143: INFO: Pod "wrapped-volume-race-74dea83e-d9c6-42b9-8bae-bf8765e5de5e-w4zkb" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-74dea83e-d9c6-42b9-8bae-bf8765e5de5e in namespace emptydir-wrapper-1427, will wait for the garbage collector to delete the pods 03/01/23 12:49:52.143
    Mar  1 12:49:52.209: INFO: Deleting ReplicationController wrapped-volume-race-74dea83e-d9c6-42b9-8bae-bf8765e5de5e took: 10.279279ms
    Mar  1 12:49:52.310: INFO: Terminating ReplicationController wrapped-volume-race-74dea83e-d9c6-42b9-8bae-bf8765e5de5e pods took: 100.859429ms
    STEP: Cleaning up the configMaps 03/01/23 12:49:55.411
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    Mar  1 12:49:55.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-1427" for this suite. 03/01/23 12:49:55.845
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:49:55.854
Mar  1 12:49:55.854: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename endpointslice 03/01/23 12:49:55.855
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:49:55.874
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:49:55.878
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
Mar  1 12:49:55.891: INFO: Endpoints addresses: [10.128.0.115 10.128.0.14 10.128.2.3] , ports: [6443]
Mar  1 12:49:55.891: INFO: EndpointSlices addresses: [10.128.0.115 10.128.0.14 10.128.2.3] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Mar  1 12:49:55.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-7377" for this suite. 03/01/23 12:49:55.898
{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","completed":232,"skipped":4365,"failed":0}
------------------------------
â€¢ [0.053 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:49:55.854
    Mar  1 12:49:55.854: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename endpointslice 03/01/23 12:49:55.855
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:49:55.874
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:49:55.878
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:65
    Mar  1 12:49:55.891: INFO: Endpoints addresses: [10.128.0.115 10.128.0.14 10.128.2.3] , ports: [6443]
    Mar  1 12:49:55.891: INFO: EndpointSlices addresses: [10.128.0.115 10.128.0.14 10.128.2.3] , ports: [6443]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Mar  1 12:49:55.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-7377" for this suite. 03/01/23 12:49:55.898
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:49:55.909
Mar  1 12:49:55.909: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename emptydir 03/01/23 12:49:55.91
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:49:55.935
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:49:55.939
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
STEP: Creating a pod to test emptydir volume type on node default medium 03/01/23 12:49:55.942
Mar  1 12:49:55.952: INFO: Waiting up to 5m0s for pod "pod-fea32aea-3a95-4b73-83fd-2e8ab6c16dc9" in namespace "emptydir-6358" to be "Succeeded or Failed"
Mar  1 12:49:55.957: INFO: Pod "pod-fea32aea-3a95-4b73-83fd-2e8ab6c16dc9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.319038ms
Mar  1 12:49:57.966: INFO: Pod "pod-fea32aea-3a95-4b73-83fd-2e8ab6c16dc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013686768s
Mar  1 12:49:59.963: INFO: Pod "pod-fea32aea-3a95-4b73-83fd-2e8ab6c16dc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010859255s
STEP: Saw pod success 03/01/23 12:49:59.963
Mar  1 12:49:59.964: INFO: Pod "pod-fea32aea-3a95-4b73-83fd-2e8ab6c16dc9" satisfied condition "Succeeded or Failed"
Mar  1 12:49:59.968: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-fea32aea-3a95-4b73-83fd-2e8ab6c16dc9 container test-container: <nil>
STEP: delete the pod 03/01/23 12:49:59.978
Mar  1 12:50:00.000: INFO: Waiting for pod pod-fea32aea-3a95-4b73-83fd-2e8ab6c16dc9 to disappear
Mar  1 12:50:00.004: INFO: Pod pod-fea32aea-3a95-4b73-83fd-2e8ab6c16dc9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar  1 12:50:00.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6358" for this suite. 03/01/23 12:50:00.013
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":233,"skipped":4381,"failed":0}
------------------------------
â€¢ [4.114 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:49:55.909
    Mar  1 12:49:55.909: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename emptydir 03/01/23 12:49:55.91
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:49:55.935
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:49:55.939
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:156
    STEP: Creating a pod to test emptydir volume type on node default medium 03/01/23 12:49:55.942
    Mar  1 12:49:55.952: INFO: Waiting up to 5m0s for pod "pod-fea32aea-3a95-4b73-83fd-2e8ab6c16dc9" in namespace "emptydir-6358" to be "Succeeded or Failed"
    Mar  1 12:49:55.957: INFO: Pod "pod-fea32aea-3a95-4b73-83fd-2e8ab6c16dc9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.319038ms
    Mar  1 12:49:57.966: INFO: Pod "pod-fea32aea-3a95-4b73-83fd-2e8ab6c16dc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013686768s
    Mar  1 12:49:59.963: INFO: Pod "pod-fea32aea-3a95-4b73-83fd-2e8ab6c16dc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010859255s
    STEP: Saw pod success 03/01/23 12:49:59.963
    Mar  1 12:49:59.964: INFO: Pod "pod-fea32aea-3a95-4b73-83fd-2e8ab6c16dc9" satisfied condition "Succeeded or Failed"
    Mar  1 12:49:59.968: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-fea32aea-3a95-4b73-83fd-2e8ab6c16dc9 container test-container: <nil>
    STEP: delete the pod 03/01/23 12:49:59.978
    Mar  1 12:50:00.000: INFO: Waiting for pod pod-fea32aea-3a95-4b73-83fd-2e8ab6c16dc9 to disappear
    Mar  1 12:50:00.004: INFO: Pod pod-fea32aea-3a95-4b73-83fd-2e8ab6c16dc9 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar  1 12:50:00.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-6358" for this suite. 03/01/23 12:50:00.013
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:50:00.023
Mar  1 12:50:00.023: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename projected 03/01/23 12:50:00.024
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:50:00.043
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:50:00.047
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
STEP: Creating secret with name projected-secret-test-1f74c90d-998e-401b-813e-396f2bcb9ceb 03/01/23 12:50:00.051
STEP: Creating a pod to test consume secrets 03/01/23 12:50:00.06
Mar  1 12:50:00.072: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3a029d90-ea75-42d3-9371-defe3fab332c" in namespace "projected-5420" to be "Succeeded or Failed"
Mar  1 12:50:00.082: INFO: Pod "pod-projected-secrets-3a029d90-ea75-42d3-9371-defe3fab332c": Phase="Pending", Reason="", readiness=false. Elapsed: 9.391947ms
Mar  1 12:50:02.090: INFO: Pod "pod-projected-secrets-3a029d90-ea75-42d3-9371-defe3fab332c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017019614s
Mar  1 12:50:04.088: INFO: Pod "pod-projected-secrets-3a029d90-ea75-42d3-9371-defe3fab332c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015395241s
STEP: Saw pod success 03/01/23 12:50:04.088
Mar  1 12:50:04.088: INFO: Pod "pod-projected-secrets-3a029d90-ea75-42d3-9371-defe3fab332c" satisfied condition "Succeeded or Failed"
Mar  1 12:50:04.094: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-projected-secrets-3a029d90-ea75-42d3-9371-defe3fab332c container secret-volume-test: <nil>
STEP: delete the pod 03/01/23 12:50:04.102
Mar  1 12:50:04.121: INFO: Waiting for pod pod-projected-secrets-3a029d90-ea75-42d3-9371-defe3fab332c to disappear
Mar  1 12:50:04.126: INFO: Pod pod-projected-secrets-3a029d90-ea75-42d3-9371-defe3fab332c no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Mar  1 12:50:04.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5420" for this suite. 03/01/23 12:50:04.135
{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":234,"skipped":4384,"failed":0}
------------------------------
â€¢ [4.123 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:50:00.023
    Mar  1 12:50:00.023: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename projected 03/01/23 12:50:00.024
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:50:00.043
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:50:00.047
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:118
    STEP: Creating secret with name projected-secret-test-1f74c90d-998e-401b-813e-396f2bcb9ceb 03/01/23 12:50:00.051
    STEP: Creating a pod to test consume secrets 03/01/23 12:50:00.06
    Mar  1 12:50:00.072: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3a029d90-ea75-42d3-9371-defe3fab332c" in namespace "projected-5420" to be "Succeeded or Failed"
    Mar  1 12:50:00.082: INFO: Pod "pod-projected-secrets-3a029d90-ea75-42d3-9371-defe3fab332c": Phase="Pending", Reason="", readiness=false. Elapsed: 9.391947ms
    Mar  1 12:50:02.090: INFO: Pod "pod-projected-secrets-3a029d90-ea75-42d3-9371-defe3fab332c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017019614s
    Mar  1 12:50:04.088: INFO: Pod "pod-projected-secrets-3a029d90-ea75-42d3-9371-defe3fab332c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015395241s
    STEP: Saw pod success 03/01/23 12:50:04.088
    Mar  1 12:50:04.088: INFO: Pod "pod-projected-secrets-3a029d90-ea75-42d3-9371-defe3fab332c" satisfied condition "Succeeded or Failed"
    Mar  1 12:50:04.094: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-projected-secrets-3a029d90-ea75-42d3-9371-defe3fab332c container secret-volume-test: <nil>
    STEP: delete the pod 03/01/23 12:50:04.102
    Mar  1 12:50:04.121: INFO: Waiting for pod pod-projected-secrets-3a029d90-ea75-42d3-9371-defe3fab332c to disappear
    Mar  1 12:50:04.126: INFO: Pod pod-projected-secrets-3a029d90-ea75-42d3-9371-defe3fab332c no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Mar  1 12:50:04.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5420" for this suite. 03/01/23 12:50:04.135
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:50:04.147
Mar  1 12:50:04.147: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename resourcequota 03/01/23 12:50:04.148
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:50:04.165
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:50:04.169
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
STEP: Creating a ResourceQuota with terminating scope 03/01/23 12:50:04.173
STEP: Ensuring ResourceQuota status is calculated 03/01/23 12:50:04.181
STEP: Creating a ResourceQuota with not terminating scope 03/01/23 12:50:06.189
STEP: Ensuring ResourceQuota status is calculated 03/01/23 12:50:06.196
STEP: Creating a long running pod 03/01/23 12:50:08.203
STEP: Ensuring resource quota with not terminating scope captures the pod usage 03/01/23 12:50:08.22
STEP: Ensuring resource quota with terminating scope ignored the pod usage 03/01/23 12:50:10.226
STEP: Deleting the pod 03/01/23 12:50:12.234
STEP: Ensuring resource quota status released the pod usage 03/01/23 12:50:12.253
STEP: Creating a terminating pod 03/01/23 12:50:14.257
STEP: Ensuring resource quota with terminating scope captures the pod usage 03/01/23 12:50:14.274
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 03/01/23 12:50:16.281
STEP: Deleting the pod 03/01/23 12:50:18.287
STEP: Ensuring resource quota status released the pod usage 03/01/23 12:50:18.303
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Mar  1 12:50:20.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-569" for this suite. 03/01/23 12:50:20.317
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","completed":235,"skipped":4385,"failed":0}
------------------------------
â€¢ [SLOW TEST] [16.181 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:50:04.147
    Mar  1 12:50:04.147: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename resourcequota 03/01/23 12:50:04.148
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:50:04.165
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:50:04.169
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:680
    STEP: Creating a ResourceQuota with terminating scope 03/01/23 12:50:04.173
    STEP: Ensuring ResourceQuota status is calculated 03/01/23 12:50:04.181
    STEP: Creating a ResourceQuota with not terminating scope 03/01/23 12:50:06.189
    STEP: Ensuring ResourceQuota status is calculated 03/01/23 12:50:06.196
    STEP: Creating a long running pod 03/01/23 12:50:08.203
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 03/01/23 12:50:08.22
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 03/01/23 12:50:10.226
    STEP: Deleting the pod 03/01/23 12:50:12.234
    STEP: Ensuring resource quota status released the pod usage 03/01/23 12:50:12.253
    STEP: Creating a terminating pod 03/01/23 12:50:14.257
    STEP: Ensuring resource quota with terminating scope captures the pod usage 03/01/23 12:50:14.274
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 03/01/23 12:50:16.281
    STEP: Deleting the pod 03/01/23 12:50:18.287
    STEP: Ensuring resource quota status released the pod usage 03/01/23 12:50:18.303
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Mar  1 12:50:20.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-569" for this suite. 03/01/23 12:50:20.317
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:50:20.33
Mar  1 12:50:20.330: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename container-probe 03/01/23 12:50:20.331
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:50:20.351
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:50:20.355
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
STEP: Creating pod liveness-b6dc0b60-dd76-4b38-87a7-d1838d20299a in namespace container-probe-7408 03/01/23 12:50:20.358
Mar  1 12:50:20.372: INFO: Waiting up to 5m0s for pod "liveness-b6dc0b60-dd76-4b38-87a7-d1838d20299a" in namespace "container-probe-7408" to be "not pending"
Mar  1 12:50:20.380: INFO: Pod "liveness-b6dc0b60-dd76-4b38-87a7-d1838d20299a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.77015ms
Mar  1 12:50:22.387: INFO: Pod "liveness-b6dc0b60-dd76-4b38-87a7-d1838d20299a": Phase="Running", Reason="", readiness=true. Elapsed: 2.01445558s
Mar  1 12:50:22.387: INFO: Pod "liveness-b6dc0b60-dd76-4b38-87a7-d1838d20299a" satisfied condition "not pending"
Mar  1 12:50:22.387: INFO: Started pod liveness-b6dc0b60-dd76-4b38-87a7-d1838d20299a in namespace container-probe-7408
STEP: checking the pod's current state and verifying that restartCount is present 03/01/23 12:50:22.387
Mar  1 12:50:22.391: INFO: Initial restart count of pod liveness-b6dc0b60-dd76-4b38-87a7-d1838d20299a is 0
Mar  1 12:50:42.456: INFO: Restart count of pod container-probe-7408/liveness-b6dc0b60-dd76-4b38-87a7-d1838d20299a is now 1 (20.065060506s elapsed)
STEP: deleting the pod 03/01/23 12:50:42.456
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Mar  1 12:50:42.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7408" for this suite. 03/01/23 12:50:42.484
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":236,"skipped":4388,"failed":0}
------------------------------
â€¢ [SLOW TEST] [22.166 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:50:20.33
    Mar  1 12:50:20.330: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename container-probe 03/01/23 12:50:20.331
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:50:20.351
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:50:20.355
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:165
    STEP: Creating pod liveness-b6dc0b60-dd76-4b38-87a7-d1838d20299a in namespace container-probe-7408 03/01/23 12:50:20.358
    Mar  1 12:50:20.372: INFO: Waiting up to 5m0s for pod "liveness-b6dc0b60-dd76-4b38-87a7-d1838d20299a" in namespace "container-probe-7408" to be "not pending"
    Mar  1 12:50:20.380: INFO: Pod "liveness-b6dc0b60-dd76-4b38-87a7-d1838d20299a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.77015ms
    Mar  1 12:50:22.387: INFO: Pod "liveness-b6dc0b60-dd76-4b38-87a7-d1838d20299a": Phase="Running", Reason="", readiness=true. Elapsed: 2.01445558s
    Mar  1 12:50:22.387: INFO: Pod "liveness-b6dc0b60-dd76-4b38-87a7-d1838d20299a" satisfied condition "not pending"
    Mar  1 12:50:22.387: INFO: Started pod liveness-b6dc0b60-dd76-4b38-87a7-d1838d20299a in namespace container-probe-7408
    STEP: checking the pod's current state and verifying that restartCount is present 03/01/23 12:50:22.387
    Mar  1 12:50:22.391: INFO: Initial restart count of pod liveness-b6dc0b60-dd76-4b38-87a7-d1838d20299a is 0
    Mar  1 12:50:42.456: INFO: Restart count of pod container-probe-7408/liveness-b6dc0b60-dd76-4b38-87a7-d1838d20299a is now 1 (20.065060506s elapsed)
    STEP: deleting the pod 03/01/23 12:50:42.456
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Mar  1 12:50:42.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-7408" for this suite. 03/01/23 12:50:42.484
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:50:42.498
Mar  1 12:50:42.498: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename var-expansion 03/01/23 12:50:42.499
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:50:42.52
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:50:42.524
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
STEP: creating the pod with failed condition 03/01/23 12:50:42.527
Mar  1 12:50:42.541: INFO: Waiting up to 2m0s for pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8" in namespace "var-expansion-8465" to be "running"
Mar  1 12:50:42.544: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.47189ms
Mar  1 12:50:44.552: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011279458s
Mar  1 12:50:46.551: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01034062s
Mar  1 12:50:48.550: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008729279s
Mar  1 12:50:50.553: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.012086855s
Mar  1 12:50:52.551: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 10.010336315s
Mar  1 12:50:54.549: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 12.008516412s
Mar  1 12:50:56.554: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 14.013505496s
Mar  1 12:50:58.552: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 16.010988224s
Mar  1 12:51:00.550: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 18.008871562s
Mar  1 12:51:02.551: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 20.010626819s
Mar  1 12:51:04.552: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 22.011586834s
Mar  1 12:51:06.549: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 24.008430844s
Mar  1 12:51:08.551: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 26.010375486s
Mar  1 12:51:10.552: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 28.010661078s
Mar  1 12:51:12.549: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 30.008423417s
Mar  1 12:51:14.551: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 32.009813507s
Mar  1 12:51:16.551: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 34.010117764s
Mar  1 12:51:18.552: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 36.011015977s
Mar  1 12:51:20.551: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 38.010633407s
Mar  1 12:51:22.549: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 40.008459461s
Mar  1 12:51:24.551: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 42.009981076s
Mar  1 12:51:26.551: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 44.009972597s
Mar  1 12:51:28.551: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 46.009796792s
Mar  1 12:51:30.552: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 48.011564186s
Mar  1 12:51:32.551: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 50.010147241s
Mar  1 12:51:34.549: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 52.008304864s
Mar  1 12:51:36.551: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 54.010224888s
Mar  1 12:51:38.551: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 56.010056799s
Mar  1 12:51:40.550: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 58.009160923s
Mar  1 12:51:42.552: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.010794715s
Mar  1 12:51:44.551: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.010124014s
Mar  1 12:51:46.551: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.009917371s
Mar  1 12:51:48.550: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.009123637s
Mar  1 12:51:50.551: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.01037783s
Mar  1 12:51:52.550: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.008942763s
Mar  1 12:51:54.551: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.010548891s
Mar  1 12:51:56.551: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.01002677s
Mar  1 12:51:58.552: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.011005847s
Mar  1 12:52:00.550: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.009642676s
Mar  1 12:52:02.549: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.008350033s
Mar  1 12:52:04.551: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.009661433s
Mar  1 12:52:06.551: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.010389865s
Mar  1 12:52:08.551: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.010370258s
Mar  1 12:52:10.551: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.010215789s
Mar  1 12:52:12.551: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.009676193s
Mar  1 12:52:14.551: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.010181384s
Mar  1 12:52:16.551: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.010029813s
Mar  1 12:52:18.551: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.010113249s
Mar  1 12:52:20.549: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.00854375s
Mar  1 12:52:22.551: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.01061945s
Mar  1 12:52:24.551: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.010101173s
Mar  1 12:52:26.551: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.009884704s
Mar  1 12:52:28.550: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.009473024s
Mar  1 12:52:30.553: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.011906902s
Mar  1 12:52:32.549: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.008260126s
Mar  1 12:52:34.550: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.009652372s
Mar  1 12:52:36.552: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.011021377s
Mar  1 12:52:38.549: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.008552389s
Mar  1 12:52:40.551: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.009862727s
Mar  1 12:52:42.551: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.010318977s
Mar  1 12:52:42.555: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.014279064s
STEP: updating the pod 03/01/23 12:52:42.555
Mar  1 12:52:43.074: INFO: Successfully updated pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8"
STEP: waiting for pod running 03/01/23 12:52:43.074
Mar  1 12:52:43.074: INFO: Waiting up to 2m0s for pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8" in namespace "var-expansion-8465" to be "running"
Mar  1 12:52:43.079: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.126005ms
Mar  1 12:52:45.084: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Running", Reason="", readiness=true. Elapsed: 2.00961352s
Mar  1 12:52:45.084: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8" satisfied condition "running"
STEP: deleting the pod gracefully 03/01/23 12:52:45.084
Mar  1 12:52:45.084: INFO: Deleting pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8" in namespace "var-expansion-8465"
Mar  1 12:52:45.100: INFO: Wait up to 5m0s for pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Mar  1 12:53:17.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8465" for this suite. 03/01/23 12:53:17.119
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","completed":237,"skipped":4388,"failed":0}
------------------------------
â€¢ [SLOW TEST] [154.630 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:50:42.498
    Mar  1 12:50:42.498: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename var-expansion 03/01/23 12:50:42.499
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:50:42.52
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:50:42.524
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:224
    STEP: creating the pod with failed condition 03/01/23 12:50:42.527
    Mar  1 12:50:42.541: INFO: Waiting up to 2m0s for pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8" in namespace "var-expansion-8465" to be "running"
    Mar  1 12:50:42.544: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.47189ms
    Mar  1 12:50:44.552: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011279458s
    Mar  1 12:50:46.551: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01034062s
    Mar  1 12:50:48.550: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008729279s
    Mar  1 12:50:50.553: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.012086855s
    Mar  1 12:50:52.551: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 10.010336315s
    Mar  1 12:50:54.549: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 12.008516412s
    Mar  1 12:50:56.554: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 14.013505496s
    Mar  1 12:50:58.552: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 16.010988224s
    Mar  1 12:51:00.550: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 18.008871562s
    Mar  1 12:51:02.551: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 20.010626819s
    Mar  1 12:51:04.552: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 22.011586834s
    Mar  1 12:51:06.549: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 24.008430844s
    Mar  1 12:51:08.551: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 26.010375486s
    Mar  1 12:51:10.552: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 28.010661078s
    Mar  1 12:51:12.549: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 30.008423417s
    Mar  1 12:51:14.551: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 32.009813507s
    Mar  1 12:51:16.551: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 34.010117764s
    Mar  1 12:51:18.552: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 36.011015977s
    Mar  1 12:51:20.551: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 38.010633407s
    Mar  1 12:51:22.549: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 40.008459461s
    Mar  1 12:51:24.551: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 42.009981076s
    Mar  1 12:51:26.551: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 44.009972597s
    Mar  1 12:51:28.551: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 46.009796792s
    Mar  1 12:51:30.552: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 48.011564186s
    Mar  1 12:51:32.551: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 50.010147241s
    Mar  1 12:51:34.549: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 52.008304864s
    Mar  1 12:51:36.551: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 54.010224888s
    Mar  1 12:51:38.551: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 56.010056799s
    Mar  1 12:51:40.550: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 58.009160923s
    Mar  1 12:51:42.552: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.010794715s
    Mar  1 12:51:44.551: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.010124014s
    Mar  1 12:51:46.551: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.009917371s
    Mar  1 12:51:48.550: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.009123637s
    Mar  1 12:51:50.551: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.01037783s
    Mar  1 12:51:52.550: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.008942763s
    Mar  1 12:51:54.551: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.010548891s
    Mar  1 12:51:56.551: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.01002677s
    Mar  1 12:51:58.552: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.011005847s
    Mar  1 12:52:00.550: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.009642676s
    Mar  1 12:52:02.549: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.008350033s
    Mar  1 12:52:04.551: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.009661433s
    Mar  1 12:52:06.551: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.010389865s
    Mar  1 12:52:08.551: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.010370258s
    Mar  1 12:52:10.551: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.010215789s
    Mar  1 12:52:12.551: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.009676193s
    Mar  1 12:52:14.551: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.010181384s
    Mar  1 12:52:16.551: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.010029813s
    Mar  1 12:52:18.551: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.010113249s
    Mar  1 12:52:20.549: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.00854375s
    Mar  1 12:52:22.551: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.01061945s
    Mar  1 12:52:24.551: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.010101173s
    Mar  1 12:52:26.551: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.009884704s
    Mar  1 12:52:28.550: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.009473024s
    Mar  1 12:52:30.553: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.011906902s
    Mar  1 12:52:32.549: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.008260126s
    Mar  1 12:52:34.550: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.009652372s
    Mar  1 12:52:36.552: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.011021377s
    Mar  1 12:52:38.549: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.008552389s
    Mar  1 12:52:40.551: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.009862727s
    Mar  1 12:52:42.551: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.010318977s
    Mar  1 12:52:42.555: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.014279064s
    STEP: updating the pod 03/01/23 12:52:42.555
    Mar  1 12:52:43.074: INFO: Successfully updated pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8"
    STEP: waiting for pod running 03/01/23 12:52:43.074
    Mar  1 12:52:43.074: INFO: Waiting up to 2m0s for pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8" in namespace "var-expansion-8465" to be "running"
    Mar  1 12:52:43.079: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.126005ms
    Mar  1 12:52:45.084: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8": Phase="Running", Reason="", readiness=true. Elapsed: 2.00961352s
    Mar  1 12:52:45.084: INFO: Pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8" satisfied condition "running"
    STEP: deleting the pod gracefully 03/01/23 12:52:45.084
    Mar  1 12:52:45.084: INFO: Deleting pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8" in namespace "var-expansion-8465"
    Mar  1 12:52:45.100: INFO: Wait up to 5m0s for pod "var-expansion-ee6685e3-a567-4ea8-89cf-a9502c306ba8" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Mar  1 12:53:17.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-8465" for this suite. 03/01/23 12:53:17.119
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:53:17.128
Mar  1 12:53:17.128: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename projected 03/01/23 12:53:17.129
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:53:17.149
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:53:17.152
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
STEP: Creating a pod to test downward API volume plugin 03/01/23 12:53:17.156
Mar  1 12:53:17.168: INFO: Waiting up to 5m0s for pod "downwardapi-volume-aa6dc3bc-57e9-42ff-bbfb-a79a94e44eb8" in namespace "projected-4297" to be "Succeeded or Failed"
Mar  1 12:53:17.176: INFO: Pod "downwardapi-volume-aa6dc3bc-57e9-42ff-bbfb-a79a94e44eb8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.612403ms
Mar  1 12:53:19.186: INFO: Pod "downwardapi-volume-aa6dc3bc-57e9-42ff-bbfb-a79a94e44eb8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017778342s
Mar  1 12:53:21.182: INFO: Pod "downwardapi-volume-aa6dc3bc-57e9-42ff-bbfb-a79a94e44eb8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013664788s
STEP: Saw pod success 03/01/23 12:53:21.182
Mar  1 12:53:21.182: INFO: Pod "downwardapi-volume-aa6dc3bc-57e9-42ff-bbfb-a79a94e44eb8" satisfied condition "Succeeded or Failed"
Mar  1 12:53:21.188: INFO: Trying to get logs from node lab1-k8s-node-3 pod downwardapi-volume-aa6dc3bc-57e9-42ff-bbfb-a79a94e44eb8 container client-container: <nil>
STEP: delete the pod 03/01/23 12:53:21.209
Mar  1 12:53:21.226: INFO: Waiting for pod downwardapi-volume-aa6dc3bc-57e9-42ff-bbfb-a79a94e44eb8 to disappear
Mar  1 12:53:21.230: INFO: Pod downwardapi-volume-aa6dc3bc-57e9-42ff-bbfb-a79a94e44eb8 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Mar  1 12:53:21.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4297" for this suite. 03/01/23 12:53:21.239
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":238,"skipped":4390,"failed":0}
------------------------------
â€¢ [4.120 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:53:17.128
    Mar  1 12:53:17.128: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename projected 03/01/23 12:53:17.129
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:53:17.149
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:53:17.152
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:260
    STEP: Creating a pod to test downward API volume plugin 03/01/23 12:53:17.156
    Mar  1 12:53:17.168: INFO: Waiting up to 5m0s for pod "downwardapi-volume-aa6dc3bc-57e9-42ff-bbfb-a79a94e44eb8" in namespace "projected-4297" to be "Succeeded or Failed"
    Mar  1 12:53:17.176: INFO: Pod "downwardapi-volume-aa6dc3bc-57e9-42ff-bbfb-a79a94e44eb8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.612403ms
    Mar  1 12:53:19.186: INFO: Pod "downwardapi-volume-aa6dc3bc-57e9-42ff-bbfb-a79a94e44eb8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017778342s
    Mar  1 12:53:21.182: INFO: Pod "downwardapi-volume-aa6dc3bc-57e9-42ff-bbfb-a79a94e44eb8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013664788s
    STEP: Saw pod success 03/01/23 12:53:21.182
    Mar  1 12:53:21.182: INFO: Pod "downwardapi-volume-aa6dc3bc-57e9-42ff-bbfb-a79a94e44eb8" satisfied condition "Succeeded or Failed"
    Mar  1 12:53:21.188: INFO: Trying to get logs from node lab1-k8s-node-3 pod downwardapi-volume-aa6dc3bc-57e9-42ff-bbfb-a79a94e44eb8 container client-container: <nil>
    STEP: delete the pod 03/01/23 12:53:21.209
    Mar  1 12:53:21.226: INFO: Waiting for pod downwardapi-volume-aa6dc3bc-57e9-42ff-bbfb-a79a94e44eb8 to disappear
    Mar  1 12:53:21.230: INFO: Pod downwardapi-volume-aa6dc3bc-57e9-42ff-bbfb-a79a94e44eb8 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Mar  1 12:53:21.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4297" for this suite. 03/01/23 12:53:21.239
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:53:21.251
Mar  1 12:53:21.252: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename pods 03/01/23 12:53:21.252
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:53:21.271
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:53:21.275
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
Mar  1 12:53:21.278: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: creating the pod 03/01/23 12:53:21.279
STEP: submitting the pod to kubernetes 03/01/23 12:53:21.279
Mar  1 12:53:21.289: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-199c3f32-9cfe-48ca-ad95-6e4493b99327" in namespace "pods-2082" to be "running and ready"
Mar  1 12:53:21.296: INFO: Pod "pod-logs-websocket-199c3f32-9cfe-48ca-ad95-6e4493b99327": Phase="Pending", Reason="", readiness=false. Elapsed: 7.20819ms
Mar  1 12:53:21.296: INFO: The phase of Pod pod-logs-websocket-199c3f32-9cfe-48ca-ad95-6e4493b99327 is Pending, waiting for it to be Running (with Ready = true)
Mar  1 12:53:23.304: INFO: Pod "pod-logs-websocket-199c3f32-9cfe-48ca-ad95-6e4493b99327": Phase="Running", Reason="", readiness=true. Elapsed: 2.014542588s
Mar  1 12:53:23.304: INFO: The phase of Pod pod-logs-websocket-199c3f32-9cfe-48ca-ad95-6e4493b99327 is Running (Ready = true)
Mar  1 12:53:23.304: INFO: Pod "pod-logs-websocket-199c3f32-9cfe-48ca-ad95-6e4493b99327" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Mar  1 12:53:23.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2082" for this suite. 03/01/23 12:53:23.342
{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","completed":239,"skipped":4401,"failed":0}
------------------------------
â€¢ [2.101 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:53:21.251
    Mar  1 12:53:21.252: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename pods 03/01/23 12:53:21.252
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:53:21.271
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:53:21.275
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:617
    Mar  1 12:53:21.278: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: creating the pod 03/01/23 12:53:21.279
    STEP: submitting the pod to kubernetes 03/01/23 12:53:21.279
    Mar  1 12:53:21.289: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-199c3f32-9cfe-48ca-ad95-6e4493b99327" in namespace "pods-2082" to be "running and ready"
    Mar  1 12:53:21.296: INFO: Pod "pod-logs-websocket-199c3f32-9cfe-48ca-ad95-6e4493b99327": Phase="Pending", Reason="", readiness=false. Elapsed: 7.20819ms
    Mar  1 12:53:21.296: INFO: The phase of Pod pod-logs-websocket-199c3f32-9cfe-48ca-ad95-6e4493b99327 is Pending, waiting for it to be Running (with Ready = true)
    Mar  1 12:53:23.304: INFO: Pod "pod-logs-websocket-199c3f32-9cfe-48ca-ad95-6e4493b99327": Phase="Running", Reason="", readiness=true. Elapsed: 2.014542588s
    Mar  1 12:53:23.304: INFO: The phase of Pod pod-logs-websocket-199c3f32-9cfe-48ca-ad95-6e4493b99327 is Running (Ready = true)
    Mar  1 12:53:23.304: INFO: Pod "pod-logs-websocket-199c3f32-9cfe-48ca-ad95-6e4493b99327" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Mar  1 12:53:23.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-2082" for this suite. 03/01/23 12:53:23.342
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:53:23.354
Mar  1 12:53:23.355: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename downward-api 03/01/23 12:53:23.355
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:53:23.375
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:53:23.379
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
STEP: Creating the pod 03/01/23 12:53:23.382
Mar  1 12:53:23.394: INFO: Waiting up to 5m0s for pod "labelsupdatef632ac29-f607-4680-935e-1772350a7301" in namespace "downward-api-4139" to be "running and ready"
Mar  1 12:53:23.401: INFO: Pod "labelsupdatef632ac29-f607-4680-935e-1772350a7301": Phase="Pending", Reason="", readiness=false. Elapsed: 7.140839ms
Mar  1 12:53:23.401: INFO: The phase of Pod labelsupdatef632ac29-f607-4680-935e-1772350a7301 is Pending, waiting for it to be Running (with Ready = true)
Mar  1 12:53:25.407: INFO: Pod "labelsupdatef632ac29-f607-4680-935e-1772350a7301": Phase="Running", Reason="", readiness=true. Elapsed: 2.012924542s
Mar  1 12:53:25.407: INFO: The phase of Pod labelsupdatef632ac29-f607-4680-935e-1772350a7301 is Running (Ready = true)
Mar  1 12:53:25.407: INFO: Pod "labelsupdatef632ac29-f607-4680-935e-1772350a7301" satisfied condition "running and ready"
Mar  1 12:53:25.937: INFO: Successfully updated pod "labelsupdatef632ac29-f607-4680-935e-1772350a7301"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Mar  1 12:53:29.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4139" for this suite. 03/01/23 12:53:29.981
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","completed":240,"skipped":4411,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.635 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:53:23.354
    Mar  1 12:53:23.355: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename downward-api 03/01/23 12:53:23.355
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:53:23.375
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:53:23.379
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:129
    STEP: Creating the pod 03/01/23 12:53:23.382
    Mar  1 12:53:23.394: INFO: Waiting up to 5m0s for pod "labelsupdatef632ac29-f607-4680-935e-1772350a7301" in namespace "downward-api-4139" to be "running and ready"
    Mar  1 12:53:23.401: INFO: Pod "labelsupdatef632ac29-f607-4680-935e-1772350a7301": Phase="Pending", Reason="", readiness=false. Elapsed: 7.140839ms
    Mar  1 12:53:23.401: INFO: The phase of Pod labelsupdatef632ac29-f607-4680-935e-1772350a7301 is Pending, waiting for it to be Running (with Ready = true)
    Mar  1 12:53:25.407: INFO: Pod "labelsupdatef632ac29-f607-4680-935e-1772350a7301": Phase="Running", Reason="", readiness=true. Elapsed: 2.012924542s
    Mar  1 12:53:25.407: INFO: The phase of Pod labelsupdatef632ac29-f607-4680-935e-1772350a7301 is Running (Ready = true)
    Mar  1 12:53:25.407: INFO: Pod "labelsupdatef632ac29-f607-4680-935e-1772350a7301" satisfied condition "running and ready"
    Mar  1 12:53:25.937: INFO: Successfully updated pod "labelsupdatef632ac29-f607-4680-935e-1772350a7301"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Mar  1 12:53:29.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-4139" for this suite. 03/01/23 12:53:29.981
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:53:29.99
Mar  1 12:53:29.990: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename pod-network-test 03/01/23 12:53:29.991
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:53:30.01
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:53:30.013
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-4085 03/01/23 12:53:30.016
STEP: creating a selector 03/01/23 12:53:30.017
STEP: Creating the service pods in kubernetes 03/01/23 12:53:30.017
Mar  1 12:53:30.017: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar  1 12:53:30.061: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-4085" to be "running and ready"
Mar  1 12:53:30.094: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 32.621238ms
Mar  1 12:53:30.094: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar  1 12:53:32.099: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.037805226s
Mar  1 12:53:32.099: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  1 12:53:34.099: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.037876537s
Mar  1 12:53:34.099: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  1 12:53:36.100: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.039135971s
Mar  1 12:53:36.100: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  1 12:53:38.106: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.044883481s
Mar  1 12:53:38.106: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  1 12:53:40.098: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.037129442s
Mar  1 12:53:40.098: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  1 12:53:42.101: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.040076661s
Mar  1 12:53:42.101: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Mar  1 12:53:42.101: INFO: Pod "netserver-0" satisfied condition "running and ready"
Mar  1 12:53:42.107: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-4085" to be "running and ready"
Mar  1 12:53:42.111: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.71033ms
Mar  1 12:53:42.111: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Mar  1 12:53:42.111: INFO: Pod "netserver-1" satisfied condition "running and ready"
Mar  1 12:53:42.115: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-4085" to be "running and ready"
Mar  1 12:53:42.120: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 4.728353ms
Mar  1 12:53:42.120: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Mar  1 12:53:42.120: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 03/01/23 12:53:42.124
Mar  1 12:53:42.147: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-4085" to be "running"
Mar  1 12:53:42.156: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.510803ms
Mar  1 12:53:44.162: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.015071372s
Mar  1 12:53:44.162: INFO: Pod "test-container-pod" satisfied condition "running"
Mar  1 12:53:44.166: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-4085" to be "running"
Mar  1 12:53:44.171: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 5.04117ms
Mar  1 12:53:44.171: INFO: Pod "host-test-container-pod" satisfied condition "running"
Mar  1 12:53:44.177: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Mar  1 12:53:44.177: INFO: Going to poll 10.233.95.167 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Mar  1 12:53:44.182: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.95.167 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4085 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  1 12:53:44.182: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
Mar  1 12:53:44.183: INFO: ExecWithOptions: Clientset creation
Mar  1 12:53:44.183: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-4085/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.95.167+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar  1 12:53:45.258: INFO: Found all 1 expected endpoints: [netserver-0]
Mar  1 12:53:45.258: INFO: Going to poll 10.233.64.126 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Mar  1 12:53:45.264: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.64.126 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4085 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  1 12:53:45.264: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
Mar  1 12:53:45.264: INFO: ExecWithOptions: Clientset creation
Mar  1 12:53:45.264: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-4085/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.64.126+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar  1 12:53:46.334: INFO: Found all 1 expected endpoints: [netserver-1]
Mar  1 12:53:46.335: INFO: Going to poll 10.233.74.53 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Mar  1 12:53:46.339: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.74.53 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4085 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  1 12:53:46.339: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
Mar  1 12:53:46.339: INFO: ExecWithOptions: Clientset creation
Mar  1 12:53:46.340: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-4085/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.74.53+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar  1 12:53:47.419: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Mar  1 12:53:47.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4085" for this suite. 03/01/23 12:53:47.429
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","completed":241,"skipped":4413,"failed":0}
------------------------------
â€¢ [SLOW TEST] [17.448 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:53:29.99
    Mar  1 12:53:29.990: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename pod-network-test 03/01/23 12:53:29.991
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:53:30.01
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:53:30.013
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-4085 03/01/23 12:53:30.016
    STEP: creating a selector 03/01/23 12:53:30.017
    STEP: Creating the service pods in kubernetes 03/01/23 12:53:30.017
    Mar  1 12:53:30.017: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Mar  1 12:53:30.061: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-4085" to be "running and ready"
    Mar  1 12:53:30.094: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 32.621238ms
    Mar  1 12:53:30.094: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Mar  1 12:53:32.099: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.037805226s
    Mar  1 12:53:32.099: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  1 12:53:34.099: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.037876537s
    Mar  1 12:53:34.099: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  1 12:53:36.100: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.039135971s
    Mar  1 12:53:36.100: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  1 12:53:38.106: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.044883481s
    Mar  1 12:53:38.106: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  1 12:53:40.098: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.037129442s
    Mar  1 12:53:40.098: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  1 12:53:42.101: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.040076661s
    Mar  1 12:53:42.101: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Mar  1 12:53:42.101: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Mar  1 12:53:42.107: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-4085" to be "running and ready"
    Mar  1 12:53:42.111: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.71033ms
    Mar  1 12:53:42.111: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Mar  1 12:53:42.111: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Mar  1 12:53:42.115: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-4085" to be "running and ready"
    Mar  1 12:53:42.120: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 4.728353ms
    Mar  1 12:53:42.120: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Mar  1 12:53:42.120: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 03/01/23 12:53:42.124
    Mar  1 12:53:42.147: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-4085" to be "running"
    Mar  1 12:53:42.156: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.510803ms
    Mar  1 12:53:44.162: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.015071372s
    Mar  1 12:53:44.162: INFO: Pod "test-container-pod" satisfied condition "running"
    Mar  1 12:53:44.166: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-4085" to be "running"
    Mar  1 12:53:44.171: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 5.04117ms
    Mar  1 12:53:44.171: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Mar  1 12:53:44.177: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Mar  1 12:53:44.177: INFO: Going to poll 10.233.95.167 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Mar  1 12:53:44.182: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.95.167 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4085 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  1 12:53:44.182: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    Mar  1 12:53:44.183: INFO: ExecWithOptions: Clientset creation
    Mar  1 12:53:44.183: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-4085/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.95.167+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar  1 12:53:45.258: INFO: Found all 1 expected endpoints: [netserver-0]
    Mar  1 12:53:45.258: INFO: Going to poll 10.233.64.126 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Mar  1 12:53:45.264: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.64.126 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4085 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  1 12:53:45.264: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    Mar  1 12:53:45.264: INFO: ExecWithOptions: Clientset creation
    Mar  1 12:53:45.264: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-4085/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.64.126+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar  1 12:53:46.334: INFO: Found all 1 expected endpoints: [netserver-1]
    Mar  1 12:53:46.335: INFO: Going to poll 10.233.74.53 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Mar  1 12:53:46.339: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.74.53 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4085 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  1 12:53:46.339: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    Mar  1 12:53:46.339: INFO: ExecWithOptions: Clientset creation
    Mar  1 12:53:46.340: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-4085/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.74.53+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar  1 12:53:47.419: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Mar  1 12:53:47.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-4085" for this suite. 03/01/23 12:53:47.429
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:53:47.439
Mar  1 12:53:47.439: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename kubectl 03/01/23 12:53:47.44
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:53:47.46
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:53:47.463
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
STEP: creating a replication controller 03/01/23 12:53:47.466
Mar  1 12:53:47.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-1151 create -f -'
Mar  1 12:53:48.281: INFO: stderr: ""
Mar  1 12:53:48.281: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 03/01/23 12:53:48.281
Mar  1 12:53:48.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-1151 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  1 12:53:48.359: INFO: stderr: ""
Mar  1 12:53:48.359: INFO: stdout: "update-demo-nautilus-44mg8 update-demo-nautilus-tbd24 "
Mar  1 12:53:48.360: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-1151 get pods update-demo-nautilus-44mg8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  1 12:53:48.421: INFO: stderr: ""
Mar  1 12:53:48.421: INFO: stdout: ""
Mar  1 12:53:48.421: INFO: update-demo-nautilus-44mg8 is created but not running
Mar  1 12:53:53.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-1151 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  1 12:53:53.485: INFO: stderr: ""
Mar  1 12:53:53.485: INFO: stdout: "update-demo-nautilus-44mg8 update-demo-nautilus-tbd24 "
Mar  1 12:53:53.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-1151 get pods update-demo-nautilus-44mg8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  1 12:53:53.555: INFO: stderr: ""
Mar  1 12:53:53.555: INFO: stdout: "true"
Mar  1 12:53:53.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-1151 get pods update-demo-nautilus-44mg8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  1 12:53:53.615: INFO: stderr: ""
Mar  1 12:53:53.615: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Mar  1 12:53:53.615: INFO: validating pod update-demo-nautilus-44mg8
Mar  1 12:53:53.623: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  1 12:53:53.623: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  1 12:53:53.623: INFO: update-demo-nautilus-44mg8 is verified up and running
Mar  1 12:53:53.623: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-1151 get pods update-demo-nautilus-tbd24 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  1 12:53:53.684: INFO: stderr: ""
Mar  1 12:53:53.684: INFO: stdout: "true"
Mar  1 12:53:53.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-1151 get pods update-demo-nautilus-tbd24 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  1 12:53:53.749: INFO: stderr: ""
Mar  1 12:53:53.749: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Mar  1 12:53:53.749: INFO: validating pod update-demo-nautilus-tbd24
Mar  1 12:53:53.757: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  1 12:53:53.757: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  1 12:53:53.757: INFO: update-demo-nautilus-tbd24 is verified up and running
STEP: using delete to clean up resources 03/01/23 12:53:53.757
Mar  1 12:53:53.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-1151 delete --grace-period=0 --force -f -'
Mar  1 12:53:53.836: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  1 12:53:53.836: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar  1 12:53:53.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-1151 get rc,svc -l name=update-demo --no-headers'
Mar  1 12:53:53.937: INFO: stderr: "No resources found in kubectl-1151 namespace.\n"
Mar  1 12:53:53.937: INFO: stdout: ""
Mar  1 12:53:53.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-1151 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  1 12:53:54.024: INFO: stderr: ""
Mar  1 12:53:54.024: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar  1 12:53:54.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1151" for this suite. 03/01/23 12:53:54.033
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","completed":242,"skipped":4419,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.603 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:337

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:53:47.439
    Mar  1 12:53:47.439: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename kubectl 03/01/23 12:53:47.44
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:53:47.46
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:53:47.463
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:337
    STEP: creating a replication controller 03/01/23 12:53:47.466
    Mar  1 12:53:47.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-1151 create -f -'
    Mar  1 12:53:48.281: INFO: stderr: ""
    Mar  1 12:53:48.281: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 03/01/23 12:53:48.281
    Mar  1 12:53:48.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-1151 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar  1 12:53:48.359: INFO: stderr: ""
    Mar  1 12:53:48.359: INFO: stdout: "update-demo-nautilus-44mg8 update-demo-nautilus-tbd24 "
    Mar  1 12:53:48.360: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-1151 get pods update-demo-nautilus-44mg8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar  1 12:53:48.421: INFO: stderr: ""
    Mar  1 12:53:48.421: INFO: stdout: ""
    Mar  1 12:53:48.421: INFO: update-demo-nautilus-44mg8 is created but not running
    Mar  1 12:53:53.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-1151 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar  1 12:53:53.485: INFO: stderr: ""
    Mar  1 12:53:53.485: INFO: stdout: "update-demo-nautilus-44mg8 update-demo-nautilus-tbd24 "
    Mar  1 12:53:53.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-1151 get pods update-demo-nautilus-44mg8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar  1 12:53:53.555: INFO: stderr: ""
    Mar  1 12:53:53.555: INFO: stdout: "true"
    Mar  1 12:53:53.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-1151 get pods update-demo-nautilus-44mg8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar  1 12:53:53.615: INFO: stderr: ""
    Mar  1 12:53:53.615: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Mar  1 12:53:53.615: INFO: validating pod update-demo-nautilus-44mg8
    Mar  1 12:53:53.623: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar  1 12:53:53.623: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar  1 12:53:53.623: INFO: update-demo-nautilus-44mg8 is verified up and running
    Mar  1 12:53:53.623: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-1151 get pods update-demo-nautilus-tbd24 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar  1 12:53:53.684: INFO: stderr: ""
    Mar  1 12:53:53.684: INFO: stdout: "true"
    Mar  1 12:53:53.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-1151 get pods update-demo-nautilus-tbd24 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar  1 12:53:53.749: INFO: stderr: ""
    Mar  1 12:53:53.749: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Mar  1 12:53:53.749: INFO: validating pod update-demo-nautilus-tbd24
    Mar  1 12:53:53.757: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar  1 12:53:53.757: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar  1 12:53:53.757: INFO: update-demo-nautilus-tbd24 is verified up and running
    STEP: using delete to clean up resources 03/01/23 12:53:53.757
    Mar  1 12:53:53.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-1151 delete --grace-period=0 --force -f -'
    Mar  1 12:53:53.836: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar  1 12:53:53.836: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Mar  1 12:53:53.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-1151 get rc,svc -l name=update-demo --no-headers'
    Mar  1 12:53:53.937: INFO: stderr: "No resources found in kubectl-1151 namespace.\n"
    Mar  1 12:53:53.937: INFO: stdout: ""
    Mar  1 12:53:53.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-1151 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Mar  1 12:53:54.024: INFO: stderr: ""
    Mar  1 12:53:54.024: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar  1 12:53:54.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-1151" for this suite. 03/01/23 12:53:54.033
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:53:54.042
Mar  1 12:53:54.042: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename secrets 03/01/23 12:53:54.043
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:53:54.062
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:53:54.068
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
STEP: creating a secret 03/01/23 12:53:54.111
STEP: listing secrets in all namespaces to ensure that there are more than zero 03/01/23 12:53:54.117
STEP: patching the secret 03/01/23 12:53:54.125
STEP: deleting the secret using a LabelSelector 03/01/23 12:53:54.136
STEP: listing secrets in all namespaces, searching for label name and value in patch 03/01/23 12:53:54.15
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Mar  1 12:53:54.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8183" for this suite. 03/01/23 12:53:54.164
{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","completed":243,"skipped":4419,"failed":0}
------------------------------
â€¢ [0.130 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:53:54.042
    Mar  1 12:53:54.042: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename secrets 03/01/23 12:53:54.043
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:53:54.062
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:53:54.068
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:153
    STEP: creating a secret 03/01/23 12:53:54.111
    STEP: listing secrets in all namespaces to ensure that there are more than zero 03/01/23 12:53:54.117
    STEP: patching the secret 03/01/23 12:53:54.125
    STEP: deleting the secret using a LabelSelector 03/01/23 12:53:54.136
    STEP: listing secrets in all namespaces, searching for label name and value in patch 03/01/23 12:53:54.15
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Mar  1 12:53:54.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-8183" for this suite. 03/01/23 12:53:54.164
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:53:54.173
Mar  1 12:53:54.173: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename projected 03/01/23 12:53:54.174
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:53:54.202
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:53:54.215
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
STEP: Creating secret with name s-test-opt-del-f57fd84b-b960-4374-a2a7-f90abaaba8b4 03/01/23 12:53:54.227
STEP: Creating secret with name s-test-opt-upd-8fee87a8-3be8-4c83-85de-1fefa3dad964 03/01/23 12:53:54.236
STEP: Creating the pod 03/01/23 12:53:54.247
Mar  1 12:53:54.266: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-dc7d09df-fd5a-40e8-bfb7-f1c11a415d02" in namespace "projected-2718" to be "running and ready"
Mar  1 12:53:54.271: INFO: Pod "pod-projected-secrets-dc7d09df-fd5a-40e8-bfb7-f1c11a415d02": Phase="Pending", Reason="", readiness=false. Elapsed: 4.604832ms
Mar  1 12:53:54.271: INFO: The phase of Pod pod-projected-secrets-dc7d09df-fd5a-40e8-bfb7-f1c11a415d02 is Pending, waiting for it to be Running (with Ready = true)
Mar  1 12:53:56.277: INFO: Pod "pod-projected-secrets-dc7d09df-fd5a-40e8-bfb7-f1c11a415d02": Phase="Running", Reason="", readiness=true. Elapsed: 2.010908265s
Mar  1 12:53:56.277: INFO: The phase of Pod pod-projected-secrets-dc7d09df-fd5a-40e8-bfb7-f1c11a415d02 is Running (Ready = true)
Mar  1 12:53:56.277: INFO: Pod "pod-projected-secrets-dc7d09df-fd5a-40e8-bfb7-f1c11a415d02" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-f57fd84b-b960-4374-a2a7-f90abaaba8b4 03/01/23 12:53:56.31
STEP: Updating secret s-test-opt-upd-8fee87a8-3be8-4c83-85de-1fefa3dad964 03/01/23 12:53:56.318
STEP: Creating secret with name s-test-opt-create-fd12ed5a-fbd4-4ffd-ac0b-40bb1219b048 03/01/23 12:53:56.325
STEP: waiting to observe update in volume 03/01/23 12:53:56.331
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Mar  1 12:53:58.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2718" for this suite. 03/01/23 12:53:58.387
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":244,"skipped":4419,"failed":0}
------------------------------
â€¢ [4.224 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:53:54.173
    Mar  1 12:53:54.173: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename projected 03/01/23 12:53:54.174
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:53:54.202
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:53:54.215
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:214
    STEP: Creating secret with name s-test-opt-del-f57fd84b-b960-4374-a2a7-f90abaaba8b4 03/01/23 12:53:54.227
    STEP: Creating secret with name s-test-opt-upd-8fee87a8-3be8-4c83-85de-1fefa3dad964 03/01/23 12:53:54.236
    STEP: Creating the pod 03/01/23 12:53:54.247
    Mar  1 12:53:54.266: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-dc7d09df-fd5a-40e8-bfb7-f1c11a415d02" in namespace "projected-2718" to be "running and ready"
    Mar  1 12:53:54.271: INFO: Pod "pod-projected-secrets-dc7d09df-fd5a-40e8-bfb7-f1c11a415d02": Phase="Pending", Reason="", readiness=false. Elapsed: 4.604832ms
    Mar  1 12:53:54.271: INFO: The phase of Pod pod-projected-secrets-dc7d09df-fd5a-40e8-bfb7-f1c11a415d02 is Pending, waiting for it to be Running (with Ready = true)
    Mar  1 12:53:56.277: INFO: Pod "pod-projected-secrets-dc7d09df-fd5a-40e8-bfb7-f1c11a415d02": Phase="Running", Reason="", readiness=true. Elapsed: 2.010908265s
    Mar  1 12:53:56.277: INFO: The phase of Pod pod-projected-secrets-dc7d09df-fd5a-40e8-bfb7-f1c11a415d02 is Running (Ready = true)
    Mar  1 12:53:56.277: INFO: Pod "pod-projected-secrets-dc7d09df-fd5a-40e8-bfb7-f1c11a415d02" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-f57fd84b-b960-4374-a2a7-f90abaaba8b4 03/01/23 12:53:56.31
    STEP: Updating secret s-test-opt-upd-8fee87a8-3be8-4c83-85de-1fefa3dad964 03/01/23 12:53:56.318
    STEP: Creating secret with name s-test-opt-create-fd12ed5a-fbd4-4ffd-ac0b-40bb1219b048 03/01/23 12:53:56.325
    STEP: waiting to observe update in volume 03/01/23 12:53:56.331
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Mar  1 12:53:58.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2718" for this suite. 03/01/23 12:53:58.387
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:53:58.401
Mar  1 12:53:58.401: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename security-context-test 03/01/23 12:53:58.402
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:53:58.42
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:53:58.423
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
Mar  1 12:53:58.438: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-0ea0399d-d2a0-43cb-8dca-c6e9cbd5f9b2" in namespace "security-context-test-5962" to be "Succeeded or Failed"
Mar  1 12:53:58.446: INFO: Pod "alpine-nnp-false-0ea0399d-d2a0-43cb-8dca-c6e9cbd5f9b2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.259546ms
Mar  1 12:54:00.451: INFO: Pod "alpine-nnp-false-0ea0399d-d2a0-43cb-8dca-c6e9cbd5f9b2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013291659s
Mar  1 12:54:02.451: INFO: Pod "alpine-nnp-false-0ea0399d-d2a0-43cb-8dca-c6e9cbd5f9b2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013512057s
Mar  1 12:54:04.451: INFO: Pod "alpine-nnp-false-0ea0399d-d2a0-43cb-8dca-c6e9cbd5f9b2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013107658s
Mar  1 12:54:04.451: INFO: Pod "alpine-nnp-false-0ea0399d-d2a0-43cb-8dca-c6e9cbd5f9b2" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Mar  1 12:54:04.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5962" for this suite. 03/01/23 12:54:04.483
{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","completed":245,"skipped":4463,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.095 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:554
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:608

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:53:58.401
    Mar  1 12:53:58.401: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename security-context-test 03/01/23 12:53:58.402
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:53:58.42
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:53:58.423
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:608
    Mar  1 12:53:58.438: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-0ea0399d-d2a0-43cb-8dca-c6e9cbd5f9b2" in namespace "security-context-test-5962" to be "Succeeded or Failed"
    Mar  1 12:53:58.446: INFO: Pod "alpine-nnp-false-0ea0399d-d2a0-43cb-8dca-c6e9cbd5f9b2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.259546ms
    Mar  1 12:54:00.451: INFO: Pod "alpine-nnp-false-0ea0399d-d2a0-43cb-8dca-c6e9cbd5f9b2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013291659s
    Mar  1 12:54:02.451: INFO: Pod "alpine-nnp-false-0ea0399d-d2a0-43cb-8dca-c6e9cbd5f9b2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013512057s
    Mar  1 12:54:04.451: INFO: Pod "alpine-nnp-false-0ea0399d-d2a0-43cb-8dca-c6e9cbd5f9b2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013107658s
    Mar  1 12:54:04.451: INFO: Pod "alpine-nnp-false-0ea0399d-d2a0-43cb-8dca-c6e9cbd5f9b2" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Mar  1 12:54:04.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-5962" for this suite. 03/01/23 12:54:04.483
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:54:04.497
Mar  1 12:54:04.497: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename container-probe 03/01/23 12:54:04.498
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:54:04.519
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:54:04.523
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
STEP: Creating pod liveness-3ef12d57-d943-4da0-80c3-d67b096749d5 in namespace container-probe-4214 03/01/23 12:54:04.526
Mar  1 12:54:04.538: INFO: Waiting up to 5m0s for pod "liveness-3ef12d57-d943-4da0-80c3-d67b096749d5" in namespace "container-probe-4214" to be "not pending"
Mar  1 12:54:04.546: INFO: Pod "liveness-3ef12d57-d943-4da0-80c3-d67b096749d5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.089567ms
Mar  1 12:54:06.551: INFO: Pod "liveness-3ef12d57-d943-4da0-80c3-d67b096749d5": Phase="Running", Reason="", readiness=true. Elapsed: 2.012705443s
Mar  1 12:54:06.551: INFO: Pod "liveness-3ef12d57-d943-4da0-80c3-d67b096749d5" satisfied condition "not pending"
Mar  1 12:54:06.551: INFO: Started pod liveness-3ef12d57-d943-4da0-80c3-d67b096749d5 in namespace container-probe-4214
STEP: checking the pod's current state and verifying that restartCount is present 03/01/23 12:54:06.551
Mar  1 12:54:06.556: INFO: Initial restart count of pod liveness-3ef12d57-d943-4da0-80c3-d67b096749d5 is 0
Mar  1 12:54:26.622: INFO: Restart count of pod container-probe-4214/liveness-3ef12d57-d943-4da0-80c3-d67b096749d5 is now 1 (20.065452012s elapsed)
Mar  1 12:54:46.686: INFO: Restart count of pod container-probe-4214/liveness-3ef12d57-d943-4da0-80c3-d67b096749d5 is now 2 (40.129304834s elapsed)
Mar  1 12:55:06.751: INFO: Restart count of pod container-probe-4214/liveness-3ef12d57-d943-4da0-80c3-d67b096749d5 is now 3 (1m0.194169539s elapsed)
Mar  1 12:55:26.817: INFO: Restart count of pod container-probe-4214/liveness-3ef12d57-d943-4da0-80c3-d67b096749d5 is now 4 (1m20.260820644s elapsed)
Mar  1 12:56:27.002: INFO: Restart count of pod container-probe-4214/liveness-3ef12d57-d943-4da0-80c3-d67b096749d5 is now 5 (2m20.445635511s elapsed)
STEP: deleting the pod 03/01/23 12:56:27.002
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Mar  1 12:56:27.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4214" for this suite. 03/01/23 12:56:27.031
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","completed":246,"skipped":4478,"failed":0}
------------------------------
â€¢ [SLOW TEST] [142.544 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:54:04.497
    Mar  1 12:54:04.497: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename container-probe 03/01/23 12:54:04.498
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:54:04.519
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:54:04.523
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:195
    STEP: Creating pod liveness-3ef12d57-d943-4da0-80c3-d67b096749d5 in namespace container-probe-4214 03/01/23 12:54:04.526
    Mar  1 12:54:04.538: INFO: Waiting up to 5m0s for pod "liveness-3ef12d57-d943-4da0-80c3-d67b096749d5" in namespace "container-probe-4214" to be "not pending"
    Mar  1 12:54:04.546: INFO: Pod "liveness-3ef12d57-d943-4da0-80c3-d67b096749d5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.089567ms
    Mar  1 12:54:06.551: INFO: Pod "liveness-3ef12d57-d943-4da0-80c3-d67b096749d5": Phase="Running", Reason="", readiness=true. Elapsed: 2.012705443s
    Mar  1 12:54:06.551: INFO: Pod "liveness-3ef12d57-d943-4da0-80c3-d67b096749d5" satisfied condition "not pending"
    Mar  1 12:54:06.551: INFO: Started pod liveness-3ef12d57-d943-4da0-80c3-d67b096749d5 in namespace container-probe-4214
    STEP: checking the pod's current state and verifying that restartCount is present 03/01/23 12:54:06.551
    Mar  1 12:54:06.556: INFO: Initial restart count of pod liveness-3ef12d57-d943-4da0-80c3-d67b096749d5 is 0
    Mar  1 12:54:26.622: INFO: Restart count of pod container-probe-4214/liveness-3ef12d57-d943-4da0-80c3-d67b096749d5 is now 1 (20.065452012s elapsed)
    Mar  1 12:54:46.686: INFO: Restart count of pod container-probe-4214/liveness-3ef12d57-d943-4da0-80c3-d67b096749d5 is now 2 (40.129304834s elapsed)
    Mar  1 12:55:06.751: INFO: Restart count of pod container-probe-4214/liveness-3ef12d57-d943-4da0-80c3-d67b096749d5 is now 3 (1m0.194169539s elapsed)
    Mar  1 12:55:26.817: INFO: Restart count of pod container-probe-4214/liveness-3ef12d57-d943-4da0-80c3-d67b096749d5 is now 4 (1m20.260820644s elapsed)
    Mar  1 12:56:27.002: INFO: Restart count of pod container-probe-4214/liveness-3ef12d57-d943-4da0-80c3-d67b096749d5 is now 5 (2m20.445635511s elapsed)
    STEP: deleting the pod 03/01/23 12:56:27.002
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Mar  1 12:56:27.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-4214" for this suite. 03/01/23 12:56:27.031
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:56:27.042
Mar  1 12:56:27.042: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename projected 03/01/23 12:56:27.043
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:56:27.066
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:56:27.075
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
STEP: Creating projection with secret that has name projected-secret-test-map-2fc61ed1-674e-40e2-9c27-0d7ce7960360 03/01/23 12:56:27.079
STEP: Creating a pod to test consume secrets 03/01/23 12:56:27.087
Mar  1 12:56:27.099: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-14dc689e-2261-442e-b012-f98ee0322972" in namespace "projected-3822" to be "Succeeded or Failed"
Mar  1 12:56:27.106: INFO: Pod "pod-projected-secrets-14dc689e-2261-442e-b012-f98ee0322972": Phase="Pending", Reason="", readiness=false. Elapsed: 6.410385ms
Mar  1 12:56:29.112: INFO: Pod "pod-projected-secrets-14dc689e-2261-442e-b012-f98ee0322972": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012561899s
Mar  1 12:56:31.112: INFO: Pod "pod-projected-secrets-14dc689e-2261-442e-b012-f98ee0322972": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012666267s
STEP: Saw pod success 03/01/23 12:56:31.112
Mar  1 12:56:31.112: INFO: Pod "pod-projected-secrets-14dc689e-2261-442e-b012-f98ee0322972" satisfied condition "Succeeded or Failed"
Mar  1 12:56:31.116: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-projected-secrets-14dc689e-2261-442e-b012-f98ee0322972 container projected-secret-volume-test: <nil>
STEP: delete the pod 03/01/23 12:56:31.134
Mar  1 12:56:31.156: INFO: Waiting for pod pod-projected-secrets-14dc689e-2261-442e-b012-f98ee0322972 to disappear
Mar  1 12:56:31.160: INFO: Pod pod-projected-secrets-14dc689e-2261-442e-b012-f98ee0322972 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Mar  1 12:56:31.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3822" for this suite. 03/01/23 12:56:31.169
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":247,"skipped":4488,"failed":0}
------------------------------
â€¢ [4.137 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:56:27.042
    Mar  1 12:56:27.042: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename projected 03/01/23 12:56:27.043
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:56:27.066
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:56:27.075
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:87
    STEP: Creating projection with secret that has name projected-secret-test-map-2fc61ed1-674e-40e2-9c27-0d7ce7960360 03/01/23 12:56:27.079
    STEP: Creating a pod to test consume secrets 03/01/23 12:56:27.087
    Mar  1 12:56:27.099: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-14dc689e-2261-442e-b012-f98ee0322972" in namespace "projected-3822" to be "Succeeded or Failed"
    Mar  1 12:56:27.106: INFO: Pod "pod-projected-secrets-14dc689e-2261-442e-b012-f98ee0322972": Phase="Pending", Reason="", readiness=false. Elapsed: 6.410385ms
    Mar  1 12:56:29.112: INFO: Pod "pod-projected-secrets-14dc689e-2261-442e-b012-f98ee0322972": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012561899s
    Mar  1 12:56:31.112: INFO: Pod "pod-projected-secrets-14dc689e-2261-442e-b012-f98ee0322972": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012666267s
    STEP: Saw pod success 03/01/23 12:56:31.112
    Mar  1 12:56:31.112: INFO: Pod "pod-projected-secrets-14dc689e-2261-442e-b012-f98ee0322972" satisfied condition "Succeeded or Failed"
    Mar  1 12:56:31.116: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-projected-secrets-14dc689e-2261-442e-b012-f98ee0322972 container projected-secret-volume-test: <nil>
    STEP: delete the pod 03/01/23 12:56:31.134
    Mar  1 12:56:31.156: INFO: Waiting for pod pod-projected-secrets-14dc689e-2261-442e-b012-f98ee0322972 to disappear
    Mar  1 12:56:31.160: INFO: Pod pod-projected-secrets-14dc689e-2261-442e-b012-f98ee0322972 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Mar  1 12:56:31.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3822" for this suite. 03/01/23 12:56:31.169
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:56:31.184
Mar  1 12:56:31.184: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename resourcequota 03/01/23 12:56:31.185
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:56:31.201
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:56:31.207
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
STEP: Counting existing ResourceQuota 03/01/23 12:56:48.215
STEP: Creating a ResourceQuota 03/01/23 12:56:53.22
STEP: Ensuring resource quota status is calculated 03/01/23 12:56:53.228
STEP: Creating a ConfigMap 03/01/23 12:56:55.237
STEP: Ensuring resource quota status captures configMap creation 03/01/23 12:56:55.254
STEP: Deleting a ConfigMap 03/01/23 12:56:57.261
STEP: Ensuring resource quota status released usage 03/01/23 12:56:57.269
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Mar  1 12:56:59.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8649" for this suite. 03/01/23 12:56:59.286
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","completed":248,"skipped":4520,"failed":0}
------------------------------
â€¢ [SLOW TEST] [28.112 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:56:31.184
    Mar  1 12:56:31.184: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename resourcequota 03/01/23 12:56:31.185
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:56:31.201
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:56:31.207
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:316
    STEP: Counting existing ResourceQuota 03/01/23 12:56:48.215
    STEP: Creating a ResourceQuota 03/01/23 12:56:53.22
    STEP: Ensuring resource quota status is calculated 03/01/23 12:56:53.228
    STEP: Creating a ConfigMap 03/01/23 12:56:55.237
    STEP: Ensuring resource quota status captures configMap creation 03/01/23 12:56:55.254
    STEP: Deleting a ConfigMap 03/01/23 12:56:57.261
    STEP: Ensuring resource quota status released usage 03/01/23 12:56:57.269
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Mar  1 12:56:59.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-8649" for this suite. 03/01/23 12:56:59.286
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:56:59.304
Mar  1 12:56:59.304: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename resourcequota 03/01/23 12:56:59.304
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:56:59.324
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:56:59.328
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
STEP: Creating a ResourceQuota 03/01/23 12:56:59.332
STEP: Getting a ResourceQuota 03/01/23 12:56:59.339
STEP: Updating a ResourceQuota 03/01/23 12:56:59.346
STEP: Verifying a ResourceQuota was modified 03/01/23 12:56:59.352
STEP: Deleting a ResourceQuota 03/01/23 12:56:59.358
STEP: Verifying the deleted ResourceQuota 03/01/23 12:56:59.365
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Mar  1 12:56:59.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4188" for this suite. 03/01/23 12:56:59.376
{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","completed":249,"skipped":4664,"failed":0}
------------------------------
â€¢ [0.082 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:56:59.304
    Mar  1 12:56:59.304: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename resourcequota 03/01/23 12:56:59.304
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:56:59.324
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:56:59.328
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:874
    STEP: Creating a ResourceQuota 03/01/23 12:56:59.332
    STEP: Getting a ResourceQuota 03/01/23 12:56:59.339
    STEP: Updating a ResourceQuota 03/01/23 12:56:59.346
    STEP: Verifying a ResourceQuota was modified 03/01/23 12:56:59.352
    STEP: Deleting a ResourceQuota 03/01/23 12:56:59.358
    STEP: Verifying the deleted ResourceQuota 03/01/23 12:56:59.365
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Mar  1 12:56:59.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-4188" for this suite. 03/01/23 12:56:59.376
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:56:59.39
Mar  1 12:56:59.391: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename kubelet-test 03/01/23 12:56:59.391
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:56:59.409
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:56:59.412
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Mar  1 12:56:59.427: INFO: Waiting up to 5m0s for pod "busybox-readonly-fscdeb6346-76f7-4078-acb2-f5eda0cbcc46" in namespace "kubelet-test-3203" to be "running and ready"
Mar  1 12:56:59.437: INFO: Pod "busybox-readonly-fscdeb6346-76f7-4078-acb2-f5eda0cbcc46": Phase="Pending", Reason="", readiness=false. Elapsed: 9.947367ms
Mar  1 12:56:59.437: INFO: The phase of Pod busybox-readonly-fscdeb6346-76f7-4078-acb2-f5eda0cbcc46 is Pending, waiting for it to be Running (with Ready = true)
Mar  1 12:57:01.444: INFO: Pod "busybox-readonly-fscdeb6346-76f7-4078-acb2-f5eda0cbcc46": Phase="Running", Reason="", readiness=true. Elapsed: 2.017305933s
Mar  1 12:57:01.444: INFO: The phase of Pod busybox-readonly-fscdeb6346-76f7-4078-acb2-f5eda0cbcc46 is Running (Ready = true)
Mar  1 12:57:01.444: INFO: Pod "busybox-readonly-fscdeb6346-76f7-4078-acb2-f5eda0cbcc46" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Mar  1 12:57:01.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3203" for this suite. 03/01/23 12:57:01.467
{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","completed":250,"skipped":4693,"failed":0}
------------------------------
â€¢ [2.088 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:56:59.39
    Mar  1 12:56:59.391: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename kubelet-test 03/01/23 12:56:59.391
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:56:59.409
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:56:59.412
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Mar  1 12:56:59.427: INFO: Waiting up to 5m0s for pod "busybox-readonly-fscdeb6346-76f7-4078-acb2-f5eda0cbcc46" in namespace "kubelet-test-3203" to be "running and ready"
    Mar  1 12:56:59.437: INFO: Pod "busybox-readonly-fscdeb6346-76f7-4078-acb2-f5eda0cbcc46": Phase="Pending", Reason="", readiness=false. Elapsed: 9.947367ms
    Mar  1 12:56:59.437: INFO: The phase of Pod busybox-readonly-fscdeb6346-76f7-4078-acb2-f5eda0cbcc46 is Pending, waiting for it to be Running (with Ready = true)
    Mar  1 12:57:01.444: INFO: Pod "busybox-readonly-fscdeb6346-76f7-4078-acb2-f5eda0cbcc46": Phase="Running", Reason="", readiness=true. Elapsed: 2.017305933s
    Mar  1 12:57:01.444: INFO: The phase of Pod busybox-readonly-fscdeb6346-76f7-4078-acb2-f5eda0cbcc46 is Running (Ready = true)
    Mar  1 12:57:01.444: INFO: Pod "busybox-readonly-fscdeb6346-76f7-4078-acb2-f5eda0cbcc46" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Mar  1 12:57:01.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-3203" for this suite. 03/01/23 12:57:01.467
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:57:01.479
Mar  1 12:57:01.479: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename secrets 03/01/23 12:57:01.48
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:57:01.5
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:57:01.503
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
STEP: Creating secret with name s-test-opt-del-add71040-b4c8-4204-8045-b7e8c4b0aafb 03/01/23 12:57:01.511
STEP: Creating secret with name s-test-opt-upd-0e83556a-f35e-4e18-86a9-3c47560379ee 03/01/23 12:57:01.518
STEP: Creating the pod 03/01/23 12:57:01.525
Mar  1 12:57:01.536: INFO: Waiting up to 5m0s for pod "pod-secrets-8659545a-ed26-4572-9edf-94ad07b04ed0" in namespace "secrets-3606" to be "running and ready"
Mar  1 12:57:01.542: INFO: Pod "pod-secrets-8659545a-ed26-4572-9edf-94ad07b04ed0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.903652ms
Mar  1 12:57:01.542: INFO: The phase of Pod pod-secrets-8659545a-ed26-4572-9edf-94ad07b04ed0 is Pending, waiting for it to be Running (with Ready = true)
Mar  1 12:57:03.547: INFO: Pod "pod-secrets-8659545a-ed26-4572-9edf-94ad07b04ed0": Phase="Running", Reason="", readiness=true. Elapsed: 2.010484696s
Mar  1 12:57:03.547: INFO: The phase of Pod pod-secrets-8659545a-ed26-4572-9edf-94ad07b04ed0 is Running (Ready = true)
Mar  1 12:57:03.547: INFO: Pod "pod-secrets-8659545a-ed26-4572-9edf-94ad07b04ed0" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-add71040-b4c8-4204-8045-b7e8c4b0aafb 03/01/23 12:57:03.581
STEP: Updating secret s-test-opt-upd-0e83556a-f35e-4e18-86a9-3c47560379ee 03/01/23 12:57:03.589
STEP: Creating secret with name s-test-opt-create-7790e8df-790d-4ba1-a866-8c81d493e56b 03/01/23 12:57:03.596
STEP: waiting to observe update in volume 03/01/23 12:57:03.602
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Mar  1 12:57:07.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3606" for this suite. 03/01/23 12:57:07.658
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":251,"skipped":4707,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.191 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:57:01.479
    Mar  1 12:57:01.479: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename secrets 03/01/23 12:57:01.48
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:57:01.5
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:57:01.503
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:204
    STEP: Creating secret with name s-test-opt-del-add71040-b4c8-4204-8045-b7e8c4b0aafb 03/01/23 12:57:01.511
    STEP: Creating secret with name s-test-opt-upd-0e83556a-f35e-4e18-86a9-3c47560379ee 03/01/23 12:57:01.518
    STEP: Creating the pod 03/01/23 12:57:01.525
    Mar  1 12:57:01.536: INFO: Waiting up to 5m0s for pod "pod-secrets-8659545a-ed26-4572-9edf-94ad07b04ed0" in namespace "secrets-3606" to be "running and ready"
    Mar  1 12:57:01.542: INFO: Pod "pod-secrets-8659545a-ed26-4572-9edf-94ad07b04ed0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.903652ms
    Mar  1 12:57:01.542: INFO: The phase of Pod pod-secrets-8659545a-ed26-4572-9edf-94ad07b04ed0 is Pending, waiting for it to be Running (with Ready = true)
    Mar  1 12:57:03.547: INFO: Pod "pod-secrets-8659545a-ed26-4572-9edf-94ad07b04ed0": Phase="Running", Reason="", readiness=true. Elapsed: 2.010484696s
    Mar  1 12:57:03.547: INFO: The phase of Pod pod-secrets-8659545a-ed26-4572-9edf-94ad07b04ed0 is Running (Ready = true)
    Mar  1 12:57:03.547: INFO: Pod "pod-secrets-8659545a-ed26-4572-9edf-94ad07b04ed0" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-add71040-b4c8-4204-8045-b7e8c4b0aafb 03/01/23 12:57:03.581
    STEP: Updating secret s-test-opt-upd-0e83556a-f35e-4e18-86a9-3c47560379ee 03/01/23 12:57:03.589
    STEP: Creating secret with name s-test-opt-create-7790e8df-790d-4ba1-a866-8c81d493e56b 03/01/23 12:57:03.596
    STEP: waiting to observe update in volume 03/01/23 12:57:03.602
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Mar  1 12:57:07.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-3606" for this suite. 03/01/23 12:57:07.658
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:57:07.671
Mar  1 12:57:07.671: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename replication-controller 03/01/23 12:57:07.671
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:57:07.693
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:57:07.697
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
STEP: Given a Pod with a 'name' label pod-adoption is created 03/01/23 12:57:07.7
Mar  1 12:57:07.715: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-558" to be "running and ready"
Mar  1 12:57:07.722: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 6.80996ms
Mar  1 12:57:07.722: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Mar  1 12:57:09.729: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.013494447s
Mar  1 12:57:09.729: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Mar  1 12:57:09.729: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 03/01/23 12:57:09.733
STEP: Then the orphan pod is adopted 03/01/23 12:57:09.741
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Mar  1 12:57:10.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-558" for this suite. 03/01/23 12:57:10.761
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","completed":252,"skipped":4714,"failed":0}
------------------------------
â€¢ [3.103 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:57:07.671
    Mar  1 12:57:07.671: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename replication-controller 03/01/23 12:57:07.671
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:57:07.693
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:57:07.697
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:91
    STEP: Given a Pod with a 'name' label pod-adoption is created 03/01/23 12:57:07.7
    Mar  1 12:57:07.715: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-558" to be "running and ready"
    Mar  1 12:57:07.722: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 6.80996ms
    Mar  1 12:57:07.722: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Mar  1 12:57:09.729: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.013494447s
    Mar  1 12:57:09.729: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Mar  1 12:57:09.729: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 03/01/23 12:57:09.733
    STEP: Then the orphan pod is adopted 03/01/23 12:57:09.741
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Mar  1 12:57:10.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-558" for this suite. 03/01/23 12:57:10.761
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:57:10.776
Mar  1 12:57:10.776: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename daemonsets 03/01/23 12:57:10.777
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:57:10.796
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:57:10.799
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
Mar  1 12:57:10.835: INFO: Create a RollingUpdate DaemonSet
Mar  1 12:57:10.841: INFO: Check that daemon pods launch on every node of the cluster
Mar  1 12:57:10.852: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 12:57:10.852: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 12:57:10.852: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 12:57:10.857: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  1 12:57:10.857: INFO: Node lab1-k8s-node-1 is running 0 daemon pod, expected 1
Mar  1 12:57:11.867: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 12:57:11.867: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 12:57:11.867: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 12:57:11.872: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  1 12:57:11.872: INFO: Node lab1-k8s-node-1 is running 0 daemon pod, expected 1
Mar  1 12:57:12.864: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 12:57:12.864: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 12:57:12.864: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 12:57:12.871: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar  1 12:57:12.871: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
Mar  1 12:57:12.871: INFO: Update the DaemonSet to trigger a rollout
Mar  1 12:57:12.885: INFO: Updating DaemonSet daemon-set
Mar  1 12:57:15.912: INFO: Roll back the DaemonSet before rollout is complete
Mar  1 12:57:15.930: INFO: Updating DaemonSet daemon-set
Mar  1 12:57:15.930: INFO: Make sure DaemonSet rollback is complete
Mar  1 12:57:15.939: INFO: Wrong image for pod: daemon-set-9q4wg. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
Mar  1 12:57:15.939: INFO: Pod daemon-set-9q4wg is not available
Mar  1 12:57:15.947: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 12:57:15.948: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 12:57:15.948: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 12:57:16.962: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 12:57:16.962: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 12:57:16.962: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 12:57:17.959: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 12:57:17.959: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 12:57:17.959: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 12:57:18.960: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 12:57:18.960: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 12:57:18.960: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 12:57:19.953: INFO: Pod daemon-set-k92xb is not available
Mar  1 12:57:19.961: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 12:57:19.962: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 12:57:19.962: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 03/01/23 12:57:19.974
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1758, will wait for the garbage collector to delete the pods 03/01/23 12:57:19.974
Mar  1 12:57:20.039: INFO: Deleting DaemonSet.extensions daemon-set took: 10.095566ms
Mar  1 12:57:20.141: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.215137ms
Mar  1 12:57:21.546: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  1 12:57:21.546: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar  1 12:57:21.551: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"38580"},"items":null}

Mar  1 12:57:21.556: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"38580"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Mar  1 12:57:21.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1758" for this suite. 03/01/23 12:57:21.589
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","completed":253,"skipped":4717,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.821 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:57:10.776
    Mar  1 12:57:10.776: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename daemonsets 03/01/23 12:57:10.777
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:57:10.796
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:57:10.799
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:431
    Mar  1 12:57:10.835: INFO: Create a RollingUpdate DaemonSet
    Mar  1 12:57:10.841: INFO: Check that daemon pods launch on every node of the cluster
    Mar  1 12:57:10.852: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 12:57:10.852: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 12:57:10.852: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 12:57:10.857: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  1 12:57:10.857: INFO: Node lab1-k8s-node-1 is running 0 daemon pod, expected 1
    Mar  1 12:57:11.867: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 12:57:11.867: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 12:57:11.867: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 12:57:11.872: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  1 12:57:11.872: INFO: Node lab1-k8s-node-1 is running 0 daemon pod, expected 1
    Mar  1 12:57:12.864: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 12:57:12.864: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 12:57:12.864: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 12:57:12.871: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar  1 12:57:12.871: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    Mar  1 12:57:12.871: INFO: Update the DaemonSet to trigger a rollout
    Mar  1 12:57:12.885: INFO: Updating DaemonSet daemon-set
    Mar  1 12:57:15.912: INFO: Roll back the DaemonSet before rollout is complete
    Mar  1 12:57:15.930: INFO: Updating DaemonSet daemon-set
    Mar  1 12:57:15.930: INFO: Make sure DaemonSet rollback is complete
    Mar  1 12:57:15.939: INFO: Wrong image for pod: daemon-set-9q4wg. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
    Mar  1 12:57:15.939: INFO: Pod daemon-set-9q4wg is not available
    Mar  1 12:57:15.947: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 12:57:15.948: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 12:57:15.948: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 12:57:16.962: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 12:57:16.962: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 12:57:16.962: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 12:57:17.959: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 12:57:17.959: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 12:57:17.959: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 12:57:18.960: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 12:57:18.960: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 12:57:18.960: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 12:57:19.953: INFO: Pod daemon-set-k92xb is not available
    Mar  1 12:57:19.961: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 12:57:19.962: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 12:57:19.962: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 03/01/23 12:57:19.974
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1758, will wait for the garbage collector to delete the pods 03/01/23 12:57:19.974
    Mar  1 12:57:20.039: INFO: Deleting DaemonSet.extensions daemon-set took: 10.095566ms
    Mar  1 12:57:20.141: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.215137ms
    Mar  1 12:57:21.546: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  1 12:57:21.546: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Mar  1 12:57:21.551: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"38580"},"items":null}

    Mar  1 12:57:21.556: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"38580"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Mar  1 12:57:21.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-1758" for this suite. 03/01/23 12:57:21.589
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:57:21.599
Mar  1 12:57:21.599: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename statefulset 03/01/23 12:57:21.599
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:57:21.622
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:57:21.625
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-2055 03/01/23 12:57:21.629
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
STEP: Creating stateful set ss in namespace statefulset-2055 03/01/23 12:57:21.636
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2055 03/01/23 12:57:21.646
Mar  1 12:57:21.654: INFO: Found 0 stateful pods, waiting for 1
Mar  1 12:57:31.660: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 03/01/23 12:57:31.66
Mar  1 12:57:31.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=statefulset-2055 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  1 12:57:31.801: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  1 12:57:31.801: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  1 12:57:31.801: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  1 12:57:31.806: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar  1 12:57:41.812: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  1 12:57:41.812: INFO: Waiting for statefulset status.replicas updated to 0
Mar  1 12:57:41.840: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
Mar  1 12:57:41.840: INFO: ss-0  lab1-k8s-node-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-01 12:57:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-01 12:57:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-01 12:57:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-01 12:57:21 +0000 UTC  }]
Mar  1 12:57:41.840: INFO: 
Mar  1 12:57:41.840: INFO: StatefulSet ss has not reached scale 3, at 1
Mar  1 12:57:42.847: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.989458297s
Mar  1 12:57:43.853: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.98277029s
Mar  1 12:57:44.860: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.975846696s
Mar  1 12:57:45.864: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.969428128s
Mar  1 12:57:46.871: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.964790155s
Mar  1 12:57:47.879: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.957407249s
Mar  1 12:57:48.884: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.950440891s
Mar  1 12:57:49.891: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.944953629s
Mar  1 12:57:50.898: INFO: Verifying statefulset ss doesn't scale past 3 for another 938.005546ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2055 03/01/23 12:57:51.899
Mar  1 12:57:51.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=statefulset-2055 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 12:57:52.047: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  1 12:57:52.047: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  1 12:57:52.047: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  1 12:57:52.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=statefulset-2055 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 12:57:52.207: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar  1 12:57:52.207: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  1 12:57:52.207: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  1 12:57:52.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=statefulset-2055 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 12:57:52.344: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar  1 12:57:52.344: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  1 12:57:52.344: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  1 12:57:52.351: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Mar  1 12:58:02.359: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  1 12:58:02.359: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  1 12:58:02.359: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 03/01/23 12:58:02.359
Mar  1 12:58:02.363: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=statefulset-2055 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  1 12:58:02.507: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  1 12:58:02.507: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  1 12:58:02.507: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  1 12:58:02.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=statefulset-2055 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  1 12:58:02.643: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  1 12:58:02.643: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  1 12:58:02.643: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  1 12:58:02.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=statefulset-2055 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  1 12:58:02.798: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  1 12:58:02.798: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  1 12:58:02.798: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  1 12:58:02.798: INFO: Waiting for statefulset status.replicas updated to 0
Mar  1 12:58:02.802: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Mar  1 12:58:12.812: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  1 12:58:12.812: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar  1 12:58:12.812: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar  1 12:58:12.829: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
Mar  1 12:58:12.829: INFO: ss-0  lab1-k8s-node-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-01 12:57:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-01 12:58:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-01 12:58:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-01 12:57:21 +0000 UTC  }]
Mar  1 12:58:12.829: INFO: ss-1  lab1-k8s-node-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-01 12:57:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-01 12:58:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-01 12:58:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-01 12:57:41 +0000 UTC  }]
Mar  1 12:58:12.829: INFO: ss-2  lab1-k8s-node-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-01 12:57:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-01 12:58:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-01 12:58:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-01 12:57:41 +0000 UTC  }]
Mar  1 12:58:12.830: INFO: 
Mar  1 12:58:12.830: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  1 12:58:13.836: INFO: Verifying statefulset ss doesn't scale past 0 for another 8.994490879s
Mar  1 12:58:14.839: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.988403704s
Mar  1 12:58:15.845: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.98447623s
Mar  1 12:58:16.852: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.978631392s
Mar  1 12:58:17.856: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.972237152s
Mar  1 12:58:18.861: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.966877552s
Mar  1 12:58:19.868: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.962449172s
Mar  1 12:58:20.874: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.955655354s
Mar  1 12:58:21.879: INFO: Verifying statefulset ss doesn't scale past 0 for another 949.812547ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2055 03/01/23 12:58:22.879
Mar  1 12:58:22.885: INFO: Scaling statefulset ss to 0
Mar  1 12:58:22.900: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Mar  1 12:58:22.906: INFO: Deleting all statefulset in ns statefulset-2055
Mar  1 12:58:22.911: INFO: Scaling statefulset ss to 0
Mar  1 12:58:22.926: INFO: Waiting for statefulset status.replicas updated to 0
Mar  1 12:58:22.934: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Mar  1 12:58:22.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2055" for this suite. 03/01/23 12:58:22.957
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","completed":254,"skipped":4717,"failed":0}
------------------------------
â€¢ [SLOW TEST] [61.372 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:695

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:57:21.599
    Mar  1 12:57:21.599: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename statefulset 03/01/23 12:57:21.599
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:57:21.622
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:57:21.625
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-2055 03/01/23 12:57:21.629
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:695
    STEP: Creating stateful set ss in namespace statefulset-2055 03/01/23 12:57:21.636
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2055 03/01/23 12:57:21.646
    Mar  1 12:57:21.654: INFO: Found 0 stateful pods, waiting for 1
    Mar  1 12:57:31.660: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 03/01/23 12:57:31.66
    Mar  1 12:57:31.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=statefulset-2055 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar  1 12:57:31.801: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar  1 12:57:31.801: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar  1 12:57:31.801: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar  1 12:57:31.806: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Mar  1 12:57:41.812: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Mar  1 12:57:41.812: INFO: Waiting for statefulset status.replicas updated to 0
    Mar  1 12:57:41.840: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
    Mar  1 12:57:41.840: INFO: ss-0  lab1-k8s-node-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-01 12:57:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-01 12:57:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-01 12:57:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-01 12:57:21 +0000 UTC  }]
    Mar  1 12:57:41.840: INFO: 
    Mar  1 12:57:41.840: INFO: StatefulSet ss has not reached scale 3, at 1
    Mar  1 12:57:42.847: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.989458297s
    Mar  1 12:57:43.853: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.98277029s
    Mar  1 12:57:44.860: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.975846696s
    Mar  1 12:57:45.864: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.969428128s
    Mar  1 12:57:46.871: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.964790155s
    Mar  1 12:57:47.879: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.957407249s
    Mar  1 12:57:48.884: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.950440891s
    Mar  1 12:57:49.891: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.944953629s
    Mar  1 12:57:50.898: INFO: Verifying statefulset ss doesn't scale past 3 for another 938.005546ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2055 03/01/23 12:57:51.899
    Mar  1 12:57:51.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=statefulset-2055 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar  1 12:57:52.047: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar  1 12:57:52.047: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar  1 12:57:52.047: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar  1 12:57:52.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=statefulset-2055 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar  1 12:57:52.207: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Mar  1 12:57:52.207: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar  1 12:57:52.207: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar  1 12:57:52.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=statefulset-2055 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar  1 12:57:52.344: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Mar  1 12:57:52.344: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar  1 12:57:52.344: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar  1 12:57:52.351: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
    Mar  1 12:58:02.359: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Mar  1 12:58:02.359: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Mar  1 12:58:02.359: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 03/01/23 12:58:02.359
    Mar  1 12:58:02.363: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=statefulset-2055 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar  1 12:58:02.507: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar  1 12:58:02.507: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar  1 12:58:02.507: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar  1 12:58:02.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=statefulset-2055 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar  1 12:58:02.643: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar  1 12:58:02.643: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar  1 12:58:02.643: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar  1 12:58:02.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=statefulset-2055 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar  1 12:58:02.798: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar  1 12:58:02.798: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar  1 12:58:02.798: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar  1 12:58:02.798: INFO: Waiting for statefulset status.replicas updated to 0
    Mar  1 12:58:02.802: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Mar  1 12:58:12.812: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Mar  1 12:58:12.812: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Mar  1 12:58:12.812: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Mar  1 12:58:12.829: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
    Mar  1 12:58:12.829: INFO: ss-0  lab1-k8s-node-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-01 12:57:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-01 12:58:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-01 12:58:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-01 12:57:21 +0000 UTC  }]
    Mar  1 12:58:12.829: INFO: ss-1  lab1-k8s-node-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-01 12:57:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-01 12:58:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-01 12:58:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-01 12:57:41 +0000 UTC  }]
    Mar  1 12:58:12.829: INFO: ss-2  lab1-k8s-node-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-01 12:57:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-01 12:58:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-01 12:58:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-01 12:57:41 +0000 UTC  }]
    Mar  1 12:58:12.830: INFO: 
    Mar  1 12:58:12.830: INFO: StatefulSet ss has not reached scale 0, at 3
    Mar  1 12:58:13.836: INFO: Verifying statefulset ss doesn't scale past 0 for another 8.994490879s
    Mar  1 12:58:14.839: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.988403704s
    Mar  1 12:58:15.845: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.98447623s
    Mar  1 12:58:16.852: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.978631392s
    Mar  1 12:58:17.856: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.972237152s
    Mar  1 12:58:18.861: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.966877552s
    Mar  1 12:58:19.868: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.962449172s
    Mar  1 12:58:20.874: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.955655354s
    Mar  1 12:58:21.879: INFO: Verifying statefulset ss doesn't scale past 0 for another 949.812547ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2055 03/01/23 12:58:22.879
    Mar  1 12:58:22.885: INFO: Scaling statefulset ss to 0
    Mar  1 12:58:22.900: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Mar  1 12:58:22.906: INFO: Deleting all statefulset in ns statefulset-2055
    Mar  1 12:58:22.911: INFO: Scaling statefulset ss to 0
    Mar  1 12:58:22.926: INFO: Waiting for statefulset status.replicas updated to 0
    Mar  1 12:58:22.934: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Mar  1 12:58:22.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-2055" for this suite. 03/01/23 12:58:22.957
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:58:22.973
Mar  1 12:58:22.973: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename projected 03/01/23 12:58:22.974
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:58:22.993
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:58:22.997
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
STEP: Creating projection with configMap that has name projected-configmap-test-upd-fe91d42a-f3a3-458c-a8d8-1b9df0424cd7 03/01/23 12:58:23.008
STEP: Creating the pod 03/01/23 12:58:23.015
Mar  1 12:58:23.025: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-fa9135af-9fbb-438b-b6ea-987a3d556420" in namespace "projected-8641" to be "running and ready"
Mar  1 12:58:23.030: INFO: Pod "pod-projected-configmaps-fa9135af-9fbb-438b-b6ea-987a3d556420": Phase="Pending", Reason="", readiness=false. Elapsed: 4.098058ms
Mar  1 12:58:23.030: INFO: The phase of Pod pod-projected-configmaps-fa9135af-9fbb-438b-b6ea-987a3d556420 is Pending, waiting for it to be Running (with Ready = true)
Mar  1 12:58:25.035: INFO: Pod "pod-projected-configmaps-fa9135af-9fbb-438b-b6ea-987a3d556420": Phase="Running", Reason="", readiness=true. Elapsed: 2.009454972s
Mar  1 12:58:25.035: INFO: The phase of Pod pod-projected-configmaps-fa9135af-9fbb-438b-b6ea-987a3d556420 is Running (Ready = true)
Mar  1 12:58:25.035: INFO: Pod "pod-projected-configmaps-fa9135af-9fbb-438b-b6ea-987a3d556420" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-fe91d42a-f3a3-458c-a8d8-1b9df0424cd7 03/01/23 12:58:25.052
STEP: waiting to observe update in volume 03/01/23 12:58:25.065
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Mar  1 12:58:27.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8641" for this suite. 03/01/23 12:58:27.093
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":255,"skipped":4737,"failed":0}
------------------------------
â€¢ [4.131 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:58:22.973
    Mar  1 12:58:22.973: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename projected 03/01/23 12:58:22.974
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:58:22.993
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:58:22.997
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:123
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-fe91d42a-f3a3-458c-a8d8-1b9df0424cd7 03/01/23 12:58:23.008
    STEP: Creating the pod 03/01/23 12:58:23.015
    Mar  1 12:58:23.025: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-fa9135af-9fbb-438b-b6ea-987a3d556420" in namespace "projected-8641" to be "running and ready"
    Mar  1 12:58:23.030: INFO: Pod "pod-projected-configmaps-fa9135af-9fbb-438b-b6ea-987a3d556420": Phase="Pending", Reason="", readiness=false. Elapsed: 4.098058ms
    Mar  1 12:58:23.030: INFO: The phase of Pod pod-projected-configmaps-fa9135af-9fbb-438b-b6ea-987a3d556420 is Pending, waiting for it to be Running (with Ready = true)
    Mar  1 12:58:25.035: INFO: Pod "pod-projected-configmaps-fa9135af-9fbb-438b-b6ea-987a3d556420": Phase="Running", Reason="", readiness=true. Elapsed: 2.009454972s
    Mar  1 12:58:25.035: INFO: The phase of Pod pod-projected-configmaps-fa9135af-9fbb-438b-b6ea-987a3d556420 is Running (Ready = true)
    Mar  1 12:58:25.035: INFO: Pod "pod-projected-configmaps-fa9135af-9fbb-438b-b6ea-987a3d556420" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-fe91d42a-f3a3-458c-a8d8-1b9df0424cd7 03/01/23 12:58:25.052
    STEP: waiting to observe update in volume 03/01/23 12:58:25.065
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Mar  1 12:58:27.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8641" for this suite. 03/01/23 12:58:27.093
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:58:27.106
Mar  1 12:58:27.106: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename configmap 03/01/23 12:58:27.107
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:58:27.13
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:58:27.134
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
STEP: Creating configMap with name configmap-test-volume-map-241569ca-3fb0-40cd-81ed-cee50b301de3 03/01/23 12:58:27.138
STEP: Creating a pod to test consume configMaps 03/01/23 12:58:27.145
Mar  1 12:58:27.158: INFO: Waiting up to 5m0s for pod "pod-configmaps-a7e46d4b-17ba-43be-b1e5-f95e655ea819" in namespace "configmap-1154" to be "Succeeded or Failed"
Mar  1 12:58:27.161: INFO: Pod "pod-configmaps-a7e46d4b-17ba-43be-b1e5-f95e655ea819": Phase="Pending", Reason="", readiness=false. Elapsed: 3.813588ms
Mar  1 12:58:29.168: INFO: Pod "pod-configmaps-a7e46d4b-17ba-43be-b1e5-f95e655ea819": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010107452s
Mar  1 12:58:31.166: INFO: Pod "pod-configmaps-a7e46d4b-17ba-43be-b1e5-f95e655ea819": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008381145s
STEP: Saw pod success 03/01/23 12:58:31.166
Mar  1 12:58:31.166: INFO: Pod "pod-configmaps-a7e46d4b-17ba-43be-b1e5-f95e655ea819" satisfied condition "Succeeded or Failed"
Mar  1 12:58:31.172: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-configmaps-a7e46d4b-17ba-43be-b1e5-f95e655ea819 container agnhost-container: <nil>
STEP: delete the pod 03/01/23 12:58:31.183
Mar  1 12:58:31.200: INFO: Waiting for pod pod-configmaps-a7e46d4b-17ba-43be-b1e5-f95e655ea819 to disappear
Mar  1 12:58:31.205: INFO: Pod pod-configmaps-a7e46d4b-17ba-43be-b1e5-f95e655ea819 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Mar  1 12:58:31.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1154" for this suite. 03/01/23 12:58:31.211
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":256,"skipped":4739,"failed":0}
------------------------------
â€¢ [4.116 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:58:27.106
    Mar  1 12:58:27.106: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename configmap 03/01/23 12:58:27.107
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:58:27.13
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:58:27.134
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:108
    STEP: Creating configMap with name configmap-test-volume-map-241569ca-3fb0-40cd-81ed-cee50b301de3 03/01/23 12:58:27.138
    STEP: Creating a pod to test consume configMaps 03/01/23 12:58:27.145
    Mar  1 12:58:27.158: INFO: Waiting up to 5m0s for pod "pod-configmaps-a7e46d4b-17ba-43be-b1e5-f95e655ea819" in namespace "configmap-1154" to be "Succeeded or Failed"
    Mar  1 12:58:27.161: INFO: Pod "pod-configmaps-a7e46d4b-17ba-43be-b1e5-f95e655ea819": Phase="Pending", Reason="", readiness=false. Elapsed: 3.813588ms
    Mar  1 12:58:29.168: INFO: Pod "pod-configmaps-a7e46d4b-17ba-43be-b1e5-f95e655ea819": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010107452s
    Mar  1 12:58:31.166: INFO: Pod "pod-configmaps-a7e46d4b-17ba-43be-b1e5-f95e655ea819": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008381145s
    STEP: Saw pod success 03/01/23 12:58:31.166
    Mar  1 12:58:31.166: INFO: Pod "pod-configmaps-a7e46d4b-17ba-43be-b1e5-f95e655ea819" satisfied condition "Succeeded or Failed"
    Mar  1 12:58:31.172: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-configmaps-a7e46d4b-17ba-43be-b1e5-f95e655ea819 container agnhost-container: <nil>
    STEP: delete the pod 03/01/23 12:58:31.183
    Mar  1 12:58:31.200: INFO: Waiting for pod pod-configmaps-a7e46d4b-17ba-43be-b1e5-f95e655ea819 to disappear
    Mar  1 12:58:31.205: INFO: Pod pod-configmaps-a7e46d4b-17ba-43be-b1e5-f95e655ea819 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Mar  1 12:58:31.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-1154" for this suite. 03/01/23 12:58:31.211
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:58:31.222
Mar  1 12:58:31.222: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename security-context-test 03/01/23 12:58:31.223
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:58:31.24
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:58:31.243
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
Mar  1 12:58:31.262: INFO: Waiting up to 5m0s for pod "busybox-user-65534-6ba95bbc-e213-495b-ae09-7569a174c10e" in namespace "security-context-test-3263" to be "Succeeded or Failed"
Mar  1 12:58:31.268: INFO: Pod "busybox-user-65534-6ba95bbc-e213-495b-ae09-7569a174c10e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.59988ms
Mar  1 12:58:33.274: INFO: Pod "busybox-user-65534-6ba95bbc-e213-495b-ae09-7569a174c10e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012207181s
Mar  1 12:58:35.275: INFO: Pod "busybox-user-65534-6ba95bbc-e213-495b-ae09-7569a174c10e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013511451s
Mar  1 12:58:35.276: INFO: Pod "busybox-user-65534-6ba95bbc-e213-495b-ae09-7569a174c10e" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Mar  1 12:58:35.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3263" for this suite. 03/01/23 12:58:35.282
{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","completed":257,"skipped":4747,"failed":0}
------------------------------
â€¢ [4.070 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:308
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:58:31.222
    Mar  1 12:58:31.222: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename security-context-test 03/01/23 12:58:31.223
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:58:31.24
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:58:31.243
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:346
    Mar  1 12:58:31.262: INFO: Waiting up to 5m0s for pod "busybox-user-65534-6ba95bbc-e213-495b-ae09-7569a174c10e" in namespace "security-context-test-3263" to be "Succeeded or Failed"
    Mar  1 12:58:31.268: INFO: Pod "busybox-user-65534-6ba95bbc-e213-495b-ae09-7569a174c10e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.59988ms
    Mar  1 12:58:33.274: INFO: Pod "busybox-user-65534-6ba95bbc-e213-495b-ae09-7569a174c10e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012207181s
    Mar  1 12:58:35.275: INFO: Pod "busybox-user-65534-6ba95bbc-e213-495b-ae09-7569a174c10e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013511451s
    Mar  1 12:58:35.276: INFO: Pod "busybox-user-65534-6ba95bbc-e213-495b-ae09-7569a174c10e" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Mar  1 12:58:35.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-3263" for this suite. 03/01/23 12:58:35.282
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:58:35.293
Mar  1 12:58:35.294: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename crd-webhook 03/01/23 12:58:35.294
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:58:35.315
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:58:35.318
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 03/01/23 12:58:35.322
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 03/01/23 12:58:35.665
STEP: Deploying the custom resource conversion webhook pod 03/01/23 12:58:35.693
STEP: Wait for the deployment to be ready 03/01/23 12:58:35.71
Mar  1 12:58:35.724: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/01/23 12:58:37.739
STEP: Verifying the service has paired with the endpoint 03/01/23 12:58:37.76
Mar  1 12:58:38.761: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Mar  1 12:58:38.767: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Creating a v1 custom resource 03/01/23 12:58:46.368
STEP: Create a v2 custom resource 03/01/23 12:58:46.397
STEP: List CRs in v1 03/01/23 12:58:46.471
STEP: List CRs in v2 03/01/23 12:58:46.479
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  1 12:58:47.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-979" for this suite. 03/01/23 12:58:47.02
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","completed":258,"skipped":4747,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.802 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:58:35.293
    Mar  1 12:58:35.294: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename crd-webhook 03/01/23 12:58:35.294
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:58:35.315
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:58:35.318
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 03/01/23 12:58:35.322
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 03/01/23 12:58:35.665
    STEP: Deploying the custom resource conversion webhook pod 03/01/23 12:58:35.693
    STEP: Wait for the deployment to be ready 03/01/23 12:58:35.71
    Mar  1 12:58:35.724: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/01/23 12:58:37.739
    STEP: Verifying the service has paired with the endpoint 03/01/23 12:58:37.76
    Mar  1 12:58:38.761: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Mar  1 12:58:38.767: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Creating a v1 custom resource 03/01/23 12:58:46.368
    STEP: Create a v2 custom resource 03/01/23 12:58:46.397
    STEP: List CRs in v1 03/01/23 12:58:46.471
    STEP: List CRs in v2 03/01/23 12:58:46.479
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  1 12:58:47.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-979" for this suite. 03/01/23 12:58:47.02
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:58:47.105
Mar  1 12:58:47.105: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename webhook 03/01/23 12:58:47.106
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:58:47.134
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:58:47.138
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/01/23 12:58:47.157
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/01/23 12:58:47.422
STEP: Deploying the webhook pod 03/01/23 12:58:47.428
STEP: Wait for the deployment to be ready 03/01/23 12:58:47.447
Mar  1 12:58:47.464: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/01/23 12:58:49.48
STEP: Verifying the service has paired with the endpoint 03/01/23 12:58:49.497
Mar  1 12:58:50.497: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
Mar  1 12:58:50.504: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4091-crds.webhook.example.com via the AdmissionRegistration API 03/01/23 12:58:56.02
STEP: Creating a custom resource that should be mutated by the webhook 03/01/23 12:58:56.04
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  1 12:58:58.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-329" for this suite. 03/01/23 12:58:58.631
STEP: Destroying namespace "webhook-329-markers" for this suite. 03/01/23 12:58:58.639
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","completed":259,"skipped":4786,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.601 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:58:47.105
    Mar  1 12:58:47.105: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename webhook 03/01/23 12:58:47.106
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:58:47.134
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:58:47.138
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/01/23 12:58:47.157
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/01/23 12:58:47.422
    STEP: Deploying the webhook pod 03/01/23 12:58:47.428
    STEP: Wait for the deployment to be ready 03/01/23 12:58:47.447
    Mar  1 12:58:47.464: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/01/23 12:58:49.48
    STEP: Verifying the service has paired with the endpoint 03/01/23 12:58:49.497
    Mar  1 12:58:50.497: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:290
    Mar  1 12:58:50.504: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4091-crds.webhook.example.com via the AdmissionRegistration API 03/01/23 12:58:56.02
    STEP: Creating a custom resource that should be mutated by the webhook 03/01/23 12:58:56.04
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  1 12:58:58.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-329" for this suite. 03/01/23 12:58:58.631
    STEP: Destroying namespace "webhook-329-markers" for this suite. 03/01/23 12:58:58.639
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:58:58.708
Mar  1 12:58:58.708: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename secrets 03/01/23 12:58:58.708
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:58:58.765
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:58:58.776
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
STEP: Creating secret with name secret-test-map-49e865d1-bfcb-483c-a93b-0de66cd1a394 03/01/23 12:58:58.781
STEP: Creating a pod to test consume secrets 03/01/23 12:58:58.795
Mar  1 12:58:58.819: INFO: Waiting up to 5m0s for pod "pod-secrets-8eab0646-df05-4984-8f35-9958ab54d7c8" in namespace "secrets-78" to be "Succeeded or Failed"
Mar  1 12:58:58.838: INFO: Pod "pod-secrets-8eab0646-df05-4984-8f35-9958ab54d7c8": Phase="Pending", Reason="", readiness=false. Elapsed: 18.212699ms
Mar  1 12:59:00.844: INFO: Pod "pod-secrets-8eab0646-df05-4984-8f35-9958ab54d7c8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025041896s
Mar  1 12:59:02.844: INFO: Pod "pod-secrets-8eab0646-df05-4984-8f35-9958ab54d7c8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024884028s
STEP: Saw pod success 03/01/23 12:59:02.844
Mar  1 12:59:02.845: INFO: Pod "pod-secrets-8eab0646-df05-4984-8f35-9958ab54d7c8" satisfied condition "Succeeded or Failed"
Mar  1 12:59:02.848: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-secrets-8eab0646-df05-4984-8f35-9958ab54d7c8 container secret-volume-test: <nil>
STEP: delete the pod 03/01/23 12:59:02.859
Mar  1 12:59:02.877: INFO: Waiting for pod pod-secrets-8eab0646-df05-4984-8f35-9958ab54d7c8 to disappear
Mar  1 12:59:02.881: INFO: Pod pod-secrets-8eab0646-df05-4984-8f35-9958ab54d7c8 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Mar  1 12:59:02.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-78" for this suite. 03/01/23 12:59:02.887
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":260,"skipped":4790,"failed":0}
------------------------------
â€¢ [4.190 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:58:58.708
    Mar  1 12:58:58.708: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename secrets 03/01/23 12:58:58.708
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:58:58.765
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:58:58.776
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:78
    STEP: Creating secret with name secret-test-map-49e865d1-bfcb-483c-a93b-0de66cd1a394 03/01/23 12:58:58.781
    STEP: Creating a pod to test consume secrets 03/01/23 12:58:58.795
    Mar  1 12:58:58.819: INFO: Waiting up to 5m0s for pod "pod-secrets-8eab0646-df05-4984-8f35-9958ab54d7c8" in namespace "secrets-78" to be "Succeeded or Failed"
    Mar  1 12:58:58.838: INFO: Pod "pod-secrets-8eab0646-df05-4984-8f35-9958ab54d7c8": Phase="Pending", Reason="", readiness=false. Elapsed: 18.212699ms
    Mar  1 12:59:00.844: INFO: Pod "pod-secrets-8eab0646-df05-4984-8f35-9958ab54d7c8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025041896s
    Mar  1 12:59:02.844: INFO: Pod "pod-secrets-8eab0646-df05-4984-8f35-9958ab54d7c8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024884028s
    STEP: Saw pod success 03/01/23 12:59:02.844
    Mar  1 12:59:02.845: INFO: Pod "pod-secrets-8eab0646-df05-4984-8f35-9958ab54d7c8" satisfied condition "Succeeded or Failed"
    Mar  1 12:59:02.848: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-secrets-8eab0646-df05-4984-8f35-9958ab54d7c8 container secret-volume-test: <nil>
    STEP: delete the pod 03/01/23 12:59:02.859
    Mar  1 12:59:02.877: INFO: Waiting for pod pod-secrets-8eab0646-df05-4984-8f35-9958ab54d7c8 to disappear
    Mar  1 12:59:02.881: INFO: Pod pod-secrets-8eab0646-df05-4984-8f35-9958ab54d7c8 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Mar  1 12:59:02.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-78" for this suite. 03/01/23 12:59:02.887
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 12:59:02.904
Mar  1 12:59:02.904: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename cronjob 03/01/23 12:59:02.904
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:59:02.924
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:59:02.928
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 03/01/23 12:59:02.931
STEP: Ensuring no jobs are scheduled 03/01/23 12:59:02.939
STEP: Ensuring no job exists by listing jobs explicitly 03/01/23 13:04:02.949
STEP: Removing cronjob 03/01/23 13:04:02.955
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Mar  1 13:04:02.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-6235" for this suite. 03/01/23 13:04:02.973
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","completed":261,"skipped":4796,"failed":0}
------------------------------
â€¢ [SLOW TEST] [300.085 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 12:59:02.904
    Mar  1 12:59:02.904: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename cronjob 03/01/23 12:59:02.904
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 12:59:02.924
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 12:59:02.928
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 03/01/23 12:59:02.931
    STEP: Ensuring no jobs are scheduled 03/01/23 12:59:02.939
    STEP: Ensuring no job exists by listing jobs explicitly 03/01/23 13:04:02.949
    STEP: Removing cronjob 03/01/23 13:04:02.955
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Mar  1 13:04:02.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-6235" for this suite. 03/01/23 13:04:02.973
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:04:02.993
Mar  1 13:04:02.994: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename ingress 03/01/23 13:04:02.994
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:04:03.015
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:04:03.019
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 03/01/23 13:04:03.022
STEP: getting /apis/networking.k8s.io 03/01/23 13:04:03.025
STEP: getting /apis/networking.k8s.iov1 03/01/23 13:04:03.026
STEP: creating 03/01/23 13:04:03.027
STEP: getting 03/01/23 13:04:03.054
STEP: listing 03/01/23 13:04:03.06
STEP: watching 03/01/23 13:04:03.065
Mar  1 13:04:03.065: INFO: starting watch
STEP: cluster-wide listing 03/01/23 13:04:03.067
STEP: cluster-wide watching 03/01/23 13:04:03.071
Mar  1 13:04:03.071: INFO: starting watch
STEP: patching 03/01/23 13:04:03.072
STEP: updating 03/01/23 13:04:03.08
Mar  1 13:04:03.090: INFO: waiting for watch events with expected annotations
Mar  1 13:04:03.091: INFO: saw patched and updated annotations
STEP: patching /status 03/01/23 13:04:03.091
STEP: updating /status 03/01/23 13:04:03.099
STEP: get /status 03/01/23 13:04:03.109
STEP: deleting 03/01/23 13:04:03.114
STEP: deleting a collection 03/01/23 13:04:03.131
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:187
Mar  1 13:04:03.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-423" for this suite. 03/01/23 13:04:03.16
{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","completed":262,"skipped":4820,"failed":0}
------------------------------
â€¢ [0.178 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:04:02.993
    Mar  1 13:04:02.994: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename ingress 03/01/23 13:04:02.994
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:04:03.015
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:04:03.019
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 03/01/23 13:04:03.022
    STEP: getting /apis/networking.k8s.io 03/01/23 13:04:03.025
    STEP: getting /apis/networking.k8s.iov1 03/01/23 13:04:03.026
    STEP: creating 03/01/23 13:04:03.027
    STEP: getting 03/01/23 13:04:03.054
    STEP: listing 03/01/23 13:04:03.06
    STEP: watching 03/01/23 13:04:03.065
    Mar  1 13:04:03.065: INFO: starting watch
    STEP: cluster-wide listing 03/01/23 13:04:03.067
    STEP: cluster-wide watching 03/01/23 13:04:03.071
    Mar  1 13:04:03.071: INFO: starting watch
    STEP: patching 03/01/23 13:04:03.072
    STEP: updating 03/01/23 13:04:03.08
    Mar  1 13:04:03.090: INFO: waiting for watch events with expected annotations
    Mar  1 13:04:03.091: INFO: saw patched and updated annotations
    STEP: patching /status 03/01/23 13:04:03.091
    STEP: updating /status 03/01/23 13:04:03.099
    STEP: get /status 03/01/23 13:04:03.109
    STEP: deleting 03/01/23 13:04:03.114
    STEP: deleting a collection 03/01/23 13:04:03.131
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:187
    Mar  1 13:04:03.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingress-423" for this suite. 03/01/23 13:04:03.16
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:309
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:04:03.174
Mar  1 13:04:03.174: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename job 03/01/23 13:04:03.175
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:04:03.194
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:04:03.198
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:309
STEP: Creating a job 03/01/23 13:04:03.202
STEP: Ensuring active pods == parallelism 03/01/23 13:04:03.209
STEP: delete a job 03/01/23 13:04:05.217
STEP: deleting Job.batch foo in namespace job-9445, will wait for the garbage collector to delete the pods 03/01/23 13:04:05.22
Mar  1 13:04:05.297: INFO: Deleting Job.batch foo took: 21.421145ms
Mar  1 13:04:05.398: INFO: Terminating Job.batch foo pods took: 101.001785ms
STEP: Ensuring job was deleted 03/01/23 13:04:38.299
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Mar  1 13:04:38.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-9445" for this suite. 03/01/23 13:04:38.315
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","completed":263,"skipped":4836,"failed":0}
------------------------------
â€¢ [SLOW TEST] [35.151 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:04:03.174
    Mar  1 13:04:03.174: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename job 03/01/23 13:04:03.175
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:04:03.194
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:04:03.198
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:309
    STEP: Creating a job 03/01/23 13:04:03.202
    STEP: Ensuring active pods == parallelism 03/01/23 13:04:03.209
    STEP: delete a job 03/01/23 13:04:05.217
    STEP: deleting Job.batch foo in namespace job-9445, will wait for the garbage collector to delete the pods 03/01/23 13:04:05.22
    Mar  1 13:04:05.297: INFO: Deleting Job.batch foo took: 21.421145ms
    Mar  1 13:04:05.398: INFO: Terminating Job.batch foo pods took: 101.001785ms
    STEP: Ensuring job was deleted 03/01/23 13:04:38.299
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Mar  1 13:04:38.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-9445" for this suite. 03/01/23 13:04:38.315
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:04:38.326
Mar  1 13:04:38.326: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename pod-network-test 03/01/23 13:04:38.327
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:04:38.346
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:04:38.349
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-9603 03/01/23 13:04:38.355
STEP: creating a selector 03/01/23 13:04:38.355
STEP: Creating the service pods in kubernetes 03/01/23 13:04:38.355
Mar  1 13:04:38.355: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar  1 13:04:38.400: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9603" to be "running and ready"
Mar  1 13:04:38.407: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.570724ms
Mar  1 13:04:38.407: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar  1 13:04:40.413: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.013716691s
Mar  1 13:04:40.413: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  1 13:04:42.414: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.013932461s
Mar  1 13:04:42.414: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  1 13:04:44.413: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.012796399s
Mar  1 13:04:44.413: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  1 13:04:46.413: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.012774943s
Mar  1 13:04:46.413: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  1 13:04:48.414: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.01421138s
Mar  1 13:04:48.414: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  1 13:04:50.412: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.012652644s
Mar  1 13:04:50.413: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  1 13:04:52.414: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.014488962s
Mar  1 13:04:52.414: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  1 13:04:54.415: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.015174576s
Mar  1 13:04:54.415: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  1 13:04:56.412: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.012464299s
Mar  1 13:04:56.412: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  1 13:04:58.413: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.013727728s
Mar  1 13:04:58.414: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  1 13:05:00.414: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.01403466s
Mar  1 13:05:00.414: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Mar  1 13:05:00.414: INFO: Pod "netserver-0" satisfied condition "running and ready"
Mar  1 13:05:00.418: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9603" to be "running and ready"
Mar  1 13:05:00.423: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 5.083044ms
Mar  1 13:05:00.423: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Mar  1 13:05:00.423: INFO: Pod "netserver-1" satisfied condition "running and ready"
Mar  1 13:05:00.429: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-9603" to be "running and ready"
Mar  1 13:05:00.433: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.843473ms
Mar  1 13:05:00.433: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Mar  1 13:05:00.433: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 03/01/23 13:05:00.438
Mar  1 13:05:00.445: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9603" to be "running"
Mar  1 13:05:00.449: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.604358ms
Mar  1 13:05:02.455: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009653645s
Mar  1 13:05:02.455: INFO: Pod "test-container-pod" satisfied condition "running"
Mar  1 13:05:02.460: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Mar  1 13:05:02.461: INFO: Breadth first check of 10.233.95.173 on host 10.128.0.178...
Mar  1 13:05:02.466: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.74.72:9080/dial?request=hostname&protocol=udp&host=10.233.95.173&port=8081&tries=1'] Namespace:pod-network-test-9603 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  1 13:05:02.466: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
Mar  1 13:05:02.467: INFO: ExecWithOptions: Clientset creation
Mar  1 13:05:02.467: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-9603/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.74.72%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.95.173%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar  1 13:05:02.542: INFO: Waiting for responses: map[]
Mar  1 13:05:02.542: INFO: reached 10.233.95.173 after 0/1 tries
Mar  1 13:05:02.542: INFO: Breadth first check of 10.233.64.129 on host 10.128.0.76...
Mar  1 13:05:02.547: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.74.72:9080/dial?request=hostname&protocol=udp&host=10.233.64.129&port=8081&tries=1'] Namespace:pod-network-test-9603 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  1 13:05:02.547: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
Mar  1 13:05:02.547: INFO: ExecWithOptions: Clientset creation
Mar  1 13:05:02.548: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-9603/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.74.72%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.64.129%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar  1 13:05:02.618: INFO: Waiting for responses: map[]
Mar  1 13:05:02.618: INFO: reached 10.233.64.129 after 0/1 tries
Mar  1 13:05:02.618: INFO: Breadth first check of 10.233.74.73 on host 10.128.2.241...
Mar  1 13:05:02.624: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.74.72:9080/dial?request=hostname&protocol=udp&host=10.233.74.73&port=8081&tries=1'] Namespace:pod-network-test-9603 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  1 13:05:02.624: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
Mar  1 13:05:02.625: INFO: ExecWithOptions: Clientset creation
Mar  1 13:05:02.625: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-9603/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.74.72%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.74.73%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar  1 13:05:02.688: INFO: Waiting for responses: map[]
Mar  1 13:05:02.688: INFO: reached 10.233.74.73 after 0/1 tries
Mar  1 13:05:02.688: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Mar  1 13:05:02.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9603" for this suite. 03/01/23 13:05:02.695
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","completed":264,"skipped":4883,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.380 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:04:38.326
    Mar  1 13:04:38.326: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename pod-network-test 03/01/23 13:04:38.327
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:04:38.346
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:04:38.349
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-9603 03/01/23 13:04:38.355
    STEP: creating a selector 03/01/23 13:04:38.355
    STEP: Creating the service pods in kubernetes 03/01/23 13:04:38.355
    Mar  1 13:04:38.355: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Mar  1 13:04:38.400: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9603" to be "running and ready"
    Mar  1 13:04:38.407: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.570724ms
    Mar  1 13:04:38.407: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Mar  1 13:04:40.413: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.013716691s
    Mar  1 13:04:40.413: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  1 13:04:42.414: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.013932461s
    Mar  1 13:04:42.414: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  1 13:04:44.413: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.012796399s
    Mar  1 13:04:44.413: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  1 13:04:46.413: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.012774943s
    Mar  1 13:04:46.413: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  1 13:04:48.414: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.01421138s
    Mar  1 13:04:48.414: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  1 13:04:50.412: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.012652644s
    Mar  1 13:04:50.413: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  1 13:04:52.414: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.014488962s
    Mar  1 13:04:52.414: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  1 13:04:54.415: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.015174576s
    Mar  1 13:04:54.415: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  1 13:04:56.412: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.012464299s
    Mar  1 13:04:56.412: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  1 13:04:58.413: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.013727728s
    Mar  1 13:04:58.414: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  1 13:05:00.414: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.01403466s
    Mar  1 13:05:00.414: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Mar  1 13:05:00.414: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Mar  1 13:05:00.418: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9603" to be "running and ready"
    Mar  1 13:05:00.423: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 5.083044ms
    Mar  1 13:05:00.423: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Mar  1 13:05:00.423: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Mar  1 13:05:00.429: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-9603" to be "running and ready"
    Mar  1 13:05:00.433: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.843473ms
    Mar  1 13:05:00.433: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Mar  1 13:05:00.433: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 03/01/23 13:05:00.438
    Mar  1 13:05:00.445: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9603" to be "running"
    Mar  1 13:05:00.449: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.604358ms
    Mar  1 13:05:02.455: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009653645s
    Mar  1 13:05:02.455: INFO: Pod "test-container-pod" satisfied condition "running"
    Mar  1 13:05:02.460: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Mar  1 13:05:02.461: INFO: Breadth first check of 10.233.95.173 on host 10.128.0.178...
    Mar  1 13:05:02.466: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.74.72:9080/dial?request=hostname&protocol=udp&host=10.233.95.173&port=8081&tries=1'] Namespace:pod-network-test-9603 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  1 13:05:02.466: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    Mar  1 13:05:02.467: INFO: ExecWithOptions: Clientset creation
    Mar  1 13:05:02.467: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-9603/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.74.72%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.95.173%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Mar  1 13:05:02.542: INFO: Waiting for responses: map[]
    Mar  1 13:05:02.542: INFO: reached 10.233.95.173 after 0/1 tries
    Mar  1 13:05:02.542: INFO: Breadth first check of 10.233.64.129 on host 10.128.0.76...
    Mar  1 13:05:02.547: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.74.72:9080/dial?request=hostname&protocol=udp&host=10.233.64.129&port=8081&tries=1'] Namespace:pod-network-test-9603 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  1 13:05:02.547: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    Mar  1 13:05:02.547: INFO: ExecWithOptions: Clientset creation
    Mar  1 13:05:02.548: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-9603/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.74.72%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.64.129%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Mar  1 13:05:02.618: INFO: Waiting for responses: map[]
    Mar  1 13:05:02.618: INFO: reached 10.233.64.129 after 0/1 tries
    Mar  1 13:05:02.618: INFO: Breadth first check of 10.233.74.73 on host 10.128.2.241...
    Mar  1 13:05:02.624: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.74.72:9080/dial?request=hostname&protocol=udp&host=10.233.74.73&port=8081&tries=1'] Namespace:pod-network-test-9603 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  1 13:05:02.624: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    Mar  1 13:05:02.625: INFO: ExecWithOptions: Clientset creation
    Mar  1 13:05:02.625: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-9603/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.74.72%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.74.73%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Mar  1 13:05:02.688: INFO: Waiting for responses: map[]
    Mar  1 13:05:02.688: INFO: reached 10.233.74.73 after 0/1 tries
    Mar  1 13:05:02.688: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Mar  1 13:05:02.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-9603" for this suite. 03/01/23 13:05:02.695
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:05:02.707
Mar  1 13:05:02.707: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename emptydir 03/01/23 13:05:02.708
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:05:02.729
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:05:02.733
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
STEP: Creating a pod to test emptydir 0666 on tmpfs 03/01/23 13:05:02.736
Mar  1 13:05:02.748: INFO: Waiting up to 5m0s for pod "pod-7d6184c0-840c-4dd7-8c49-22365ff0c988" in namespace "emptydir-6286" to be "Succeeded or Failed"
Mar  1 13:05:02.754: INFO: Pod "pod-7d6184c0-840c-4dd7-8c49-22365ff0c988": Phase="Pending", Reason="", readiness=false. Elapsed: 5.696104ms
Mar  1 13:05:04.760: INFO: Pod "pod-7d6184c0-840c-4dd7-8c49-22365ff0c988": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011544233s
Mar  1 13:05:06.762: INFO: Pod "pod-7d6184c0-840c-4dd7-8c49-22365ff0c988": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012992776s
STEP: Saw pod success 03/01/23 13:05:06.762
Mar  1 13:05:06.762: INFO: Pod "pod-7d6184c0-840c-4dd7-8c49-22365ff0c988" satisfied condition "Succeeded or Failed"
Mar  1 13:05:06.766: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-7d6184c0-840c-4dd7-8c49-22365ff0c988 container test-container: <nil>
STEP: delete the pod 03/01/23 13:05:06.786
Mar  1 13:05:06.808: INFO: Waiting for pod pod-7d6184c0-840c-4dd7-8c49-22365ff0c988 to disappear
Mar  1 13:05:06.812: INFO: Pod pod-7d6184c0-840c-4dd7-8c49-22365ff0c988 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar  1 13:05:06.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6286" for this suite. 03/01/23 13:05:06.818
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":265,"skipped":4890,"failed":0}
------------------------------
â€¢ [4.122 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:05:02.707
    Mar  1 13:05:02.707: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename emptydir 03/01/23 13:05:02.708
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:05:02.729
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:05:02.733
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:136
    STEP: Creating a pod to test emptydir 0666 on tmpfs 03/01/23 13:05:02.736
    Mar  1 13:05:02.748: INFO: Waiting up to 5m0s for pod "pod-7d6184c0-840c-4dd7-8c49-22365ff0c988" in namespace "emptydir-6286" to be "Succeeded or Failed"
    Mar  1 13:05:02.754: INFO: Pod "pod-7d6184c0-840c-4dd7-8c49-22365ff0c988": Phase="Pending", Reason="", readiness=false. Elapsed: 5.696104ms
    Mar  1 13:05:04.760: INFO: Pod "pod-7d6184c0-840c-4dd7-8c49-22365ff0c988": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011544233s
    Mar  1 13:05:06.762: INFO: Pod "pod-7d6184c0-840c-4dd7-8c49-22365ff0c988": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012992776s
    STEP: Saw pod success 03/01/23 13:05:06.762
    Mar  1 13:05:06.762: INFO: Pod "pod-7d6184c0-840c-4dd7-8c49-22365ff0c988" satisfied condition "Succeeded or Failed"
    Mar  1 13:05:06.766: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-7d6184c0-840c-4dd7-8c49-22365ff0c988 container test-container: <nil>
    STEP: delete the pod 03/01/23 13:05:06.786
    Mar  1 13:05:06.808: INFO: Waiting for pod pod-7d6184c0-840c-4dd7-8c49-22365ff0c988 to disappear
    Mar  1 13:05:06.812: INFO: Pod pod-7d6184c0-840c-4dd7-8c49-22365ff0c988 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar  1 13:05:06.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-6286" for this suite. 03/01/23 13:05:06.818
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:05:06.83
Mar  1 13:05:06.830: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename webhook 03/01/23 13:05:06.831
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:05:06.848
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:05:06.851
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/01/23 13:05:06.879
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/01/23 13:05:07.311
STEP: Deploying the webhook pod 03/01/23 13:05:07.325
STEP: Wait for the deployment to be ready 03/01/23 13:05:07.343
Mar  1 13:05:07.354: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/01/23 13:05:09.37
STEP: Verifying the service has paired with the endpoint 03/01/23 13:05:09.39
Mar  1 13:05:10.392: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
STEP: Registering the webhook via the AdmissionRegistration API 03/01/23 13:05:10.398
STEP: create a pod that should be denied by the webhook 03/01/23 13:05:10.418
STEP: create a pod that causes the webhook to hang 03/01/23 13:05:10.431
STEP: create a configmap that should be denied by the webhook 03/01/23 13:05:20.443
STEP: create a configmap that should be admitted by the webhook 03/01/23 13:05:20.464
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 03/01/23 13:05:20.476
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 03/01/23 13:05:20.488
STEP: create a namespace that bypass the webhook 03/01/23 13:05:20.497
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 03/01/23 13:05:20.509
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  1 13:05:20.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8145" for this suite. 03/01/23 13:05:20.552
STEP: Destroying namespace "webhook-8145-markers" for this suite. 03/01/23 13:05:20.561
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","completed":266,"skipped":4890,"failed":0}
------------------------------
â€¢ [SLOW TEST] [13.812 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:05:06.83
    Mar  1 13:05:06.830: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename webhook 03/01/23 13:05:06.831
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:05:06.848
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:05:06.851
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/01/23 13:05:06.879
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/01/23 13:05:07.311
    STEP: Deploying the webhook pod 03/01/23 13:05:07.325
    STEP: Wait for the deployment to be ready 03/01/23 13:05:07.343
    Mar  1 13:05:07.354: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/01/23 13:05:09.37
    STEP: Verifying the service has paired with the endpoint 03/01/23 13:05:09.39
    Mar  1 13:05:10.392: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:196
    STEP: Registering the webhook via the AdmissionRegistration API 03/01/23 13:05:10.398
    STEP: create a pod that should be denied by the webhook 03/01/23 13:05:10.418
    STEP: create a pod that causes the webhook to hang 03/01/23 13:05:10.431
    STEP: create a configmap that should be denied by the webhook 03/01/23 13:05:20.443
    STEP: create a configmap that should be admitted by the webhook 03/01/23 13:05:20.464
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 03/01/23 13:05:20.476
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 03/01/23 13:05:20.488
    STEP: create a namespace that bypass the webhook 03/01/23 13:05:20.497
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 03/01/23 13:05:20.509
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  1 13:05:20.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-8145" for this suite. 03/01/23 13:05:20.552
    STEP: Destroying namespace "webhook-8145-markers" for this suite. 03/01/23 13:05:20.561
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:05:20.651
Mar  1 13:05:20.651: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename sched-pred 03/01/23 13:05:20.652
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:05:20.682
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:05:20.687
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Mar  1 13:05:20.690: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar  1 13:05:20.704: INFO: Waiting for terminating namespaces to be deleted...
Mar  1 13:05:20.709: INFO: 
Logging pods the apiserver thinks is on node lab1-k8s-node-1 before test
Mar  1 13:05:20.722: INFO: calico-node-kjj57 from kube-system started at 2023-03-01 11:36:06 +0000 UTC (1 container statuses recorded)
Mar  1 13:05:20.722: INFO: 	Container calico-node ready: true, restart count 1
Mar  1 13:05:20.722: INFO: csi-cinder-nodeplugin-fjt6c from kube-system started at 2023-03-01 11:38:24 +0000 UTC (3 container statuses recorded)
Mar  1 13:05:20.722: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Mar  1 13:05:20.722: INFO: 	Container liveness-probe ready: true, restart count 0
Mar  1 13:05:20.722: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar  1 13:05:20.722: INFO: kube-proxy-xmdzj from kube-system started at 2023-03-01 11:35:32 +0000 UTC (1 container statuses recorded)
Mar  1 13:05:20.722: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  1 13:05:20.722: INFO: metrics-server-6bd8d699c5-pwxfp from kube-system started at 2023-03-01 11:38:24 +0000 UTC (1 container statuses recorded)
Mar  1 13:05:20.722: INFO: 	Container metrics-server ready: true, restart count 0
Mar  1 13:05:20.722: INFO: nginx-proxy-lab1-k8s-node-1 from kube-system started at 2023-03-01 11:36:22 +0000 UTC (1 container statuses recorded)
Mar  1 13:05:20.722: INFO: 	Container nginx-proxy ready: true, restart count 0
Mar  1 13:05:20.722: INFO: nodelocaldns-mclb6 from kube-system started at 2023-03-01 11:37:23 +0000 UTC (1 container statuses recorded)
Mar  1 13:05:20.722: INFO: 	Container node-cache ready: true, restart count 0
Mar  1 13:05:20.722: INFO: snapshot-controller-7d445c66c9-m2qf9 from kube-system started at 2023-03-01 12:18:37 +0000 UTC (1 container statuses recorded)
Mar  1 13:05:20.722: INFO: 	Container snapshot-controller ready: true, restart count 0
Mar  1 13:05:20.722: INFO: sonobuoy-e2e-job-5f1d4571b8c24260 from sonobuoy started at 2023-03-01 11:42:09 +0000 UTC (2 container statuses recorded)
Mar  1 13:05:20.722: INFO: 	Container e2e ready: true, restart count 0
Mar  1 13:05:20.722: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  1 13:05:20.722: INFO: sonobuoy-systemd-logs-daemon-set-1321f7b040854c53-9ckr7 from sonobuoy started at 2023-03-01 11:42:09 +0000 UTC (2 container statuses recorded)
Mar  1 13:05:20.722: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  1 13:05:20.722: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  1 13:05:20.722: INFO: 
Logging pods the apiserver thinks is on node lab1-k8s-node-2 before test
Mar  1 13:05:20.735: INFO: calico-kube-controllers-ff45567bb-9k2q7 from kube-system started at 2023-03-01 11:37:06 +0000 UTC (1 container statuses recorded)
Mar  1 13:05:20.735: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Mar  1 13:05:20.735: INFO: calico-node-5vzf7 from kube-system started at 2023-03-01 11:36:06 +0000 UTC (1 container statuses recorded)
Mar  1 13:05:20.735: INFO: 	Container calico-node ready: true, restart count 1
Mar  1 13:05:20.735: INFO: csi-cinder-controllerplugin-6f68fbd578-krvcc from kube-system started at 2023-03-01 11:38:24 +0000 UTC (6 container statuses recorded)
Mar  1 13:05:20.735: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Mar  1 13:05:20.735: INFO: 	Container csi-attacher ready: true, restart count 0
Mar  1 13:05:20.735: INFO: 	Container csi-provisioner ready: true, restart count 0
Mar  1 13:05:20.735: INFO: 	Container csi-resizer ready: true, restart count 0
Mar  1 13:05:20.735: INFO: 	Container csi-snapshotter ready: true, restart count 0
Mar  1 13:05:20.736: INFO: 	Container liveness-probe ready: true, restart count 0
Mar  1 13:05:20.736: INFO: csi-cinder-nodeplugin-zl564 from kube-system started at 2023-03-01 11:38:24 +0000 UTC (3 container statuses recorded)
Mar  1 13:05:20.736: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Mar  1 13:05:20.736: INFO: 	Container liveness-probe ready: true, restart count 0
Mar  1 13:05:20.736: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar  1 13:05:20.736: INFO: kube-proxy-jllc6 from kube-system started at 2023-03-01 11:35:32 +0000 UTC (1 container statuses recorded)
Mar  1 13:05:20.736: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  1 13:05:20.736: INFO: nginx-proxy-lab1-k8s-node-2 from kube-system started at 2023-03-01 11:36:22 +0000 UTC (1 container statuses recorded)
Mar  1 13:05:20.736: INFO: 	Container nginx-proxy ready: true, restart count 0
Mar  1 13:05:20.736: INFO: nodelocaldns-mhm2s from kube-system started at 2023-03-01 11:37:23 +0000 UTC (1 container statuses recorded)
Mar  1 13:05:20.736: INFO: 	Container node-cache ready: true, restart count 0
Mar  1 13:05:20.736: INFO: sonobuoy-systemd-logs-daemon-set-1321f7b040854c53-nklmj from sonobuoy started at 2023-03-01 11:42:09 +0000 UTC (2 container statuses recorded)
Mar  1 13:05:20.736: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  1 13:05:20.736: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  1 13:05:20.736: INFO: 
Logging pods the apiserver thinks is on node lab1-k8s-node-3 before test
Mar  1 13:05:20.758: INFO: calico-node-zjksl from kube-system started at 2023-03-01 11:36:06 +0000 UTC (1 container statuses recorded)
Mar  1 13:05:20.759: INFO: 	Container calico-node ready: true, restart count 1
Mar  1 13:05:20.759: INFO: csi-cinder-nodeplugin-xh6tw from kube-system started at 2023-03-01 12:27:48 +0000 UTC (3 container statuses recorded)
Mar  1 13:05:20.759: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Mar  1 13:05:20.759: INFO: 	Container liveness-probe ready: true, restart count 0
Mar  1 13:05:20.759: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar  1 13:05:20.759: INFO: kube-proxy-2jcfl from kube-system started at 2023-03-01 11:35:29 +0000 UTC (1 container statuses recorded)
Mar  1 13:05:20.759: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  1 13:05:20.759: INFO: nginx-proxy-lab1-k8s-node-3 from kube-system started at 2023-03-01 11:38:59 +0000 UTC (1 container statuses recorded)
Mar  1 13:05:20.759: INFO: 	Container nginx-proxy ready: true, restart count 0
Mar  1 13:05:20.759: INFO: nodelocaldns-5tw4l from kube-system started at 2023-03-01 11:37:23 +0000 UTC (1 container statuses recorded)
Mar  1 13:05:20.759: INFO: 	Container node-cache ready: true, restart count 0
Mar  1 13:05:20.759: INFO: sonobuoy from sonobuoy started at 2023-03-01 11:42:06 +0000 UTC (1 container statuses recorded)
Mar  1 13:05:20.760: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar  1 13:05:20.760: INFO: sonobuoy-systemd-logs-daemon-set-1321f7b040854c53-9b52c from sonobuoy started at 2023-03-01 11:42:09 +0000 UTC (2 container statuses recorded)
Mar  1 13:05:20.760: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  1 13:05:20.760: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
STEP: verifying the node has the label node lab1-k8s-node-1 03/01/23 13:05:20.807
STEP: verifying the node has the label node lab1-k8s-node-2 03/01/23 13:05:20.835
STEP: verifying the node has the label node lab1-k8s-node-3 03/01/23 13:05:20.855
Mar  1 13:05:20.887: INFO: Pod calico-kube-controllers-ff45567bb-9k2q7 requesting resource cpu=30m on Node lab1-k8s-node-2
Mar  1 13:05:20.887: INFO: Pod calico-node-5vzf7 requesting resource cpu=150m on Node lab1-k8s-node-2
Mar  1 13:05:20.887: INFO: Pod calico-node-kjj57 requesting resource cpu=150m on Node lab1-k8s-node-1
Mar  1 13:05:20.887: INFO: Pod calico-node-zjksl requesting resource cpu=150m on Node lab1-k8s-node-3
Mar  1 13:05:20.887: INFO: Pod csi-cinder-controllerplugin-6f68fbd578-krvcc requesting resource cpu=0m on Node lab1-k8s-node-2
Mar  1 13:05:20.887: INFO: Pod csi-cinder-nodeplugin-fjt6c requesting resource cpu=0m on Node lab1-k8s-node-1
Mar  1 13:05:20.887: INFO: Pod csi-cinder-nodeplugin-xh6tw requesting resource cpu=0m on Node lab1-k8s-node-3
Mar  1 13:05:20.887: INFO: Pod csi-cinder-nodeplugin-zl564 requesting resource cpu=0m on Node lab1-k8s-node-2
Mar  1 13:05:20.887: INFO: Pod kube-proxy-2jcfl requesting resource cpu=0m on Node lab1-k8s-node-3
Mar  1 13:05:20.887: INFO: Pod kube-proxy-jllc6 requesting resource cpu=0m on Node lab1-k8s-node-2
Mar  1 13:05:20.887: INFO: Pod kube-proxy-xmdzj requesting resource cpu=0m on Node lab1-k8s-node-1
Mar  1 13:05:20.887: INFO: Pod metrics-server-6bd8d699c5-pwxfp requesting resource cpu=100m on Node lab1-k8s-node-1
Mar  1 13:05:20.887: INFO: Pod nginx-proxy-lab1-k8s-node-1 requesting resource cpu=25m on Node lab1-k8s-node-1
Mar  1 13:05:20.887: INFO: Pod nginx-proxy-lab1-k8s-node-2 requesting resource cpu=25m on Node lab1-k8s-node-2
Mar  1 13:05:20.887: INFO: Pod nginx-proxy-lab1-k8s-node-3 requesting resource cpu=25m on Node lab1-k8s-node-3
Mar  1 13:05:20.887: INFO: Pod nodelocaldns-5tw4l requesting resource cpu=100m on Node lab1-k8s-node-3
Mar  1 13:05:20.887: INFO: Pod nodelocaldns-mclb6 requesting resource cpu=100m on Node lab1-k8s-node-1
Mar  1 13:05:20.887: INFO: Pod nodelocaldns-mhm2s requesting resource cpu=100m on Node lab1-k8s-node-2
Mar  1 13:05:20.887: INFO: Pod snapshot-controller-7d445c66c9-m2qf9 requesting resource cpu=0m on Node lab1-k8s-node-1
Mar  1 13:05:20.887: INFO: Pod sonobuoy requesting resource cpu=0m on Node lab1-k8s-node-3
Mar  1 13:05:20.888: INFO: Pod sonobuoy-e2e-job-5f1d4571b8c24260 requesting resource cpu=0m on Node lab1-k8s-node-1
Mar  1 13:05:20.888: INFO: Pod sonobuoy-systemd-logs-daemon-set-1321f7b040854c53-9b52c requesting resource cpu=0m on Node lab1-k8s-node-3
Mar  1 13:05:20.888: INFO: Pod sonobuoy-systemd-logs-daemon-set-1321f7b040854c53-9ckr7 requesting resource cpu=0m on Node lab1-k8s-node-1
Mar  1 13:05:20.888: INFO: Pod sonobuoy-systemd-logs-daemon-set-1321f7b040854c53-nklmj requesting resource cpu=0m on Node lab1-k8s-node-2
STEP: Starting Pods to consume most of the cluster CPU. 03/01/23 13:05:20.888
Mar  1 13:05:20.888: INFO: Creating a pod which consumes cpu=787m on Node lab1-k8s-node-1
Mar  1 13:05:20.903: INFO: Creating a pod which consumes cpu=836m on Node lab1-k8s-node-2
Mar  1 13:05:20.914: INFO: Creating a pod which consumes cpu=857m on Node lab1-k8s-node-3
Mar  1 13:05:20.939: INFO: Waiting up to 5m0s for pod "filler-pod-4c56e772-d26c-415c-b32b-808e2de10c9c" in namespace "sched-pred-6613" to be "running"
Mar  1 13:05:20.944: INFO: Pod "filler-pod-4c56e772-d26c-415c-b32b-808e2de10c9c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.887843ms
Mar  1 13:05:22.950: INFO: Pod "filler-pod-4c56e772-d26c-415c-b32b-808e2de10c9c": Phase="Running", Reason="", readiness=true. Elapsed: 2.010755154s
Mar  1 13:05:22.950: INFO: Pod "filler-pod-4c56e772-d26c-415c-b32b-808e2de10c9c" satisfied condition "running"
Mar  1 13:05:22.950: INFO: Waiting up to 5m0s for pod "filler-pod-5a8b0f34-ea72-480e-a29c-0e36c2dfa556" in namespace "sched-pred-6613" to be "running"
Mar  1 13:05:22.955: INFO: Pod "filler-pod-5a8b0f34-ea72-480e-a29c-0e36c2dfa556": Phase="Running", Reason="", readiness=true. Elapsed: 5.231059ms
Mar  1 13:05:22.955: INFO: Pod "filler-pod-5a8b0f34-ea72-480e-a29c-0e36c2dfa556" satisfied condition "running"
Mar  1 13:05:22.955: INFO: Waiting up to 5m0s for pod "filler-pod-85f1a2d0-c743-4500-8e80-cdd33b95d225" in namespace "sched-pred-6613" to be "running"
Mar  1 13:05:22.959: INFO: Pod "filler-pod-85f1a2d0-c743-4500-8e80-cdd33b95d225": Phase="Running", Reason="", readiness=true. Elapsed: 3.659176ms
Mar  1 13:05:22.959: INFO: Pod "filler-pod-85f1a2d0-c743-4500-8e80-cdd33b95d225" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 03/01/23 13:05:22.959
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4c56e772-d26c-415c-b32b-808e2de10c9c.17484d5c838cd3ca], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6613/filler-pod-4c56e772-d26c-415c-b32b-808e2de10c9c to lab1-k8s-node-1] 03/01/23 13:05:22.965
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4c56e772-d26c-415c-b32b-808e2de10c9c.17484d5cac429f92], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 03/01/23 13:05:22.965
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4c56e772-d26c-415c-b32b-808e2de10c9c.17484d5cacd586f4], Reason = [Created], Message = [Created container filler-pod-4c56e772-d26c-415c-b32b-808e2de10c9c] 03/01/23 13:05:22.965
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4c56e772-d26c-415c-b32b-808e2de10c9c.17484d5cb27a7291], Reason = [Started], Message = [Started container filler-pod-4c56e772-d26c-415c-b32b-808e2de10c9c] 03/01/23 13:05:22.966
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5a8b0f34-ea72-480e-a29c-0e36c2dfa556.17484d5c84754dcc], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6613/filler-pod-5a8b0f34-ea72-480e-a29c-0e36c2dfa556 to lab1-k8s-node-2] 03/01/23 13:05:22.966
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5a8b0f34-ea72-480e-a29c-0e36c2dfa556.17484d5cabf0fc39], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 03/01/23 13:05:22.966
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5a8b0f34-ea72-480e-a29c-0e36c2dfa556.17484d5cacfc0aa3], Reason = [Created], Message = [Created container filler-pod-5a8b0f34-ea72-480e-a29c-0e36c2dfa556] 03/01/23 13:05:22.966
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5a8b0f34-ea72-480e-a29c-0e36c2dfa556.17484d5cb34c3bbc], Reason = [Started], Message = [Started container filler-pod-5a8b0f34-ea72-480e-a29c-0e36c2dfa556] 03/01/23 13:05:22.966
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-85f1a2d0-c743-4500-8e80-cdd33b95d225.17484d5c85c907c5], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6613/filler-pod-85f1a2d0-c743-4500-8e80-cdd33b95d225 to lab1-k8s-node-3] 03/01/23 13:05:22.966
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-85f1a2d0-c743-4500-8e80-cdd33b95d225.17484d5cacfda354], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 03/01/23 13:05:22.966
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-85f1a2d0-c743-4500-8e80-cdd33b95d225.17484d5cadda3090], Reason = [Created], Message = [Created container filler-pod-85f1a2d0-c743-4500-8e80-cdd33b95d225] 03/01/23 13:05:22.966
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-85f1a2d0-c743-4500-8e80-cdd33b95d225.17484d5cb3cf98ce], Reason = [Started], Message = [Started container filler-pod-85f1a2d0-c743-4500-8e80-cdd33b95d225] 03/01/23 13:05:22.966
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.17484d5cfe74f99a], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 Insufficient cpu, 3 node(s) had untolerated taint {node-role.kubernetes.io/master: }. preemption: 0/6 nodes are available: 3 No preemption victims found for incoming pod, 3 Preemption is not helpful for scheduling.] 03/01/23 13:05:22.982
STEP: removing the label node off the node lab1-k8s-node-1 03/01/23 13:05:23.982
STEP: verifying the node doesn't have the label node 03/01/23 13:05:24.007
STEP: removing the label node off the node lab1-k8s-node-2 03/01/23 13:05:24.012
STEP: verifying the node doesn't have the label node 03/01/23 13:05:24.03
STEP: removing the label node off the node lab1-k8s-node-3 03/01/23 13:05:24.037
STEP: verifying the node doesn't have the label node 03/01/23 13:05:24.056
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Mar  1 13:05:24.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6613" for this suite. 03/01/23 13:05:24.072
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","completed":267,"skipped":4919,"failed":0}
------------------------------
â€¢ [3.430 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:05:20.651
    Mar  1 13:05:20.651: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename sched-pred 03/01/23 13:05:20.652
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:05:20.682
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:05:20.687
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Mar  1 13:05:20.690: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Mar  1 13:05:20.704: INFO: Waiting for terminating namespaces to be deleted...
    Mar  1 13:05:20.709: INFO: 
    Logging pods the apiserver thinks is on node lab1-k8s-node-1 before test
    Mar  1 13:05:20.722: INFO: calico-node-kjj57 from kube-system started at 2023-03-01 11:36:06 +0000 UTC (1 container statuses recorded)
    Mar  1 13:05:20.722: INFO: 	Container calico-node ready: true, restart count 1
    Mar  1 13:05:20.722: INFO: csi-cinder-nodeplugin-fjt6c from kube-system started at 2023-03-01 11:38:24 +0000 UTC (3 container statuses recorded)
    Mar  1 13:05:20.722: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Mar  1 13:05:20.722: INFO: 	Container liveness-probe ready: true, restart count 0
    Mar  1 13:05:20.722: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Mar  1 13:05:20.722: INFO: kube-proxy-xmdzj from kube-system started at 2023-03-01 11:35:32 +0000 UTC (1 container statuses recorded)
    Mar  1 13:05:20.722: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar  1 13:05:20.722: INFO: metrics-server-6bd8d699c5-pwxfp from kube-system started at 2023-03-01 11:38:24 +0000 UTC (1 container statuses recorded)
    Mar  1 13:05:20.722: INFO: 	Container metrics-server ready: true, restart count 0
    Mar  1 13:05:20.722: INFO: nginx-proxy-lab1-k8s-node-1 from kube-system started at 2023-03-01 11:36:22 +0000 UTC (1 container statuses recorded)
    Mar  1 13:05:20.722: INFO: 	Container nginx-proxy ready: true, restart count 0
    Mar  1 13:05:20.722: INFO: nodelocaldns-mclb6 from kube-system started at 2023-03-01 11:37:23 +0000 UTC (1 container statuses recorded)
    Mar  1 13:05:20.722: INFO: 	Container node-cache ready: true, restart count 0
    Mar  1 13:05:20.722: INFO: snapshot-controller-7d445c66c9-m2qf9 from kube-system started at 2023-03-01 12:18:37 +0000 UTC (1 container statuses recorded)
    Mar  1 13:05:20.722: INFO: 	Container snapshot-controller ready: true, restart count 0
    Mar  1 13:05:20.722: INFO: sonobuoy-e2e-job-5f1d4571b8c24260 from sonobuoy started at 2023-03-01 11:42:09 +0000 UTC (2 container statuses recorded)
    Mar  1 13:05:20.722: INFO: 	Container e2e ready: true, restart count 0
    Mar  1 13:05:20.722: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar  1 13:05:20.722: INFO: sonobuoy-systemd-logs-daemon-set-1321f7b040854c53-9ckr7 from sonobuoy started at 2023-03-01 11:42:09 +0000 UTC (2 container statuses recorded)
    Mar  1 13:05:20.722: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar  1 13:05:20.722: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar  1 13:05:20.722: INFO: 
    Logging pods the apiserver thinks is on node lab1-k8s-node-2 before test
    Mar  1 13:05:20.735: INFO: calico-kube-controllers-ff45567bb-9k2q7 from kube-system started at 2023-03-01 11:37:06 +0000 UTC (1 container statuses recorded)
    Mar  1 13:05:20.735: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Mar  1 13:05:20.735: INFO: calico-node-5vzf7 from kube-system started at 2023-03-01 11:36:06 +0000 UTC (1 container statuses recorded)
    Mar  1 13:05:20.735: INFO: 	Container calico-node ready: true, restart count 1
    Mar  1 13:05:20.735: INFO: csi-cinder-controllerplugin-6f68fbd578-krvcc from kube-system started at 2023-03-01 11:38:24 +0000 UTC (6 container statuses recorded)
    Mar  1 13:05:20.735: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Mar  1 13:05:20.735: INFO: 	Container csi-attacher ready: true, restart count 0
    Mar  1 13:05:20.735: INFO: 	Container csi-provisioner ready: true, restart count 0
    Mar  1 13:05:20.735: INFO: 	Container csi-resizer ready: true, restart count 0
    Mar  1 13:05:20.735: INFO: 	Container csi-snapshotter ready: true, restart count 0
    Mar  1 13:05:20.736: INFO: 	Container liveness-probe ready: true, restart count 0
    Mar  1 13:05:20.736: INFO: csi-cinder-nodeplugin-zl564 from kube-system started at 2023-03-01 11:38:24 +0000 UTC (3 container statuses recorded)
    Mar  1 13:05:20.736: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Mar  1 13:05:20.736: INFO: 	Container liveness-probe ready: true, restart count 0
    Mar  1 13:05:20.736: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Mar  1 13:05:20.736: INFO: kube-proxy-jllc6 from kube-system started at 2023-03-01 11:35:32 +0000 UTC (1 container statuses recorded)
    Mar  1 13:05:20.736: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar  1 13:05:20.736: INFO: nginx-proxy-lab1-k8s-node-2 from kube-system started at 2023-03-01 11:36:22 +0000 UTC (1 container statuses recorded)
    Mar  1 13:05:20.736: INFO: 	Container nginx-proxy ready: true, restart count 0
    Mar  1 13:05:20.736: INFO: nodelocaldns-mhm2s from kube-system started at 2023-03-01 11:37:23 +0000 UTC (1 container statuses recorded)
    Mar  1 13:05:20.736: INFO: 	Container node-cache ready: true, restart count 0
    Mar  1 13:05:20.736: INFO: sonobuoy-systemd-logs-daemon-set-1321f7b040854c53-nklmj from sonobuoy started at 2023-03-01 11:42:09 +0000 UTC (2 container statuses recorded)
    Mar  1 13:05:20.736: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar  1 13:05:20.736: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar  1 13:05:20.736: INFO: 
    Logging pods the apiserver thinks is on node lab1-k8s-node-3 before test
    Mar  1 13:05:20.758: INFO: calico-node-zjksl from kube-system started at 2023-03-01 11:36:06 +0000 UTC (1 container statuses recorded)
    Mar  1 13:05:20.759: INFO: 	Container calico-node ready: true, restart count 1
    Mar  1 13:05:20.759: INFO: csi-cinder-nodeplugin-xh6tw from kube-system started at 2023-03-01 12:27:48 +0000 UTC (3 container statuses recorded)
    Mar  1 13:05:20.759: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Mar  1 13:05:20.759: INFO: 	Container liveness-probe ready: true, restart count 0
    Mar  1 13:05:20.759: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Mar  1 13:05:20.759: INFO: kube-proxy-2jcfl from kube-system started at 2023-03-01 11:35:29 +0000 UTC (1 container statuses recorded)
    Mar  1 13:05:20.759: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar  1 13:05:20.759: INFO: nginx-proxy-lab1-k8s-node-3 from kube-system started at 2023-03-01 11:38:59 +0000 UTC (1 container statuses recorded)
    Mar  1 13:05:20.759: INFO: 	Container nginx-proxy ready: true, restart count 0
    Mar  1 13:05:20.759: INFO: nodelocaldns-5tw4l from kube-system started at 2023-03-01 11:37:23 +0000 UTC (1 container statuses recorded)
    Mar  1 13:05:20.759: INFO: 	Container node-cache ready: true, restart count 0
    Mar  1 13:05:20.759: INFO: sonobuoy from sonobuoy started at 2023-03-01 11:42:06 +0000 UTC (1 container statuses recorded)
    Mar  1 13:05:20.760: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Mar  1 13:05:20.760: INFO: sonobuoy-systemd-logs-daemon-set-1321f7b040854c53-9b52c from sonobuoy started at 2023-03-01 11:42:09 +0000 UTC (2 container statuses recorded)
    Mar  1 13:05:20.760: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar  1 13:05:20.760: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:326
    STEP: verifying the node has the label node lab1-k8s-node-1 03/01/23 13:05:20.807
    STEP: verifying the node has the label node lab1-k8s-node-2 03/01/23 13:05:20.835
    STEP: verifying the node has the label node lab1-k8s-node-3 03/01/23 13:05:20.855
    Mar  1 13:05:20.887: INFO: Pod calico-kube-controllers-ff45567bb-9k2q7 requesting resource cpu=30m on Node lab1-k8s-node-2
    Mar  1 13:05:20.887: INFO: Pod calico-node-5vzf7 requesting resource cpu=150m on Node lab1-k8s-node-2
    Mar  1 13:05:20.887: INFO: Pod calico-node-kjj57 requesting resource cpu=150m on Node lab1-k8s-node-1
    Mar  1 13:05:20.887: INFO: Pod calico-node-zjksl requesting resource cpu=150m on Node lab1-k8s-node-3
    Mar  1 13:05:20.887: INFO: Pod csi-cinder-controllerplugin-6f68fbd578-krvcc requesting resource cpu=0m on Node lab1-k8s-node-2
    Mar  1 13:05:20.887: INFO: Pod csi-cinder-nodeplugin-fjt6c requesting resource cpu=0m on Node lab1-k8s-node-1
    Mar  1 13:05:20.887: INFO: Pod csi-cinder-nodeplugin-xh6tw requesting resource cpu=0m on Node lab1-k8s-node-3
    Mar  1 13:05:20.887: INFO: Pod csi-cinder-nodeplugin-zl564 requesting resource cpu=0m on Node lab1-k8s-node-2
    Mar  1 13:05:20.887: INFO: Pod kube-proxy-2jcfl requesting resource cpu=0m on Node lab1-k8s-node-3
    Mar  1 13:05:20.887: INFO: Pod kube-proxy-jllc6 requesting resource cpu=0m on Node lab1-k8s-node-2
    Mar  1 13:05:20.887: INFO: Pod kube-proxy-xmdzj requesting resource cpu=0m on Node lab1-k8s-node-1
    Mar  1 13:05:20.887: INFO: Pod metrics-server-6bd8d699c5-pwxfp requesting resource cpu=100m on Node lab1-k8s-node-1
    Mar  1 13:05:20.887: INFO: Pod nginx-proxy-lab1-k8s-node-1 requesting resource cpu=25m on Node lab1-k8s-node-1
    Mar  1 13:05:20.887: INFO: Pod nginx-proxy-lab1-k8s-node-2 requesting resource cpu=25m on Node lab1-k8s-node-2
    Mar  1 13:05:20.887: INFO: Pod nginx-proxy-lab1-k8s-node-3 requesting resource cpu=25m on Node lab1-k8s-node-3
    Mar  1 13:05:20.887: INFO: Pod nodelocaldns-5tw4l requesting resource cpu=100m on Node lab1-k8s-node-3
    Mar  1 13:05:20.887: INFO: Pod nodelocaldns-mclb6 requesting resource cpu=100m on Node lab1-k8s-node-1
    Mar  1 13:05:20.887: INFO: Pod nodelocaldns-mhm2s requesting resource cpu=100m on Node lab1-k8s-node-2
    Mar  1 13:05:20.887: INFO: Pod snapshot-controller-7d445c66c9-m2qf9 requesting resource cpu=0m on Node lab1-k8s-node-1
    Mar  1 13:05:20.887: INFO: Pod sonobuoy requesting resource cpu=0m on Node lab1-k8s-node-3
    Mar  1 13:05:20.888: INFO: Pod sonobuoy-e2e-job-5f1d4571b8c24260 requesting resource cpu=0m on Node lab1-k8s-node-1
    Mar  1 13:05:20.888: INFO: Pod sonobuoy-systemd-logs-daemon-set-1321f7b040854c53-9b52c requesting resource cpu=0m on Node lab1-k8s-node-3
    Mar  1 13:05:20.888: INFO: Pod sonobuoy-systemd-logs-daemon-set-1321f7b040854c53-9ckr7 requesting resource cpu=0m on Node lab1-k8s-node-1
    Mar  1 13:05:20.888: INFO: Pod sonobuoy-systemd-logs-daemon-set-1321f7b040854c53-nklmj requesting resource cpu=0m on Node lab1-k8s-node-2
    STEP: Starting Pods to consume most of the cluster CPU. 03/01/23 13:05:20.888
    Mar  1 13:05:20.888: INFO: Creating a pod which consumes cpu=787m on Node lab1-k8s-node-1
    Mar  1 13:05:20.903: INFO: Creating a pod which consumes cpu=836m on Node lab1-k8s-node-2
    Mar  1 13:05:20.914: INFO: Creating a pod which consumes cpu=857m on Node lab1-k8s-node-3
    Mar  1 13:05:20.939: INFO: Waiting up to 5m0s for pod "filler-pod-4c56e772-d26c-415c-b32b-808e2de10c9c" in namespace "sched-pred-6613" to be "running"
    Mar  1 13:05:20.944: INFO: Pod "filler-pod-4c56e772-d26c-415c-b32b-808e2de10c9c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.887843ms
    Mar  1 13:05:22.950: INFO: Pod "filler-pod-4c56e772-d26c-415c-b32b-808e2de10c9c": Phase="Running", Reason="", readiness=true. Elapsed: 2.010755154s
    Mar  1 13:05:22.950: INFO: Pod "filler-pod-4c56e772-d26c-415c-b32b-808e2de10c9c" satisfied condition "running"
    Mar  1 13:05:22.950: INFO: Waiting up to 5m0s for pod "filler-pod-5a8b0f34-ea72-480e-a29c-0e36c2dfa556" in namespace "sched-pred-6613" to be "running"
    Mar  1 13:05:22.955: INFO: Pod "filler-pod-5a8b0f34-ea72-480e-a29c-0e36c2dfa556": Phase="Running", Reason="", readiness=true. Elapsed: 5.231059ms
    Mar  1 13:05:22.955: INFO: Pod "filler-pod-5a8b0f34-ea72-480e-a29c-0e36c2dfa556" satisfied condition "running"
    Mar  1 13:05:22.955: INFO: Waiting up to 5m0s for pod "filler-pod-85f1a2d0-c743-4500-8e80-cdd33b95d225" in namespace "sched-pred-6613" to be "running"
    Mar  1 13:05:22.959: INFO: Pod "filler-pod-85f1a2d0-c743-4500-8e80-cdd33b95d225": Phase="Running", Reason="", readiness=true. Elapsed: 3.659176ms
    Mar  1 13:05:22.959: INFO: Pod "filler-pod-85f1a2d0-c743-4500-8e80-cdd33b95d225" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 03/01/23 13:05:22.959
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-4c56e772-d26c-415c-b32b-808e2de10c9c.17484d5c838cd3ca], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6613/filler-pod-4c56e772-d26c-415c-b32b-808e2de10c9c to lab1-k8s-node-1] 03/01/23 13:05:22.965
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-4c56e772-d26c-415c-b32b-808e2de10c9c.17484d5cac429f92], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 03/01/23 13:05:22.965
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-4c56e772-d26c-415c-b32b-808e2de10c9c.17484d5cacd586f4], Reason = [Created], Message = [Created container filler-pod-4c56e772-d26c-415c-b32b-808e2de10c9c] 03/01/23 13:05:22.965
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-4c56e772-d26c-415c-b32b-808e2de10c9c.17484d5cb27a7291], Reason = [Started], Message = [Started container filler-pod-4c56e772-d26c-415c-b32b-808e2de10c9c] 03/01/23 13:05:22.966
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-5a8b0f34-ea72-480e-a29c-0e36c2dfa556.17484d5c84754dcc], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6613/filler-pod-5a8b0f34-ea72-480e-a29c-0e36c2dfa556 to lab1-k8s-node-2] 03/01/23 13:05:22.966
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-5a8b0f34-ea72-480e-a29c-0e36c2dfa556.17484d5cabf0fc39], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 03/01/23 13:05:22.966
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-5a8b0f34-ea72-480e-a29c-0e36c2dfa556.17484d5cacfc0aa3], Reason = [Created], Message = [Created container filler-pod-5a8b0f34-ea72-480e-a29c-0e36c2dfa556] 03/01/23 13:05:22.966
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-5a8b0f34-ea72-480e-a29c-0e36c2dfa556.17484d5cb34c3bbc], Reason = [Started], Message = [Started container filler-pod-5a8b0f34-ea72-480e-a29c-0e36c2dfa556] 03/01/23 13:05:22.966
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-85f1a2d0-c743-4500-8e80-cdd33b95d225.17484d5c85c907c5], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6613/filler-pod-85f1a2d0-c743-4500-8e80-cdd33b95d225 to lab1-k8s-node-3] 03/01/23 13:05:22.966
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-85f1a2d0-c743-4500-8e80-cdd33b95d225.17484d5cacfda354], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 03/01/23 13:05:22.966
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-85f1a2d0-c743-4500-8e80-cdd33b95d225.17484d5cadda3090], Reason = [Created], Message = [Created container filler-pod-85f1a2d0-c743-4500-8e80-cdd33b95d225] 03/01/23 13:05:22.966
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-85f1a2d0-c743-4500-8e80-cdd33b95d225.17484d5cb3cf98ce], Reason = [Started], Message = [Started container filler-pod-85f1a2d0-c743-4500-8e80-cdd33b95d225] 03/01/23 13:05:22.966
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.17484d5cfe74f99a], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 Insufficient cpu, 3 node(s) had untolerated taint {node-role.kubernetes.io/master: }. preemption: 0/6 nodes are available: 3 No preemption victims found for incoming pod, 3 Preemption is not helpful for scheduling.] 03/01/23 13:05:22.982
    STEP: removing the label node off the node lab1-k8s-node-1 03/01/23 13:05:23.982
    STEP: verifying the node doesn't have the label node 03/01/23 13:05:24.007
    STEP: removing the label node off the node lab1-k8s-node-2 03/01/23 13:05:24.012
    STEP: verifying the node doesn't have the label node 03/01/23 13:05:24.03
    STEP: removing the label node off the node lab1-k8s-node-3 03/01/23 13:05:24.037
    STEP: verifying the node doesn't have the label node 03/01/23 13:05:24.056
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Mar  1 13:05:24.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-6613" for this suite. 03/01/23 13:05:24.072
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:05:24.083
Mar  1 13:05:24.083: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename resourcequota 03/01/23 13:05:24.084
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:05:24.106
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:05:24.111
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
STEP: Creating a ResourceQuota 03/01/23 13:05:24.115
STEP: Getting a ResourceQuota 03/01/23 13:05:24.12
STEP: Listing all ResourceQuotas with LabelSelector 03/01/23 13:05:24.127
STEP: Patching the ResourceQuota 03/01/23 13:05:24.131
STEP: Deleting a Collection of ResourceQuotas 03/01/23 13:05:24.142
STEP: Verifying the deleted ResourceQuota 03/01/23 13:05:24.154
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Mar  1 13:05:24.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3225" for this suite. 03/01/23 13:05:24.163
{"msg":"PASSED [sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]","completed":268,"skipped":4923,"failed":0}
------------------------------
â€¢ [0.090 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:05:24.083
    Mar  1 13:05:24.083: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename resourcequota 03/01/23 13:05:24.084
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:05:24.106
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:05:24.111
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:933
    STEP: Creating a ResourceQuota 03/01/23 13:05:24.115
    STEP: Getting a ResourceQuota 03/01/23 13:05:24.12
    STEP: Listing all ResourceQuotas with LabelSelector 03/01/23 13:05:24.127
    STEP: Patching the ResourceQuota 03/01/23 13:05:24.131
    STEP: Deleting a Collection of ResourceQuotas 03/01/23 13:05:24.142
    STEP: Verifying the deleted ResourceQuota 03/01/23 13:05:24.154
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Mar  1 13:05:24.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-3225" for this suite. 03/01/23 13:05:24.163
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:05:24.18
Mar  1 13:05:24.180: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename disruption 03/01/23 13:05:24.181
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:05:24.202
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:05:24.205
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
STEP: Waiting for the pdb to be processed 03/01/23 13:05:24.218
STEP: Waiting for all pods to be running 03/01/23 13:05:24.257
Mar  1 13:05:24.270: INFO: running pods: 0 < 3
Mar  1 13:05:26.276: INFO: running pods: 2 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Mar  1 13:05:28.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-5015" for this suite. 03/01/23 13:05:28.287
{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","completed":269,"skipped":5001,"failed":0}
------------------------------
â€¢ [4.116 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:05:24.18
    Mar  1 13:05:24.180: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename disruption 03/01/23 13:05:24.181
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:05:24.202
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:05:24.205
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:140
    STEP: Waiting for the pdb to be processed 03/01/23 13:05:24.218
    STEP: Waiting for all pods to be running 03/01/23 13:05:24.257
    Mar  1 13:05:24.270: INFO: running pods: 0 < 3
    Mar  1 13:05:26.276: INFO: running pods: 2 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Mar  1 13:05:28.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-5015" for this suite. 03/01/23 13:05:28.287
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:05:28.297
Mar  1 13:05:28.297: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename containers 03/01/23 13:05:28.298
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:05:28.318
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:05:28.324
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
Mar  1 13:05:28.338: INFO: Waiting up to 5m0s for pod "client-containers-f1c1ac13-4d5b-4052-b4b9-419063b21675" in namespace "containers-3420" to be "running"
Mar  1 13:05:28.346: INFO: Pod "client-containers-f1c1ac13-4d5b-4052-b4b9-419063b21675": Phase="Pending", Reason="", readiness=false. Elapsed: 7.332323ms
Mar  1 13:05:30.352: INFO: Pod "client-containers-f1c1ac13-4d5b-4052-b4b9-419063b21675": Phase="Running", Reason="", readiness=true. Elapsed: 2.013467688s
Mar  1 13:05:30.352: INFO: Pod "client-containers-f1c1ac13-4d5b-4052-b4b9-419063b21675" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Mar  1 13:05:30.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3420" for this suite. 03/01/23 13:05:30.37
{"msg":"PASSED [sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","completed":270,"skipped":5013,"failed":0}
------------------------------
â€¢ [2.084 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:05:28.297
    Mar  1 13:05:28.297: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename containers 03/01/23 13:05:28.298
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:05:28.318
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:05:28.324
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:38
    Mar  1 13:05:28.338: INFO: Waiting up to 5m0s for pod "client-containers-f1c1ac13-4d5b-4052-b4b9-419063b21675" in namespace "containers-3420" to be "running"
    Mar  1 13:05:28.346: INFO: Pod "client-containers-f1c1ac13-4d5b-4052-b4b9-419063b21675": Phase="Pending", Reason="", readiness=false. Elapsed: 7.332323ms
    Mar  1 13:05:30.352: INFO: Pod "client-containers-f1c1ac13-4d5b-4052-b4b9-419063b21675": Phase="Running", Reason="", readiness=true. Elapsed: 2.013467688s
    Mar  1 13:05:30.352: INFO: Pod "client-containers-f1c1ac13-4d5b-4052-b4b9-419063b21675" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Mar  1 13:05:30.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-3420" for this suite. 03/01/23 13:05:30.37
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:05:30.383
Mar  1 13:05:30.383: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename services 03/01/23 13:05:30.384
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:05:30.415
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:05:30.421
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
STEP: creating service multi-endpoint-test in namespace services-3818 03/01/23 13:05:30.428
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3818 to expose endpoints map[] 03/01/23 13:05:30.446
Mar  1 13:05:30.459: INFO: successfully validated that service multi-endpoint-test in namespace services-3818 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-3818 03/01/23 13:05:30.459
Mar  1 13:05:30.471: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-3818" to be "running and ready"
Mar  1 13:05:30.479: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.336121ms
Mar  1 13:05:30.480: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Mar  1 13:05:32.486: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.015400037s
Mar  1 13:05:32.486: INFO: The phase of Pod pod1 is Running (Ready = true)
Mar  1 13:05:32.486: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3818 to expose endpoints map[pod1:[100]] 03/01/23 13:05:32.492
Mar  1 13:05:32.508: INFO: successfully validated that service multi-endpoint-test in namespace services-3818 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-3818 03/01/23 13:05:32.508
Mar  1 13:05:32.516: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-3818" to be "running and ready"
Mar  1 13:05:32.524: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.435214ms
Mar  1 13:05:32.525: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Mar  1 13:05:34.532: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.015702579s
Mar  1 13:05:34.532: INFO: The phase of Pod pod2 is Running (Ready = true)
Mar  1 13:05:34.532: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3818 to expose endpoints map[pod1:[100] pod2:[101]] 03/01/23 13:05:34.536
Mar  1 13:05:34.559: INFO: successfully validated that service multi-endpoint-test in namespace services-3818 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 03/01/23 13:05:34.559
Mar  1 13:05:34.559: INFO: Creating new exec pod
Mar  1 13:05:34.565: INFO: Waiting up to 5m0s for pod "execpod4b2hk" in namespace "services-3818" to be "running"
Mar  1 13:05:34.572: INFO: Pod "execpod4b2hk": Phase="Pending", Reason="", readiness=false. Elapsed: 7.279521ms
Mar  1 13:05:36.582: INFO: Pod "execpod4b2hk": Phase="Running", Reason="", readiness=true. Elapsed: 2.017606608s
Mar  1 13:05:36.583: INFO: Pod "execpod4b2hk" satisfied condition "running"
Mar  1 13:05:37.584: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-3818 exec execpod4b2hk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
Mar  1 13:05:37.717: INFO: stderr: "+ nc -v -t -w 2 multi-endpoint-test 80\n+ echo hostName\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Mar  1 13:05:37.717: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  1 13:05:37.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-3818 exec execpod4b2hk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.41.114 80'
Mar  1 13:05:37.850: INFO: stderr: "+ + echonc hostName\n -v -t -w 2 10.233.41.114 80\nConnection to 10.233.41.114 80 port [tcp/http] succeeded!\n"
Mar  1 13:05:37.850: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  1 13:05:37.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-3818 exec execpod4b2hk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
Mar  1 13:05:37.990: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Mar  1 13:05:37.990: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  1 13:05:37.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-3818 exec execpod4b2hk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.41.114 81'
Mar  1 13:05:38.160: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.41.114 81\nConnection to 10.233.41.114 81 port [tcp/*] succeeded!\n"
Mar  1 13:05:38.160: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-3818 03/01/23 13:05:38.16
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3818 to expose endpoints map[pod2:[101]] 03/01/23 13:05:38.201
Mar  1 13:05:39.235: INFO: successfully validated that service multi-endpoint-test in namespace services-3818 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-3818 03/01/23 13:05:39.235
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3818 to expose endpoints map[] 03/01/23 13:05:39.261
Mar  1 13:05:39.282: INFO: successfully validated that service multi-endpoint-test in namespace services-3818 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar  1 13:05:39.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3818" for this suite. 03/01/23 13:05:39.326
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","completed":271,"skipped":5025,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.957 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:05:30.383
    Mar  1 13:05:30.383: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename services 03/01/23 13:05:30.384
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:05:30.415
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:05:30.421
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:852
    STEP: creating service multi-endpoint-test in namespace services-3818 03/01/23 13:05:30.428
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3818 to expose endpoints map[] 03/01/23 13:05:30.446
    Mar  1 13:05:30.459: INFO: successfully validated that service multi-endpoint-test in namespace services-3818 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-3818 03/01/23 13:05:30.459
    Mar  1 13:05:30.471: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-3818" to be "running and ready"
    Mar  1 13:05:30.479: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.336121ms
    Mar  1 13:05:30.480: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Mar  1 13:05:32.486: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.015400037s
    Mar  1 13:05:32.486: INFO: The phase of Pod pod1 is Running (Ready = true)
    Mar  1 13:05:32.486: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3818 to expose endpoints map[pod1:[100]] 03/01/23 13:05:32.492
    Mar  1 13:05:32.508: INFO: successfully validated that service multi-endpoint-test in namespace services-3818 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-3818 03/01/23 13:05:32.508
    Mar  1 13:05:32.516: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-3818" to be "running and ready"
    Mar  1 13:05:32.524: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.435214ms
    Mar  1 13:05:32.525: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Mar  1 13:05:34.532: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.015702579s
    Mar  1 13:05:34.532: INFO: The phase of Pod pod2 is Running (Ready = true)
    Mar  1 13:05:34.532: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3818 to expose endpoints map[pod1:[100] pod2:[101]] 03/01/23 13:05:34.536
    Mar  1 13:05:34.559: INFO: successfully validated that service multi-endpoint-test in namespace services-3818 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 03/01/23 13:05:34.559
    Mar  1 13:05:34.559: INFO: Creating new exec pod
    Mar  1 13:05:34.565: INFO: Waiting up to 5m0s for pod "execpod4b2hk" in namespace "services-3818" to be "running"
    Mar  1 13:05:34.572: INFO: Pod "execpod4b2hk": Phase="Pending", Reason="", readiness=false. Elapsed: 7.279521ms
    Mar  1 13:05:36.582: INFO: Pod "execpod4b2hk": Phase="Running", Reason="", readiness=true. Elapsed: 2.017606608s
    Mar  1 13:05:36.583: INFO: Pod "execpod4b2hk" satisfied condition "running"
    Mar  1 13:05:37.584: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-3818 exec execpod4b2hk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
    Mar  1 13:05:37.717: INFO: stderr: "+ nc -v -t -w 2 multi-endpoint-test 80\n+ echo hostName\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Mar  1 13:05:37.717: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  1 13:05:37.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-3818 exec execpod4b2hk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.41.114 80'
    Mar  1 13:05:37.850: INFO: stderr: "+ + echonc hostName\n -v -t -w 2 10.233.41.114 80\nConnection to 10.233.41.114 80 port [tcp/http] succeeded!\n"
    Mar  1 13:05:37.850: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  1 13:05:37.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-3818 exec execpod4b2hk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
    Mar  1 13:05:37.990: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Mar  1 13:05:37.990: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  1 13:05:37.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-3818 exec execpod4b2hk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.41.114 81'
    Mar  1 13:05:38.160: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.41.114 81\nConnection to 10.233.41.114 81 port [tcp/*] succeeded!\n"
    Mar  1 13:05:38.160: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-3818 03/01/23 13:05:38.16
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3818 to expose endpoints map[pod2:[101]] 03/01/23 13:05:38.201
    Mar  1 13:05:39.235: INFO: successfully validated that service multi-endpoint-test in namespace services-3818 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-3818 03/01/23 13:05:39.235
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3818 to expose endpoints map[] 03/01/23 13:05:39.261
    Mar  1 13:05:39.282: INFO: successfully validated that service multi-endpoint-test in namespace services-3818 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar  1 13:05:39.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3818" for this suite. 03/01/23 13:05:39.326
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:05:39.342
Mar  1 13:05:39.343: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename crd-publish-openapi 03/01/23 13:05:39.343
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:05:39.363
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:05:39.367
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 03/01/23 13:05:39.371
Mar  1 13:05:39.371: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 03/01/23 13:05:56.921
Mar  1 13:05:56.922: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
Mar  1 13:06:04.676: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  1 13:06:20.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6552" for this suite. 03/01/23 13:06:20.429
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","completed":272,"skipped":5034,"failed":0}
------------------------------
â€¢ [SLOW TEST] [41.095 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:05:39.342
    Mar  1 13:05:39.343: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename crd-publish-openapi 03/01/23 13:05:39.343
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:05:39.363
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:05:39.367
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:308
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 03/01/23 13:05:39.371
    Mar  1 13:05:39.371: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 03/01/23 13:05:56.921
    Mar  1 13:05:56.922: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    Mar  1 13:06:04.676: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  1 13:06:20.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-6552" for this suite. 03/01/23 13:06:20.429
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:06:20.439
Mar  1 13:06:20.439: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename svcaccounts 03/01/23 13:06:20.44
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:06:20.46
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:06:20.463
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
Mar  1 13:06:20.481: INFO: Waiting up to 5m0s for pod "pod-service-account-326a778b-1ac4-4e6a-8adb-88c303f46339" in namespace "svcaccounts-569" to be "running"
Mar  1 13:06:20.486: INFO: Pod "pod-service-account-326a778b-1ac4-4e6a-8adb-88c303f46339": Phase="Pending", Reason="", readiness=false. Elapsed: 4.830848ms
Mar  1 13:06:22.490: INFO: Pod "pod-service-account-326a778b-1ac4-4e6a-8adb-88c303f46339": Phase="Running", Reason="", readiness=true. Elapsed: 2.00951383s
Mar  1 13:06:22.491: INFO: Pod "pod-service-account-326a778b-1ac4-4e6a-8adb-88c303f46339" satisfied condition "running"
STEP: reading a file in the container 03/01/23 13:06:22.491
Mar  1 13:06:22.491: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-569 pod-service-account-326a778b-1ac4-4e6a-8adb-88c303f46339 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 03/01/23 13:06:22.625
Mar  1 13:06:22.625: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-569 pod-service-account-326a778b-1ac4-4e6a-8adb-88c303f46339 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 03/01/23 13:06:22.767
Mar  1 13:06:22.767: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-569 pod-service-account-326a778b-1ac4-4e6a-8adb-88c303f46339 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Mar  1 13:06:22.912: INFO: Got root ca configmap in namespace "svcaccounts-569"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Mar  1 13:06:22.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-569" for this suite. 03/01/23 13:06:22.919
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","completed":273,"skipped":5039,"failed":0}
------------------------------
â€¢ [2.489 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:06:20.439
    Mar  1 13:06:20.439: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename svcaccounts 03/01/23 13:06:20.44
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:06:20.46
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:06:20.463
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:75
    Mar  1 13:06:20.481: INFO: Waiting up to 5m0s for pod "pod-service-account-326a778b-1ac4-4e6a-8adb-88c303f46339" in namespace "svcaccounts-569" to be "running"
    Mar  1 13:06:20.486: INFO: Pod "pod-service-account-326a778b-1ac4-4e6a-8adb-88c303f46339": Phase="Pending", Reason="", readiness=false. Elapsed: 4.830848ms
    Mar  1 13:06:22.490: INFO: Pod "pod-service-account-326a778b-1ac4-4e6a-8adb-88c303f46339": Phase="Running", Reason="", readiness=true. Elapsed: 2.00951383s
    Mar  1 13:06:22.491: INFO: Pod "pod-service-account-326a778b-1ac4-4e6a-8adb-88c303f46339" satisfied condition "running"
    STEP: reading a file in the container 03/01/23 13:06:22.491
    Mar  1 13:06:22.491: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-569 pod-service-account-326a778b-1ac4-4e6a-8adb-88c303f46339 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 03/01/23 13:06:22.625
    Mar  1 13:06:22.625: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-569 pod-service-account-326a778b-1ac4-4e6a-8adb-88c303f46339 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 03/01/23 13:06:22.767
    Mar  1 13:06:22.767: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-569 pod-service-account-326a778b-1ac4-4e6a-8adb-88c303f46339 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Mar  1 13:06:22.912: INFO: Got root ca configmap in namespace "svcaccounts-569"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Mar  1 13:06:22.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-569" for this suite. 03/01/23 13:06:22.919
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:06:22.929
Mar  1 13:06:22.929: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename proxy 03/01/23 13:06:22.93
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:06:22.951
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:06:22.954
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Mar  1 13:06:22.956: INFO: Creating pod...
Mar  1 13:06:22.965: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-812" to be "running"
Mar  1 13:06:22.970: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 4.818959ms
Mar  1 13:06:24.976: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.010159113s
Mar  1 13:06:24.976: INFO: Pod "agnhost" satisfied condition "running"
Mar  1 13:06:24.976: INFO: Creating service...
Mar  1 13:06:24.991: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-812/pods/agnhost/proxy/some/path/with/DELETE
Mar  1 13:06:24.999: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Mar  1 13:06:24.999: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-812/pods/agnhost/proxy/some/path/with/GET
Mar  1 13:06:25.006: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Mar  1 13:06:25.006: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-812/pods/agnhost/proxy/some/path/with/HEAD
Mar  1 13:06:25.012: INFO: http.Client request:HEAD | StatusCode:200
Mar  1 13:06:25.013: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-812/pods/agnhost/proxy/some/path/with/OPTIONS
Mar  1 13:06:25.017: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Mar  1 13:06:25.017: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-812/pods/agnhost/proxy/some/path/with/PATCH
Mar  1 13:06:25.022: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Mar  1 13:06:25.022: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-812/pods/agnhost/proxy/some/path/with/POST
Mar  1 13:06:25.027: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Mar  1 13:06:25.027: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-812/pods/agnhost/proxy/some/path/with/PUT
Mar  1 13:06:25.032: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Mar  1 13:06:25.032: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-812/services/test-service/proxy/some/path/with/DELETE
Mar  1 13:06:25.039: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Mar  1 13:06:25.039: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-812/services/test-service/proxy/some/path/with/GET
Mar  1 13:06:25.049: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Mar  1 13:06:25.049: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-812/services/test-service/proxy/some/path/with/HEAD
Mar  1 13:06:25.061: INFO: http.Client request:HEAD | StatusCode:200
Mar  1 13:06:25.061: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-812/services/test-service/proxy/some/path/with/OPTIONS
Mar  1 13:06:25.073: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Mar  1 13:06:25.073: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-812/services/test-service/proxy/some/path/with/PATCH
Mar  1 13:06:25.081: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Mar  1 13:06:25.081: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-812/services/test-service/proxy/some/path/with/POST
Mar  1 13:06:25.088: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Mar  1 13:06:25.089: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-812/services/test-service/proxy/some/path/with/PUT
Mar  1 13:06:25.097: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Mar  1 13:06:25.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-812" for this suite. 03/01/23 13:06:25.103
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","completed":274,"skipped":5046,"failed":0}
------------------------------
â€¢ [2.182 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:06:22.929
    Mar  1 13:06:22.929: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename proxy 03/01/23 13:06:22.93
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:06:22.951
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:06:22.954
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Mar  1 13:06:22.956: INFO: Creating pod...
    Mar  1 13:06:22.965: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-812" to be "running"
    Mar  1 13:06:22.970: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 4.818959ms
    Mar  1 13:06:24.976: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.010159113s
    Mar  1 13:06:24.976: INFO: Pod "agnhost" satisfied condition "running"
    Mar  1 13:06:24.976: INFO: Creating service...
    Mar  1 13:06:24.991: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-812/pods/agnhost/proxy/some/path/with/DELETE
    Mar  1 13:06:24.999: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Mar  1 13:06:24.999: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-812/pods/agnhost/proxy/some/path/with/GET
    Mar  1 13:06:25.006: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Mar  1 13:06:25.006: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-812/pods/agnhost/proxy/some/path/with/HEAD
    Mar  1 13:06:25.012: INFO: http.Client request:HEAD | StatusCode:200
    Mar  1 13:06:25.013: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-812/pods/agnhost/proxy/some/path/with/OPTIONS
    Mar  1 13:06:25.017: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Mar  1 13:06:25.017: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-812/pods/agnhost/proxy/some/path/with/PATCH
    Mar  1 13:06:25.022: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Mar  1 13:06:25.022: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-812/pods/agnhost/proxy/some/path/with/POST
    Mar  1 13:06:25.027: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Mar  1 13:06:25.027: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-812/pods/agnhost/proxy/some/path/with/PUT
    Mar  1 13:06:25.032: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Mar  1 13:06:25.032: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-812/services/test-service/proxy/some/path/with/DELETE
    Mar  1 13:06:25.039: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Mar  1 13:06:25.039: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-812/services/test-service/proxy/some/path/with/GET
    Mar  1 13:06:25.049: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Mar  1 13:06:25.049: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-812/services/test-service/proxy/some/path/with/HEAD
    Mar  1 13:06:25.061: INFO: http.Client request:HEAD | StatusCode:200
    Mar  1 13:06:25.061: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-812/services/test-service/proxy/some/path/with/OPTIONS
    Mar  1 13:06:25.073: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Mar  1 13:06:25.073: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-812/services/test-service/proxy/some/path/with/PATCH
    Mar  1 13:06:25.081: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Mar  1 13:06:25.081: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-812/services/test-service/proxy/some/path/with/POST
    Mar  1 13:06:25.088: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Mar  1 13:06:25.089: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-812/services/test-service/proxy/some/path/with/PUT
    Mar  1 13:06:25.097: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Mar  1 13:06:25.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-812" for this suite. 03/01/23 13:06:25.103
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:06:25.116
Mar  1 13:06:25.116: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename downward-api 03/01/23 13:06:25.116
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:06:25.137
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:06:25.14
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
STEP: Creating a pod to test downward API volume plugin 03/01/23 13:06:25.143
Mar  1 13:06:25.155: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0ab5c47b-c8e2-4f0c-a17d-72bd2da6ece3" in namespace "downward-api-6396" to be "Succeeded or Failed"
Mar  1 13:06:25.166: INFO: Pod "downwardapi-volume-0ab5c47b-c8e2-4f0c-a17d-72bd2da6ece3": Phase="Pending", Reason="", readiness=false. Elapsed: 11.069855ms
Mar  1 13:06:27.173: INFO: Pod "downwardapi-volume-0ab5c47b-c8e2-4f0c-a17d-72bd2da6ece3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017723026s
Mar  1 13:06:29.172: INFO: Pod "downwardapi-volume-0ab5c47b-c8e2-4f0c-a17d-72bd2da6ece3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017210394s
STEP: Saw pod success 03/01/23 13:06:29.172
Mar  1 13:06:29.173: INFO: Pod "downwardapi-volume-0ab5c47b-c8e2-4f0c-a17d-72bd2da6ece3" satisfied condition "Succeeded or Failed"
Mar  1 13:06:29.177: INFO: Trying to get logs from node lab1-k8s-node-3 pod downwardapi-volume-0ab5c47b-c8e2-4f0c-a17d-72bd2da6ece3 container client-container: <nil>
STEP: delete the pod 03/01/23 13:06:29.193
Mar  1 13:06:29.215: INFO: Waiting for pod downwardapi-volume-0ab5c47b-c8e2-4f0c-a17d-72bd2da6ece3 to disappear
Mar  1 13:06:29.219: INFO: Pod downwardapi-volume-0ab5c47b-c8e2-4f0c-a17d-72bd2da6ece3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Mar  1 13:06:29.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6396" for this suite. 03/01/23 13:06:29.226
{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":275,"skipped":5067,"failed":0}
------------------------------
â€¢ [4.120 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:06:25.116
    Mar  1 13:06:25.116: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename downward-api 03/01/23 13:06:25.116
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:06:25.137
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:06:25.14
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:83
    STEP: Creating a pod to test downward API volume plugin 03/01/23 13:06:25.143
    Mar  1 13:06:25.155: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0ab5c47b-c8e2-4f0c-a17d-72bd2da6ece3" in namespace "downward-api-6396" to be "Succeeded or Failed"
    Mar  1 13:06:25.166: INFO: Pod "downwardapi-volume-0ab5c47b-c8e2-4f0c-a17d-72bd2da6ece3": Phase="Pending", Reason="", readiness=false. Elapsed: 11.069855ms
    Mar  1 13:06:27.173: INFO: Pod "downwardapi-volume-0ab5c47b-c8e2-4f0c-a17d-72bd2da6ece3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017723026s
    Mar  1 13:06:29.172: INFO: Pod "downwardapi-volume-0ab5c47b-c8e2-4f0c-a17d-72bd2da6ece3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017210394s
    STEP: Saw pod success 03/01/23 13:06:29.172
    Mar  1 13:06:29.173: INFO: Pod "downwardapi-volume-0ab5c47b-c8e2-4f0c-a17d-72bd2da6ece3" satisfied condition "Succeeded or Failed"
    Mar  1 13:06:29.177: INFO: Trying to get logs from node lab1-k8s-node-3 pod downwardapi-volume-0ab5c47b-c8e2-4f0c-a17d-72bd2da6ece3 container client-container: <nil>
    STEP: delete the pod 03/01/23 13:06:29.193
    Mar  1 13:06:29.215: INFO: Waiting for pod downwardapi-volume-0ab5c47b-c8e2-4f0c-a17d-72bd2da6ece3 to disappear
    Mar  1 13:06:29.219: INFO: Pod downwardapi-volume-0ab5c47b-c8e2-4f0c-a17d-72bd2da6ece3 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Mar  1 13:06:29.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-6396" for this suite. 03/01/23 13:06:29.226
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:06:29.239
Mar  1 13:06:29.239: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename statefulset 03/01/23 13:06:29.24
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:06:29.261
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:06:29.264
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-7062 03/01/23 13:06:29.267
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
STEP: Creating a new StatefulSet 03/01/23 13:06:29.272
Mar  1 13:06:29.288: INFO: Found 0 stateful pods, waiting for 3
Mar  1 13:06:39.294: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  1 13:06:39.294: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  1 13:06:39.294: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 03/01/23 13:06:39.307
Mar  1 13:06:39.331: INFO: Updating stateful set ss2
STEP: Creating a new revision 03/01/23 13:06:39.331
STEP: Not applying an update when the partition is greater than the number of replicas 03/01/23 13:06:49.359
STEP: Performing a canary update 03/01/23 13:06:49.36
Mar  1 13:06:49.382: INFO: Updating stateful set ss2
Mar  1 13:06:49.394: INFO: Waiting for Pod statefulset-7062/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
STEP: Restoring Pods to the correct revision when they are deleted 03/01/23 13:06:59.404
Mar  1 13:06:59.452: INFO: Found 1 stateful pods, waiting for 3
Mar  1 13:07:09.458: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  1 13:07:09.458: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  1 13:07:09.458: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 03/01/23 13:07:09.467
Mar  1 13:07:09.491: INFO: Updating stateful set ss2
Mar  1 13:07:09.509: INFO: Waiting for Pod statefulset-7062/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
Mar  1 13:07:19.542: INFO: Updating stateful set ss2
Mar  1 13:07:19.576: INFO: Waiting for StatefulSet statefulset-7062/ss2 to complete update
Mar  1 13:07:19.576: INFO: Waiting for Pod statefulset-7062/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Mar  1 13:07:29.589: INFO: Deleting all statefulset in ns statefulset-7062
Mar  1 13:07:29.592: INFO: Scaling statefulset ss2 to 0
Mar  1 13:07:39.614: INFO: Waiting for statefulset status.replicas updated to 0
Mar  1 13:07:39.618: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Mar  1 13:07:39.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7062" for this suite. 03/01/23 13:07:39.643
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","completed":276,"skipped":5078,"failed":0}
------------------------------
â€¢ [SLOW TEST] [70.413 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:315

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:06:29.239
    Mar  1 13:06:29.239: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename statefulset 03/01/23 13:06:29.24
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:06:29.261
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:06:29.264
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-7062 03/01/23 13:06:29.267
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:315
    STEP: Creating a new StatefulSet 03/01/23 13:06:29.272
    Mar  1 13:06:29.288: INFO: Found 0 stateful pods, waiting for 3
    Mar  1 13:06:39.294: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Mar  1 13:06:39.294: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Mar  1 13:06:39.294: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 03/01/23 13:06:39.307
    Mar  1 13:06:39.331: INFO: Updating stateful set ss2
    STEP: Creating a new revision 03/01/23 13:06:39.331
    STEP: Not applying an update when the partition is greater than the number of replicas 03/01/23 13:06:49.359
    STEP: Performing a canary update 03/01/23 13:06:49.36
    Mar  1 13:06:49.382: INFO: Updating stateful set ss2
    Mar  1 13:06:49.394: INFO: Waiting for Pod statefulset-7062/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    STEP: Restoring Pods to the correct revision when they are deleted 03/01/23 13:06:59.404
    Mar  1 13:06:59.452: INFO: Found 1 stateful pods, waiting for 3
    Mar  1 13:07:09.458: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Mar  1 13:07:09.458: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Mar  1 13:07:09.458: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 03/01/23 13:07:09.467
    Mar  1 13:07:09.491: INFO: Updating stateful set ss2
    Mar  1 13:07:09.509: INFO: Waiting for Pod statefulset-7062/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    Mar  1 13:07:19.542: INFO: Updating stateful set ss2
    Mar  1 13:07:19.576: INFO: Waiting for StatefulSet statefulset-7062/ss2 to complete update
    Mar  1 13:07:19.576: INFO: Waiting for Pod statefulset-7062/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Mar  1 13:07:29.589: INFO: Deleting all statefulset in ns statefulset-7062
    Mar  1 13:07:29.592: INFO: Scaling statefulset ss2 to 0
    Mar  1 13:07:39.614: INFO: Waiting for statefulset status.replicas updated to 0
    Mar  1 13:07:39.618: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Mar  1 13:07:39.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-7062" for this suite. 03/01/23 13:07:39.643
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:07:39.656
Mar  1 13:07:39.657: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename emptydir 03/01/23 13:07:39.657
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:07:39.676
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:07:39.679
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
STEP: Creating a pod to test emptydir 0777 on tmpfs 03/01/23 13:07:39.681
Mar  1 13:07:39.692: INFO: Waiting up to 5m0s for pod "pod-13e2d247-f4af-4b05-93a3-2f2dbe2eac22" in namespace "emptydir-9941" to be "Succeeded or Failed"
Mar  1 13:07:39.696: INFO: Pod "pod-13e2d247-f4af-4b05-93a3-2f2dbe2eac22": Phase="Pending", Reason="", readiness=false. Elapsed: 4.367963ms
Mar  1 13:07:41.701: INFO: Pod "pod-13e2d247-f4af-4b05-93a3-2f2dbe2eac22": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009190392s
Mar  1 13:07:43.701: INFO: Pod "pod-13e2d247-f4af-4b05-93a3-2f2dbe2eac22": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009675888s
STEP: Saw pod success 03/01/23 13:07:43.702
Mar  1 13:07:43.702: INFO: Pod "pod-13e2d247-f4af-4b05-93a3-2f2dbe2eac22" satisfied condition "Succeeded or Failed"
Mar  1 13:07:43.707: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-13e2d247-f4af-4b05-93a3-2f2dbe2eac22 container test-container: <nil>
STEP: delete the pod 03/01/23 13:07:43.717
Mar  1 13:07:43.736: INFO: Waiting for pod pod-13e2d247-f4af-4b05-93a3-2f2dbe2eac22 to disappear
Mar  1 13:07:43.739: INFO: Pod pod-13e2d247-f4af-4b05-93a3-2f2dbe2eac22 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar  1 13:07:43.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9941" for this suite. 03/01/23 13:07:43.745
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":277,"skipped":5141,"failed":0}
------------------------------
â€¢ [4.096 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:07:39.656
    Mar  1 13:07:39.657: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename emptydir 03/01/23 13:07:39.657
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:07:39.676
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:07:39.679
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:146
    STEP: Creating a pod to test emptydir 0777 on tmpfs 03/01/23 13:07:39.681
    Mar  1 13:07:39.692: INFO: Waiting up to 5m0s for pod "pod-13e2d247-f4af-4b05-93a3-2f2dbe2eac22" in namespace "emptydir-9941" to be "Succeeded or Failed"
    Mar  1 13:07:39.696: INFO: Pod "pod-13e2d247-f4af-4b05-93a3-2f2dbe2eac22": Phase="Pending", Reason="", readiness=false. Elapsed: 4.367963ms
    Mar  1 13:07:41.701: INFO: Pod "pod-13e2d247-f4af-4b05-93a3-2f2dbe2eac22": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009190392s
    Mar  1 13:07:43.701: INFO: Pod "pod-13e2d247-f4af-4b05-93a3-2f2dbe2eac22": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009675888s
    STEP: Saw pod success 03/01/23 13:07:43.702
    Mar  1 13:07:43.702: INFO: Pod "pod-13e2d247-f4af-4b05-93a3-2f2dbe2eac22" satisfied condition "Succeeded or Failed"
    Mar  1 13:07:43.707: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-13e2d247-f4af-4b05-93a3-2f2dbe2eac22 container test-container: <nil>
    STEP: delete the pod 03/01/23 13:07:43.717
    Mar  1 13:07:43.736: INFO: Waiting for pod pod-13e2d247-f4af-4b05-93a3-2f2dbe2eac22 to disappear
    Mar  1 13:07:43.739: INFO: Pod pod-13e2d247-f4af-4b05-93a3-2f2dbe2eac22 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar  1 13:07:43.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-9941" for this suite. 03/01/23 13:07:43.745
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:07:43.76
Mar  1 13:07:43.760: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename projected 03/01/23 13:07:43.761
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:07:43.781
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:07:43.784
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
STEP: Creating the pod 03/01/23 13:07:43.787
Mar  1 13:07:43.796: INFO: Waiting up to 5m0s for pod "annotationupdate13d9bdd2-9660-44ce-a279-fd56feaea40a" in namespace "projected-8268" to be "running and ready"
Mar  1 13:07:43.803: INFO: Pod "annotationupdate13d9bdd2-9660-44ce-a279-fd56feaea40a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.694178ms
Mar  1 13:07:43.803: INFO: The phase of Pod annotationupdate13d9bdd2-9660-44ce-a279-fd56feaea40a is Pending, waiting for it to be Running (with Ready = true)
Mar  1 13:07:45.809: INFO: Pod "annotationupdate13d9bdd2-9660-44ce-a279-fd56feaea40a": Phase="Running", Reason="", readiness=true. Elapsed: 2.012653954s
Mar  1 13:07:45.809: INFO: The phase of Pod annotationupdate13d9bdd2-9660-44ce-a279-fd56feaea40a is Running (Ready = true)
Mar  1 13:07:45.809: INFO: Pod "annotationupdate13d9bdd2-9660-44ce-a279-fd56feaea40a" satisfied condition "running and ready"
Mar  1 13:07:46.337: INFO: Successfully updated pod "annotationupdate13d9bdd2-9660-44ce-a279-fd56feaea40a"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Mar  1 13:07:50.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8268" for this suite. 03/01/23 13:07:50.372
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","completed":278,"skipped":5168,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.622 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:07:43.76
    Mar  1 13:07:43.760: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename projected 03/01/23 13:07:43.761
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:07:43.781
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:07:43.784
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:161
    STEP: Creating the pod 03/01/23 13:07:43.787
    Mar  1 13:07:43.796: INFO: Waiting up to 5m0s for pod "annotationupdate13d9bdd2-9660-44ce-a279-fd56feaea40a" in namespace "projected-8268" to be "running and ready"
    Mar  1 13:07:43.803: INFO: Pod "annotationupdate13d9bdd2-9660-44ce-a279-fd56feaea40a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.694178ms
    Mar  1 13:07:43.803: INFO: The phase of Pod annotationupdate13d9bdd2-9660-44ce-a279-fd56feaea40a is Pending, waiting for it to be Running (with Ready = true)
    Mar  1 13:07:45.809: INFO: Pod "annotationupdate13d9bdd2-9660-44ce-a279-fd56feaea40a": Phase="Running", Reason="", readiness=true. Elapsed: 2.012653954s
    Mar  1 13:07:45.809: INFO: The phase of Pod annotationupdate13d9bdd2-9660-44ce-a279-fd56feaea40a is Running (Ready = true)
    Mar  1 13:07:45.809: INFO: Pod "annotationupdate13d9bdd2-9660-44ce-a279-fd56feaea40a" satisfied condition "running and ready"
    Mar  1 13:07:46.337: INFO: Successfully updated pod "annotationupdate13d9bdd2-9660-44ce-a279-fd56feaea40a"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Mar  1 13:07:50.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8268" for this suite. 03/01/23 13:07:50.372
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:07:50.387
Mar  1 13:07:50.387: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename init-container 03/01/23 13:07:50.388
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:07:50.412
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:07:50.414
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
STEP: creating the pod 03/01/23 13:07:50.417
Mar  1 13:07:50.417: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Mar  1 13:07:54.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9726" for this suite. 03/01/23 13:07:54.735
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","completed":279,"skipped":5211,"failed":0}
------------------------------
â€¢ [4.356 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:07:50.387
    Mar  1 13:07:50.387: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename init-container 03/01/23 13:07:50.388
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:07:50.412
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:07:50.414
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:457
    STEP: creating the pod 03/01/23 13:07:50.417
    Mar  1 13:07:50.417: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Mar  1 13:07:54.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-9726" for this suite. 03/01/23 13:07:54.735
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:07:54.744
Mar  1 13:07:54.744: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename emptydir 03/01/23 13:07:54.745
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:07:54.768
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:07:54.771
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
STEP: Creating Pod 03/01/23 13:07:54.774
Mar  1 13:07:54.786: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-14d7ad55-d038-4866-a4b8-08ce368758f0" in namespace "emptydir-2459" to be "running"
Mar  1 13:07:54.790: INFO: Pod "pod-sharedvolume-14d7ad55-d038-4866-a4b8-08ce368758f0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.907134ms
Mar  1 13:07:56.796: INFO: Pod "pod-sharedvolume-14d7ad55-d038-4866-a4b8-08ce368758f0": Phase="Running", Reason="", readiness=false. Elapsed: 2.009590739s
Mar  1 13:07:56.796: INFO: Pod "pod-sharedvolume-14d7ad55-d038-4866-a4b8-08ce368758f0" satisfied condition "running"
STEP: Reading file content from the nginx-container 03/01/23 13:07:56.796
Mar  1 13:07:56.796: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-2459 PodName:pod-sharedvolume-14d7ad55-d038-4866-a4b8-08ce368758f0 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  1 13:07:56.796: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
Mar  1 13:07:56.797: INFO: ExecWithOptions: Clientset creation
Mar  1 13:07:56.797: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/emptydir-2459/pods/pod-sharedvolume-14d7ad55-d038-4866-a4b8-08ce368758f0/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Mar  1 13:07:56.862: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar  1 13:07:56.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2459" for this suite. 03/01/23 13:07:56.869
{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","completed":280,"skipped":5230,"failed":0}
------------------------------
â€¢ [2.134 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:07:54.744
    Mar  1 13:07:54.744: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename emptydir 03/01/23 13:07:54.745
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:07:54.768
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:07:54.771
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:226
    STEP: Creating Pod 03/01/23 13:07:54.774
    Mar  1 13:07:54.786: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-14d7ad55-d038-4866-a4b8-08ce368758f0" in namespace "emptydir-2459" to be "running"
    Mar  1 13:07:54.790: INFO: Pod "pod-sharedvolume-14d7ad55-d038-4866-a4b8-08ce368758f0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.907134ms
    Mar  1 13:07:56.796: INFO: Pod "pod-sharedvolume-14d7ad55-d038-4866-a4b8-08ce368758f0": Phase="Running", Reason="", readiness=false. Elapsed: 2.009590739s
    Mar  1 13:07:56.796: INFO: Pod "pod-sharedvolume-14d7ad55-d038-4866-a4b8-08ce368758f0" satisfied condition "running"
    STEP: Reading file content from the nginx-container 03/01/23 13:07:56.796
    Mar  1 13:07:56.796: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-2459 PodName:pod-sharedvolume-14d7ad55-d038-4866-a4b8-08ce368758f0 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  1 13:07:56.796: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    Mar  1 13:07:56.797: INFO: ExecWithOptions: Clientset creation
    Mar  1 13:07:56.797: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/emptydir-2459/pods/pod-sharedvolume-14d7ad55-d038-4866-a4b8-08ce368758f0/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Mar  1 13:07:56.862: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar  1 13:07:56.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-2459" for this suite. 03/01/23 13:07:56.869
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:07:56.88
Mar  1 13:07:56.880: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename crd-watch 03/01/23 13:07:56.881
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:07:56.904
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:07:56.907
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Mar  1 13:07:56.910: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Creating first CR  03/01/23 13:08:04.47
Mar  1 13:08:04.477: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-01T13:08:04Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-01T13:08:04Z]] name:name1 resourceVersion:42302 uid:144a1843-9ad2-4b56-ad9f-10342d10b7d4] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 03/01/23 13:08:14.477
Mar  1 13:08:14.485: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-01T13:08:14Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-01T13:08:14Z]] name:name2 resourceVersion:42333 uid:cb3388f0-3f8e-4c39-a06a-1712ef48df68] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 03/01/23 13:08:24.485
Mar  1 13:08:24.495: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-01T13:08:04Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-01T13:08:24Z]] name:name1 resourceVersion:42365 uid:144a1843-9ad2-4b56-ad9f-10342d10b7d4] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 03/01/23 13:08:34.495
Mar  1 13:08:34.503: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-01T13:08:14Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-01T13:08:34Z]] name:name2 resourceVersion:42396 uid:cb3388f0-3f8e-4c39-a06a-1712ef48df68] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 03/01/23 13:08:44.503
Mar  1 13:08:44.513: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-01T13:08:04Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-01T13:08:24Z]] name:name1 resourceVersion:42427 uid:144a1843-9ad2-4b56-ad9f-10342d10b7d4] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 03/01/23 13:08:54.514
Mar  1 13:08:54.528: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-01T13:08:14Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-01T13:08:34Z]] name:name2 resourceVersion:42458 uid:cb3388f0-3f8e-4c39-a06a-1712ef48df68] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  1 13:09:05.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-7546" for this suite. 03/01/23 13:09:05.053
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","completed":281,"skipped":5234,"failed":0}
------------------------------
â€¢ [SLOW TEST] [68.189 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:07:56.88
    Mar  1 13:07:56.880: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename crd-watch 03/01/23 13:07:56.881
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:07:56.904
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:07:56.907
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Mar  1 13:07:56.910: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Creating first CR  03/01/23 13:08:04.47
    Mar  1 13:08:04.477: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-01T13:08:04Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-01T13:08:04Z]] name:name1 resourceVersion:42302 uid:144a1843-9ad2-4b56-ad9f-10342d10b7d4] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 03/01/23 13:08:14.477
    Mar  1 13:08:14.485: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-01T13:08:14Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-01T13:08:14Z]] name:name2 resourceVersion:42333 uid:cb3388f0-3f8e-4c39-a06a-1712ef48df68] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 03/01/23 13:08:24.485
    Mar  1 13:08:24.495: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-01T13:08:04Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-01T13:08:24Z]] name:name1 resourceVersion:42365 uid:144a1843-9ad2-4b56-ad9f-10342d10b7d4] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 03/01/23 13:08:34.495
    Mar  1 13:08:34.503: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-01T13:08:14Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-01T13:08:34Z]] name:name2 resourceVersion:42396 uid:cb3388f0-3f8e-4c39-a06a-1712ef48df68] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 03/01/23 13:08:44.503
    Mar  1 13:08:44.513: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-01T13:08:04Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-01T13:08:24Z]] name:name1 resourceVersion:42427 uid:144a1843-9ad2-4b56-ad9f-10342d10b7d4] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 03/01/23 13:08:54.514
    Mar  1 13:08:54.528: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-01T13:08:14Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-01T13:08:34Z]] name:name2 resourceVersion:42458 uid:cb3388f0-3f8e-4c39-a06a-1712ef48df68] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  1 13:09:05.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-watch-7546" for this suite. 03/01/23 13:09:05.053
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:09:05.07
Mar  1 13:09:05.070: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename deployment 03/01/23 13:09:05.071
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:09:05.093
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:09:05.099
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 03/01/23 13:09:05.11
Mar  1 13:09:05.110: INFO: Creating simple deployment test-deployment-t4vnj
Mar  1 13:09:05.126: INFO: deployment "test-deployment-t4vnj" doesn't have the required revision set
STEP: Getting /status 03/01/23 13:09:07.144
Mar  1 13:09:07.150: INFO: Deployment test-deployment-t4vnj has Conditions: [{Available True 2023-03-01 13:09:06 +0000 UTC 2023-03-01 13:09:06 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-03-01 13:09:06 +0000 UTC 2023-03-01 13:09:05 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-t4vnj-777898ffcc" has successfully progressed.}]
STEP: updating Deployment Status 03/01/23 13:09:07.15
Mar  1 13:09:07.161: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 1, 13, 9, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 1, 13, 9, 6, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 1, 13, 9, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 1, 13, 9, 5, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-t4vnj-777898ffcc\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 03/01/23 13:09:07.162
Mar  1 13:09:07.163: INFO: Observed &Deployment event: ADDED
Mar  1 13:09:07.163: INFO: Observed Deployment test-deployment-t4vnj in namespace deployment-9395 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-01 13:09:05 +0000 UTC 2023-03-01 13:09:05 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-t4vnj-777898ffcc"}
Mar  1 13:09:07.163: INFO: Observed &Deployment event: MODIFIED
Mar  1 13:09:07.163: INFO: Observed Deployment test-deployment-t4vnj in namespace deployment-9395 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-01 13:09:05 +0000 UTC 2023-03-01 13:09:05 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-t4vnj-777898ffcc"}
Mar  1 13:09:07.164: INFO: Observed Deployment test-deployment-t4vnj in namespace deployment-9395 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-01 13:09:05 +0000 UTC 2023-03-01 13:09:05 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Mar  1 13:09:07.164: INFO: Observed &Deployment event: MODIFIED
Mar  1 13:09:07.164: INFO: Observed Deployment test-deployment-t4vnj in namespace deployment-9395 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-01 13:09:05 +0000 UTC 2023-03-01 13:09:05 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Mar  1 13:09:07.164: INFO: Observed Deployment test-deployment-t4vnj in namespace deployment-9395 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-01 13:09:05 +0000 UTC 2023-03-01 13:09:05 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-t4vnj-777898ffcc" is progressing.}
Mar  1 13:09:07.164: INFO: Observed &Deployment event: MODIFIED
Mar  1 13:09:07.164: INFO: Observed Deployment test-deployment-t4vnj in namespace deployment-9395 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-01 13:09:06 +0000 UTC 2023-03-01 13:09:06 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Mar  1 13:09:07.164: INFO: Observed Deployment test-deployment-t4vnj in namespace deployment-9395 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-01 13:09:06 +0000 UTC 2023-03-01 13:09:05 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-t4vnj-777898ffcc" has successfully progressed.}
Mar  1 13:09:07.164: INFO: Observed &Deployment event: MODIFIED
Mar  1 13:09:07.164: INFO: Observed Deployment test-deployment-t4vnj in namespace deployment-9395 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-01 13:09:06 +0000 UTC 2023-03-01 13:09:06 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Mar  1 13:09:07.164: INFO: Observed Deployment test-deployment-t4vnj in namespace deployment-9395 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-01 13:09:06 +0000 UTC 2023-03-01 13:09:05 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-t4vnj-777898ffcc" has successfully progressed.}
Mar  1 13:09:07.165: INFO: Found Deployment test-deployment-t4vnj in namespace deployment-9395 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Mar  1 13:09:07.165: INFO: Deployment test-deployment-t4vnj has an updated status
STEP: patching the Statefulset Status 03/01/23 13:09:07.165
Mar  1 13:09:07.165: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Mar  1 13:09:07.175: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 03/01/23 13:09:07.175
Mar  1 13:09:07.176: INFO: Observed &Deployment event: ADDED
Mar  1 13:09:07.176: INFO: Observed deployment test-deployment-t4vnj in namespace deployment-9395 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-01 13:09:05 +0000 UTC 2023-03-01 13:09:05 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-t4vnj-777898ffcc"}
Mar  1 13:09:07.177: INFO: Observed &Deployment event: MODIFIED
Mar  1 13:09:07.177: INFO: Observed deployment test-deployment-t4vnj in namespace deployment-9395 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-01 13:09:05 +0000 UTC 2023-03-01 13:09:05 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-t4vnj-777898ffcc"}
Mar  1 13:09:07.177: INFO: Observed deployment test-deployment-t4vnj in namespace deployment-9395 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-01 13:09:05 +0000 UTC 2023-03-01 13:09:05 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Mar  1 13:09:07.177: INFO: Observed &Deployment event: MODIFIED
Mar  1 13:09:07.177: INFO: Observed deployment test-deployment-t4vnj in namespace deployment-9395 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-01 13:09:05 +0000 UTC 2023-03-01 13:09:05 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Mar  1 13:09:07.177: INFO: Observed deployment test-deployment-t4vnj in namespace deployment-9395 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-01 13:09:05 +0000 UTC 2023-03-01 13:09:05 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-t4vnj-777898ffcc" is progressing.}
Mar  1 13:09:07.177: INFO: Observed &Deployment event: MODIFIED
Mar  1 13:09:07.177: INFO: Observed deployment test-deployment-t4vnj in namespace deployment-9395 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-01 13:09:06 +0000 UTC 2023-03-01 13:09:06 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Mar  1 13:09:07.177: INFO: Observed deployment test-deployment-t4vnj in namespace deployment-9395 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-01 13:09:06 +0000 UTC 2023-03-01 13:09:05 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-t4vnj-777898ffcc" has successfully progressed.}
Mar  1 13:09:07.178: INFO: Observed &Deployment event: MODIFIED
Mar  1 13:09:07.178: INFO: Observed deployment test-deployment-t4vnj in namespace deployment-9395 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-01 13:09:06 +0000 UTC 2023-03-01 13:09:06 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Mar  1 13:09:07.178: INFO: Observed deployment test-deployment-t4vnj in namespace deployment-9395 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-01 13:09:06 +0000 UTC 2023-03-01 13:09:05 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-t4vnj-777898ffcc" has successfully progressed.}
Mar  1 13:09:07.178: INFO: Observed deployment test-deployment-t4vnj in namespace deployment-9395 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Mar  1 13:09:07.178: INFO: Observed &Deployment event: MODIFIED
Mar  1 13:09:07.178: INFO: Found deployment test-deployment-t4vnj in namespace deployment-9395 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Mar  1 13:09:07.178: INFO: Deployment test-deployment-t4vnj has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar  1 13:09:07.184: INFO: Deployment "test-deployment-t4vnj":
&Deployment{ObjectMeta:{test-deployment-t4vnj  deployment-9395  96ee8545-5b9f-4095-a83f-e7dcd179e01d 42529 1 2023-03-01 13:09:05 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-03-01 13:09:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-03-01 13:09:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-03-01 13:09:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00435dfd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-t4vnj-777898ffcc",LastUpdateTime:2023-03-01 13:09:07 +0000 UTC,LastTransitionTime:2023-03-01 13:09:07 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar  1 13:09:07.191: INFO: New ReplicaSet "test-deployment-t4vnj-777898ffcc" of Deployment "test-deployment-t4vnj":
&ReplicaSet{ObjectMeta:{test-deployment-t4vnj-777898ffcc  deployment-9395  ab70aad7-8bdb-4fcc-bad7-79da9434fb35 42522 1 2023-03-01 13:09:05 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-t4vnj 96ee8545-5b9f-4095-a83f-e7dcd179e01d 0xc0028ce390 0xc0028ce391}] [] [{kube-controller-manager Update apps/v1 2023-03-01 13:09:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"96ee8545-5b9f-4095-a83f-e7dcd179e01d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-01 13:09:06 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 777898ffcc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0028ce438 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar  1 13:09:07.196: INFO: Pod "test-deployment-t4vnj-777898ffcc-5zqdt" is available:
&Pod{ObjectMeta:{test-deployment-t4vnj-777898ffcc-5zqdt test-deployment-t4vnj-777898ffcc- deployment-9395  0ae3ed6c-ff91-45fe-ae2b-b280a791071e 42521 0 2023-03-01 13:09:05 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[cni.projectcalico.org/containerID:7b1736473be591089735a3d17e1fcebf29bde3baf74aa57556fd57e4d1a90e9a cni.projectcalico.org/podIP:10.233.74.115/32 cni.projectcalico.org/podIPs:10.233.74.115/32] [{apps/v1 ReplicaSet test-deployment-t4vnj-777898ffcc ab70aad7-8bdb-4fcc-bad7-79da9434fb35 0xc0043e6740 0xc0043e6741}] [] [{calico Update v1 2023-03-01 13:09:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-01 13:09:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ab70aad7-8bdb-4fcc-bad7-79da9434fb35\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-01 13:09:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.74.115\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5r7kp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5r7kp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:09:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:09:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:09:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:09:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.2.241,PodIP:10.233.74.115,StartTime:2023-03-01 13:09:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-01 13:09:05 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://6953dc0b8aeebe402dcb7e1bbe869f248a388c3dc6a55ce28ad0da80e16cf79f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.74.115,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Mar  1 13:09:07.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9395" for this suite. 03/01/23 13:09:07.203
{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","completed":282,"skipped":5239,"failed":0}
------------------------------
â€¢ [2.143 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:09:05.07
    Mar  1 13:09:05.070: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename deployment 03/01/23 13:09:05.071
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:09:05.093
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:09:05.099
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 03/01/23 13:09:05.11
    Mar  1 13:09:05.110: INFO: Creating simple deployment test-deployment-t4vnj
    Mar  1 13:09:05.126: INFO: deployment "test-deployment-t4vnj" doesn't have the required revision set
    STEP: Getting /status 03/01/23 13:09:07.144
    Mar  1 13:09:07.150: INFO: Deployment test-deployment-t4vnj has Conditions: [{Available True 2023-03-01 13:09:06 +0000 UTC 2023-03-01 13:09:06 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-03-01 13:09:06 +0000 UTC 2023-03-01 13:09:05 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-t4vnj-777898ffcc" has successfully progressed.}]
    STEP: updating Deployment Status 03/01/23 13:09:07.15
    Mar  1 13:09:07.161: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 1, 13, 9, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 1, 13, 9, 6, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 1, 13, 9, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 1, 13, 9, 5, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-t4vnj-777898ffcc\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 03/01/23 13:09:07.162
    Mar  1 13:09:07.163: INFO: Observed &Deployment event: ADDED
    Mar  1 13:09:07.163: INFO: Observed Deployment test-deployment-t4vnj in namespace deployment-9395 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-01 13:09:05 +0000 UTC 2023-03-01 13:09:05 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-t4vnj-777898ffcc"}
    Mar  1 13:09:07.163: INFO: Observed &Deployment event: MODIFIED
    Mar  1 13:09:07.163: INFO: Observed Deployment test-deployment-t4vnj in namespace deployment-9395 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-01 13:09:05 +0000 UTC 2023-03-01 13:09:05 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-t4vnj-777898ffcc"}
    Mar  1 13:09:07.164: INFO: Observed Deployment test-deployment-t4vnj in namespace deployment-9395 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-01 13:09:05 +0000 UTC 2023-03-01 13:09:05 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Mar  1 13:09:07.164: INFO: Observed &Deployment event: MODIFIED
    Mar  1 13:09:07.164: INFO: Observed Deployment test-deployment-t4vnj in namespace deployment-9395 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-01 13:09:05 +0000 UTC 2023-03-01 13:09:05 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Mar  1 13:09:07.164: INFO: Observed Deployment test-deployment-t4vnj in namespace deployment-9395 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-01 13:09:05 +0000 UTC 2023-03-01 13:09:05 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-t4vnj-777898ffcc" is progressing.}
    Mar  1 13:09:07.164: INFO: Observed &Deployment event: MODIFIED
    Mar  1 13:09:07.164: INFO: Observed Deployment test-deployment-t4vnj in namespace deployment-9395 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-01 13:09:06 +0000 UTC 2023-03-01 13:09:06 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Mar  1 13:09:07.164: INFO: Observed Deployment test-deployment-t4vnj in namespace deployment-9395 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-01 13:09:06 +0000 UTC 2023-03-01 13:09:05 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-t4vnj-777898ffcc" has successfully progressed.}
    Mar  1 13:09:07.164: INFO: Observed &Deployment event: MODIFIED
    Mar  1 13:09:07.164: INFO: Observed Deployment test-deployment-t4vnj in namespace deployment-9395 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-01 13:09:06 +0000 UTC 2023-03-01 13:09:06 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Mar  1 13:09:07.164: INFO: Observed Deployment test-deployment-t4vnj in namespace deployment-9395 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-01 13:09:06 +0000 UTC 2023-03-01 13:09:05 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-t4vnj-777898ffcc" has successfully progressed.}
    Mar  1 13:09:07.165: INFO: Found Deployment test-deployment-t4vnj in namespace deployment-9395 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Mar  1 13:09:07.165: INFO: Deployment test-deployment-t4vnj has an updated status
    STEP: patching the Statefulset Status 03/01/23 13:09:07.165
    Mar  1 13:09:07.165: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Mar  1 13:09:07.175: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 03/01/23 13:09:07.175
    Mar  1 13:09:07.176: INFO: Observed &Deployment event: ADDED
    Mar  1 13:09:07.176: INFO: Observed deployment test-deployment-t4vnj in namespace deployment-9395 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-01 13:09:05 +0000 UTC 2023-03-01 13:09:05 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-t4vnj-777898ffcc"}
    Mar  1 13:09:07.177: INFO: Observed &Deployment event: MODIFIED
    Mar  1 13:09:07.177: INFO: Observed deployment test-deployment-t4vnj in namespace deployment-9395 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-01 13:09:05 +0000 UTC 2023-03-01 13:09:05 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-t4vnj-777898ffcc"}
    Mar  1 13:09:07.177: INFO: Observed deployment test-deployment-t4vnj in namespace deployment-9395 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-01 13:09:05 +0000 UTC 2023-03-01 13:09:05 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Mar  1 13:09:07.177: INFO: Observed &Deployment event: MODIFIED
    Mar  1 13:09:07.177: INFO: Observed deployment test-deployment-t4vnj in namespace deployment-9395 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-01 13:09:05 +0000 UTC 2023-03-01 13:09:05 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Mar  1 13:09:07.177: INFO: Observed deployment test-deployment-t4vnj in namespace deployment-9395 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-01 13:09:05 +0000 UTC 2023-03-01 13:09:05 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-t4vnj-777898ffcc" is progressing.}
    Mar  1 13:09:07.177: INFO: Observed &Deployment event: MODIFIED
    Mar  1 13:09:07.177: INFO: Observed deployment test-deployment-t4vnj in namespace deployment-9395 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-01 13:09:06 +0000 UTC 2023-03-01 13:09:06 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Mar  1 13:09:07.177: INFO: Observed deployment test-deployment-t4vnj in namespace deployment-9395 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-01 13:09:06 +0000 UTC 2023-03-01 13:09:05 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-t4vnj-777898ffcc" has successfully progressed.}
    Mar  1 13:09:07.178: INFO: Observed &Deployment event: MODIFIED
    Mar  1 13:09:07.178: INFO: Observed deployment test-deployment-t4vnj in namespace deployment-9395 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-01 13:09:06 +0000 UTC 2023-03-01 13:09:06 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Mar  1 13:09:07.178: INFO: Observed deployment test-deployment-t4vnj in namespace deployment-9395 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-01 13:09:06 +0000 UTC 2023-03-01 13:09:05 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-t4vnj-777898ffcc" has successfully progressed.}
    Mar  1 13:09:07.178: INFO: Observed deployment test-deployment-t4vnj in namespace deployment-9395 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Mar  1 13:09:07.178: INFO: Observed &Deployment event: MODIFIED
    Mar  1 13:09:07.178: INFO: Found deployment test-deployment-t4vnj in namespace deployment-9395 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Mar  1 13:09:07.178: INFO: Deployment test-deployment-t4vnj has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar  1 13:09:07.184: INFO: Deployment "test-deployment-t4vnj":
    &Deployment{ObjectMeta:{test-deployment-t4vnj  deployment-9395  96ee8545-5b9f-4095-a83f-e7dcd179e01d 42529 1 2023-03-01 13:09:05 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-03-01 13:09:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-03-01 13:09:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-03-01 13:09:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00435dfd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-t4vnj-777898ffcc",LastUpdateTime:2023-03-01 13:09:07 +0000 UTC,LastTransitionTime:2023-03-01 13:09:07 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Mar  1 13:09:07.191: INFO: New ReplicaSet "test-deployment-t4vnj-777898ffcc" of Deployment "test-deployment-t4vnj":
    &ReplicaSet{ObjectMeta:{test-deployment-t4vnj-777898ffcc  deployment-9395  ab70aad7-8bdb-4fcc-bad7-79da9434fb35 42522 1 2023-03-01 13:09:05 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-t4vnj 96ee8545-5b9f-4095-a83f-e7dcd179e01d 0xc0028ce390 0xc0028ce391}] [] [{kube-controller-manager Update apps/v1 2023-03-01 13:09:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"96ee8545-5b9f-4095-a83f-e7dcd179e01d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-01 13:09:06 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 777898ffcc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0028ce438 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Mar  1 13:09:07.196: INFO: Pod "test-deployment-t4vnj-777898ffcc-5zqdt" is available:
    &Pod{ObjectMeta:{test-deployment-t4vnj-777898ffcc-5zqdt test-deployment-t4vnj-777898ffcc- deployment-9395  0ae3ed6c-ff91-45fe-ae2b-b280a791071e 42521 0 2023-03-01 13:09:05 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[cni.projectcalico.org/containerID:7b1736473be591089735a3d17e1fcebf29bde3baf74aa57556fd57e4d1a90e9a cni.projectcalico.org/podIP:10.233.74.115/32 cni.projectcalico.org/podIPs:10.233.74.115/32] [{apps/v1 ReplicaSet test-deployment-t4vnj-777898ffcc ab70aad7-8bdb-4fcc-bad7-79da9434fb35 0xc0043e6740 0xc0043e6741}] [] [{calico Update v1 2023-03-01 13:09:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-01 13:09:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ab70aad7-8bdb-4fcc-bad7-79da9434fb35\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-01 13:09:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.74.115\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5r7kp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5r7kp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:09:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:09:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:09:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:09:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.2.241,PodIP:10.233.74.115,StartTime:2023-03-01 13:09:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-01 13:09:05 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://6953dc0b8aeebe402dcb7e1bbe869f248a388c3dc6a55ce28ad0da80e16cf79f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.74.115,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Mar  1 13:09:07.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-9395" for this suite. 03/01/23 13:09:07.203
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:09:07.214
Mar  1 13:09:07.214: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename deployment 03/01/23 13:09:07.215
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:09:07.235
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:09:07.238
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
Mar  1 13:09:07.254: INFO: Pod name rollover-pod: Found 0 pods out of 1
Mar  1 13:09:12.260: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 03/01/23 13:09:12.26
Mar  1 13:09:12.261: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Mar  1 13:09:14.267: INFO: Creating deployment "test-rollover-deployment"
Mar  1 13:09:14.278: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Mar  1 13:09:16.287: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Mar  1 13:09:16.295: INFO: Ensure that both replica sets have 1 created replica
Mar  1 13:09:16.303: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Mar  1 13:09:16.316: INFO: Updating deployment test-rollover-deployment
Mar  1 13:09:16.316: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Mar  1 13:09:18.325: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Mar  1 13:09:18.334: INFO: Make sure deployment "test-rollover-deployment" is complete
Mar  1 13:09:18.341: INFO: all replica sets need to contain the pod-template-hash label
Mar  1 13:09:18.341: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 1, 13, 9, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 1, 13, 9, 14, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 1, 13, 9, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 1, 13, 9, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 13:09:20.351: INFO: all replica sets need to contain the pod-template-hash label
Mar  1 13:09:20.351: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 1, 13, 9, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 1, 13, 9, 14, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 1, 13, 9, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 1, 13, 9, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 13:09:22.351: INFO: all replica sets need to contain the pod-template-hash label
Mar  1 13:09:22.351: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 1, 13, 9, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 1, 13, 9, 14, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 1, 13, 9, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 1, 13, 9, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 13:09:24.353: INFO: all replica sets need to contain the pod-template-hash label
Mar  1 13:09:24.353: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 1, 13, 9, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 1, 13, 9, 14, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 1, 13, 9, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 1, 13, 9, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 13:09:26.351: INFO: all replica sets need to contain the pod-template-hash label
Mar  1 13:09:26.351: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 1, 13, 9, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 1, 13, 9, 14, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 1, 13, 9, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 1, 13, 9, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 13:09:28.351: INFO: 
Mar  1 13:09:28.352: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar  1 13:09:28.363: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-4665  e59bf434-0175-47b6-b8c1-58e8ee032e4c 42709 2 2023-03-01 13:09:14 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-03-01 13:09:16 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-01 13:09:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001f9e6a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-01 13:09:14 +0000 UTC,LastTransitionTime:2023-03-01 13:09:14 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6d45fd857b" has successfully progressed.,LastUpdateTime:2023-03-01 13:09:27 +0000 UTC,LastTransitionTime:2023-03-01 13:09:14 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar  1 13:09:28.368: INFO: New ReplicaSet "test-rollover-deployment-6d45fd857b" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6d45fd857b  deployment-4665  29567de9-9210-4441-ae4c-20d8210295d2 42698 2 2023-03-01 13:09:16 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment e59bf434-0175-47b6-b8c1-58e8ee032e4c 0xc002117e57 0xc002117e58}] [] [{kube-controller-manager Update apps/v1 2023-03-01 13:09:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e59bf434-0175-47b6-b8c1-58e8ee032e4c\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-01 13:09:27 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6d45fd857b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002117f08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar  1 13:09:28.368: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Mar  1 13:09:28.368: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-4665  18e54168-8f8c-49a3-a0d9-f581b3e2fdf1 42708 2 2023-03-01 13:09:07 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment e59bf434-0175-47b6-b8c1-58e8ee032e4c 0xc002117c07 0xc002117c08}] [] [{e2e.test Update apps/v1 2023-03-01 13:09:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-01 13:09:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e59bf434-0175-47b6-b8c1-58e8ee032e4c\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-03-01 13:09:27 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002117cc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  1 13:09:28.368: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-4665  d156aea4-74d8-4713-aea1-987c15739717 42644 2 2023-03-01 13:09:14 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment e59bf434-0175-47b6-b8c1-58e8ee032e4c 0xc002117d37 0xc002117d38}] [] [{kube-controller-manager Update apps/v1 2023-03-01 13:09:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e59bf434-0175-47b6-b8c1-58e8ee032e4c\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-01 13:09:16 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002117de8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  1 13:09:28.372: INFO: Pod "test-rollover-deployment-6d45fd857b-vfh6r" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6d45fd857b-vfh6r test-rollover-deployment-6d45fd857b- deployment-4665  c761a1e3-4387-4764-927a-9a460d970503 42663 0 2023-03-01 13:09:16 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[cni.projectcalico.org/containerID:3defe489748dc68077e7fb03fa87c383724e312966968d912bb055228be4c4b2 cni.projectcalico.org/podIP:10.233.74.103/32 cni.projectcalico.org/podIPs:10.233.74.103/32] [{apps/v1 ReplicaSet test-rollover-deployment-6d45fd857b 29567de9-9210-4441-ae4c-20d8210295d2 0xc001f9f267 0xc001f9f268}] [] [{calico Update v1 2023-03-01 13:09:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-01 13:09:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"29567de9-9210-4441-ae4c-20d8210295d2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-01 13:09:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.74.103\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cq6dd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cq6dd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:09:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:09:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:09:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:09:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.2.241,PodIP:10.233.74.103,StartTime:2023-03-01 13:09:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-01 13:09:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://4c39c7975917a8e89afd3c134d6361e010a6ad6d2b8cd000c382baaa456c460f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.74.103,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Mar  1 13:09:28.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4665" for this suite. 03/01/23 13:09:28.379
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","completed":283,"skipped":5249,"failed":0}
------------------------------
â€¢ [SLOW TEST] [21.176 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:09:07.214
    Mar  1 13:09:07.214: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename deployment 03/01/23 13:09:07.215
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:09:07.235
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:09:07.238
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    Mar  1 13:09:07.254: INFO: Pod name rollover-pod: Found 0 pods out of 1
    Mar  1 13:09:12.260: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 03/01/23 13:09:12.26
    Mar  1 13:09:12.261: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Mar  1 13:09:14.267: INFO: Creating deployment "test-rollover-deployment"
    Mar  1 13:09:14.278: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Mar  1 13:09:16.287: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Mar  1 13:09:16.295: INFO: Ensure that both replica sets have 1 created replica
    Mar  1 13:09:16.303: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Mar  1 13:09:16.316: INFO: Updating deployment test-rollover-deployment
    Mar  1 13:09:16.316: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Mar  1 13:09:18.325: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Mar  1 13:09:18.334: INFO: Make sure deployment "test-rollover-deployment" is complete
    Mar  1 13:09:18.341: INFO: all replica sets need to contain the pod-template-hash label
    Mar  1 13:09:18.341: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 1, 13, 9, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 1, 13, 9, 14, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 1, 13, 9, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 1, 13, 9, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar  1 13:09:20.351: INFO: all replica sets need to contain the pod-template-hash label
    Mar  1 13:09:20.351: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 1, 13, 9, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 1, 13, 9, 14, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 1, 13, 9, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 1, 13, 9, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar  1 13:09:22.351: INFO: all replica sets need to contain the pod-template-hash label
    Mar  1 13:09:22.351: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 1, 13, 9, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 1, 13, 9, 14, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 1, 13, 9, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 1, 13, 9, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar  1 13:09:24.353: INFO: all replica sets need to contain the pod-template-hash label
    Mar  1 13:09:24.353: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 1, 13, 9, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 1, 13, 9, 14, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 1, 13, 9, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 1, 13, 9, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar  1 13:09:26.351: INFO: all replica sets need to contain the pod-template-hash label
    Mar  1 13:09:26.351: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 1, 13, 9, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 1, 13, 9, 14, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 1, 13, 9, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 1, 13, 9, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar  1 13:09:28.351: INFO: 
    Mar  1 13:09:28.352: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar  1 13:09:28.363: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-4665  e59bf434-0175-47b6-b8c1-58e8ee032e4c 42709 2 2023-03-01 13:09:14 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-03-01 13:09:16 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-01 13:09:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001f9e6a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-01 13:09:14 +0000 UTC,LastTransitionTime:2023-03-01 13:09:14 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6d45fd857b" has successfully progressed.,LastUpdateTime:2023-03-01 13:09:27 +0000 UTC,LastTransitionTime:2023-03-01 13:09:14 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Mar  1 13:09:28.368: INFO: New ReplicaSet "test-rollover-deployment-6d45fd857b" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6d45fd857b  deployment-4665  29567de9-9210-4441-ae4c-20d8210295d2 42698 2 2023-03-01 13:09:16 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment e59bf434-0175-47b6-b8c1-58e8ee032e4c 0xc002117e57 0xc002117e58}] [] [{kube-controller-manager Update apps/v1 2023-03-01 13:09:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e59bf434-0175-47b6-b8c1-58e8ee032e4c\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-01 13:09:27 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6d45fd857b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002117f08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Mar  1 13:09:28.368: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Mar  1 13:09:28.368: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-4665  18e54168-8f8c-49a3-a0d9-f581b3e2fdf1 42708 2 2023-03-01 13:09:07 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment e59bf434-0175-47b6-b8c1-58e8ee032e4c 0xc002117c07 0xc002117c08}] [] [{e2e.test Update apps/v1 2023-03-01 13:09:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-01 13:09:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e59bf434-0175-47b6-b8c1-58e8ee032e4c\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-03-01 13:09:27 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002117cc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Mar  1 13:09:28.368: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-4665  d156aea4-74d8-4713-aea1-987c15739717 42644 2 2023-03-01 13:09:14 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment e59bf434-0175-47b6-b8c1-58e8ee032e4c 0xc002117d37 0xc002117d38}] [] [{kube-controller-manager Update apps/v1 2023-03-01 13:09:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e59bf434-0175-47b6-b8c1-58e8ee032e4c\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-01 13:09:16 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002117de8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Mar  1 13:09:28.372: INFO: Pod "test-rollover-deployment-6d45fd857b-vfh6r" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6d45fd857b-vfh6r test-rollover-deployment-6d45fd857b- deployment-4665  c761a1e3-4387-4764-927a-9a460d970503 42663 0 2023-03-01 13:09:16 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[cni.projectcalico.org/containerID:3defe489748dc68077e7fb03fa87c383724e312966968d912bb055228be4c4b2 cni.projectcalico.org/podIP:10.233.74.103/32 cni.projectcalico.org/podIPs:10.233.74.103/32] [{apps/v1 ReplicaSet test-rollover-deployment-6d45fd857b 29567de9-9210-4441-ae4c-20d8210295d2 0xc001f9f267 0xc001f9f268}] [] [{calico Update v1 2023-03-01 13:09:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-01 13:09:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"29567de9-9210-4441-ae4c-20d8210295d2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-01 13:09:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.74.103\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cq6dd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cq6dd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:09:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:09:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:09:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:09:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.2.241,PodIP:10.233.74.103,StartTime:2023-03-01 13:09:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-01 13:09:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://4c39c7975917a8e89afd3c134d6361e010a6ad6d2b8cd000c382baaa456c460f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.74.103,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Mar  1 13:09:28.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-4665" for this suite. 03/01/23 13:09:28.379
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:09:28.393
Mar  1 13:09:28.393: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename projected 03/01/23 13:09:28.394
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:09:28.413
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:09:28.416
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
STEP: Creating projection with secret that has name projected-secret-test-75a7182d-a8e0-43c9-bea5-b1350629fe40 03/01/23 13:09:28.417
STEP: Creating a pod to test consume secrets 03/01/23 13:09:28.424
Mar  1 13:09:28.435: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-61252d85-d544-4afe-8c5b-e8367603d5eb" in namespace "projected-7910" to be "Succeeded or Failed"
Mar  1 13:09:28.440: INFO: Pod "pod-projected-secrets-61252d85-d544-4afe-8c5b-e8367603d5eb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.157457ms
Mar  1 13:09:30.446: INFO: Pod "pod-projected-secrets-61252d85-d544-4afe-8c5b-e8367603d5eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01015469s
Mar  1 13:09:32.446: INFO: Pod "pod-projected-secrets-61252d85-d544-4afe-8c5b-e8367603d5eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010516563s
STEP: Saw pod success 03/01/23 13:09:32.446
Mar  1 13:09:32.446: INFO: Pod "pod-projected-secrets-61252d85-d544-4afe-8c5b-e8367603d5eb" satisfied condition "Succeeded or Failed"
Mar  1 13:09:32.450: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-projected-secrets-61252d85-d544-4afe-8c5b-e8367603d5eb container projected-secret-volume-test: <nil>
STEP: delete the pod 03/01/23 13:09:32.469
Mar  1 13:09:32.484: INFO: Waiting for pod pod-projected-secrets-61252d85-d544-4afe-8c5b-e8367603d5eb to disappear
Mar  1 13:09:32.488: INFO: Pod pod-projected-secrets-61252d85-d544-4afe-8c5b-e8367603d5eb no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Mar  1 13:09:32.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7910" for this suite. 03/01/23 13:09:32.495
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","completed":284,"skipped":5302,"failed":0}
------------------------------
â€¢ [4.111 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:09:28.393
    Mar  1 13:09:28.393: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename projected 03/01/23 13:09:28.394
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:09:28.413
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:09:28.416
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:45
    STEP: Creating projection with secret that has name projected-secret-test-75a7182d-a8e0-43c9-bea5-b1350629fe40 03/01/23 13:09:28.417
    STEP: Creating a pod to test consume secrets 03/01/23 13:09:28.424
    Mar  1 13:09:28.435: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-61252d85-d544-4afe-8c5b-e8367603d5eb" in namespace "projected-7910" to be "Succeeded or Failed"
    Mar  1 13:09:28.440: INFO: Pod "pod-projected-secrets-61252d85-d544-4afe-8c5b-e8367603d5eb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.157457ms
    Mar  1 13:09:30.446: INFO: Pod "pod-projected-secrets-61252d85-d544-4afe-8c5b-e8367603d5eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01015469s
    Mar  1 13:09:32.446: INFO: Pod "pod-projected-secrets-61252d85-d544-4afe-8c5b-e8367603d5eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010516563s
    STEP: Saw pod success 03/01/23 13:09:32.446
    Mar  1 13:09:32.446: INFO: Pod "pod-projected-secrets-61252d85-d544-4afe-8c5b-e8367603d5eb" satisfied condition "Succeeded or Failed"
    Mar  1 13:09:32.450: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-projected-secrets-61252d85-d544-4afe-8c5b-e8367603d5eb container projected-secret-volume-test: <nil>
    STEP: delete the pod 03/01/23 13:09:32.469
    Mar  1 13:09:32.484: INFO: Waiting for pod pod-projected-secrets-61252d85-d544-4afe-8c5b-e8367603d5eb to disappear
    Mar  1 13:09:32.488: INFO: Pod pod-projected-secrets-61252d85-d544-4afe-8c5b-e8367603d5eb no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Mar  1 13:09:32.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7910" for this suite. 03/01/23 13:09:32.495
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:09:32.509
Mar  1 13:09:32.510: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename ephemeral-containers-test 03/01/23 13:09:32.51
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:09:32.534
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:09:32.536
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 03/01/23 13:09:32.539
Mar  1 13:09:32.548: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-9707" to be "running and ready"
Mar  1 13:09:32.554: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.42749ms
Mar  1 13:09:32.554: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Mar  1 13:09:34.559: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010447137s
Mar  1 13:09:34.559: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Mar  1 13:09:34.559: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 03/01/23 13:09:34.564
Mar  1 13:09:34.582: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-9707" to be "container debugger running"
Mar  1 13:09:34.587: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.139855ms
Mar  1 13:09:36.592: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009938854s
Mar  1 13:09:38.593: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.010598474s
Mar  1 13:09:38.593: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 03/01/23 13:09:38.593
Mar  1 13:09:38.593: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-9707 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  1 13:09:38.593: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
Mar  1 13:09:38.594: INFO: ExecWithOptions: Clientset creation
Mar  1 13:09:38.594: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/ephemeral-containers-test-9707/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Mar  1 13:09:38.669: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:187
Mar  1 13:09:38.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ephemeral-containers-test-9707" for this suite. 03/01/23 13:09:38.685
{"msg":"PASSED [sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]","completed":285,"skipped":5345,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.186 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:09:32.509
    Mar  1 13:09:32.510: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename ephemeral-containers-test 03/01/23 13:09:32.51
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:09:32.534
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:09:32.536
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 03/01/23 13:09:32.539
    Mar  1 13:09:32.548: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-9707" to be "running and ready"
    Mar  1 13:09:32.554: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.42749ms
    Mar  1 13:09:32.554: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Mar  1 13:09:34.559: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010447137s
    Mar  1 13:09:34.559: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Mar  1 13:09:34.559: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 03/01/23 13:09:34.564
    Mar  1 13:09:34.582: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-9707" to be "container debugger running"
    Mar  1 13:09:34.587: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.139855ms
    Mar  1 13:09:36.592: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009938854s
    Mar  1 13:09:38.593: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.010598474s
    Mar  1 13:09:38.593: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 03/01/23 13:09:38.593
    Mar  1 13:09:38.593: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-9707 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  1 13:09:38.593: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    Mar  1 13:09:38.594: INFO: ExecWithOptions: Clientset creation
    Mar  1 13:09:38.594: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/ephemeral-containers-test-9707/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Mar  1 13:09:38.669: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:187
    Mar  1 13:09:38.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ephemeral-containers-test-9707" for this suite. 03/01/23 13:09:38.685
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:09:38.698
Mar  1 13:09:38.698: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename projected 03/01/23 13:09:38.699
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:09:38.723
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:09:38.727
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
STEP: Creating a pod to test downward API volume plugin 03/01/23 13:09:38.729
Mar  1 13:09:38.741: INFO: Waiting up to 5m0s for pod "downwardapi-volume-53b35d89-17eb-46fb-b27d-ebe88f9f8876" in namespace "projected-6626" to be "Succeeded or Failed"
Mar  1 13:09:38.748: INFO: Pod "downwardapi-volume-53b35d89-17eb-46fb-b27d-ebe88f9f8876": Phase="Pending", Reason="", readiness=false. Elapsed: 7.604396ms
Mar  1 13:09:40.754: INFO: Pod "downwardapi-volume-53b35d89-17eb-46fb-b27d-ebe88f9f8876": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012733807s
Mar  1 13:09:42.754: INFO: Pod "downwardapi-volume-53b35d89-17eb-46fb-b27d-ebe88f9f8876": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012927797s
STEP: Saw pod success 03/01/23 13:09:42.754
Mar  1 13:09:42.754: INFO: Pod "downwardapi-volume-53b35d89-17eb-46fb-b27d-ebe88f9f8876" satisfied condition "Succeeded or Failed"
Mar  1 13:09:42.758: INFO: Trying to get logs from node lab1-k8s-node-3 pod downwardapi-volume-53b35d89-17eb-46fb-b27d-ebe88f9f8876 container client-container: <nil>
STEP: delete the pod 03/01/23 13:09:42.766
Mar  1 13:09:42.785: INFO: Waiting for pod downwardapi-volume-53b35d89-17eb-46fb-b27d-ebe88f9f8876 to disappear
Mar  1 13:09:42.788: INFO: Pod downwardapi-volume-53b35d89-17eb-46fb-b27d-ebe88f9f8876 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Mar  1 13:09:42.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6626" for this suite. 03/01/23 13:09:42.795
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","completed":286,"skipped":5356,"failed":0}
------------------------------
â€¢ [4.106 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:09:38.698
    Mar  1 13:09:38.698: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename projected 03/01/23 13:09:38.699
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:09:38.723
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:09:38.727
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:192
    STEP: Creating a pod to test downward API volume plugin 03/01/23 13:09:38.729
    Mar  1 13:09:38.741: INFO: Waiting up to 5m0s for pod "downwardapi-volume-53b35d89-17eb-46fb-b27d-ebe88f9f8876" in namespace "projected-6626" to be "Succeeded or Failed"
    Mar  1 13:09:38.748: INFO: Pod "downwardapi-volume-53b35d89-17eb-46fb-b27d-ebe88f9f8876": Phase="Pending", Reason="", readiness=false. Elapsed: 7.604396ms
    Mar  1 13:09:40.754: INFO: Pod "downwardapi-volume-53b35d89-17eb-46fb-b27d-ebe88f9f8876": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012733807s
    Mar  1 13:09:42.754: INFO: Pod "downwardapi-volume-53b35d89-17eb-46fb-b27d-ebe88f9f8876": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012927797s
    STEP: Saw pod success 03/01/23 13:09:42.754
    Mar  1 13:09:42.754: INFO: Pod "downwardapi-volume-53b35d89-17eb-46fb-b27d-ebe88f9f8876" satisfied condition "Succeeded or Failed"
    Mar  1 13:09:42.758: INFO: Trying to get logs from node lab1-k8s-node-3 pod downwardapi-volume-53b35d89-17eb-46fb-b27d-ebe88f9f8876 container client-container: <nil>
    STEP: delete the pod 03/01/23 13:09:42.766
    Mar  1 13:09:42.785: INFO: Waiting for pod downwardapi-volume-53b35d89-17eb-46fb-b27d-ebe88f9f8876 to disappear
    Mar  1 13:09:42.788: INFO: Pod downwardapi-volume-53b35d89-17eb-46fb-b27d-ebe88f9f8876 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Mar  1 13:09:42.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6626" for this suite. 03/01/23 13:09:42.795
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:09:42.807
Mar  1 13:09:42.807: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename webhook 03/01/23 13:09:42.808
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:09:42.828
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:09:42.83
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/01/23 13:09:42.853
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/01/23 13:09:43.198
STEP: Deploying the webhook pod 03/01/23 13:09:43.209
STEP: Wait for the deployment to be ready 03/01/23 13:09:43.223
Mar  1 13:09:43.233: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/01/23 13:09:45.249
STEP: Verifying the service has paired with the endpoint 03/01/23 13:09:45.263
Mar  1 13:09:46.263: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 03/01/23 13:09:46.267
STEP: create a namespace for the webhook 03/01/23 13:09:46.283
STEP: create a configmap should be unconditionally rejected by the webhook 03/01/23 13:09:46.293
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  1 13:09:46.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2547" for this suite. 03/01/23 13:09:46.335
STEP: Destroying namespace "webhook-2547-markers" for this suite. 03/01/23 13:09:46.343
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","completed":287,"skipped":5363,"failed":0}
------------------------------
â€¢ [3.593 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:09:42.807
    Mar  1 13:09:42.807: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename webhook 03/01/23 13:09:42.808
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:09:42.828
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:09:42.83
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/01/23 13:09:42.853
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/01/23 13:09:43.198
    STEP: Deploying the webhook pod 03/01/23 13:09:43.209
    STEP: Wait for the deployment to be ready 03/01/23 13:09:43.223
    Mar  1 13:09:43.233: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/01/23 13:09:45.249
    STEP: Verifying the service has paired with the endpoint 03/01/23 13:09:45.263
    Mar  1 13:09:46.263: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:238
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 03/01/23 13:09:46.267
    STEP: create a namespace for the webhook 03/01/23 13:09:46.283
    STEP: create a configmap should be unconditionally rejected by the webhook 03/01/23 13:09:46.293
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  1 13:09:46.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-2547" for this suite. 03/01/23 13:09:46.335
    STEP: Destroying namespace "webhook-2547-markers" for this suite. 03/01/23 13:09:46.343
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:09:46.4
Mar  1 13:09:46.400: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename pods 03/01/23 13:09:46.401
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:09:46.423
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:09:46.427
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 03/01/23 13:09:46.43
STEP: submitting the pod to kubernetes 03/01/23 13:09:46.43
STEP: verifying QOS class is set on the pod 03/01/23 13:09:46.44
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:187
Mar  1 13:09:46.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8243" for this suite. 03/01/23 13:09:46.453
{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","completed":288,"skipped":5378,"failed":0}
------------------------------
â€¢ [0.064 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:09:46.4
    Mar  1 13:09:46.400: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename pods 03/01/23 13:09:46.401
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:09:46.423
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:09:46.427
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 03/01/23 13:09:46.43
    STEP: submitting the pod to kubernetes 03/01/23 13:09:46.43
    STEP: verifying QOS class is set on the pod 03/01/23 13:09:46.44
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:187
    Mar  1 13:09:46.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-8243" for this suite. 03/01/23 13:09:46.453
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:09:46.464
Mar  1 13:09:46.465: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename emptydir 03/01/23 13:09:46.465
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:09:46.489
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:09:46.492
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
STEP: Creating a pod to test emptydir 0644 on tmpfs 03/01/23 13:09:46.495
Mar  1 13:09:46.507: INFO: Waiting up to 5m0s for pod "pod-fc16dae9-a746-486d-93a2-0ac284b41972" in namespace "emptydir-5570" to be "Succeeded or Failed"
Mar  1 13:09:46.513: INFO: Pod "pod-fc16dae9-a746-486d-93a2-0ac284b41972": Phase="Pending", Reason="", readiness=false. Elapsed: 6.213353ms
Mar  1 13:09:48.520: INFO: Pod "pod-fc16dae9-a746-486d-93a2-0ac284b41972": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012918389s
Mar  1 13:09:50.519: INFO: Pod "pod-fc16dae9-a746-486d-93a2-0ac284b41972": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011660222s
STEP: Saw pod success 03/01/23 13:09:50.519
Mar  1 13:09:50.519: INFO: Pod "pod-fc16dae9-a746-486d-93a2-0ac284b41972" satisfied condition "Succeeded or Failed"
Mar  1 13:09:50.523: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-fc16dae9-a746-486d-93a2-0ac284b41972 container test-container: <nil>
STEP: delete the pod 03/01/23 13:09:50.532
Mar  1 13:09:50.548: INFO: Waiting for pod pod-fc16dae9-a746-486d-93a2-0ac284b41972 to disappear
Mar  1 13:09:50.552: INFO: Pod pod-fc16dae9-a746-486d-93a2-0ac284b41972 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar  1 13:09:50.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5570" for this suite. 03/01/23 13:09:50.558
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":289,"skipped":5378,"failed":0}
------------------------------
â€¢ [4.102 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:09:46.464
    Mar  1 13:09:46.465: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename emptydir 03/01/23 13:09:46.465
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:09:46.489
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:09:46.492
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:96
    STEP: Creating a pod to test emptydir 0644 on tmpfs 03/01/23 13:09:46.495
    Mar  1 13:09:46.507: INFO: Waiting up to 5m0s for pod "pod-fc16dae9-a746-486d-93a2-0ac284b41972" in namespace "emptydir-5570" to be "Succeeded or Failed"
    Mar  1 13:09:46.513: INFO: Pod "pod-fc16dae9-a746-486d-93a2-0ac284b41972": Phase="Pending", Reason="", readiness=false. Elapsed: 6.213353ms
    Mar  1 13:09:48.520: INFO: Pod "pod-fc16dae9-a746-486d-93a2-0ac284b41972": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012918389s
    Mar  1 13:09:50.519: INFO: Pod "pod-fc16dae9-a746-486d-93a2-0ac284b41972": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011660222s
    STEP: Saw pod success 03/01/23 13:09:50.519
    Mar  1 13:09:50.519: INFO: Pod "pod-fc16dae9-a746-486d-93a2-0ac284b41972" satisfied condition "Succeeded or Failed"
    Mar  1 13:09:50.523: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-fc16dae9-a746-486d-93a2-0ac284b41972 container test-container: <nil>
    STEP: delete the pod 03/01/23 13:09:50.532
    Mar  1 13:09:50.548: INFO: Waiting for pod pod-fc16dae9-a746-486d-93a2-0ac284b41972 to disappear
    Mar  1 13:09:50.552: INFO: Pod pod-fc16dae9-a746-486d-93a2-0ac284b41972 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar  1 13:09:50.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-5570" for this suite. 03/01/23 13:09:50.558
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:09:50.572
Mar  1 13:09:50.572: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename kubectl 03/01/23 13:09:50.573
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:09:50.592
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:09:50.595
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
STEP: creating Agnhost RC 03/01/23 13:09:50.598
Mar  1 13:09:50.598: INFO: namespace kubectl-3342
Mar  1 13:09:50.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-3342 create -f -'
Mar  1 13:09:51.403: INFO: stderr: ""
Mar  1 13:09:51.403: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 03/01/23 13:09:51.403
Mar  1 13:09:52.408: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  1 13:09:52.408: INFO: Found 0 / 1
Mar  1 13:09:53.408: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  1 13:09:53.408: INFO: Found 1 / 1
Mar  1 13:09:53.408: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar  1 13:09:53.414: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  1 13:09:53.414: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar  1 13:09:53.414: INFO: wait on agnhost-primary startup in kubectl-3342 
Mar  1 13:09:53.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-3342 logs agnhost-primary-dmrfh agnhost-primary'
Mar  1 13:09:53.486: INFO: stderr: ""
Mar  1 13:09:53.486: INFO: stdout: "Paused\n"
STEP: exposing RC 03/01/23 13:09:53.486
Mar  1 13:09:53.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-3342 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Mar  1 13:09:53.587: INFO: stderr: ""
Mar  1 13:09:53.587: INFO: stdout: "service/rm2 exposed\n"
Mar  1 13:09:53.598: INFO: Service rm2 in namespace kubectl-3342 found.
STEP: exposing service 03/01/23 13:09:55.605
Mar  1 13:09:55.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-3342 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Mar  1 13:09:55.683: INFO: stderr: ""
Mar  1 13:09:55.683: INFO: stdout: "service/rm3 exposed\n"
Mar  1 13:09:55.691: INFO: Service rm3 in namespace kubectl-3342 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar  1 13:09:57.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3342" for this suite. 03/01/23 13:09:57.707
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","completed":290,"skipped":5408,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.149 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1407
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:09:50.572
    Mar  1 13:09:50.572: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename kubectl 03/01/23 13:09:50.573
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:09:50.592
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:09:50.595
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1413
    STEP: creating Agnhost RC 03/01/23 13:09:50.598
    Mar  1 13:09:50.598: INFO: namespace kubectl-3342
    Mar  1 13:09:50.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-3342 create -f -'
    Mar  1 13:09:51.403: INFO: stderr: ""
    Mar  1 13:09:51.403: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 03/01/23 13:09:51.403
    Mar  1 13:09:52.408: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar  1 13:09:52.408: INFO: Found 0 / 1
    Mar  1 13:09:53.408: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar  1 13:09:53.408: INFO: Found 1 / 1
    Mar  1 13:09:53.408: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Mar  1 13:09:53.414: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar  1 13:09:53.414: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Mar  1 13:09:53.414: INFO: wait on agnhost-primary startup in kubectl-3342 
    Mar  1 13:09:53.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-3342 logs agnhost-primary-dmrfh agnhost-primary'
    Mar  1 13:09:53.486: INFO: stderr: ""
    Mar  1 13:09:53.486: INFO: stdout: "Paused\n"
    STEP: exposing RC 03/01/23 13:09:53.486
    Mar  1 13:09:53.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-3342 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Mar  1 13:09:53.587: INFO: stderr: ""
    Mar  1 13:09:53.587: INFO: stdout: "service/rm2 exposed\n"
    Mar  1 13:09:53.598: INFO: Service rm2 in namespace kubectl-3342 found.
    STEP: exposing service 03/01/23 13:09:55.605
    Mar  1 13:09:55.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-3342 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Mar  1 13:09:55.683: INFO: stderr: ""
    Mar  1 13:09:55.683: INFO: stdout: "service/rm3 exposed\n"
    Mar  1 13:09:55.691: INFO: Service rm3 in namespace kubectl-3342 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar  1 13:09:57.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-3342" for this suite. 03/01/23 13:09:57.707
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:09:57.721
Mar  1 13:09:57.721: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename downward-api 03/01/23 13:09:57.722
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:09:57.741
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:09:57.745
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
STEP: Creating a pod to test downward API volume plugin 03/01/23 13:09:57.747
Mar  1 13:09:57.759: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2d26b08f-39ba-4739-9977-2e9a5defd2d2" in namespace "downward-api-611" to be "Succeeded or Failed"
Mar  1 13:09:57.765: INFO: Pod "downwardapi-volume-2d26b08f-39ba-4739-9977-2e9a5defd2d2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.634133ms
Mar  1 13:09:59.772: INFO: Pod "downwardapi-volume-2d26b08f-39ba-4739-9977-2e9a5defd2d2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013003945s
Mar  1 13:10:01.773: INFO: Pod "downwardapi-volume-2d26b08f-39ba-4739-9977-2e9a5defd2d2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01446178s
STEP: Saw pod success 03/01/23 13:10:01.773
Mar  1 13:10:01.773: INFO: Pod "downwardapi-volume-2d26b08f-39ba-4739-9977-2e9a5defd2d2" satisfied condition "Succeeded or Failed"
Mar  1 13:10:01.780: INFO: Trying to get logs from node lab1-k8s-node-3 pod downwardapi-volume-2d26b08f-39ba-4739-9977-2e9a5defd2d2 container client-container: <nil>
STEP: delete the pod 03/01/23 13:10:01.796
Mar  1 13:10:01.823: INFO: Waiting for pod downwardapi-volume-2d26b08f-39ba-4739-9977-2e9a5defd2d2 to disappear
Mar  1 13:10:01.828: INFO: Pod downwardapi-volume-2d26b08f-39ba-4739-9977-2e9a5defd2d2 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Mar  1 13:10:01.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-611" for this suite. 03/01/23 13:10:01.836
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":291,"skipped":5416,"failed":0}
------------------------------
â€¢ [4.126 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:09:57.721
    Mar  1 13:09:57.721: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename downward-api 03/01/23 13:09:57.722
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:09:57.741
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:09:57.745
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:248
    STEP: Creating a pod to test downward API volume plugin 03/01/23 13:09:57.747
    Mar  1 13:09:57.759: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2d26b08f-39ba-4739-9977-2e9a5defd2d2" in namespace "downward-api-611" to be "Succeeded or Failed"
    Mar  1 13:09:57.765: INFO: Pod "downwardapi-volume-2d26b08f-39ba-4739-9977-2e9a5defd2d2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.634133ms
    Mar  1 13:09:59.772: INFO: Pod "downwardapi-volume-2d26b08f-39ba-4739-9977-2e9a5defd2d2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013003945s
    Mar  1 13:10:01.773: INFO: Pod "downwardapi-volume-2d26b08f-39ba-4739-9977-2e9a5defd2d2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01446178s
    STEP: Saw pod success 03/01/23 13:10:01.773
    Mar  1 13:10:01.773: INFO: Pod "downwardapi-volume-2d26b08f-39ba-4739-9977-2e9a5defd2d2" satisfied condition "Succeeded or Failed"
    Mar  1 13:10:01.780: INFO: Trying to get logs from node lab1-k8s-node-3 pod downwardapi-volume-2d26b08f-39ba-4739-9977-2e9a5defd2d2 container client-container: <nil>
    STEP: delete the pod 03/01/23 13:10:01.796
    Mar  1 13:10:01.823: INFO: Waiting for pod downwardapi-volume-2d26b08f-39ba-4739-9977-2e9a5defd2d2 to disappear
    Mar  1 13:10:01.828: INFO: Pod downwardapi-volume-2d26b08f-39ba-4739-9977-2e9a5defd2d2 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Mar  1 13:10:01.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-611" for this suite. 03/01/23 13:10:01.836
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:10:01.848
Mar  1 13:10:01.848: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename downward-api 03/01/23 13:10:01.849
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:10:01.871
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:10:01.874
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
STEP: Creating a pod to test downward api env vars 03/01/23 13:10:01.876
Mar  1 13:10:01.889: INFO: Waiting up to 5m0s for pod "downward-api-05e33ef5-4928-4f09-bfa7-1c3efb7b23b1" in namespace "downward-api-3920" to be "Succeeded or Failed"
Mar  1 13:10:01.894: INFO: Pod "downward-api-05e33ef5-4928-4f09-bfa7-1c3efb7b23b1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.288607ms
Mar  1 13:10:03.899: INFO: Pod "downward-api-05e33ef5-4928-4f09-bfa7-1c3efb7b23b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01055752s
Mar  1 13:10:05.900: INFO: Pod "downward-api-05e33ef5-4928-4f09-bfa7-1c3efb7b23b1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011439717s
STEP: Saw pod success 03/01/23 13:10:05.9
Mar  1 13:10:05.901: INFO: Pod "downward-api-05e33ef5-4928-4f09-bfa7-1c3efb7b23b1" satisfied condition "Succeeded or Failed"
Mar  1 13:10:05.905: INFO: Trying to get logs from node lab1-k8s-node-1 pod downward-api-05e33ef5-4928-4f09-bfa7-1c3efb7b23b1 container dapi-container: <nil>
STEP: delete the pod 03/01/23 13:10:05.924
Mar  1 13:10:05.945: INFO: Waiting for pod downward-api-05e33ef5-4928-4f09-bfa7-1c3efb7b23b1 to disappear
Mar  1 13:10:05.949: INFO: Pod downward-api-05e33ef5-4928-4f09-bfa7-1c3efb7b23b1 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Mar  1 13:10:05.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3920" for this suite. 03/01/23 13:10:05.956
{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","completed":292,"skipped":5417,"failed":0}
------------------------------
â€¢ [4.118 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:10:01.848
    Mar  1 13:10:01.848: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename downward-api 03/01/23 13:10:01.849
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:10:01.871
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:10:01.874
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:165
    STEP: Creating a pod to test downward api env vars 03/01/23 13:10:01.876
    Mar  1 13:10:01.889: INFO: Waiting up to 5m0s for pod "downward-api-05e33ef5-4928-4f09-bfa7-1c3efb7b23b1" in namespace "downward-api-3920" to be "Succeeded or Failed"
    Mar  1 13:10:01.894: INFO: Pod "downward-api-05e33ef5-4928-4f09-bfa7-1c3efb7b23b1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.288607ms
    Mar  1 13:10:03.899: INFO: Pod "downward-api-05e33ef5-4928-4f09-bfa7-1c3efb7b23b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01055752s
    Mar  1 13:10:05.900: INFO: Pod "downward-api-05e33ef5-4928-4f09-bfa7-1c3efb7b23b1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011439717s
    STEP: Saw pod success 03/01/23 13:10:05.9
    Mar  1 13:10:05.901: INFO: Pod "downward-api-05e33ef5-4928-4f09-bfa7-1c3efb7b23b1" satisfied condition "Succeeded or Failed"
    Mar  1 13:10:05.905: INFO: Trying to get logs from node lab1-k8s-node-1 pod downward-api-05e33ef5-4928-4f09-bfa7-1c3efb7b23b1 container dapi-container: <nil>
    STEP: delete the pod 03/01/23 13:10:05.924
    Mar  1 13:10:05.945: INFO: Waiting for pod downward-api-05e33ef5-4928-4f09-bfa7-1c3efb7b23b1 to disappear
    Mar  1 13:10:05.949: INFO: Pod downward-api-05e33ef5-4928-4f09-bfa7-1c3efb7b23b1 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Mar  1 13:10:05.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-3920" for this suite. 03/01/23 13:10:05.956
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:10:05.969
Mar  1 13:10:05.969: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename csistoragecapacity 03/01/23 13:10:05.97
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:10:05.99
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:10:05.993
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 03/01/23 13:10:05.995
STEP: getting /apis/storage.k8s.io 03/01/23 13:10:05.997
STEP: getting /apis/storage.k8s.io/v1 03/01/23 13:10:05.998
STEP: creating 03/01/23 13:10:05.999
STEP: watching 03/01/23 13:10:06.021
Mar  1 13:10:06.021: INFO: starting watch
STEP: getting 03/01/23 13:10:06.031
STEP: listing in namespace 03/01/23 13:10:06.035
STEP: listing across namespaces 03/01/23 13:10:06.041
STEP: patching 03/01/23 13:10:06.044
STEP: updating 03/01/23 13:10:06.051
Mar  1 13:10:06.057: INFO: waiting for watch events with expected annotations in namespace
Mar  1 13:10:06.057: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 03/01/23 13:10:06.057
STEP: deleting a collection 03/01/23 13:10:06.074
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:187
Mar  1 13:10:06.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "csistoragecapacity-7452" for this suite. 03/01/23 13:10:06.103
{"msg":"PASSED [sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]","completed":293,"skipped":5430,"failed":0}
------------------------------
â€¢ [0.147 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:10:05.969
    Mar  1 13:10:05.969: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename csistoragecapacity 03/01/23 13:10:05.97
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:10:05.99
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:10:05.993
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 03/01/23 13:10:05.995
    STEP: getting /apis/storage.k8s.io 03/01/23 13:10:05.997
    STEP: getting /apis/storage.k8s.io/v1 03/01/23 13:10:05.998
    STEP: creating 03/01/23 13:10:05.999
    STEP: watching 03/01/23 13:10:06.021
    Mar  1 13:10:06.021: INFO: starting watch
    STEP: getting 03/01/23 13:10:06.031
    STEP: listing in namespace 03/01/23 13:10:06.035
    STEP: listing across namespaces 03/01/23 13:10:06.041
    STEP: patching 03/01/23 13:10:06.044
    STEP: updating 03/01/23 13:10:06.051
    Mar  1 13:10:06.057: INFO: waiting for watch events with expected annotations in namespace
    Mar  1 13:10:06.057: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 03/01/23 13:10:06.057
    STEP: deleting a collection 03/01/23 13:10:06.074
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:187
    Mar  1 13:10:06.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "csistoragecapacity-7452" for this suite. 03/01/23 13:10:06.103
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:10:06.117
Mar  1 13:10:06.117: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename container-runtime 03/01/23 13:10:06.118
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:10:06.148
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:10:06.151
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
STEP: create the container 03/01/23 13:10:06.153
STEP: wait for the container to reach Succeeded 03/01/23 13:10:06.163
STEP: get the container status 03/01/23 13:10:09.186
STEP: the container should be terminated 03/01/23 13:10:09.19
STEP: the termination message should be set 03/01/23 13:10:09.19
Mar  1 13:10:09.190: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 03/01/23 13:10:09.19
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Mar  1 13:10:09.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4848" for this suite. 03/01/23 13:10:09.216
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":294,"skipped":5431,"failed":0}
------------------------------
â€¢ [3.108 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:10:06.117
    Mar  1 13:10:06.117: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename container-runtime 03/01/23 13:10:06.118
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:10:06.148
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:10:06.151
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247
    STEP: create the container 03/01/23 13:10:06.153
    STEP: wait for the container to reach Succeeded 03/01/23 13:10:06.163
    STEP: get the container status 03/01/23 13:10:09.186
    STEP: the container should be terminated 03/01/23 13:10:09.19
    STEP: the termination message should be set 03/01/23 13:10:09.19
    Mar  1 13:10:09.190: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 03/01/23 13:10:09.19
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Mar  1 13:10:09.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-4848" for this suite. 03/01/23 13:10:09.216
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:10:09.233
Mar  1 13:10:09.233: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename downward-api 03/01/23 13:10:09.234
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:10:09.256
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:10:09.262
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
STEP: Creating a pod to test downward API volume plugin 03/01/23 13:10:09.267
Mar  1 13:10:09.276: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b6d1b13c-8700-48d7-9667-e6dbd43bc56e" in namespace "downward-api-6655" to be "Succeeded or Failed"
Mar  1 13:10:09.280: INFO: Pod "downwardapi-volume-b6d1b13c-8700-48d7-9667-e6dbd43bc56e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.566936ms
Mar  1 13:10:11.285: INFO: Pod "downwardapi-volume-b6d1b13c-8700-48d7-9667-e6dbd43bc56e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008507111s
Mar  1 13:10:13.285: INFO: Pod "downwardapi-volume-b6d1b13c-8700-48d7-9667-e6dbd43bc56e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008347948s
STEP: Saw pod success 03/01/23 13:10:13.285
Mar  1 13:10:13.285: INFO: Pod "downwardapi-volume-b6d1b13c-8700-48d7-9667-e6dbd43bc56e" satisfied condition "Succeeded or Failed"
Mar  1 13:10:13.289: INFO: Trying to get logs from node lab1-k8s-node-3 pod downwardapi-volume-b6d1b13c-8700-48d7-9667-e6dbd43bc56e container client-container: <nil>
STEP: delete the pod 03/01/23 13:10:13.301
Mar  1 13:10:13.318: INFO: Waiting for pod downwardapi-volume-b6d1b13c-8700-48d7-9667-e6dbd43bc56e to disappear
Mar  1 13:10:13.324: INFO: Pod downwardapi-volume-b6d1b13c-8700-48d7-9667-e6dbd43bc56e no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Mar  1 13:10:13.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6655" for this suite. 03/01/23 13:10:13.331
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":295,"skipped":5443,"failed":0}
------------------------------
â€¢ [4.109 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:10:09.233
    Mar  1 13:10:09.233: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename downward-api 03/01/23 13:10:09.234
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:10:09.256
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:10:09.262
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:260
    STEP: Creating a pod to test downward API volume plugin 03/01/23 13:10:09.267
    Mar  1 13:10:09.276: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b6d1b13c-8700-48d7-9667-e6dbd43bc56e" in namespace "downward-api-6655" to be "Succeeded or Failed"
    Mar  1 13:10:09.280: INFO: Pod "downwardapi-volume-b6d1b13c-8700-48d7-9667-e6dbd43bc56e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.566936ms
    Mar  1 13:10:11.285: INFO: Pod "downwardapi-volume-b6d1b13c-8700-48d7-9667-e6dbd43bc56e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008507111s
    Mar  1 13:10:13.285: INFO: Pod "downwardapi-volume-b6d1b13c-8700-48d7-9667-e6dbd43bc56e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008347948s
    STEP: Saw pod success 03/01/23 13:10:13.285
    Mar  1 13:10:13.285: INFO: Pod "downwardapi-volume-b6d1b13c-8700-48d7-9667-e6dbd43bc56e" satisfied condition "Succeeded or Failed"
    Mar  1 13:10:13.289: INFO: Trying to get logs from node lab1-k8s-node-3 pod downwardapi-volume-b6d1b13c-8700-48d7-9667-e6dbd43bc56e container client-container: <nil>
    STEP: delete the pod 03/01/23 13:10:13.301
    Mar  1 13:10:13.318: INFO: Waiting for pod downwardapi-volume-b6d1b13c-8700-48d7-9667-e6dbd43bc56e to disappear
    Mar  1 13:10:13.324: INFO: Pod downwardapi-volume-b6d1b13c-8700-48d7-9667-e6dbd43bc56e no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Mar  1 13:10:13.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-6655" for this suite. 03/01/23 13:10:13.331
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:10:13.345
Mar  1 13:10:13.346: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename configmap 03/01/23 13:10:13.347
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:10:13.376
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:10:13.379
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
STEP: Creating configMap with name configmap-test-volume-ceee4090-9a88-44a7-b596-fd2f32528ea1 03/01/23 13:10:13.382
STEP: Creating a pod to test consume configMaps 03/01/23 13:10:13.388
Mar  1 13:10:13.400: INFO: Waiting up to 5m0s for pod "pod-configmaps-7f2face8-5a6a-4840-824b-e06a43457cb0" in namespace "configmap-9261" to be "Succeeded or Failed"
Mar  1 13:10:13.405: INFO: Pod "pod-configmaps-7f2face8-5a6a-4840-824b-e06a43457cb0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.751483ms
Mar  1 13:10:15.410: INFO: Pod "pod-configmaps-7f2face8-5a6a-4840-824b-e06a43457cb0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010417497s
Mar  1 13:10:17.411: INFO: Pod "pod-configmaps-7f2face8-5a6a-4840-824b-e06a43457cb0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011098828s
STEP: Saw pod success 03/01/23 13:10:17.411
Mar  1 13:10:17.411: INFO: Pod "pod-configmaps-7f2face8-5a6a-4840-824b-e06a43457cb0" satisfied condition "Succeeded or Failed"
Mar  1 13:10:17.415: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-configmaps-7f2face8-5a6a-4840-824b-e06a43457cb0 container agnhost-container: <nil>
STEP: delete the pod 03/01/23 13:10:17.423
Mar  1 13:10:17.439: INFO: Waiting for pod pod-configmaps-7f2face8-5a6a-4840-824b-e06a43457cb0 to disappear
Mar  1 13:10:17.443: INFO: Pod pod-configmaps-7f2face8-5a6a-4840-824b-e06a43457cb0 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Mar  1 13:10:17.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9261" for this suite. 03/01/23 13:10:17.45
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":296,"skipped":5452,"failed":0}
------------------------------
â€¢ [4.113 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:10:13.345
    Mar  1 13:10:13.346: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename configmap 03/01/23 13:10:13.347
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:10:13.376
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:10:13.379
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:46
    STEP: Creating configMap with name configmap-test-volume-ceee4090-9a88-44a7-b596-fd2f32528ea1 03/01/23 13:10:13.382
    STEP: Creating a pod to test consume configMaps 03/01/23 13:10:13.388
    Mar  1 13:10:13.400: INFO: Waiting up to 5m0s for pod "pod-configmaps-7f2face8-5a6a-4840-824b-e06a43457cb0" in namespace "configmap-9261" to be "Succeeded or Failed"
    Mar  1 13:10:13.405: INFO: Pod "pod-configmaps-7f2face8-5a6a-4840-824b-e06a43457cb0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.751483ms
    Mar  1 13:10:15.410: INFO: Pod "pod-configmaps-7f2face8-5a6a-4840-824b-e06a43457cb0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010417497s
    Mar  1 13:10:17.411: INFO: Pod "pod-configmaps-7f2face8-5a6a-4840-824b-e06a43457cb0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011098828s
    STEP: Saw pod success 03/01/23 13:10:17.411
    Mar  1 13:10:17.411: INFO: Pod "pod-configmaps-7f2face8-5a6a-4840-824b-e06a43457cb0" satisfied condition "Succeeded or Failed"
    Mar  1 13:10:17.415: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-configmaps-7f2face8-5a6a-4840-824b-e06a43457cb0 container agnhost-container: <nil>
    STEP: delete the pod 03/01/23 13:10:17.423
    Mar  1 13:10:17.439: INFO: Waiting for pod pod-configmaps-7f2face8-5a6a-4840-824b-e06a43457cb0 to disappear
    Mar  1 13:10:17.443: INFO: Pod pod-configmaps-7f2face8-5a6a-4840-824b-e06a43457cb0 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Mar  1 13:10:17.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-9261" for this suite. 03/01/23 13:10:17.45
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:10:17.461
Mar  1 13:10:17.461: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename runtimeclass 03/01/23 13:10:17.462
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:10:17.482
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:10:17.486
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Mar  1 13:10:17.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-3088" for this suite. 03/01/23 13:10:17.507
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]","completed":297,"skipped":5469,"failed":0}
------------------------------
â€¢ [0.055 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:10:17.461
    Mar  1 13:10:17.461: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename runtimeclass 03/01/23 13:10:17.462
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:10:17.482
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:10:17.486
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Mar  1 13:10:17.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-3088" for this suite. 03/01/23 13:10:17.507
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:10:17.518
Mar  1 13:10:17.518: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename projected 03/01/23 13:10:17.519
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:10:17.542
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:10:17.545
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
STEP: Creating a pod to test downward API volume plugin 03/01/23 13:10:17.547
Mar  1 13:10:17.557: INFO: Waiting up to 5m0s for pod "downwardapi-volume-70c41152-322f-42ca-8d44-a17bc9fec0fa" in namespace "projected-9379" to be "Succeeded or Failed"
Mar  1 13:10:17.561: INFO: Pod "downwardapi-volume-70c41152-322f-42ca-8d44-a17bc9fec0fa": Phase="Pending", Reason="", readiness=false. Elapsed: 3.880338ms
Mar  1 13:10:19.567: INFO: Pod "downwardapi-volume-70c41152-322f-42ca-8d44-a17bc9fec0fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010183959s
Mar  1 13:10:21.566: INFO: Pod "downwardapi-volume-70c41152-322f-42ca-8d44-a17bc9fec0fa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009236008s
STEP: Saw pod success 03/01/23 13:10:21.566
Mar  1 13:10:21.567: INFO: Pod "downwardapi-volume-70c41152-322f-42ca-8d44-a17bc9fec0fa" satisfied condition "Succeeded or Failed"
Mar  1 13:10:21.571: INFO: Trying to get logs from node lab1-k8s-node-3 pod downwardapi-volume-70c41152-322f-42ca-8d44-a17bc9fec0fa container client-container: <nil>
STEP: delete the pod 03/01/23 13:10:21.58
Mar  1 13:10:21.614: INFO: Waiting for pod downwardapi-volume-70c41152-322f-42ca-8d44-a17bc9fec0fa to disappear
Mar  1 13:10:21.618: INFO: Pod downwardapi-volume-70c41152-322f-42ca-8d44-a17bc9fec0fa no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Mar  1 13:10:21.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9379" for this suite. 03/01/23 13:10:21.626
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":298,"skipped":5474,"failed":0}
------------------------------
â€¢ [4.119 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:10:17.518
    Mar  1 13:10:17.518: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename projected 03/01/23 13:10:17.519
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:10:17.542
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:10:17.545
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:248
    STEP: Creating a pod to test downward API volume plugin 03/01/23 13:10:17.547
    Mar  1 13:10:17.557: INFO: Waiting up to 5m0s for pod "downwardapi-volume-70c41152-322f-42ca-8d44-a17bc9fec0fa" in namespace "projected-9379" to be "Succeeded or Failed"
    Mar  1 13:10:17.561: INFO: Pod "downwardapi-volume-70c41152-322f-42ca-8d44-a17bc9fec0fa": Phase="Pending", Reason="", readiness=false. Elapsed: 3.880338ms
    Mar  1 13:10:19.567: INFO: Pod "downwardapi-volume-70c41152-322f-42ca-8d44-a17bc9fec0fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010183959s
    Mar  1 13:10:21.566: INFO: Pod "downwardapi-volume-70c41152-322f-42ca-8d44-a17bc9fec0fa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009236008s
    STEP: Saw pod success 03/01/23 13:10:21.566
    Mar  1 13:10:21.567: INFO: Pod "downwardapi-volume-70c41152-322f-42ca-8d44-a17bc9fec0fa" satisfied condition "Succeeded or Failed"
    Mar  1 13:10:21.571: INFO: Trying to get logs from node lab1-k8s-node-3 pod downwardapi-volume-70c41152-322f-42ca-8d44-a17bc9fec0fa container client-container: <nil>
    STEP: delete the pod 03/01/23 13:10:21.58
    Mar  1 13:10:21.614: INFO: Waiting for pod downwardapi-volume-70c41152-322f-42ca-8d44-a17bc9fec0fa to disappear
    Mar  1 13:10:21.618: INFO: Pod downwardapi-volume-70c41152-322f-42ca-8d44-a17bc9fec0fa no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Mar  1 13:10:21.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9379" for this suite. 03/01/23 13:10:21.626
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:10:21.64
Mar  1 13:10:21.640: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename kubectl 03/01/23 13:10:21.641
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:10:21.667
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:10:21.67
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
STEP: create deployment with httpd image 03/01/23 13:10:21.672
Mar  1 13:10:21.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-604 create -f -'
Mar  1 13:10:22.457: INFO: stderr: ""
Mar  1 13:10:22.457: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 03/01/23 13:10:22.457
Mar  1 13:10:22.458: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-604 diff -f -'
Mar  1 13:10:22.667: INFO: rc: 1
Mar  1 13:10:22.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-604 delete -f -'
Mar  1 13:10:22.745: INFO: stderr: ""
Mar  1 13:10:22.745: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar  1 13:10:22.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-604" for this suite. 03/01/23 13:10:22.752
{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","completed":299,"skipped":5494,"failed":0}
------------------------------
â€¢ [1.121 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:923
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:929

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:10:21.64
    Mar  1 13:10:21.640: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename kubectl 03/01/23 13:10:21.641
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:10:21.667
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:10:21.67
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:929
    STEP: create deployment with httpd image 03/01/23 13:10:21.672
    Mar  1 13:10:21.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-604 create -f -'
    Mar  1 13:10:22.457: INFO: stderr: ""
    Mar  1 13:10:22.457: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 03/01/23 13:10:22.457
    Mar  1 13:10:22.458: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-604 diff -f -'
    Mar  1 13:10:22.667: INFO: rc: 1
    Mar  1 13:10:22.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-604 delete -f -'
    Mar  1 13:10:22.745: INFO: stderr: ""
    Mar  1 13:10:22.745: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar  1 13:10:22.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-604" for this suite. 03/01/23 13:10:22.752
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:10:22.761
Mar  1 13:10:22.761: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename gc 03/01/23 13:10:22.761
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:10:22.783
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:10:22.786
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Mar  1 13:10:22.828: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"8ecbf560-b38c-44a5-83c6-91924bd34eca", Controller:(*bool)(0xc0054dc966), BlockOwnerDeletion:(*bool)(0xc0054dc967)}}
Mar  1 13:10:22.838: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"89646f3a-1fa8-4150-b992-401a4127935d", Controller:(*bool)(0xc0054dcbb6), BlockOwnerDeletion:(*bool)(0xc0054dcbb7)}}
Mar  1 13:10:22.845: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"8d2d854b-7367-4fd8-8f2e-55cbb8f82fcb", Controller:(*bool)(0xc003fd64ae), BlockOwnerDeletion:(*bool)(0xc003fd64af)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Mar  1 13:10:27.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2644" for this suite. 03/01/23 13:10:27.867
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","completed":300,"skipped":5494,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.119 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:10:22.761
    Mar  1 13:10:22.761: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename gc 03/01/23 13:10:22.761
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:10:22.783
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:10:22.786
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Mar  1 13:10:22.828: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"8ecbf560-b38c-44a5-83c6-91924bd34eca", Controller:(*bool)(0xc0054dc966), BlockOwnerDeletion:(*bool)(0xc0054dc967)}}
    Mar  1 13:10:22.838: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"89646f3a-1fa8-4150-b992-401a4127935d", Controller:(*bool)(0xc0054dcbb6), BlockOwnerDeletion:(*bool)(0xc0054dcbb7)}}
    Mar  1 13:10:22.845: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"8d2d854b-7367-4fd8-8f2e-55cbb8f82fcb", Controller:(*bool)(0xc003fd64ae), BlockOwnerDeletion:(*bool)(0xc003fd64af)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Mar  1 13:10:27.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-2644" for this suite. 03/01/23 13:10:27.867
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:10:27.882
Mar  1 13:10:27.882: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename replicaset 03/01/23 13:10:27.883
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:10:27.912
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:10:27.916
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 03/01/23 13:10:27.918
STEP: Verify that the required pods have come up 03/01/23 13:10:27.928
Mar  1 13:10:27.932: INFO: Pod name sample-pod: Found 0 pods out of 3
Mar  1 13:10:32.939: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 03/01/23 13:10:32.939
Mar  1 13:10:32.944: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 03/01/23 13:10:32.944
STEP: DeleteCollection of the ReplicaSets 03/01/23 13:10:32.952
STEP: After DeleteCollection verify that ReplicaSets have been deleted 03/01/23 13:10:32.965
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Mar  1 13:10:32.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3865" for this suite. 03/01/23 13:10:32.976
{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","completed":301,"skipped":5534,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.103 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:10:27.882
    Mar  1 13:10:27.882: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename replicaset 03/01/23 13:10:27.883
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:10:27.912
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:10:27.916
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 03/01/23 13:10:27.918
    STEP: Verify that the required pods have come up 03/01/23 13:10:27.928
    Mar  1 13:10:27.932: INFO: Pod name sample-pod: Found 0 pods out of 3
    Mar  1 13:10:32.939: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 03/01/23 13:10:32.939
    Mar  1 13:10:32.944: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 03/01/23 13:10:32.944
    STEP: DeleteCollection of the ReplicaSets 03/01/23 13:10:32.952
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 03/01/23 13:10:32.965
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Mar  1 13:10:32.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-3865" for this suite. 03/01/23 13:10:32.976
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:10:32.987
Mar  1 13:10:32.987: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename namespaces 03/01/23 13:10:32.994
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:10:33.034
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:10:33.052
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
STEP: Creating a test namespace 03/01/23 13:10:33.062
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:10:33.087
STEP: Creating a service in the namespace 03/01/23 13:10:33.092
STEP: Deleting the namespace 03/01/23 13:10:33.109
STEP: Waiting for the namespace to be removed. 03/01/23 13:10:33.125
STEP: Recreating the namespace 03/01/23 13:10:39.131
STEP: Verifying there is no service in the namespace 03/01/23 13:10:39.161
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Mar  1 13:10:39.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9727" for this suite. 03/01/23 13:10:39.172
STEP: Destroying namespace "nsdeletetest-1577" for this suite. 03/01/23 13:10:39.181
Mar  1 13:10:39.185: INFO: Namespace nsdeletetest-1577 was already deleted
STEP: Destroying namespace "nsdeletetest-2599" for this suite. 03/01/23 13:10:39.186
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","completed":302,"skipped":5534,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.207 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:10:32.987
    Mar  1 13:10:32.987: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename namespaces 03/01/23 13:10:32.994
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:10:33.034
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:10:33.052
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:250
    STEP: Creating a test namespace 03/01/23 13:10:33.062
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:10:33.087
    STEP: Creating a service in the namespace 03/01/23 13:10:33.092
    STEP: Deleting the namespace 03/01/23 13:10:33.109
    STEP: Waiting for the namespace to be removed. 03/01/23 13:10:33.125
    STEP: Recreating the namespace 03/01/23 13:10:39.131
    STEP: Verifying there is no service in the namespace 03/01/23 13:10:39.161
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Mar  1 13:10:39.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-9727" for this suite. 03/01/23 13:10:39.172
    STEP: Destroying namespace "nsdeletetest-1577" for this suite. 03/01/23 13:10:39.181
    Mar  1 13:10:39.185: INFO: Namespace nsdeletetest-1577 was already deleted
    STEP: Destroying namespace "nsdeletetest-2599" for this suite. 03/01/23 13:10:39.186
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:10:39.194
Mar  1 13:10:39.194: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename gc 03/01/23 13:10:39.195
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:10:39.215
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:10:39.218
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 03/01/23 13:10:39.22
STEP: delete the rc 03/01/23 13:10:44.232
STEP: wait for all pods to be garbage collected 03/01/23 13:10:44.242
STEP: Gathering metrics 03/01/23 13:10:49.251
Mar  1 13:10:49.284: INFO: Waiting up to 5m0s for pod "kube-controller-manager-lab1-k8s-master-3" in namespace "kube-system" to be "running and ready"
Mar  1 13:10:49.289: INFO: Pod "kube-controller-manager-lab1-k8s-master-3": Phase="Running", Reason="", readiness=true. Elapsed: 4.744732ms
Mar  1 13:10:49.289: INFO: The phase of Pod kube-controller-manager-lab1-k8s-master-3 is Running (Ready = true)
Mar  1 13:10:49.289: INFO: Pod "kube-controller-manager-lab1-k8s-master-3" satisfied condition "running and ready"
Mar  1 13:10:49.346: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Mar  1 13:10:49.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6032" for this suite. 03/01/23 13:10:49.353
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","completed":303,"skipped":5562,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.167 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:10:39.194
    Mar  1 13:10:39.194: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename gc 03/01/23 13:10:39.195
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:10:39.215
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:10:39.218
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 03/01/23 13:10:39.22
    STEP: delete the rc 03/01/23 13:10:44.232
    STEP: wait for all pods to be garbage collected 03/01/23 13:10:44.242
    STEP: Gathering metrics 03/01/23 13:10:49.251
    Mar  1 13:10:49.284: INFO: Waiting up to 5m0s for pod "kube-controller-manager-lab1-k8s-master-3" in namespace "kube-system" to be "running and ready"
    Mar  1 13:10:49.289: INFO: Pod "kube-controller-manager-lab1-k8s-master-3": Phase="Running", Reason="", readiness=true. Elapsed: 4.744732ms
    Mar  1 13:10:49.289: INFO: The phase of Pod kube-controller-manager-lab1-k8s-master-3 is Running (Ready = true)
    Mar  1 13:10:49.289: INFO: Pod "kube-controller-manager-lab1-k8s-master-3" satisfied condition "running and ready"
    Mar  1 13:10:49.346: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Mar  1 13:10:49.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-6032" for this suite. 03/01/23 13:10:49.353
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:10:49.367
Mar  1 13:10:49.367: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename emptydir 03/01/23 13:10:49.368
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:10:49.388
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:10:49.391
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
STEP: Creating a pod to test emptydir 0666 on node default medium 03/01/23 13:10:49.394
Mar  1 13:10:49.405: INFO: Waiting up to 5m0s for pod "pod-b4c3155a-31f6-4091-9df2-152661966d41" in namespace "emptydir-7293" to be "Succeeded or Failed"
Mar  1 13:10:49.410: INFO: Pod "pod-b4c3155a-31f6-4091-9df2-152661966d41": Phase="Pending", Reason="", readiness=false. Elapsed: 5.533502ms
Mar  1 13:10:51.415: INFO: Pod "pod-b4c3155a-31f6-4091-9df2-152661966d41": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010181716s
Mar  1 13:10:53.415: INFO: Pod "pod-b4c3155a-31f6-4091-9df2-152661966d41": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01054335s
STEP: Saw pod success 03/01/23 13:10:53.415
Mar  1 13:10:53.416: INFO: Pod "pod-b4c3155a-31f6-4091-9df2-152661966d41" satisfied condition "Succeeded or Failed"
Mar  1 13:10:53.420: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-b4c3155a-31f6-4091-9df2-152661966d41 container test-container: <nil>
STEP: delete the pod 03/01/23 13:10:53.429
Mar  1 13:10:53.448: INFO: Waiting for pod pod-b4c3155a-31f6-4091-9df2-152661966d41 to disappear
Mar  1 13:10:53.452: INFO: Pod pod-b4c3155a-31f6-4091-9df2-152661966d41 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar  1 13:10:53.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7293" for this suite. 03/01/23 13:10:53.457
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":304,"skipped":5568,"failed":0}
------------------------------
â€¢ [4.100 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:10:49.367
    Mar  1 13:10:49.367: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename emptydir 03/01/23 13:10:49.368
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:10:49.388
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:10:49.391
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:206
    STEP: Creating a pod to test emptydir 0666 on node default medium 03/01/23 13:10:49.394
    Mar  1 13:10:49.405: INFO: Waiting up to 5m0s for pod "pod-b4c3155a-31f6-4091-9df2-152661966d41" in namespace "emptydir-7293" to be "Succeeded or Failed"
    Mar  1 13:10:49.410: INFO: Pod "pod-b4c3155a-31f6-4091-9df2-152661966d41": Phase="Pending", Reason="", readiness=false. Elapsed: 5.533502ms
    Mar  1 13:10:51.415: INFO: Pod "pod-b4c3155a-31f6-4091-9df2-152661966d41": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010181716s
    Mar  1 13:10:53.415: INFO: Pod "pod-b4c3155a-31f6-4091-9df2-152661966d41": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01054335s
    STEP: Saw pod success 03/01/23 13:10:53.415
    Mar  1 13:10:53.416: INFO: Pod "pod-b4c3155a-31f6-4091-9df2-152661966d41" satisfied condition "Succeeded or Failed"
    Mar  1 13:10:53.420: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-b4c3155a-31f6-4091-9df2-152661966d41 container test-container: <nil>
    STEP: delete the pod 03/01/23 13:10:53.429
    Mar  1 13:10:53.448: INFO: Waiting for pod pod-b4c3155a-31f6-4091-9df2-152661966d41 to disappear
    Mar  1 13:10:53.452: INFO: Pod pod-b4c3155a-31f6-4091-9df2-152661966d41 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar  1 13:10:53.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-7293" for this suite. 03/01/23 13:10:53.457
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:10:53.468
Mar  1 13:10:53.469: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename services 03/01/23 13:10:53.469
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:10:53.49
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:10:53.492
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-278 03/01/23 13:10:53.495
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 03/01/23 13:10:53.509
STEP: creating service externalsvc in namespace services-278 03/01/23 13:10:53.511
STEP: creating replication controller externalsvc in namespace services-278 03/01/23 13:10:53.525
I0301 13:10:53.533352      19 runners.go:193] Created replication controller with name: externalsvc, namespace: services-278, replica count: 2
I0301 13:10:56.584964      19 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 03/01/23 13:10:56.59
Mar  1 13:10:56.607: INFO: Creating new exec pod
Mar  1 13:10:56.620: INFO: Waiting up to 5m0s for pod "execpodvtstd" in namespace "services-278" to be "running"
Mar  1 13:10:56.626: INFO: Pod "execpodvtstd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.553003ms
Mar  1 13:10:58.631: INFO: Pod "execpodvtstd": Phase="Running", Reason="", readiness=true. Elapsed: 2.009330393s
Mar  1 13:10:58.631: INFO: Pod "execpodvtstd" satisfied condition "running"
Mar  1 13:10:58.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-278 exec execpodvtstd -- /bin/sh -x -c nslookup clusterip-service.services-278.svc.cluster.local'
Mar  1 13:10:58.819: INFO: stderr: "+ nslookup clusterip-service.services-278.svc.cluster.local\n"
Mar  1 13:10:58.819: INFO: stdout: "Server:\t\t169.254.25.10\nAddress:\t169.254.25.10#53\n\nclusterip-service.services-278.svc.cluster.local\tcanonical name = externalsvc.services-278.svc.cluster.local.\nName:\texternalsvc.services-278.svc.cluster.local\nAddress: 10.233.22.26\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-278, will wait for the garbage collector to delete the pods 03/01/23 13:10:58.819
Mar  1 13:10:58.884: INFO: Deleting ReplicationController externalsvc took: 9.112861ms
Mar  1 13:10:58.984: INFO: Terminating ReplicationController externalsvc pods took: 100.469059ms
Mar  1 13:11:01.316: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar  1 13:11:01.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-278" for this suite. 03/01/23 13:11:01.339
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","completed":305,"skipped":5568,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.881 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:10:53.468
    Mar  1 13:10:53.469: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename services 03/01/23 13:10:53.469
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:10:53.49
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:10:53.492
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1481
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-278 03/01/23 13:10:53.495
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 03/01/23 13:10:53.509
    STEP: creating service externalsvc in namespace services-278 03/01/23 13:10:53.511
    STEP: creating replication controller externalsvc in namespace services-278 03/01/23 13:10:53.525
    I0301 13:10:53.533352      19 runners.go:193] Created replication controller with name: externalsvc, namespace: services-278, replica count: 2
    I0301 13:10:56.584964      19 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 03/01/23 13:10:56.59
    Mar  1 13:10:56.607: INFO: Creating new exec pod
    Mar  1 13:10:56.620: INFO: Waiting up to 5m0s for pod "execpodvtstd" in namespace "services-278" to be "running"
    Mar  1 13:10:56.626: INFO: Pod "execpodvtstd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.553003ms
    Mar  1 13:10:58.631: INFO: Pod "execpodvtstd": Phase="Running", Reason="", readiness=true. Elapsed: 2.009330393s
    Mar  1 13:10:58.631: INFO: Pod "execpodvtstd" satisfied condition "running"
    Mar  1 13:10:58.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-278 exec execpodvtstd -- /bin/sh -x -c nslookup clusterip-service.services-278.svc.cluster.local'
    Mar  1 13:10:58.819: INFO: stderr: "+ nslookup clusterip-service.services-278.svc.cluster.local\n"
    Mar  1 13:10:58.819: INFO: stdout: "Server:\t\t169.254.25.10\nAddress:\t169.254.25.10#53\n\nclusterip-service.services-278.svc.cluster.local\tcanonical name = externalsvc.services-278.svc.cluster.local.\nName:\texternalsvc.services-278.svc.cluster.local\nAddress: 10.233.22.26\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-278, will wait for the garbage collector to delete the pods 03/01/23 13:10:58.819
    Mar  1 13:10:58.884: INFO: Deleting ReplicationController externalsvc took: 9.112861ms
    Mar  1 13:10:58.984: INFO: Terminating ReplicationController externalsvc pods took: 100.469059ms
    Mar  1 13:11:01.316: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar  1 13:11:01.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-278" for this suite. 03/01/23 13:11:01.339
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:11:01.351
Mar  1 13:11:01.351: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename dns 03/01/23 13:11:01.352
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:11:01.372
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:11:01.375
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 03/01/23 13:11:01.377
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 03/01/23 13:11:01.377
STEP: creating a pod to probe DNS 03/01/23 13:11:01.377
STEP: submitting the pod to kubernetes 03/01/23 13:11:01.377
Mar  1 13:11:01.393: INFO: Waiting up to 15m0s for pod "dns-test-1e964c3c-c78f-48a9-9408-7d4cbd85bfdd" in namespace "dns-5609" to be "running"
Mar  1 13:11:01.399: INFO: Pod "dns-test-1e964c3c-c78f-48a9-9408-7d4cbd85bfdd": Phase="Pending", Reason="", readiness=false. Elapsed: 5.61972ms
Mar  1 13:11:03.405: INFO: Pod "dns-test-1e964c3c-c78f-48a9-9408-7d4cbd85bfdd": Phase="Running", Reason="", readiness=true. Elapsed: 2.011089064s
Mar  1 13:11:03.405: INFO: Pod "dns-test-1e964c3c-c78f-48a9-9408-7d4cbd85bfdd" satisfied condition "running"
STEP: retrieving the pod 03/01/23 13:11:03.405
STEP: looking for the results for each expected name from probers 03/01/23 13:11:03.409
Mar  1 13:11:03.429: INFO: DNS probes using dns-5609/dns-test-1e964c3c-c78f-48a9-9408-7d4cbd85bfdd succeeded

STEP: deleting the pod 03/01/23 13:11:03.43
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Mar  1 13:11:03.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5609" for this suite. 03/01/23 13:11:03.451
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","completed":306,"skipped":5605,"failed":0}
------------------------------
â€¢ [2.108 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:11:01.351
    Mar  1 13:11:01.351: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename dns 03/01/23 13:11:01.352
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:11:01.372
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:11:01.375
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     03/01/23 13:11:01.377
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     03/01/23 13:11:01.377
    STEP: creating a pod to probe DNS 03/01/23 13:11:01.377
    STEP: submitting the pod to kubernetes 03/01/23 13:11:01.377
    Mar  1 13:11:01.393: INFO: Waiting up to 15m0s for pod "dns-test-1e964c3c-c78f-48a9-9408-7d4cbd85bfdd" in namespace "dns-5609" to be "running"
    Mar  1 13:11:01.399: INFO: Pod "dns-test-1e964c3c-c78f-48a9-9408-7d4cbd85bfdd": Phase="Pending", Reason="", readiness=false. Elapsed: 5.61972ms
    Mar  1 13:11:03.405: INFO: Pod "dns-test-1e964c3c-c78f-48a9-9408-7d4cbd85bfdd": Phase="Running", Reason="", readiness=true. Elapsed: 2.011089064s
    Mar  1 13:11:03.405: INFO: Pod "dns-test-1e964c3c-c78f-48a9-9408-7d4cbd85bfdd" satisfied condition "running"
    STEP: retrieving the pod 03/01/23 13:11:03.405
    STEP: looking for the results for each expected name from probers 03/01/23 13:11:03.409
    Mar  1 13:11:03.429: INFO: DNS probes using dns-5609/dns-test-1e964c3c-c78f-48a9-9408-7d4cbd85bfdd succeeded

    STEP: deleting the pod 03/01/23 13:11:03.43
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Mar  1 13:11:03.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-5609" for this suite. 03/01/23 13:11:03.451
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:11:03.468
Mar  1 13:11:03.470: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename webhook 03/01/23 13:11:03.47
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:11:03.495
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:11:03.497
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/01/23 13:11:03.517
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/01/23 13:11:04.071
STEP: Deploying the webhook pod 03/01/23 13:11:04.087
STEP: Wait for the deployment to be ready 03/01/23 13:11:04.1
Mar  1 13:11:04.109: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/01/23 13:11:06.125
STEP: Verifying the service has paired with the endpoint 03/01/23 13:11:06.14
Mar  1 13:11:07.140: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 03/01/23 13:11:07.145
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 03/01/23 13:11:07.162
STEP: Creating a dummy validating-webhook-configuration object 03/01/23 13:11:07.177
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 03/01/23 13:11:07.188
STEP: Creating a dummy mutating-webhook-configuration object 03/01/23 13:11:07.197
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 03/01/23 13:11:07.206
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  1 13:11:07.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9422" for this suite. 03/01/23 13:11:07.241
STEP: Destroying namespace "webhook-9422-markers" for this suite. 03/01/23 13:11:07.249
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","completed":307,"skipped":5611,"failed":0}
------------------------------
â€¢ [3.859 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:11:03.468
    Mar  1 13:11:03.470: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename webhook 03/01/23 13:11:03.47
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:11:03.495
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:11:03.497
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/01/23 13:11:03.517
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/01/23 13:11:04.071
    STEP: Deploying the webhook pod 03/01/23 13:11:04.087
    STEP: Wait for the deployment to be ready 03/01/23 13:11:04.1
    Mar  1 13:11:04.109: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/01/23 13:11:06.125
    STEP: Verifying the service has paired with the endpoint 03/01/23 13:11:06.14
    Mar  1 13:11:07.140: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:276
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 03/01/23 13:11:07.145
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 03/01/23 13:11:07.162
    STEP: Creating a dummy validating-webhook-configuration object 03/01/23 13:11:07.177
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 03/01/23 13:11:07.188
    STEP: Creating a dummy mutating-webhook-configuration object 03/01/23 13:11:07.197
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 03/01/23 13:11:07.206
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  1 13:11:07.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-9422" for this suite. 03/01/23 13:11:07.241
    STEP: Destroying namespace "webhook-9422-markers" for this suite. 03/01/23 13:11:07.249
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:11:07.32
Mar  1 13:11:07.321: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename downward-api 03/01/23 13:11:07.321
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:11:07.343
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:11:07.346
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
STEP: Creating a pod to test downward api env vars 03/01/23 13:11:07.349
Mar  1 13:11:07.362: INFO: Waiting up to 5m0s for pod "downward-api-eace37c2-8452-45b2-911d-53772348abef" in namespace "downward-api-3207" to be "Succeeded or Failed"
Mar  1 13:11:07.367: INFO: Pod "downward-api-eace37c2-8452-45b2-911d-53772348abef": Phase="Pending", Reason="", readiness=false. Elapsed: 4.391115ms
Mar  1 13:11:09.372: INFO: Pod "downward-api-eace37c2-8452-45b2-911d-53772348abef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00990395s
Mar  1 13:11:11.372: INFO: Pod "downward-api-eace37c2-8452-45b2-911d-53772348abef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010189293s
STEP: Saw pod success 03/01/23 13:11:11.372
Mar  1 13:11:11.373: INFO: Pod "downward-api-eace37c2-8452-45b2-911d-53772348abef" satisfied condition "Succeeded or Failed"
Mar  1 13:11:11.377: INFO: Trying to get logs from node lab1-k8s-node-3 pod downward-api-eace37c2-8452-45b2-911d-53772348abef container dapi-container: <nil>
STEP: delete the pod 03/01/23 13:11:11.387
Mar  1 13:11:11.405: INFO: Waiting for pod downward-api-eace37c2-8452-45b2-911d-53772348abef to disappear
Mar  1 13:11:11.412: INFO: Pod downward-api-eace37c2-8452-45b2-911d-53772348abef no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Mar  1 13:11:11.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3207" for this suite. 03/01/23 13:11:11.418
{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","completed":308,"skipped":5619,"failed":0}
------------------------------
â€¢ [4.105 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:11:07.32
    Mar  1 13:11:07.321: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename downward-api 03/01/23 13:11:07.321
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:11:07.343
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:11:07.346
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:43
    STEP: Creating a pod to test downward api env vars 03/01/23 13:11:07.349
    Mar  1 13:11:07.362: INFO: Waiting up to 5m0s for pod "downward-api-eace37c2-8452-45b2-911d-53772348abef" in namespace "downward-api-3207" to be "Succeeded or Failed"
    Mar  1 13:11:07.367: INFO: Pod "downward-api-eace37c2-8452-45b2-911d-53772348abef": Phase="Pending", Reason="", readiness=false. Elapsed: 4.391115ms
    Mar  1 13:11:09.372: INFO: Pod "downward-api-eace37c2-8452-45b2-911d-53772348abef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00990395s
    Mar  1 13:11:11.372: INFO: Pod "downward-api-eace37c2-8452-45b2-911d-53772348abef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010189293s
    STEP: Saw pod success 03/01/23 13:11:11.372
    Mar  1 13:11:11.373: INFO: Pod "downward-api-eace37c2-8452-45b2-911d-53772348abef" satisfied condition "Succeeded or Failed"
    Mar  1 13:11:11.377: INFO: Trying to get logs from node lab1-k8s-node-3 pod downward-api-eace37c2-8452-45b2-911d-53772348abef container dapi-container: <nil>
    STEP: delete the pod 03/01/23 13:11:11.387
    Mar  1 13:11:11.405: INFO: Waiting for pod downward-api-eace37c2-8452-45b2-911d-53772348abef to disappear
    Mar  1 13:11:11.412: INFO: Pod downward-api-eace37c2-8452-45b2-911d-53772348abef no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Mar  1 13:11:11.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-3207" for this suite. 03/01/23 13:11:11.418
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:11:11.427
Mar  1 13:11:11.427: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename projected 03/01/23 13:11:11.428
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:11:11.45
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:11:11.453
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
STEP: Creating configMap with name configmap-projected-all-test-volume-23c14ea1-b998-4685-9f49-e4146f7c7e38 03/01/23 13:11:11.456
STEP: Creating secret with name secret-projected-all-test-volume-0ea51c2e-6f2f-401f-a069-28b647102200 03/01/23 13:11:11.462
STEP: Creating a pod to test Check all projections for projected volume plugin 03/01/23 13:11:11.467
Mar  1 13:11:11.477: INFO: Waiting up to 5m0s for pod "projected-volume-fc498264-d045-44dc-9833-8089d8e59839" in namespace "projected-726" to be "Succeeded or Failed"
Mar  1 13:11:11.482: INFO: Pod "projected-volume-fc498264-d045-44dc-9833-8089d8e59839": Phase="Pending", Reason="", readiness=false. Elapsed: 4.376171ms
Mar  1 13:11:13.487: INFO: Pod "projected-volume-fc498264-d045-44dc-9833-8089d8e59839": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009719051s
Mar  1 13:11:15.487: INFO: Pod "projected-volume-fc498264-d045-44dc-9833-8089d8e59839": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009540983s
STEP: Saw pod success 03/01/23 13:11:15.487
Mar  1 13:11:15.487: INFO: Pod "projected-volume-fc498264-d045-44dc-9833-8089d8e59839" satisfied condition "Succeeded or Failed"
Mar  1 13:11:15.492: INFO: Trying to get logs from node lab1-k8s-node-3 pod projected-volume-fc498264-d045-44dc-9833-8089d8e59839 container projected-all-volume-test: <nil>
STEP: delete the pod 03/01/23 13:11:15.5
Mar  1 13:11:15.514: INFO: Waiting for pod projected-volume-fc498264-d045-44dc-9833-8089d8e59839 to disappear
Mar  1 13:11:15.518: INFO: Pod projected-volume-fc498264-d045-44dc-9833-8089d8e59839 no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:187
Mar  1 13:11:15.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-726" for this suite. 03/01/23 13:11:15.531
{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","completed":309,"skipped":5627,"failed":0}
------------------------------
â€¢ [4.121 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:11:11.427
    Mar  1 13:11:11.427: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename projected 03/01/23 13:11:11.428
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:11:11.45
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:11:11.453
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:43
    STEP: Creating configMap with name configmap-projected-all-test-volume-23c14ea1-b998-4685-9f49-e4146f7c7e38 03/01/23 13:11:11.456
    STEP: Creating secret with name secret-projected-all-test-volume-0ea51c2e-6f2f-401f-a069-28b647102200 03/01/23 13:11:11.462
    STEP: Creating a pod to test Check all projections for projected volume plugin 03/01/23 13:11:11.467
    Mar  1 13:11:11.477: INFO: Waiting up to 5m0s for pod "projected-volume-fc498264-d045-44dc-9833-8089d8e59839" in namespace "projected-726" to be "Succeeded or Failed"
    Mar  1 13:11:11.482: INFO: Pod "projected-volume-fc498264-d045-44dc-9833-8089d8e59839": Phase="Pending", Reason="", readiness=false. Elapsed: 4.376171ms
    Mar  1 13:11:13.487: INFO: Pod "projected-volume-fc498264-d045-44dc-9833-8089d8e59839": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009719051s
    Mar  1 13:11:15.487: INFO: Pod "projected-volume-fc498264-d045-44dc-9833-8089d8e59839": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009540983s
    STEP: Saw pod success 03/01/23 13:11:15.487
    Mar  1 13:11:15.487: INFO: Pod "projected-volume-fc498264-d045-44dc-9833-8089d8e59839" satisfied condition "Succeeded or Failed"
    Mar  1 13:11:15.492: INFO: Trying to get logs from node lab1-k8s-node-3 pod projected-volume-fc498264-d045-44dc-9833-8089d8e59839 container projected-all-volume-test: <nil>
    STEP: delete the pod 03/01/23 13:11:15.5
    Mar  1 13:11:15.514: INFO: Waiting for pod projected-volume-fc498264-d045-44dc-9833-8089d8e59839 to disappear
    Mar  1 13:11:15.518: INFO: Pod projected-volume-fc498264-d045-44dc-9833-8089d8e59839 no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:187
    Mar  1 13:11:15.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-726" for this suite. 03/01/23 13:11:15.531
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:11:15.549
Mar  1 13:11:15.549: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename configmap 03/01/23 13:11:15.55
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:11:15.573
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:11:15.577
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
STEP: Creating configMap with name configmap-test-volume-0dbc7b9a-947d-41b7-9036-8e0e98e81c80 03/01/23 13:11:15.579
STEP: Creating a pod to test consume configMaps 03/01/23 13:11:15.584
Mar  1 13:11:15.594: INFO: Waiting up to 5m0s for pod "pod-configmaps-c6d8bc50-6b27-4ad5-a57c-f200d8bfc306" in namespace "configmap-6542" to be "Succeeded or Failed"
Mar  1 13:11:15.598: INFO: Pod "pod-configmaps-c6d8bc50-6b27-4ad5-a57c-f200d8bfc306": Phase="Pending", Reason="", readiness=false. Elapsed: 3.814131ms
Mar  1 13:11:17.605: INFO: Pod "pod-configmaps-c6d8bc50-6b27-4ad5-a57c-f200d8bfc306": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01051841s
Mar  1 13:11:19.604: INFO: Pod "pod-configmaps-c6d8bc50-6b27-4ad5-a57c-f200d8bfc306": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009684582s
STEP: Saw pod success 03/01/23 13:11:19.604
Mar  1 13:11:19.604: INFO: Pod "pod-configmaps-c6d8bc50-6b27-4ad5-a57c-f200d8bfc306" satisfied condition "Succeeded or Failed"
Mar  1 13:11:19.608: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-configmaps-c6d8bc50-6b27-4ad5-a57c-f200d8bfc306 container configmap-volume-test: <nil>
STEP: delete the pod 03/01/23 13:11:19.622
Mar  1 13:11:19.637: INFO: Waiting for pod pod-configmaps-c6d8bc50-6b27-4ad5-a57c-f200d8bfc306 to disappear
Mar  1 13:11:19.642: INFO: Pod pod-configmaps-c6d8bc50-6b27-4ad5-a57c-f200d8bfc306 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Mar  1 13:11:19.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6542" for this suite. 03/01/23 13:11:19.65
{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":310,"skipped":5642,"failed":0}
------------------------------
â€¢ [4.111 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:11:15.549
    Mar  1 13:11:15.549: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename configmap 03/01/23 13:11:15.55
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:11:15.573
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:11:15.577
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:422
    STEP: Creating configMap with name configmap-test-volume-0dbc7b9a-947d-41b7-9036-8e0e98e81c80 03/01/23 13:11:15.579
    STEP: Creating a pod to test consume configMaps 03/01/23 13:11:15.584
    Mar  1 13:11:15.594: INFO: Waiting up to 5m0s for pod "pod-configmaps-c6d8bc50-6b27-4ad5-a57c-f200d8bfc306" in namespace "configmap-6542" to be "Succeeded or Failed"
    Mar  1 13:11:15.598: INFO: Pod "pod-configmaps-c6d8bc50-6b27-4ad5-a57c-f200d8bfc306": Phase="Pending", Reason="", readiness=false. Elapsed: 3.814131ms
    Mar  1 13:11:17.605: INFO: Pod "pod-configmaps-c6d8bc50-6b27-4ad5-a57c-f200d8bfc306": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01051841s
    Mar  1 13:11:19.604: INFO: Pod "pod-configmaps-c6d8bc50-6b27-4ad5-a57c-f200d8bfc306": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009684582s
    STEP: Saw pod success 03/01/23 13:11:19.604
    Mar  1 13:11:19.604: INFO: Pod "pod-configmaps-c6d8bc50-6b27-4ad5-a57c-f200d8bfc306" satisfied condition "Succeeded or Failed"
    Mar  1 13:11:19.608: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-configmaps-c6d8bc50-6b27-4ad5-a57c-f200d8bfc306 container configmap-volume-test: <nil>
    STEP: delete the pod 03/01/23 13:11:19.622
    Mar  1 13:11:19.637: INFO: Waiting for pod pod-configmaps-c6d8bc50-6b27-4ad5-a57c-f200d8bfc306 to disappear
    Mar  1 13:11:19.642: INFO: Pod pod-configmaps-c6d8bc50-6b27-4ad5-a57c-f200d8bfc306 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Mar  1 13:11:19.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-6542" for this suite. 03/01/23 13:11:19.65
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:11:19.662
Mar  1 13:11:19.662: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename emptydir 03/01/23 13:11:19.663
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:11:19.682
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:11:19.685
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
STEP: Creating a pod to test emptydir 0644 on node default medium 03/01/23 13:11:19.688
Mar  1 13:11:19.698: INFO: Waiting up to 5m0s for pod "pod-ac262ccc-9f8a-4cd0-94a2-5b35239dc8c4" in namespace "emptydir-5326" to be "Succeeded or Failed"
Mar  1 13:11:19.703: INFO: Pod "pod-ac262ccc-9f8a-4cd0-94a2-5b35239dc8c4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.28459ms
Mar  1 13:11:21.709: INFO: Pod "pod-ac262ccc-9f8a-4cd0-94a2-5b35239dc8c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010354801s
Mar  1 13:11:23.709: INFO: Pod "pod-ac262ccc-9f8a-4cd0-94a2-5b35239dc8c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010018762s
STEP: Saw pod success 03/01/23 13:11:23.709
Mar  1 13:11:23.709: INFO: Pod "pod-ac262ccc-9f8a-4cd0-94a2-5b35239dc8c4" satisfied condition "Succeeded or Failed"
Mar  1 13:11:23.713: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-ac262ccc-9f8a-4cd0-94a2-5b35239dc8c4 container test-container: <nil>
STEP: delete the pod 03/01/23 13:11:23.723
Mar  1 13:11:23.738: INFO: Waiting for pod pod-ac262ccc-9f8a-4cd0-94a2-5b35239dc8c4 to disappear
Mar  1 13:11:23.742: INFO: Pod pod-ac262ccc-9f8a-4cd0-94a2-5b35239dc8c4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar  1 13:11:23.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5326" for this suite. 03/01/23 13:11:23.749
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":311,"skipped":5671,"failed":0}
------------------------------
â€¢ [4.097 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:11:19.662
    Mar  1 13:11:19.662: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename emptydir 03/01/23 13:11:19.663
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:11:19.682
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:11:19.685
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:166
    STEP: Creating a pod to test emptydir 0644 on node default medium 03/01/23 13:11:19.688
    Mar  1 13:11:19.698: INFO: Waiting up to 5m0s for pod "pod-ac262ccc-9f8a-4cd0-94a2-5b35239dc8c4" in namespace "emptydir-5326" to be "Succeeded or Failed"
    Mar  1 13:11:19.703: INFO: Pod "pod-ac262ccc-9f8a-4cd0-94a2-5b35239dc8c4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.28459ms
    Mar  1 13:11:21.709: INFO: Pod "pod-ac262ccc-9f8a-4cd0-94a2-5b35239dc8c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010354801s
    Mar  1 13:11:23.709: INFO: Pod "pod-ac262ccc-9f8a-4cd0-94a2-5b35239dc8c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010018762s
    STEP: Saw pod success 03/01/23 13:11:23.709
    Mar  1 13:11:23.709: INFO: Pod "pod-ac262ccc-9f8a-4cd0-94a2-5b35239dc8c4" satisfied condition "Succeeded or Failed"
    Mar  1 13:11:23.713: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-ac262ccc-9f8a-4cd0-94a2-5b35239dc8c4 container test-container: <nil>
    STEP: delete the pod 03/01/23 13:11:23.723
    Mar  1 13:11:23.738: INFO: Waiting for pod pod-ac262ccc-9f8a-4cd0-94a2-5b35239dc8c4 to disappear
    Mar  1 13:11:23.742: INFO: Pod pod-ac262ccc-9f8a-4cd0-94a2-5b35239dc8c4 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar  1 13:11:23.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-5326" for this suite. 03/01/23 13:11:23.749
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:11:23.761
Mar  1 13:11:23.761: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename events 03/01/23 13:11:23.762
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:11:23.787
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:11:23.789
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 03/01/23 13:11:23.791
Mar  1 13:11:23.801: INFO: created test-event-1
Mar  1 13:11:23.805: INFO: created test-event-2
Mar  1 13:11:23.811: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 03/01/23 13:11:23.811
STEP: delete collection of events 03/01/23 13:11:23.816
Mar  1 13:11:23.816: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 03/01/23 13:11:23.847
Mar  1 13:11:23.847: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
Mar  1 13:11:23.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5718" for this suite. 03/01/23 13:11:23.857
{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","completed":312,"skipped":5684,"failed":0}
------------------------------
â€¢ [0.105 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:11:23.761
    Mar  1 13:11:23.761: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename events 03/01/23 13:11:23.762
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:11:23.787
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:11:23.789
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 03/01/23 13:11:23.791
    Mar  1 13:11:23.801: INFO: created test-event-1
    Mar  1 13:11:23.805: INFO: created test-event-2
    Mar  1 13:11:23.811: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 03/01/23 13:11:23.811
    STEP: delete collection of events 03/01/23 13:11:23.816
    Mar  1 13:11:23.816: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 03/01/23 13:11:23.847
    Mar  1 13:11:23.847: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    Mar  1 13:11:23.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-5718" for this suite. 03/01/23 13:11:23.857
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:11:23.869
Mar  1 13:11:23.869: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename svcaccounts 03/01/23 13:11:23.87
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:11:23.893
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:11:23.896
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
Mar  1 13:11:23.923: INFO: created pod pod-service-account-defaultsa
Mar  1 13:11:23.923: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Mar  1 13:11:23.936: INFO: created pod pod-service-account-mountsa
Mar  1 13:11:23.936: INFO: pod pod-service-account-mountsa service account token volume mount: true
Mar  1 13:11:23.945: INFO: created pod pod-service-account-nomountsa
Mar  1 13:11:23.945: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Mar  1 13:11:23.954: INFO: created pod pod-service-account-defaultsa-mountspec
Mar  1 13:11:23.954: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Mar  1 13:11:23.967: INFO: created pod pod-service-account-mountsa-mountspec
Mar  1 13:11:23.967: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Mar  1 13:11:23.981: INFO: created pod pod-service-account-nomountsa-mountspec
Mar  1 13:11:23.982: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Mar  1 13:11:24.000: INFO: created pod pod-service-account-defaultsa-nomountspec
Mar  1 13:11:24.000: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Mar  1 13:11:24.012: INFO: created pod pod-service-account-mountsa-nomountspec
Mar  1 13:11:24.013: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Mar  1 13:11:24.020: INFO: created pod pod-service-account-nomountsa-nomountspec
Mar  1 13:11:24.020: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Mar  1 13:11:24.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7326" for this suite. 03/01/23 13:11:24.028
{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","completed":313,"skipped":5741,"failed":0}
------------------------------
â€¢ [0.179 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:11:23.869
    Mar  1 13:11:23.869: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename svcaccounts 03/01/23 13:11:23.87
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:11:23.893
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:11:23.896
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:158
    Mar  1 13:11:23.923: INFO: created pod pod-service-account-defaultsa
    Mar  1 13:11:23.923: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Mar  1 13:11:23.936: INFO: created pod pod-service-account-mountsa
    Mar  1 13:11:23.936: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Mar  1 13:11:23.945: INFO: created pod pod-service-account-nomountsa
    Mar  1 13:11:23.945: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Mar  1 13:11:23.954: INFO: created pod pod-service-account-defaultsa-mountspec
    Mar  1 13:11:23.954: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Mar  1 13:11:23.967: INFO: created pod pod-service-account-mountsa-mountspec
    Mar  1 13:11:23.967: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Mar  1 13:11:23.981: INFO: created pod pod-service-account-nomountsa-mountspec
    Mar  1 13:11:23.982: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Mar  1 13:11:24.000: INFO: created pod pod-service-account-defaultsa-nomountspec
    Mar  1 13:11:24.000: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Mar  1 13:11:24.012: INFO: created pod pod-service-account-mountsa-nomountspec
    Mar  1 13:11:24.013: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Mar  1 13:11:24.020: INFO: created pod pod-service-account-nomountsa-nomountspec
    Mar  1 13:11:24.020: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Mar  1 13:11:24.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-7326" for this suite. 03/01/23 13:11:24.028
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:11:24.049
Mar  1 13:11:24.050: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename replicaset 03/01/23 13:11:24.051
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:11:24.075
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:11:24.078
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Mar  1 13:11:24.080: INFO: Creating ReplicaSet my-hostname-basic-5c19cbc8-6d1a-47e1-a0ca-27108aa11fb8
Mar  1 13:11:24.091: INFO: Pod name my-hostname-basic-5c19cbc8-6d1a-47e1-a0ca-27108aa11fb8: Found 0 pods out of 1
Mar  1 13:11:29.097: INFO: Pod name my-hostname-basic-5c19cbc8-6d1a-47e1-a0ca-27108aa11fb8: Found 1 pods out of 1
Mar  1 13:11:29.097: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-5c19cbc8-6d1a-47e1-a0ca-27108aa11fb8" is running
Mar  1 13:11:29.097: INFO: Waiting up to 5m0s for pod "my-hostname-basic-5c19cbc8-6d1a-47e1-a0ca-27108aa11fb8-4fr5b" in namespace "replicaset-8768" to be "running"
Mar  1 13:11:29.103: INFO: Pod "my-hostname-basic-5c19cbc8-6d1a-47e1-a0ca-27108aa11fb8-4fr5b": Phase="Running", Reason="", readiness=true. Elapsed: 6.162451ms
Mar  1 13:11:29.104: INFO: Pod "my-hostname-basic-5c19cbc8-6d1a-47e1-a0ca-27108aa11fb8-4fr5b" satisfied condition "running"
Mar  1 13:11:29.104: INFO: Pod "my-hostname-basic-5c19cbc8-6d1a-47e1-a0ca-27108aa11fb8-4fr5b" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-01 13:11:24 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-01 13:11:25 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-01 13:11:25 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-01 13:11:24 +0000 UTC Reason: Message:}])
Mar  1 13:11:29.104: INFO: Trying to dial the pod
Mar  1 13:11:34.121: INFO: Controller my-hostname-basic-5c19cbc8-6d1a-47e1-a0ca-27108aa11fb8: Got expected result from replica 1 [my-hostname-basic-5c19cbc8-6d1a-47e1-a0ca-27108aa11fb8-4fr5b]: "my-hostname-basic-5c19cbc8-6d1a-47e1-a0ca-27108aa11fb8-4fr5b", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Mar  1 13:11:34.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8768" for this suite. 03/01/23 13:11:34.127
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","completed":314,"skipped":5762,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.085 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:11:24.049
    Mar  1 13:11:24.050: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename replicaset 03/01/23 13:11:24.051
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:11:24.075
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:11:24.078
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Mar  1 13:11:24.080: INFO: Creating ReplicaSet my-hostname-basic-5c19cbc8-6d1a-47e1-a0ca-27108aa11fb8
    Mar  1 13:11:24.091: INFO: Pod name my-hostname-basic-5c19cbc8-6d1a-47e1-a0ca-27108aa11fb8: Found 0 pods out of 1
    Mar  1 13:11:29.097: INFO: Pod name my-hostname-basic-5c19cbc8-6d1a-47e1-a0ca-27108aa11fb8: Found 1 pods out of 1
    Mar  1 13:11:29.097: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-5c19cbc8-6d1a-47e1-a0ca-27108aa11fb8" is running
    Mar  1 13:11:29.097: INFO: Waiting up to 5m0s for pod "my-hostname-basic-5c19cbc8-6d1a-47e1-a0ca-27108aa11fb8-4fr5b" in namespace "replicaset-8768" to be "running"
    Mar  1 13:11:29.103: INFO: Pod "my-hostname-basic-5c19cbc8-6d1a-47e1-a0ca-27108aa11fb8-4fr5b": Phase="Running", Reason="", readiness=true. Elapsed: 6.162451ms
    Mar  1 13:11:29.104: INFO: Pod "my-hostname-basic-5c19cbc8-6d1a-47e1-a0ca-27108aa11fb8-4fr5b" satisfied condition "running"
    Mar  1 13:11:29.104: INFO: Pod "my-hostname-basic-5c19cbc8-6d1a-47e1-a0ca-27108aa11fb8-4fr5b" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-01 13:11:24 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-01 13:11:25 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-01 13:11:25 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-01 13:11:24 +0000 UTC Reason: Message:}])
    Mar  1 13:11:29.104: INFO: Trying to dial the pod
    Mar  1 13:11:34.121: INFO: Controller my-hostname-basic-5c19cbc8-6d1a-47e1-a0ca-27108aa11fb8: Got expected result from replica 1 [my-hostname-basic-5c19cbc8-6d1a-47e1-a0ca-27108aa11fb8-4fr5b]: "my-hostname-basic-5c19cbc8-6d1a-47e1-a0ca-27108aa11fb8-4fr5b", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Mar  1 13:11:34.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-8768" for this suite. 03/01/23 13:11:34.127
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:11:34.136
Mar  1 13:11:34.136: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename services 03/01/23 13:11:34.137
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:11:34.159
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:11:34.161
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
STEP: creating a service externalname-service with the type=ExternalName in namespace services-304 03/01/23 13:11:34.164
STEP: changing the ExternalName service to type=ClusterIP 03/01/23 13:11:34.17
STEP: creating replication controller externalname-service in namespace services-304 03/01/23 13:11:34.188
I0301 13:11:34.197311      19 runners.go:193] Created replication controller with name: externalname-service, namespace: services-304, replica count: 2
I0301 13:11:37.248642      19 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  1 13:11:37.248: INFO: Creating new exec pod
Mar  1 13:11:37.259: INFO: Waiting up to 5m0s for pod "execpodpbgjn" in namespace "services-304" to be "running"
Mar  1 13:11:37.264: INFO: Pod "execpodpbgjn": Phase="Pending", Reason="", readiness=false. Elapsed: 4.486069ms
Mar  1 13:11:39.270: INFO: Pod "execpodpbgjn": Phase="Running", Reason="", readiness=true. Elapsed: 2.010095215s
Mar  1 13:11:39.270: INFO: Pod "execpodpbgjn" satisfied condition "running"
Mar  1 13:11:40.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-304 exec execpodpbgjn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Mar  1 13:11:40.432: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar  1 13:11:40.432: INFO: stdout: "externalname-service-82zgr"
Mar  1 13:11:40.432: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-304 exec execpodpbgjn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.1.152 80'
Mar  1 13:11:40.591: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.1.152 80\nConnection to 10.233.1.152 80 port [tcp/http] succeeded!\n"
Mar  1 13:11:40.591: INFO: stdout: "externalname-service-82zgr"
Mar  1 13:11:40.591: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar  1 13:11:40.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-304" for this suite. 03/01/23 13:11:40.628
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","completed":315,"skipped":5763,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.501 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:11:34.136
    Mar  1 13:11:34.136: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename services 03/01/23 13:11:34.137
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:11:34.159
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:11:34.161
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1404
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-304 03/01/23 13:11:34.164
    STEP: changing the ExternalName service to type=ClusterIP 03/01/23 13:11:34.17
    STEP: creating replication controller externalname-service in namespace services-304 03/01/23 13:11:34.188
    I0301 13:11:34.197311      19 runners.go:193] Created replication controller with name: externalname-service, namespace: services-304, replica count: 2
    I0301 13:11:37.248642      19 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar  1 13:11:37.248: INFO: Creating new exec pod
    Mar  1 13:11:37.259: INFO: Waiting up to 5m0s for pod "execpodpbgjn" in namespace "services-304" to be "running"
    Mar  1 13:11:37.264: INFO: Pod "execpodpbgjn": Phase="Pending", Reason="", readiness=false. Elapsed: 4.486069ms
    Mar  1 13:11:39.270: INFO: Pod "execpodpbgjn": Phase="Running", Reason="", readiness=true. Elapsed: 2.010095215s
    Mar  1 13:11:39.270: INFO: Pod "execpodpbgjn" satisfied condition "running"
    Mar  1 13:11:40.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-304 exec execpodpbgjn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Mar  1 13:11:40.432: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Mar  1 13:11:40.432: INFO: stdout: "externalname-service-82zgr"
    Mar  1 13:11:40.432: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=services-304 exec execpodpbgjn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.1.152 80'
    Mar  1 13:11:40.591: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.1.152 80\nConnection to 10.233.1.152 80 port [tcp/http] succeeded!\n"
    Mar  1 13:11:40.591: INFO: stdout: "externalname-service-82zgr"
    Mar  1 13:11:40.591: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar  1 13:11:40.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-304" for this suite. 03/01/23 13:11:40.628
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:11:40.638
Mar  1 13:11:40.638: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename deployment 03/01/23 13:11:40.638
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:11:40.664
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:11:40.666
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Mar  1 13:11:40.669: INFO: Creating deployment "webserver-deployment"
Mar  1 13:11:40.676: INFO: Waiting for observed generation 1
Mar  1 13:11:42.686: INFO: Waiting for all required pods to come up
Mar  1 13:11:42.694: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 03/01/23 13:11:42.694
Mar  1 13:11:42.694: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-2gfp2" in namespace "deployment-9252" to be "running"
Mar  1 13:11:42.694: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-k6987" in namespace "deployment-9252" to be "running"
Mar  1 13:11:42.694: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-q24cx" in namespace "deployment-9252" to be "running"
Mar  1 13:11:42.698: INFO: Pod "webserver-deployment-845c8977d9-k6987": Phase="Pending", Reason="", readiness=false. Elapsed: 4.070476ms
Mar  1 13:11:42.699: INFO: Pod "webserver-deployment-845c8977d9-q24cx": Phase="Pending", Reason="", readiness=false. Elapsed: 4.775136ms
Mar  1 13:11:42.699: INFO: Pod "webserver-deployment-845c8977d9-2gfp2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.440045ms
Mar  1 13:11:44.705: INFO: Pod "webserver-deployment-845c8977d9-q24cx": Phase="Running", Reason="", readiness=true. Elapsed: 2.010685072s
Mar  1 13:11:44.705: INFO: Pod "webserver-deployment-845c8977d9-q24cx" satisfied condition "running"
Mar  1 13:11:44.705: INFO: Pod "webserver-deployment-845c8977d9-k6987": Phase="Running", Reason="", readiness=true. Elapsed: 2.011003664s
Mar  1 13:11:44.705: INFO: Pod "webserver-deployment-845c8977d9-k6987" satisfied condition "running"
Mar  1 13:11:44.705: INFO: Pod "webserver-deployment-845c8977d9-2gfp2": Phase="Running", Reason="", readiness=true. Elapsed: 2.0113694s
Mar  1 13:11:44.705: INFO: Pod "webserver-deployment-845c8977d9-2gfp2" satisfied condition "running"
Mar  1 13:11:44.705: INFO: Waiting for deployment "webserver-deployment" to complete
Mar  1 13:11:44.715: INFO: Updating deployment "webserver-deployment" with a non-existent image
Mar  1 13:11:44.727: INFO: Updating deployment webserver-deployment
Mar  1 13:11:44.727: INFO: Waiting for observed generation 2
Mar  1 13:11:46.739: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Mar  1 13:11:46.744: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Mar  1 13:11:46.748: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Mar  1 13:11:46.761: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Mar  1 13:11:46.761: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Mar  1 13:11:46.765: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Mar  1 13:11:46.772: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Mar  1 13:11:46.773: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Mar  1 13:11:46.785: INFO: Updating deployment webserver-deployment
Mar  1 13:11:46.785: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Mar  1 13:11:46.794: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Mar  1 13:11:46.804: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar  1 13:11:46.829: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-9252  7b4552cd-4e7d-42c2-a81b-b6ab9794b7f3 44798 3 2023-03-01 13:11:40 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-03-01 13:11:46 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-01 13:11:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001d25488 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2023-03-01 13:11:44 +0000 UTC,LastTransitionTime:2023-03-01 13:11:40 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-03-01 13:11:46 +0000 UTC,LastTransitionTime:2023-03-01 13:11:46 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Mar  1 13:11:46.844: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-9252  f8a83454-9b40-4540-a14e-b59ab16fdaaa 44789 3 2023-03-01 13:11:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 7b4552cd-4e7d-42c2-a81b-b6ab9794b7f3 0xc00482edf7 0xc00482edf8}] [] [{kube-controller-manager Update apps/v1 2023-03-01 13:11:44 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-03-01 13:11:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7b4552cd-4e7d-42c2-a81b-b6ab9794b7f3\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00482ee98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  1 13:11:46.844: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Mar  1 13:11:46.844: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-845c8977d9  deployment-9252  174a02b9-2cfe-45cb-8bbb-69a603d52f6e 44785 3 2023-03-01 13:11:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 7b4552cd-4e7d-42c2-a81b-b6ab9794b7f3 0xc00482ef07 0xc00482ef08}] [] [{kube-controller-manager Update apps/v1 2023-03-01 13:11:44 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-03-01 13:11:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7b4552cd-4e7d-42c2-a81b-b6ab9794b7f3\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00482ef98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Mar  1 13:11:46.868: INFO: Pod "webserver-deployment-69b7448995-8dqsw" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-8dqsw webserver-deployment-69b7448995- deployment-9252  be7ca848-ad20-45d1-b10b-d5f3535fb235 44817 0 2023-03-01 13:11:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 f8a83454-9b40-4540-a14e-b59ab16fdaaa 0xc00482f497 0xc00482f498}] [] [{kube-controller-manager Update v1 2023-03-01 13:11:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f8a83454-9b40-4540-a14e-b59ab16fdaaa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k4bzz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k4bzz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 13:11:46.869: INFO: Pod "webserver-deployment-69b7448995-8vxht" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-8vxht webserver-deployment-69b7448995- deployment-9252  9ea3bdf6-d025-4375-a8ee-9aff1a1677f2 44822 0 2023-03-01 13:11:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 f8a83454-9b40-4540-a14e-b59ab16fdaaa 0xc00482f610 0xc00482f611}] [] [{kube-controller-manager Update v1 2023-03-01 13:11:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f8a83454-9b40-4540-a14e-b59ab16fdaaa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ckdl2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ckdl2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 13:11:46.869: INFO: Pod "webserver-deployment-69b7448995-b4lvh" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-b4lvh webserver-deployment-69b7448995- deployment-9252  53b09e2a-88a6-4693-a81b-0cf6e2ccbfa1 44739 0 2023-03-01 13:11:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:5792d5563ebec734e417967ca79722c432711f12c5ef5568331c04b3aba86607 cni.projectcalico.org/podIP:10.233.95.187/32 cni.projectcalico.org/podIPs:10.233.95.187/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 f8a83454-9b40-4540-a14e-b59ab16fdaaa 0xc00482f777 0xc00482f778}] [] [{kube-controller-manager Update v1 2023-03-01 13:11:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f8a83454-9b40-4540-a14e-b59ab16fdaaa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-01 13:11:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-01 13:11:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f9rhl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f9rhl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.178,PodIP:,StartTime:2023-03-01 13:11:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 13:11:46.870: INFO: Pod "webserver-deployment-69b7448995-hdkbn" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-hdkbn webserver-deployment-69b7448995- deployment-9252  31d3c512-01c8-4d64-ab03-5668450de24c 44741 0 2023-03-01 13:11:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:ae7d2c4b7d693ab8d7306095ecd9f78a47a77d90e38a476a2975d33a796bb93b cni.projectcalico.org/podIP:10.233.64.142/32 cni.projectcalico.org/podIPs:10.233.64.142/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 f8a83454-9b40-4540-a14e-b59ab16fdaaa 0xc00482f9a7 0xc00482f9a8}] [] [{kube-controller-manager Update v1 2023-03-01 13:11:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f8a83454-9b40-4540-a14e-b59ab16fdaaa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-01 13:11:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-01 13:11:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jqrm2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jqrm2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.76,PodIP:,StartTime:2023-03-01 13:11:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 13:11:46.870: INFO: Pod "webserver-deployment-69b7448995-nj45t" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-nj45t webserver-deployment-69b7448995- deployment-9252  0ec4d9e5-3c05-49b1-bf8d-9b3afe4b4743 44797 0 2023-03-01 13:11:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 f8a83454-9b40-4540-a14e-b59ab16fdaaa 0xc00482fbb0 0xc00482fbb1}] [] [{kube-controller-manager Update v1 2023-03-01 13:11:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f8a83454-9b40-4540-a14e-b59ab16fdaaa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hlm6m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hlm6m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 13:11:46.870: INFO: Pod "webserver-deployment-69b7448995-rdbjk" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-rdbjk webserver-deployment-69b7448995- deployment-9252  f57f4895-d7c4-4036-922d-8a17b7976e6c 44729 0 2023-03-01 13:11:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:dead0dff2a4a1a0cd35476906067135dd8ee9eb1a07a7e72fe428734542c4c71 cni.projectcalico.org/podIP:10.233.64.141/32 cni.projectcalico.org/podIPs:10.233.64.141/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 f8a83454-9b40-4540-a14e-b59ab16fdaaa 0xc00482fd40 0xc00482fd41}] [] [{kube-controller-manager Update v1 2023-03-01 13:11:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f8a83454-9b40-4540-a14e-b59ab16fdaaa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-01 13:11:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-01 13:11:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2hkdd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2hkdd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.76,PodIP:,StartTime:2023-03-01 13:11:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 13:11:46.870: INFO: Pod "webserver-deployment-69b7448995-rjqqr" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-rjqqr webserver-deployment-69b7448995- deployment-9252  b7944a58-ccc7-4016-85ad-a3ca99824c8c 44736 0 2023-03-01 13:11:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:4e69d55654a981a08da6a7b1fcf9e289584c2f71e51e3895d7234a5ecd951c63 cni.projectcalico.org/podIP:10.233.74.127/32 cni.projectcalico.org/podIPs:10.233.74.127/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 f8a83454-9b40-4540-a14e-b59ab16fdaaa 0xc00482ff60 0xc00482ff61}] [] [{kube-controller-manager Update v1 2023-03-01 13:11:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f8a83454-9b40-4540-a14e-b59ab16fdaaa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-01 13:11:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-01 13:11:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-drk2c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-drk2c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.2.241,PodIP:,StartTime:2023-03-01 13:11:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 13:11:46.871: INFO: Pod "webserver-deployment-69b7448995-t72w2" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-t72w2 webserver-deployment-69b7448995- deployment-9252  c0052c66-296e-4958-974c-a0ab804590f7 44812 0 2023-03-01 13:11:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 f8a83454-9b40-4540-a14e-b59ab16fdaaa 0xc003610167 0xc003610168}] [] [{kube-controller-manager Update v1 2023-03-01 13:11:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f8a83454-9b40-4540-a14e-b59ab16fdaaa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jr667,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jr667,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 13:11:46.871: INFO: Pod "webserver-deployment-69b7448995-tzj2d" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-tzj2d webserver-deployment-69b7448995- deployment-9252  bc531941-28d3-400e-b18e-8a122f11a26d 44744 0 2023-03-01 13:11:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:d2580bb231bda712f5258eaaa561f177e25351de0a7ddae3867ee2348dab9d9f cni.projectcalico.org/podIP:10.233.74.128/32 cni.projectcalico.org/podIPs:10.233.74.128/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 f8a83454-9b40-4540-a14e-b59ab16fdaaa 0xc003610300 0xc003610301}] [] [{kube-controller-manager Update v1 2023-03-01 13:11:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f8a83454-9b40-4540-a14e-b59ab16fdaaa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-01 13:11:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-01 13:11:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jhsh4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jhsh4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.2.241,PodIP:,StartTime:2023-03-01 13:11:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 13:11:46.871: INFO: Pod "webserver-deployment-845c8977d9-7mzgg" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-7mzgg webserver-deployment-845c8977d9- deployment-9252  6fa01954-1864-4211-bef4-c53a63d2c7b7 44815 0 2023-03-01 13:11:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 174a02b9-2cfe-45cb-8bbb-69a603d52f6e 0xc003610507 0xc003610508}] [] [{kube-controller-manager Update v1 2023-03-01 13:11:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"174a02b9-2cfe-45cb-8bbb-69a603d52f6e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dbq8f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dbq8f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 13:11:46.871: INFO: Pod "webserver-deployment-845c8977d9-8nq65" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-8nq65 webserver-deployment-845c8977d9- deployment-9252  4add41c6-2593-45d9-9339-7a077d6f5e2d 44818 0 2023-03-01 13:11:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 174a02b9-2cfe-45cb-8bbb-69a603d52f6e 0xc003610680 0xc003610681}] [] [{kube-controller-manager Update v1 2023-03-01 13:11:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"174a02b9-2cfe-45cb-8bbb-69a603d52f6e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-01 13:11:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2z7ss,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2z7ss,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.2.241,PodIP:,StartTime:2023-03-01 13:11:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 13:11:46.872: INFO: Pod "webserver-deployment-845c8977d9-9gns4" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-9gns4 webserver-deployment-845c8977d9- deployment-9252  b0b0856e-2038-4a33-9807-eb8d00f6261d 44629 0 2023-03-01 13:11:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:cfde810c843bed1edc984dbe8978633d0053d596f4777d3660bb3c36e37b1e74 cni.projectcalico.org/podIP:10.233.95.186/32 cni.projectcalico.org/podIPs:10.233.95.186/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 174a02b9-2cfe-45cb-8bbb-69a603d52f6e 0xc003610877 0xc003610878}] [] [{kube-controller-manager Update v1 2023-03-01 13:11:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"174a02b9-2cfe-45cb-8bbb-69a603d52f6e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-01 13:11:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-01 13:11:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.95.186\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6zvl4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6zvl4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.178,PodIP:10.233.95.186,StartTime:2023-03-01 13:11:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-01 13:11:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://b5ff85a089488138883afb8da3c13601b290fe2bc0673a077fcad85a2d9f18dd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.95.186,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 13:11:46.872: INFO: Pod "webserver-deployment-845c8977d9-9vg5k" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-9vg5k webserver-deployment-845c8977d9- deployment-9252  73c5c12d-669a-4e88-ae9f-febbd603e345 44653 0 2023-03-01 13:11:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:b1d2cd8265b5dcdb0c25e89d36b4fff0c3dd808f144da0ac686e3a1788b855f1 cni.projectcalico.org/podIP:10.233.64.139/32 cni.projectcalico.org/podIPs:10.233.64.139/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 174a02b9-2cfe-45cb-8bbb-69a603d52f6e 0xc003610aa7 0xc003610aa8}] [] [{kube-controller-manager Update v1 2023-03-01 13:11:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"174a02b9-2cfe-45cb-8bbb-69a603d52f6e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-01 13:11:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-01 13:11:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.139\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4tbhm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4tbhm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.76,PodIP:10.233.64.139,StartTime:2023-03-01 13:11:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-01 13:11:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://2f311f3264f9283edae4b98f0b6f95cf58e0942b03f826fc84ebc4191e755f71,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.139,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 13:11:46.873: INFO: Pod "webserver-deployment-845c8977d9-9zxvc" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-9zxvc webserver-deployment-845c8977d9- deployment-9252  05122087-44c2-4f87-9a09-a6b1c7af9729 44810 0 2023-03-01 13:11:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 174a02b9-2cfe-45cb-8bbb-69a603d52f6e 0xc003610cb7 0xc003610cb8}] [] [{kube-controller-manager Update v1 2023-03-01 13:11:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"174a02b9-2cfe-45cb-8bbb-69a603d52f6e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ldsg9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ldsg9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 13:11:46.873: INFO: Pod "webserver-deployment-845c8977d9-hvqfn" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-hvqfn webserver-deployment-845c8977d9- deployment-9252  423970c1-313b-469e-8b75-0913747464a3 44646 0 2023-03-01 13:11:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:7fe454aa8e12601194ccd05905aea54e2710040187fffd4134f59018361d3f14 cni.projectcalico.org/podIP:10.233.64.138/32 cni.projectcalico.org/podIPs:10.233.64.138/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 174a02b9-2cfe-45cb-8bbb-69a603d52f6e 0xc003610e50 0xc003610e51}] [] [{kube-controller-manager Update v1 2023-03-01 13:11:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"174a02b9-2cfe-45cb-8bbb-69a603d52f6e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-01 13:11:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-01 13:11:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.138\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b4db8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b4db8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.76,PodIP:10.233.64.138,StartTime:2023-03-01 13:11:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-01 13:11:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://e91ed0632da63cc73c180f1ac86ff5c61ace0a40d01898f843445b207870fe6f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.138,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 13:11:46.873: INFO: Pod "webserver-deployment-845c8977d9-jgzdd" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-jgzdd webserver-deployment-845c8977d9- deployment-9252  6f9e4ac7-5559-4ae4-a046-aedba7722bd6 44626 0 2023-03-01 13:11:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:60147d7f2fedaf51b041e26f854dbaf1973a8bcb056db3b0810e7ccee8be6089 cni.projectcalico.org/podIP:10.233.95.185/32 cni.projectcalico.org/podIPs:10.233.95.185/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 174a02b9-2cfe-45cb-8bbb-69a603d52f6e 0xc003611077 0xc003611078}] [] [{kube-controller-manager Update v1 2023-03-01 13:11:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"174a02b9-2cfe-45cb-8bbb-69a603d52f6e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-01 13:11:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-01 13:11:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.95.185\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lhcpn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lhcpn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.178,PodIP:10.233.95.185,StartTime:2023-03-01 13:11:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-01 13:11:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://27dc77eadde5f0bda9b059e06a4c6f663212e8325326ad817dfc82b49ae19a41,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.95.185,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 13:11:46.874: INFO: Pod "webserver-deployment-845c8977d9-lcxq6" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-lcxq6 webserver-deployment-845c8977d9- deployment-9252  fc03d4d1-0f11-412e-8f64-b74156635b52 44821 0 2023-03-01 13:11:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 174a02b9-2cfe-45cb-8bbb-69a603d52f6e 0xc003611287 0xc003611288}] [] [{kube-controller-manager Update v1 2023-03-01 13:11:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"174a02b9-2cfe-45cb-8bbb-69a603d52f6e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-01 13:11:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sm79c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sm79c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.76,PodIP:,StartTime:2023-03-01 13:11:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 13:11:46.874: INFO: Pod "webserver-deployment-845c8977d9-mhx2h" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-mhx2h webserver-deployment-845c8977d9- deployment-9252  863abe70-c946-4a9b-978d-089bf758409b 44816 0 2023-03-01 13:11:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 174a02b9-2cfe-45cb-8bbb-69a603d52f6e 0xc003611457 0xc003611458}] [] [{kube-controller-manager Update v1 2023-03-01 13:11:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"174a02b9-2cfe-45cb-8bbb-69a603d52f6e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vthth,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vthth,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 13:11:46.874: INFO: Pod "webserver-deployment-845c8977d9-q24cx" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-q24cx webserver-deployment-845c8977d9- deployment-9252  fb57c198-454f-410a-9cc8-72b7ee441669 44662 0 2023-03-01 13:11:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:99de5b8cee018593ab1e0d5cf65c0b447946bac77e995a26dfe947614a0db8dd cni.projectcalico.org/podIP:10.233.74.121/32 cni.projectcalico.org/podIPs:10.233.74.121/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 174a02b9-2cfe-45cb-8bbb-69a603d52f6e 0xc0036115e0 0xc0036115e1}] [] [{kube-controller-manager Update v1 2023-03-01 13:11:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"174a02b9-2cfe-45cb-8bbb-69a603d52f6e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-01 13:11:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-01 13:11:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.74.121\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-swrkf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-swrkf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.2.241,PodIP:10.233.74.121,StartTime:2023-03-01 13:11:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-01 13:11:42 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://a22d46c1f44077325adc8ddc9048c1087b959f116119e8566e449fd1ced8b65f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.74.121,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 13:11:46.874: INFO: Pod "webserver-deployment-845c8977d9-qzkd5" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-qzkd5 webserver-deployment-845c8977d9- deployment-9252  2622927b-4e74-458f-90d4-49b182176f9d 44631 0 2023-03-01 13:11:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:ac389d362415614639fded029df978389481aca866ac2dcae57c84faf1e6aed6 cni.projectcalico.org/podIP:10.233.95.184/32 cni.projectcalico.org/podIPs:10.233.95.184/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 174a02b9-2cfe-45cb-8bbb-69a603d52f6e 0xc003611807 0xc003611808}] [] [{kube-controller-manager Update v1 2023-03-01 13:11:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"174a02b9-2cfe-45cb-8bbb-69a603d52f6e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-01 13:11:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-01 13:11:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.95.184\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vm2sm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vm2sm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.178,PodIP:10.233.95.184,StartTime:2023-03-01 13:11:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-01 13:11:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://95a2e033b0eacffa688e4d0757ac0cd71ec5a8091f54e650b422b99c67489d3d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.95.184,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 13:11:46.877: INFO: Pod "webserver-deployment-845c8977d9-st2sf" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-st2sf webserver-deployment-845c8977d9- deployment-9252  fc8580e4-52a0-4ff1-8ae4-34d9ed760157 44641 0 2023-03-01 13:11:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:726af8d36b8935eb5ccfb5e9c71b809bad8c02c9806d190d4ffc45d122ba030a cni.projectcalico.org/podIP:10.233.74.96/32 cni.projectcalico.org/podIPs:10.233.74.96/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 174a02b9-2cfe-45cb-8bbb-69a603d52f6e 0xc003611a37 0xc003611a38}] [] [{kube-controller-manager Update v1 2023-03-01 13:11:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"174a02b9-2cfe-45cb-8bbb-69a603d52f6e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-01 13:11:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-01 13:11:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.74.96\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t8vl4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t8vl4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.2.241,PodIP:10.233.74.96,StartTime:2023-03-01 13:11:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-01 13:11:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://0b23a5f4cb92ee47443f2972425132ddb8f180dc9cc5b0b87b4d4b6d95d719f0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.74.96,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 13:11:46.877: INFO: Pod "webserver-deployment-845c8977d9-t6j4d" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-t6j4d webserver-deployment-845c8977d9- deployment-9252  529f768b-faa9-46cc-9134-0501865eac2d 44799 0 2023-03-01 13:11:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 174a02b9-2cfe-45cb-8bbb-69a603d52f6e 0xc003611c47 0xc003611c48}] [] [{kube-controller-manager Update v1 2023-03-01 13:11:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"174a02b9-2cfe-45cb-8bbb-69a603d52f6e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b56jv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b56jv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 13:11:46.878: INFO: Pod "webserver-deployment-845c8977d9-tlf24" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-tlf24 webserver-deployment-845c8977d9- deployment-9252  9e2f1582-3348-44f9-a8d5-b974630cb043 44651 0 2023-03-01 13:11:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:905821bc5cdb9802f0ebbac0e6c608c1009d01f4392819599c4018e43e97857a cni.projectcalico.org/podIP:10.233.64.140/32 cni.projectcalico.org/podIPs:10.233.64.140/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 174a02b9-2cfe-45cb-8bbb-69a603d52f6e 0xc003611dd0 0xc003611dd1}] [] [{kube-controller-manager Update v1 2023-03-01 13:11:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"174a02b9-2cfe-45cb-8bbb-69a603d52f6e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-01 13:11:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-01 13:11:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.140\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2c2zb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2c2zb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.76,PodIP:10.233.64.140,StartTime:2023-03-01 13:11:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-01 13:11:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://2dc1402ceaa432bc20e1e3711910330022e73cb5c7f14970d845c55994f4db61,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.140,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 13:11:46.878: INFO: Pod "webserver-deployment-845c8977d9-xkxgg" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-xkxgg webserver-deployment-845c8977d9- deployment-9252  9a2bda6f-df91-45be-bafb-b31a8ef2969b 44824 0 2023-03-01 13:11:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 174a02b9-2cfe-45cb-8bbb-69a603d52f6e 0xc003611fd7 0xc003611fd8}] [] [{kube-controller-manager Update v1 2023-03-01 13:11:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"174a02b9-2cfe-45cb-8bbb-69a603d52f6e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b74qn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b74qn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Mar  1 13:11:46.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9252" for this suite. 03/01/23 13:11:46.893
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","completed":316,"skipped":5773,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.273 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:11:40.638
    Mar  1 13:11:40.638: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename deployment 03/01/23 13:11:40.638
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:11:40.664
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:11:40.666
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Mar  1 13:11:40.669: INFO: Creating deployment "webserver-deployment"
    Mar  1 13:11:40.676: INFO: Waiting for observed generation 1
    Mar  1 13:11:42.686: INFO: Waiting for all required pods to come up
    Mar  1 13:11:42.694: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 03/01/23 13:11:42.694
    Mar  1 13:11:42.694: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-2gfp2" in namespace "deployment-9252" to be "running"
    Mar  1 13:11:42.694: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-k6987" in namespace "deployment-9252" to be "running"
    Mar  1 13:11:42.694: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-q24cx" in namespace "deployment-9252" to be "running"
    Mar  1 13:11:42.698: INFO: Pod "webserver-deployment-845c8977d9-k6987": Phase="Pending", Reason="", readiness=false. Elapsed: 4.070476ms
    Mar  1 13:11:42.699: INFO: Pod "webserver-deployment-845c8977d9-q24cx": Phase="Pending", Reason="", readiness=false. Elapsed: 4.775136ms
    Mar  1 13:11:42.699: INFO: Pod "webserver-deployment-845c8977d9-2gfp2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.440045ms
    Mar  1 13:11:44.705: INFO: Pod "webserver-deployment-845c8977d9-q24cx": Phase="Running", Reason="", readiness=true. Elapsed: 2.010685072s
    Mar  1 13:11:44.705: INFO: Pod "webserver-deployment-845c8977d9-q24cx" satisfied condition "running"
    Mar  1 13:11:44.705: INFO: Pod "webserver-deployment-845c8977d9-k6987": Phase="Running", Reason="", readiness=true. Elapsed: 2.011003664s
    Mar  1 13:11:44.705: INFO: Pod "webserver-deployment-845c8977d9-k6987" satisfied condition "running"
    Mar  1 13:11:44.705: INFO: Pod "webserver-deployment-845c8977d9-2gfp2": Phase="Running", Reason="", readiness=true. Elapsed: 2.0113694s
    Mar  1 13:11:44.705: INFO: Pod "webserver-deployment-845c8977d9-2gfp2" satisfied condition "running"
    Mar  1 13:11:44.705: INFO: Waiting for deployment "webserver-deployment" to complete
    Mar  1 13:11:44.715: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Mar  1 13:11:44.727: INFO: Updating deployment webserver-deployment
    Mar  1 13:11:44.727: INFO: Waiting for observed generation 2
    Mar  1 13:11:46.739: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Mar  1 13:11:46.744: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Mar  1 13:11:46.748: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Mar  1 13:11:46.761: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Mar  1 13:11:46.761: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Mar  1 13:11:46.765: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Mar  1 13:11:46.772: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Mar  1 13:11:46.773: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Mar  1 13:11:46.785: INFO: Updating deployment webserver-deployment
    Mar  1 13:11:46.785: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Mar  1 13:11:46.794: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Mar  1 13:11:46.804: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar  1 13:11:46.829: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-9252  7b4552cd-4e7d-42c2-a81b-b6ab9794b7f3 44798 3 2023-03-01 13:11:40 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-03-01 13:11:46 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-01 13:11:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001d25488 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2023-03-01 13:11:44 +0000 UTC,LastTransitionTime:2023-03-01 13:11:40 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-03-01 13:11:46 +0000 UTC,LastTransitionTime:2023-03-01 13:11:46 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    Mar  1 13:11:46.844: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-9252  f8a83454-9b40-4540-a14e-b59ab16fdaaa 44789 3 2023-03-01 13:11:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 7b4552cd-4e7d-42c2-a81b-b6ab9794b7f3 0xc00482edf7 0xc00482edf8}] [] [{kube-controller-manager Update apps/v1 2023-03-01 13:11:44 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-03-01 13:11:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7b4552cd-4e7d-42c2-a81b-b6ab9794b7f3\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00482ee98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Mar  1 13:11:46.844: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Mar  1 13:11:46.844: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-845c8977d9  deployment-9252  174a02b9-2cfe-45cb-8bbb-69a603d52f6e 44785 3 2023-03-01 13:11:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 7b4552cd-4e7d-42c2-a81b-b6ab9794b7f3 0xc00482ef07 0xc00482ef08}] [] [{kube-controller-manager Update apps/v1 2023-03-01 13:11:44 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-03-01 13:11:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7b4552cd-4e7d-42c2-a81b-b6ab9794b7f3\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00482ef98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    Mar  1 13:11:46.868: INFO: Pod "webserver-deployment-69b7448995-8dqsw" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-8dqsw webserver-deployment-69b7448995- deployment-9252  be7ca848-ad20-45d1-b10b-d5f3535fb235 44817 0 2023-03-01 13:11:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 f8a83454-9b40-4540-a14e-b59ab16fdaaa 0xc00482f497 0xc00482f498}] [] [{kube-controller-manager Update v1 2023-03-01 13:11:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f8a83454-9b40-4540-a14e-b59ab16fdaaa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k4bzz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k4bzz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  1 13:11:46.869: INFO: Pod "webserver-deployment-69b7448995-8vxht" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-8vxht webserver-deployment-69b7448995- deployment-9252  9ea3bdf6-d025-4375-a8ee-9aff1a1677f2 44822 0 2023-03-01 13:11:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 f8a83454-9b40-4540-a14e-b59ab16fdaaa 0xc00482f610 0xc00482f611}] [] [{kube-controller-manager Update v1 2023-03-01 13:11:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f8a83454-9b40-4540-a14e-b59ab16fdaaa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ckdl2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ckdl2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  1 13:11:46.869: INFO: Pod "webserver-deployment-69b7448995-b4lvh" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-b4lvh webserver-deployment-69b7448995- deployment-9252  53b09e2a-88a6-4693-a81b-0cf6e2ccbfa1 44739 0 2023-03-01 13:11:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:5792d5563ebec734e417967ca79722c432711f12c5ef5568331c04b3aba86607 cni.projectcalico.org/podIP:10.233.95.187/32 cni.projectcalico.org/podIPs:10.233.95.187/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 f8a83454-9b40-4540-a14e-b59ab16fdaaa 0xc00482f777 0xc00482f778}] [] [{kube-controller-manager Update v1 2023-03-01 13:11:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f8a83454-9b40-4540-a14e-b59ab16fdaaa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-01 13:11:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-01 13:11:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f9rhl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f9rhl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.178,PodIP:,StartTime:2023-03-01 13:11:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  1 13:11:46.870: INFO: Pod "webserver-deployment-69b7448995-hdkbn" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-hdkbn webserver-deployment-69b7448995- deployment-9252  31d3c512-01c8-4d64-ab03-5668450de24c 44741 0 2023-03-01 13:11:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:ae7d2c4b7d693ab8d7306095ecd9f78a47a77d90e38a476a2975d33a796bb93b cni.projectcalico.org/podIP:10.233.64.142/32 cni.projectcalico.org/podIPs:10.233.64.142/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 f8a83454-9b40-4540-a14e-b59ab16fdaaa 0xc00482f9a7 0xc00482f9a8}] [] [{kube-controller-manager Update v1 2023-03-01 13:11:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f8a83454-9b40-4540-a14e-b59ab16fdaaa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-01 13:11:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-01 13:11:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jqrm2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jqrm2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.76,PodIP:,StartTime:2023-03-01 13:11:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  1 13:11:46.870: INFO: Pod "webserver-deployment-69b7448995-nj45t" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-nj45t webserver-deployment-69b7448995- deployment-9252  0ec4d9e5-3c05-49b1-bf8d-9b3afe4b4743 44797 0 2023-03-01 13:11:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 f8a83454-9b40-4540-a14e-b59ab16fdaaa 0xc00482fbb0 0xc00482fbb1}] [] [{kube-controller-manager Update v1 2023-03-01 13:11:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f8a83454-9b40-4540-a14e-b59ab16fdaaa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hlm6m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hlm6m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  1 13:11:46.870: INFO: Pod "webserver-deployment-69b7448995-rdbjk" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-rdbjk webserver-deployment-69b7448995- deployment-9252  f57f4895-d7c4-4036-922d-8a17b7976e6c 44729 0 2023-03-01 13:11:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:dead0dff2a4a1a0cd35476906067135dd8ee9eb1a07a7e72fe428734542c4c71 cni.projectcalico.org/podIP:10.233.64.141/32 cni.projectcalico.org/podIPs:10.233.64.141/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 f8a83454-9b40-4540-a14e-b59ab16fdaaa 0xc00482fd40 0xc00482fd41}] [] [{kube-controller-manager Update v1 2023-03-01 13:11:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f8a83454-9b40-4540-a14e-b59ab16fdaaa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-01 13:11:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-01 13:11:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2hkdd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2hkdd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.76,PodIP:,StartTime:2023-03-01 13:11:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  1 13:11:46.870: INFO: Pod "webserver-deployment-69b7448995-rjqqr" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-rjqqr webserver-deployment-69b7448995- deployment-9252  b7944a58-ccc7-4016-85ad-a3ca99824c8c 44736 0 2023-03-01 13:11:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:4e69d55654a981a08da6a7b1fcf9e289584c2f71e51e3895d7234a5ecd951c63 cni.projectcalico.org/podIP:10.233.74.127/32 cni.projectcalico.org/podIPs:10.233.74.127/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 f8a83454-9b40-4540-a14e-b59ab16fdaaa 0xc00482ff60 0xc00482ff61}] [] [{kube-controller-manager Update v1 2023-03-01 13:11:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f8a83454-9b40-4540-a14e-b59ab16fdaaa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-01 13:11:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-01 13:11:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-drk2c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-drk2c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.2.241,PodIP:,StartTime:2023-03-01 13:11:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  1 13:11:46.871: INFO: Pod "webserver-deployment-69b7448995-t72w2" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-t72w2 webserver-deployment-69b7448995- deployment-9252  c0052c66-296e-4958-974c-a0ab804590f7 44812 0 2023-03-01 13:11:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 f8a83454-9b40-4540-a14e-b59ab16fdaaa 0xc003610167 0xc003610168}] [] [{kube-controller-manager Update v1 2023-03-01 13:11:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f8a83454-9b40-4540-a14e-b59ab16fdaaa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jr667,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jr667,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  1 13:11:46.871: INFO: Pod "webserver-deployment-69b7448995-tzj2d" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-tzj2d webserver-deployment-69b7448995- deployment-9252  bc531941-28d3-400e-b18e-8a122f11a26d 44744 0 2023-03-01 13:11:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:d2580bb231bda712f5258eaaa561f177e25351de0a7ddae3867ee2348dab9d9f cni.projectcalico.org/podIP:10.233.74.128/32 cni.projectcalico.org/podIPs:10.233.74.128/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 f8a83454-9b40-4540-a14e-b59ab16fdaaa 0xc003610300 0xc003610301}] [] [{kube-controller-manager Update v1 2023-03-01 13:11:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f8a83454-9b40-4540-a14e-b59ab16fdaaa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-01 13:11:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-01 13:11:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jhsh4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jhsh4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.2.241,PodIP:,StartTime:2023-03-01 13:11:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  1 13:11:46.871: INFO: Pod "webserver-deployment-845c8977d9-7mzgg" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-7mzgg webserver-deployment-845c8977d9- deployment-9252  6fa01954-1864-4211-bef4-c53a63d2c7b7 44815 0 2023-03-01 13:11:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 174a02b9-2cfe-45cb-8bbb-69a603d52f6e 0xc003610507 0xc003610508}] [] [{kube-controller-manager Update v1 2023-03-01 13:11:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"174a02b9-2cfe-45cb-8bbb-69a603d52f6e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dbq8f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dbq8f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  1 13:11:46.871: INFO: Pod "webserver-deployment-845c8977d9-8nq65" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-8nq65 webserver-deployment-845c8977d9- deployment-9252  4add41c6-2593-45d9-9339-7a077d6f5e2d 44818 0 2023-03-01 13:11:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 174a02b9-2cfe-45cb-8bbb-69a603d52f6e 0xc003610680 0xc003610681}] [] [{kube-controller-manager Update v1 2023-03-01 13:11:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"174a02b9-2cfe-45cb-8bbb-69a603d52f6e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-01 13:11:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2z7ss,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2z7ss,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.2.241,PodIP:,StartTime:2023-03-01 13:11:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  1 13:11:46.872: INFO: Pod "webserver-deployment-845c8977d9-9gns4" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-9gns4 webserver-deployment-845c8977d9- deployment-9252  b0b0856e-2038-4a33-9807-eb8d00f6261d 44629 0 2023-03-01 13:11:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:cfde810c843bed1edc984dbe8978633d0053d596f4777d3660bb3c36e37b1e74 cni.projectcalico.org/podIP:10.233.95.186/32 cni.projectcalico.org/podIPs:10.233.95.186/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 174a02b9-2cfe-45cb-8bbb-69a603d52f6e 0xc003610877 0xc003610878}] [] [{kube-controller-manager Update v1 2023-03-01 13:11:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"174a02b9-2cfe-45cb-8bbb-69a603d52f6e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-01 13:11:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-01 13:11:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.95.186\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6zvl4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6zvl4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.178,PodIP:10.233.95.186,StartTime:2023-03-01 13:11:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-01 13:11:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://b5ff85a089488138883afb8da3c13601b290fe2bc0673a077fcad85a2d9f18dd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.95.186,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  1 13:11:46.872: INFO: Pod "webserver-deployment-845c8977d9-9vg5k" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-9vg5k webserver-deployment-845c8977d9- deployment-9252  73c5c12d-669a-4e88-ae9f-febbd603e345 44653 0 2023-03-01 13:11:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:b1d2cd8265b5dcdb0c25e89d36b4fff0c3dd808f144da0ac686e3a1788b855f1 cni.projectcalico.org/podIP:10.233.64.139/32 cni.projectcalico.org/podIPs:10.233.64.139/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 174a02b9-2cfe-45cb-8bbb-69a603d52f6e 0xc003610aa7 0xc003610aa8}] [] [{kube-controller-manager Update v1 2023-03-01 13:11:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"174a02b9-2cfe-45cb-8bbb-69a603d52f6e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-01 13:11:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-01 13:11:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.139\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4tbhm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4tbhm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.76,PodIP:10.233.64.139,StartTime:2023-03-01 13:11:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-01 13:11:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://2f311f3264f9283edae4b98f0b6f95cf58e0942b03f826fc84ebc4191e755f71,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.139,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  1 13:11:46.873: INFO: Pod "webserver-deployment-845c8977d9-9zxvc" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-9zxvc webserver-deployment-845c8977d9- deployment-9252  05122087-44c2-4f87-9a09-a6b1c7af9729 44810 0 2023-03-01 13:11:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 174a02b9-2cfe-45cb-8bbb-69a603d52f6e 0xc003610cb7 0xc003610cb8}] [] [{kube-controller-manager Update v1 2023-03-01 13:11:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"174a02b9-2cfe-45cb-8bbb-69a603d52f6e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ldsg9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ldsg9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  1 13:11:46.873: INFO: Pod "webserver-deployment-845c8977d9-hvqfn" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-hvqfn webserver-deployment-845c8977d9- deployment-9252  423970c1-313b-469e-8b75-0913747464a3 44646 0 2023-03-01 13:11:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:7fe454aa8e12601194ccd05905aea54e2710040187fffd4134f59018361d3f14 cni.projectcalico.org/podIP:10.233.64.138/32 cni.projectcalico.org/podIPs:10.233.64.138/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 174a02b9-2cfe-45cb-8bbb-69a603d52f6e 0xc003610e50 0xc003610e51}] [] [{kube-controller-manager Update v1 2023-03-01 13:11:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"174a02b9-2cfe-45cb-8bbb-69a603d52f6e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-01 13:11:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-01 13:11:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.138\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b4db8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b4db8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.76,PodIP:10.233.64.138,StartTime:2023-03-01 13:11:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-01 13:11:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://e91ed0632da63cc73c180f1ac86ff5c61ace0a40d01898f843445b207870fe6f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.138,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  1 13:11:46.873: INFO: Pod "webserver-deployment-845c8977d9-jgzdd" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-jgzdd webserver-deployment-845c8977d9- deployment-9252  6f9e4ac7-5559-4ae4-a046-aedba7722bd6 44626 0 2023-03-01 13:11:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:60147d7f2fedaf51b041e26f854dbaf1973a8bcb056db3b0810e7ccee8be6089 cni.projectcalico.org/podIP:10.233.95.185/32 cni.projectcalico.org/podIPs:10.233.95.185/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 174a02b9-2cfe-45cb-8bbb-69a603d52f6e 0xc003611077 0xc003611078}] [] [{kube-controller-manager Update v1 2023-03-01 13:11:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"174a02b9-2cfe-45cb-8bbb-69a603d52f6e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-01 13:11:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-01 13:11:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.95.185\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lhcpn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lhcpn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.178,PodIP:10.233.95.185,StartTime:2023-03-01 13:11:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-01 13:11:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://27dc77eadde5f0bda9b059e06a4c6f663212e8325326ad817dfc82b49ae19a41,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.95.185,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  1 13:11:46.874: INFO: Pod "webserver-deployment-845c8977d9-lcxq6" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-lcxq6 webserver-deployment-845c8977d9- deployment-9252  fc03d4d1-0f11-412e-8f64-b74156635b52 44821 0 2023-03-01 13:11:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 174a02b9-2cfe-45cb-8bbb-69a603d52f6e 0xc003611287 0xc003611288}] [] [{kube-controller-manager Update v1 2023-03-01 13:11:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"174a02b9-2cfe-45cb-8bbb-69a603d52f6e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-01 13:11:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sm79c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sm79c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.76,PodIP:,StartTime:2023-03-01 13:11:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  1 13:11:46.874: INFO: Pod "webserver-deployment-845c8977d9-mhx2h" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-mhx2h webserver-deployment-845c8977d9- deployment-9252  863abe70-c946-4a9b-978d-089bf758409b 44816 0 2023-03-01 13:11:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 174a02b9-2cfe-45cb-8bbb-69a603d52f6e 0xc003611457 0xc003611458}] [] [{kube-controller-manager Update v1 2023-03-01 13:11:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"174a02b9-2cfe-45cb-8bbb-69a603d52f6e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vthth,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vthth,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  1 13:11:46.874: INFO: Pod "webserver-deployment-845c8977d9-q24cx" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-q24cx webserver-deployment-845c8977d9- deployment-9252  fb57c198-454f-410a-9cc8-72b7ee441669 44662 0 2023-03-01 13:11:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:99de5b8cee018593ab1e0d5cf65c0b447946bac77e995a26dfe947614a0db8dd cni.projectcalico.org/podIP:10.233.74.121/32 cni.projectcalico.org/podIPs:10.233.74.121/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 174a02b9-2cfe-45cb-8bbb-69a603d52f6e 0xc0036115e0 0xc0036115e1}] [] [{kube-controller-manager Update v1 2023-03-01 13:11:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"174a02b9-2cfe-45cb-8bbb-69a603d52f6e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-01 13:11:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-01 13:11:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.74.121\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-swrkf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-swrkf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.2.241,PodIP:10.233.74.121,StartTime:2023-03-01 13:11:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-01 13:11:42 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://a22d46c1f44077325adc8ddc9048c1087b959f116119e8566e449fd1ced8b65f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.74.121,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  1 13:11:46.874: INFO: Pod "webserver-deployment-845c8977d9-qzkd5" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-qzkd5 webserver-deployment-845c8977d9- deployment-9252  2622927b-4e74-458f-90d4-49b182176f9d 44631 0 2023-03-01 13:11:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:ac389d362415614639fded029df978389481aca866ac2dcae57c84faf1e6aed6 cni.projectcalico.org/podIP:10.233.95.184/32 cni.projectcalico.org/podIPs:10.233.95.184/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 174a02b9-2cfe-45cb-8bbb-69a603d52f6e 0xc003611807 0xc003611808}] [] [{kube-controller-manager Update v1 2023-03-01 13:11:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"174a02b9-2cfe-45cb-8bbb-69a603d52f6e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-01 13:11:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-01 13:11:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.95.184\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vm2sm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vm2sm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.178,PodIP:10.233.95.184,StartTime:2023-03-01 13:11:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-01 13:11:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://95a2e033b0eacffa688e4d0757ac0cd71ec5a8091f54e650b422b99c67489d3d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.95.184,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  1 13:11:46.877: INFO: Pod "webserver-deployment-845c8977d9-st2sf" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-st2sf webserver-deployment-845c8977d9- deployment-9252  fc8580e4-52a0-4ff1-8ae4-34d9ed760157 44641 0 2023-03-01 13:11:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:726af8d36b8935eb5ccfb5e9c71b809bad8c02c9806d190d4ffc45d122ba030a cni.projectcalico.org/podIP:10.233.74.96/32 cni.projectcalico.org/podIPs:10.233.74.96/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 174a02b9-2cfe-45cb-8bbb-69a603d52f6e 0xc003611a37 0xc003611a38}] [] [{kube-controller-manager Update v1 2023-03-01 13:11:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"174a02b9-2cfe-45cb-8bbb-69a603d52f6e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-01 13:11:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-01 13:11:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.74.96\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t8vl4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t8vl4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.2.241,PodIP:10.233.74.96,StartTime:2023-03-01 13:11:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-01 13:11:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://0b23a5f4cb92ee47443f2972425132ddb8f180dc9cc5b0b87b4d4b6d95d719f0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.74.96,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  1 13:11:46.877: INFO: Pod "webserver-deployment-845c8977d9-t6j4d" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-t6j4d webserver-deployment-845c8977d9- deployment-9252  529f768b-faa9-46cc-9134-0501865eac2d 44799 0 2023-03-01 13:11:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 174a02b9-2cfe-45cb-8bbb-69a603d52f6e 0xc003611c47 0xc003611c48}] [] [{kube-controller-manager Update v1 2023-03-01 13:11:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"174a02b9-2cfe-45cb-8bbb-69a603d52f6e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b56jv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b56jv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  1 13:11:46.878: INFO: Pod "webserver-deployment-845c8977d9-tlf24" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-tlf24 webserver-deployment-845c8977d9- deployment-9252  9e2f1582-3348-44f9-a8d5-b974630cb043 44651 0 2023-03-01 13:11:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:905821bc5cdb9802f0ebbac0e6c608c1009d01f4392819599c4018e43e97857a cni.projectcalico.org/podIP:10.233.64.140/32 cni.projectcalico.org/podIPs:10.233.64.140/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 174a02b9-2cfe-45cb-8bbb-69a603d52f6e 0xc003611dd0 0xc003611dd1}] [] [{kube-controller-manager Update v1 2023-03-01 13:11:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"174a02b9-2cfe-45cb-8bbb-69a603d52f6e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-01 13:11:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-01 13:11:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.140\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2c2zb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2c2zb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.76,PodIP:10.233.64.140,StartTime:2023-03-01 13:11:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-01 13:11:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://2dc1402ceaa432bc20e1e3711910330022e73cb5c7f14970d845c55994f4db61,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.140,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  1 13:11:46.878: INFO: Pod "webserver-deployment-845c8977d9-xkxgg" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-xkxgg webserver-deployment-845c8977d9- deployment-9252  9a2bda6f-df91-45be-bafb-b31a8ef2969b 44824 0 2023-03-01 13:11:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 174a02b9-2cfe-45cb-8bbb-69a603d52f6e 0xc003611fd7 0xc003611fd8}] [] [{kube-controller-manager Update v1 2023-03-01 13:11:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"174a02b9-2cfe-45cb-8bbb-69a603d52f6e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b74qn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b74qn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:11:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Mar  1 13:11:46.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-9252" for this suite. 03/01/23 13:11:46.893
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:11:46.917
Mar  1 13:11:46.917: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename dns 03/01/23 13:11:46.918
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:11:46.959
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:11:46.962
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 03/01/23 13:11:46.965
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2868.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2868.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2868.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2868.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2868.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2868.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2868.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2868.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2868.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2868.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2868.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2868.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 54.19.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.19.54_udp@PTR;check="$$(dig +tcp +noall +answer +search 54.19.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.19.54_tcp@PTR;sleep 1; done
 03/01/23 13:11:46.988
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2868.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2868.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2868.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2868.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2868.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2868.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2868.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2868.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2868.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2868.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2868.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2868.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 54.19.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.19.54_udp@PTR;check="$$(dig +tcp +noall +answer +search 54.19.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.19.54_tcp@PTR;sleep 1; done
 03/01/23 13:11:46.989
STEP: creating a pod to probe DNS 03/01/23 13:11:46.989
STEP: submitting the pod to kubernetes 03/01/23 13:11:46.989
Mar  1 13:11:47.005: INFO: Waiting up to 15m0s for pod "dns-test-8e285bab-b12e-4611-891b-a51c2c92600b" in namespace "dns-2868" to be "running"
Mar  1 13:11:47.010: INFO: Pod "dns-test-8e285bab-b12e-4611-891b-a51c2c92600b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.769134ms
Mar  1 13:11:49.016: INFO: Pod "dns-test-8e285bab-b12e-4611-891b-a51c2c92600b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010991893s
Mar  1 13:11:51.017: INFO: Pod "dns-test-8e285bab-b12e-4611-891b-a51c2c92600b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011143683s
Mar  1 13:11:53.016: INFO: Pod "dns-test-8e285bab-b12e-4611-891b-a51c2c92600b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.010718147s
Mar  1 13:11:55.015: INFO: Pod "dns-test-8e285bab-b12e-4611-891b-a51c2c92600b": Phase="Running", Reason="", readiness=true. Elapsed: 8.009961949s
Mar  1 13:11:55.015: INFO: Pod "dns-test-8e285bab-b12e-4611-891b-a51c2c92600b" satisfied condition "running"
STEP: retrieving the pod 03/01/23 13:11:55.016
STEP: looking for the results for each expected name from probers 03/01/23 13:11:55.021
Mar  1 13:11:55.027: INFO: Unable to read wheezy_udp@dns-test-service.dns-2868.svc.cluster.local from pod dns-2868/dns-test-8e285bab-b12e-4611-891b-a51c2c92600b: the server could not find the requested resource (get pods dns-test-8e285bab-b12e-4611-891b-a51c2c92600b)
Mar  1 13:11:55.033: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2868.svc.cluster.local from pod dns-2868/dns-test-8e285bab-b12e-4611-891b-a51c2c92600b: the server could not find the requested resource (get pods dns-test-8e285bab-b12e-4611-891b-a51c2c92600b)
Mar  1 13:11:55.038: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2868.svc.cluster.local from pod dns-2868/dns-test-8e285bab-b12e-4611-891b-a51c2c92600b: the server could not find the requested resource (get pods dns-test-8e285bab-b12e-4611-891b-a51c2c92600b)
Mar  1 13:11:55.043: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2868.svc.cluster.local from pod dns-2868/dns-test-8e285bab-b12e-4611-891b-a51c2c92600b: the server could not find the requested resource (get pods dns-test-8e285bab-b12e-4611-891b-a51c2c92600b)
Mar  1 13:11:55.068: INFO: Unable to read jessie_udp@dns-test-service.dns-2868.svc.cluster.local from pod dns-2868/dns-test-8e285bab-b12e-4611-891b-a51c2c92600b: the server could not find the requested resource (get pods dns-test-8e285bab-b12e-4611-891b-a51c2c92600b)
Mar  1 13:11:55.073: INFO: Unable to read jessie_tcp@dns-test-service.dns-2868.svc.cluster.local from pod dns-2868/dns-test-8e285bab-b12e-4611-891b-a51c2c92600b: the server could not find the requested resource (get pods dns-test-8e285bab-b12e-4611-891b-a51c2c92600b)
Mar  1 13:11:55.078: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2868.svc.cluster.local from pod dns-2868/dns-test-8e285bab-b12e-4611-891b-a51c2c92600b: the server could not find the requested resource (get pods dns-test-8e285bab-b12e-4611-891b-a51c2c92600b)
Mar  1 13:11:55.083: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2868.svc.cluster.local from pod dns-2868/dns-test-8e285bab-b12e-4611-891b-a51c2c92600b: the server could not find the requested resource (get pods dns-test-8e285bab-b12e-4611-891b-a51c2c92600b)
Mar  1 13:11:55.105: INFO: Lookups using dns-2868/dns-test-8e285bab-b12e-4611-891b-a51c2c92600b failed for: [wheezy_udp@dns-test-service.dns-2868.svc.cluster.local wheezy_tcp@dns-test-service.dns-2868.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2868.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2868.svc.cluster.local jessie_udp@dns-test-service.dns-2868.svc.cluster.local jessie_tcp@dns-test-service.dns-2868.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2868.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2868.svc.cluster.local]

Mar  1 13:12:00.196: INFO: DNS probes using dns-2868/dns-test-8e285bab-b12e-4611-891b-a51c2c92600b succeeded

STEP: deleting the pod 03/01/23 13:12:00.196
STEP: deleting the test service 03/01/23 13:12:00.22
STEP: deleting the test headless service 03/01/23 13:12:00.249
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Mar  1 13:12:00.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2868" for this suite. 03/01/23 13:12:00.281
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","completed":317,"skipped":5776,"failed":0}
------------------------------
â€¢ [SLOW TEST] [13.374 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:11:46.917
    Mar  1 13:11:46.917: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename dns 03/01/23 13:11:46.918
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:11:46.959
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:11:46.962
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 03/01/23 13:11:46.965
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2868.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2868.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2868.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2868.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2868.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2868.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2868.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2868.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2868.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2868.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2868.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2868.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 54.19.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.19.54_udp@PTR;check="$$(dig +tcp +noall +answer +search 54.19.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.19.54_tcp@PTR;sleep 1; done
     03/01/23 13:11:46.988
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2868.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2868.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2868.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2868.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2868.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2868.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2868.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2868.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2868.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2868.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2868.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2868.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 54.19.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.19.54_udp@PTR;check="$$(dig +tcp +noall +answer +search 54.19.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.19.54_tcp@PTR;sleep 1; done
     03/01/23 13:11:46.989
    STEP: creating a pod to probe DNS 03/01/23 13:11:46.989
    STEP: submitting the pod to kubernetes 03/01/23 13:11:46.989
    Mar  1 13:11:47.005: INFO: Waiting up to 15m0s for pod "dns-test-8e285bab-b12e-4611-891b-a51c2c92600b" in namespace "dns-2868" to be "running"
    Mar  1 13:11:47.010: INFO: Pod "dns-test-8e285bab-b12e-4611-891b-a51c2c92600b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.769134ms
    Mar  1 13:11:49.016: INFO: Pod "dns-test-8e285bab-b12e-4611-891b-a51c2c92600b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010991893s
    Mar  1 13:11:51.017: INFO: Pod "dns-test-8e285bab-b12e-4611-891b-a51c2c92600b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011143683s
    Mar  1 13:11:53.016: INFO: Pod "dns-test-8e285bab-b12e-4611-891b-a51c2c92600b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.010718147s
    Mar  1 13:11:55.015: INFO: Pod "dns-test-8e285bab-b12e-4611-891b-a51c2c92600b": Phase="Running", Reason="", readiness=true. Elapsed: 8.009961949s
    Mar  1 13:11:55.015: INFO: Pod "dns-test-8e285bab-b12e-4611-891b-a51c2c92600b" satisfied condition "running"
    STEP: retrieving the pod 03/01/23 13:11:55.016
    STEP: looking for the results for each expected name from probers 03/01/23 13:11:55.021
    Mar  1 13:11:55.027: INFO: Unable to read wheezy_udp@dns-test-service.dns-2868.svc.cluster.local from pod dns-2868/dns-test-8e285bab-b12e-4611-891b-a51c2c92600b: the server could not find the requested resource (get pods dns-test-8e285bab-b12e-4611-891b-a51c2c92600b)
    Mar  1 13:11:55.033: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2868.svc.cluster.local from pod dns-2868/dns-test-8e285bab-b12e-4611-891b-a51c2c92600b: the server could not find the requested resource (get pods dns-test-8e285bab-b12e-4611-891b-a51c2c92600b)
    Mar  1 13:11:55.038: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2868.svc.cluster.local from pod dns-2868/dns-test-8e285bab-b12e-4611-891b-a51c2c92600b: the server could not find the requested resource (get pods dns-test-8e285bab-b12e-4611-891b-a51c2c92600b)
    Mar  1 13:11:55.043: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2868.svc.cluster.local from pod dns-2868/dns-test-8e285bab-b12e-4611-891b-a51c2c92600b: the server could not find the requested resource (get pods dns-test-8e285bab-b12e-4611-891b-a51c2c92600b)
    Mar  1 13:11:55.068: INFO: Unable to read jessie_udp@dns-test-service.dns-2868.svc.cluster.local from pod dns-2868/dns-test-8e285bab-b12e-4611-891b-a51c2c92600b: the server could not find the requested resource (get pods dns-test-8e285bab-b12e-4611-891b-a51c2c92600b)
    Mar  1 13:11:55.073: INFO: Unable to read jessie_tcp@dns-test-service.dns-2868.svc.cluster.local from pod dns-2868/dns-test-8e285bab-b12e-4611-891b-a51c2c92600b: the server could not find the requested resource (get pods dns-test-8e285bab-b12e-4611-891b-a51c2c92600b)
    Mar  1 13:11:55.078: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2868.svc.cluster.local from pod dns-2868/dns-test-8e285bab-b12e-4611-891b-a51c2c92600b: the server could not find the requested resource (get pods dns-test-8e285bab-b12e-4611-891b-a51c2c92600b)
    Mar  1 13:11:55.083: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2868.svc.cluster.local from pod dns-2868/dns-test-8e285bab-b12e-4611-891b-a51c2c92600b: the server could not find the requested resource (get pods dns-test-8e285bab-b12e-4611-891b-a51c2c92600b)
    Mar  1 13:11:55.105: INFO: Lookups using dns-2868/dns-test-8e285bab-b12e-4611-891b-a51c2c92600b failed for: [wheezy_udp@dns-test-service.dns-2868.svc.cluster.local wheezy_tcp@dns-test-service.dns-2868.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2868.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2868.svc.cluster.local jessie_udp@dns-test-service.dns-2868.svc.cluster.local jessie_tcp@dns-test-service.dns-2868.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2868.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2868.svc.cluster.local]

    Mar  1 13:12:00.196: INFO: DNS probes using dns-2868/dns-test-8e285bab-b12e-4611-891b-a51c2c92600b succeeded

    STEP: deleting the pod 03/01/23 13:12:00.196
    STEP: deleting the test service 03/01/23 13:12:00.22
    STEP: deleting the test headless service 03/01/23 13:12:00.249
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Mar  1 13:12:00.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-2868" for this suite. 03/01/23 13:12:00.281
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:12:00.292
Mar  1 13:12:00.292: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename subpath 03/01/23 13:12:00.292
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:12:00.316
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:12:00.318
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 03/01/23 13:12:00.321
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-vd5l 03/01/23 13:12:00.332
STEP: Creating a pod to test atomic-volume-subpath 03/01/23 13:12:00.332
Mar  1 13:12:00.347: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-vd5l" in namespace "subpath-2219" to be "Succeeded or Failed"
Mar  1 13:12:00.354: INFO: Pod "pod-subpath-test-projected-vd5l": Phase="Pending", Reason="", readiness=false. Elapsed: 7.413055ms
Mar  1 13:12:02.360: INFO: Pod "pod-subpath-test-projected-vd5l": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013193174s
Mar  1 13:12:04.361: INFO: Pod "pod-subpath-test-projected-vd5l": Phase="Running", Reason="", readiness=true. Elapsed: 4.014395845s
Mar  1 13:12:06.359: INFO: Pod "pod-subpath-test-projected-vd5l": Phase="Running", Reason="", readiness=true. Elapsed: 6.012908834s
Mar  1 13:12:08.360: INFO: Pod "pod-subpath-test-projected-vd5l": Phase="Running", Reason="", readiness=true. Elapsed: 8.013037899s
Mar  1 13:12:10.360: INFO: Pod "pod-subpath-test-projected-vd5l": Phase="Running", Reason="", readiness=true. Elapsed: 10.013028542s
Mar  1 13:12:12.360: INFO: Pod "pod-subpath-test-projected-vd5l": Phase="Running", Reason="", readiness=true. Elapsed: 12.013786673s
Mar  1 13:12:14.359: INFO: Pod "pod-subpath-test-projected-vd5l": Phase="Running", Reason="", readiness=true. Elapsed: 14.012594054s
Mar  1 13:12:16.359: INFO: Pod "pod-subpath-test-projected-vd5l": Phase="Running", Reason="", readiness=true. Elapsed: 16.012625019s
Mar  1 13:12:18.360: INFO: Pod "pod-subpath-test-projected-vd5l": Phase="Running", Reason="", readiness=true. Elapsed: 18.013859758s
Mar  1 13:12:20.359: INFO: Pod "pod-subpath-test-projected-vd5l": Phase="Running", Reason="", readiness=true. Elapsed: 20.012709603s
Mar  1 13:12:22.359: INFO: Pod "pod-subpath-test-projected-vd5l": Phase="Running", Reason="", readiness=true. Elapsed: 22.012355544s
Mar  1 13:12:24.362: INFO: Pod "pod-subpath-test-projected-vd5l": Phase="Running", Reason="", readiness=false. Elapsed: 24.015418024s
Mar  1 13:12:26.361: INFO: Pod "pod-subpath-test-projected-vd5l": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.014487693s
STEP: Saw pod success 03/01/23 13:12:26.361
Mar  1 13:12:26.361: INFO: Pod "pod-subpath-test-projected-vd5l" satisfied condition "Succeeded or Failed"
Mar  1 13:12:26.366: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-subpath-test-projected-vd5l container test-container-subpath-projected-vd5l: <nil>
STEP: delete the pod 03/01/23 13:12:26.376
Mar  1 13:12:26.392: INFO: Waiting for pod pod-subpath-test-projected-vd5l to disappear
Mar  1 13:12:26.397: INFO: Pod pod-subpath-test-projected-vd5l no longer exists
STEP: Deleting pod pod-subpath-test-projected-vd5l 03/01/23 13:12:26.397
Mar  1 13:12:26.397: INFO: Deleting pod "pod-subpath-test-projected-vd5l" in namespace "subpath-2219"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Mar  1 13:12:26.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2219" for this suite. 03/01/23 13:12:26.408
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]","completed":318,"skipped":5779,"failed":0}
------------------------------
â€¢ [SLOW TEST] [26.126 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:12:00.292
    Mar  1 13:12:00.292: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename subpath 03/01/23 13:12:00.292
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:12:00.316
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:12:00.318
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 03/01/23 13:12:00.321
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-vd5l 03/01/23 13:12:00.332
    STEP: Creating a pod to test atomic-volume-subpath 03/01/23 13:12:00.332
    Mar  1 13:12:00.347: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-vd5l" in namespace "subpath-2219" to be "Succeeded or Failed"
    Mar  1 13:12:00.354: INFO: Pod "pod-subpath-test-projected-vd5l": Phase="Pending", Reason="", readiness=false. Elapsed: 7.413055ms
    Mar  1 13:12:02.360: INFO: Pod "pod-subpath-test-projected-vd5l": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013193174s
    Mar  1 13:12:04.361: INFO: Pod "pod-subpath-test-projected-vd5l": Phase="Running", Reason="", readiness=true. Elapsed: 4.014395845s
    Mar  1 13:12:06.359: INFO: Pod "pod-subpath-test-projected-vd5l": Phase="Running", Reason="", readiness=true. Elapsed: 6.012908834s
    Mar  1 13:12:08.360: INFO: Pod "pod-subpath-test-projected-vd5l": Phase="Running", Reason="", readiness=true. Elapsed: 8.013037899s
    Mar  1 13:12:10.360: INFO: Pod "pod-subpath-test-projected-vd5l": Phase="Running", Reason="", readiness=true. Elapsed: 10.013028542s
    Mar  1 13:12:12.360: INFO: Pod "pod-subpath-test-projected-vd5l": Phase="Running", Reason="", readiness=true. Elapsed: 12.013786673s
    Mar  1 13:12:14.359: INFO: Pod "pod-subpath-test-projected-vd5l": Phase="Running", Reason="", readiness=true. Elapsed: 14.012594054s
    Mar  1 13:12:16.359: INFO: Pod "pod-subpath-test-projected-vd5l": Phase="Running", Reason="", readiness=true. Elapsed: 16.012625019s
    Mar  1 13:12:18.360: INFO: Pod "pod-subpath-test-projected-vd5l": Phase="Running", Reason="", readiness=true. Elapsed: 18.013859758s
    Mar  1 13:12:20.359: INFO: Pod "pod-subpath-test-projected-vd5l": Phase="Running", Reason="", readiness=true. Elapsed: 20.012709603s
    Mar  1 13:12:22.359: INFO: Pod "pod-subpath-test-projected-vd5l": Phase="Running", Reason="", readiness=true. Elapsed: 22.012355544s
    Mar  1 13:12:24.362: INFO: Pod "pod-subpath-test-projected-vd5l": Phase="Running", Reason="", readiness=false. Elapsed: 24.015418024s
    Mar  1 13:12:26.361: INFO: Pod "pod-subpath-test-projected-vd5l": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.014487693s
    STEP: Saw pod success 03/01/23 13:12:26.361
    Mar  1 13:12:26.361: INFO: Pod "pod-subpath-test-projected-vd5l" satisfied condition "Succeeded or Failed"
    Mar  1 13:12:26.366: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-subpath-test-projected-vd5l container test-container-subpath-projected-vd5l: <nil>
    STEP: delete the pod 03/01/23 13:12:26.376
    Mar  1 13:12:26.392: INFO: Waiting for pod pod-subpath-test-projected-vd5l to disappear
    Mar  1 13:12:26.397: INFO: Pod pod-subpath-test-projected-vd5l no longer exists
    STEP: Deleting pod pod-subpath-test-projected-vd5l 03/01/23 13:12:26.397
    Mar  1 13:12:26.397: INFO: Deleting pod "pod-subpath-test-projected-vd5l" in namespace "subpath-2219"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Mar  1 13:12:26.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-2219" for this suite. 03/01/23 13:12:26.408
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:12:26.421
Mar  1 13:12:26.421: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename security-context-test 03/01/23 13:12:26.422
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:12:26.443
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:12:26.447
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
Mar  1 13:12:26.458: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-1cf18b96-fe96-4e9c-9646-d699b3895a0f" in namespace "security-context-test-714" to be "Succeeded or Failed"
Mar  1 13:12:26.463: INFO: Pod "busybox-readonly-false-1cf18b96-fe96-4e9c-9646-d699b3895a0f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.462102ms
Mar  1 13:12:28.468: INFO: Pod "busybox-readonly-false-1cf18b96-fe96-4e9c-9646-d699b3895a0f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009698712s
Mar  1 13:12:30.467: INFO: Pod "busybox-readonly-false-1cf18b96-fe96-4e9c-9646-d699b3895a0f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009072522s
Mar  1 13:12:30.467: INFO: Pod "busybox-readonly-false-1cf18b96-fe96-4e9c-9646-d699b3895a0f" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Mar  1 13:12:30.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-714" for this suite. 03/01/23 13:12:30.473
{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","completed":319,"skipped":5810,"failed":0}
------------------------------
â€¢ [4.063 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:429
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:485

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:12:26.421
    Mar  1 13:12:26.421: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename security-context-test 03/01/23 13:12:26.422
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:12:26.443
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:12:26.447
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:485
    Mar  1 13:12:26.458: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-1cf18b96-fe96-4e9c-9646-d699b3895a0f" in namespace "security-context-test-714" to be "Succeeded or Failed"
    Mar  1 13:12:26.463: INFO: Pod "busybox-readonly-false-1cf18b96-fe96-4e9c-9646-d699b3895a0f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.462102ms
    Mar  1 13:12:28.468: INFO: Pod "busybox-readonly-false-1cf18b96-fe96-4e9c-9646-d699b3895a0f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009698712s
    Mar  1 13:12:30.467: INFO: Pod "busybox-readonly-false-1cf18b96-fe96-4e9c-9646-d699b3895a0f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009072522s
    Mar  1 13:12:30.467: INFO: Pod "busybox-readonly-false-1cf18b96-fe96-4e9c-9646-d699b3895a0f" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Mar  1 13:12:30.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-714" for this suite. 03/01/23 13:12:30.473
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:12:30.485
Mar  1 13:12:30.485: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename sysctl 03/01/23 13:12:30.486
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:12:30.511
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:12:30.513
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 03/01/23 13:12:30.515
STEP: Watching for error events or started pod 03/01/23 13:12:30.528
STEP: Waiting for pod completion 03/01/23 13:12:32.534
Mar  1 13:12:32.534: INFO: Waiting up to 3m0s for pod "sysctl-9c82bb83-b6cf-49a3-b29e-76c4fbe0ee28" in namespace "sysctl-6009" to be "completed"
Mar  1 13:12:32.538: INFO: Pod "sysctl-9c82bb83-b6cf-49a3-b29e-76c4fbe0ee28": Phase="Pending", Reason="", readiness=false. Elapsed: 4.499495ms
Mar  1 13:12:34.544: INFO: Pod "sysctl-9c82bb83-b6cf-49a3-b29e-76c4fbe0ee28": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01032754s
Mar  1 13:12:34.544: INFO: Pod "sysctl-9c82bb83-b6cf-49a3-b29e-76c4fbe0ee28" satisfied condition "completed"
STEP: Checking that the pod succeeded 03/01/23 13:12:34.548
STEP: Getting logs from the pod 03/01/23 13:12:34.548
STEP: Checking that the sysctl is actually updated 03/01/23 13:12:34.558
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
Mar  1 13:12:34.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-6009" for this suite. 03/01/23 13:12:34.563
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":320,"skipped":5821,"failed":0}
------------------------------
â€¢ [4.087 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:12:30.485
    Mar  1 13:12:30.485: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename sysctl 03/01/23 13:12:30.486
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:12:30.511
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:12:30.513
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 03/01/23 13:12:30.515
    STEP: Watching for error events or started pod 03/01/23 13:12:30.528
    STEP: Waiting for pod completion 03/01/23 13:12:32.534
    Mar  1 13:12:32.534: INFO: Waiting up to 3m0s for pod "sysctl-9c82bb83-b6cf-49a3-b29e-76c4fbe0ee28" in namespace "sysctl-6009" to be "completed"
    Mar  1 13:12:32.538: INFO: Pod "sysctl-9c82bb83-b6cf-49a3-b29e-76c4fbe0ee28": Phase="Pending", Reason="", readiness=false. Elapsed: 4.499495ms
    Mar  1 13:12:34.544: INFO: Pod "sysctl-9c82bb83-b6cf-49a3-b29e-76c4fbe0ee28": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01032754s
    Mar  1 13:12:34.544: INFO: Pod "sysctl-9c82bb83-b6cf-49a3-b29e-76c4fbe0ee28" satisfied condition "completed"
    STEP: Checking that the pod succeeded 03/01/23 13:12:34.548
    STEP: Getting logs from the pod 03/01/23 13:12:34.548
    STEP: Checking that the sysctl is actually updated 03/01/23 13:12:34.558
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    Mar  1 13:12:34.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-6009" for this suite. 03/01/23 13:12:34.563
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:12:34.574
Mar  1 13:12:34.574: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename projected 03/01/23 13:12:34.575
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:12:34.605
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:12:34.607
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
STEP: Creating projection with secret that has name projected-secret-test-map-7596dff5-5dec-4e75-8fe2-af3d6b3059cc 03/01/23 13:12:34.61
STEP: Creating a pod to test consume secrets 03/01/23 13:12:34.615
Mar  1 13:12:34.626: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1710472b-9969-40a5-9e46-b2019514e5c0" in namespace "projected-487" to be "Succeeded or Failed"
Mar  1 13:12:34.631: INFO: Pod "pod-projected-secrets-1710472b-9969-40a5-9e46-b2019514e5c0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.224714ms
Mar  1 13:12:36.637: INFO: Pod "pod-projected-secrets-1710472b-9969-40a5-9e46-b2019514e5c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010893716s
Mar  1 13:12:38.636: INFO: Pod "pod-projected-secrets-1710472b-9969-40a5-9e46-b2019514e5c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009955266s
STEP: Saw pod success 03/01/23 13:12:38.636
Mar  1 13:12:38.636: INFO: Pod "pod-projected-secrets-1710472b-9969-40a5-9e46-b2019514e5c0" satisfied condition "Succeeded or Failed"
Mar  1 13:12:38.641: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-projected-secrets-1710472b-9969-40a5-9e46-b2019514e5c0 container projected-secret-volume-test: <nil>
STEP: delete the pod 03/01/23 13:12:38.65
Mar  1 13:12:38.665: INFO: Waiting for pod pod-projected-secrets-1710472b-9969-40a5-9e46-b2019514e5c0 to disappear
Mar  1 13:12:38.669: INFO: Pod pod-projected-secrets-1710472b-9969-40a5-9e46-b2019514e5c0 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Mar  1 13:12:38.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-487" for this suite. 03/01/23 13:12:38.675
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":321,"skipped":5875,"failed":0}
------------------------------
â€¢ [4.109 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:12:34.574
    Mar  1 13:12:34.574: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename projected 03/01/23 13:12:34.575
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:12:34.605
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:12:34.607
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:77
    STEP: Creating projection with secret that has name projected-secret-test-map-7596dff5-5dec-4e75-8fe2-af3d6b3059cc 03/01/23 13:12:34.61
    STEP: Creating a pod to test consume secrets 03/01/23 13:12:34.615
    Mar  1 13:12:34.626: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1710472b-9969-40a5-9e46-b2019514e5c0" in namespace "projected-487" to be "Succeeded or Failed"
    Mar  1 13:12:34.631: INFO: Pod "pod-projected-secrets-1710472b-9969-40a5-9e46-b2019514e5c0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.224714ms
    Mar  1 13:12:36.637: INFO: Pod "pod-projected-secrets-1710472b-9969-40a5-9e46-b2019514e5c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010893716s
    Mar  1 13:12:38.636: INFO: Pod "pod-projected-secrets-1710472b-9969-40a5-9e46-b2019514e5c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009955266s
    STEP: Saw pod success 03/01/23 13:12:38.636
    Mar  1 13:12:38.636: INFO: Pod "pod-projected-secrets-1710472b-9969-40a5-9e46-b2019514e5c0" satisfied condition "Succeeded or Failed"
    Mar  1 13:12:38.641: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-projected-secrets-1710472b-9969-40a5-9e46-b2019514e5c0 container projected-secret-volume-test: <nil>
    STEP: delete the pod 03/01/23 13:12:38.65
    Mar  1 13:12:38.665: INFO: Waiting for pod pod-projected-secrets-1710472b-9969-40a5-9e46-b2019514e5c0 to disappear
    Mar  1 13:12:38.669: INFO: Pod pod-projected-secrets-1710472b-9969-40a5-9e46-b2019514e5c0 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Mar  1 13:12:38.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-487" for this suite. 03/01/23 13:12:38.675
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:12:38.685
Mar  1 13:12:38.685: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename projected 03/01/23 13:12:38.686
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:12:38.707
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:12:38.71
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
STEP: Creating configMap with name projected-configmap-test-volume-84c23c34-850b-49e9-ada9-5704901d6004 03/01/23 13:12:38.712
STEP: Creating a pod to test consume configMaps 03/01/23 13:12:38.718
Mar  1 13:12:38.728: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9d945c7d-b279-4bd3-9a18-4e3f8ef93d16" in namespace "projected-5828" to be "Succeeded or Failed"
Mar  1 13:12:38.733: INFO: Pod "pod-projected-configmaps-9d945c7d-b279-4bd3-9a18-4e3f8ef93d16": Phase="Pending", Reason="", readiness=false. Elapsed: 5.040091ms
Mar  1 13:12:40.739: INFO: Pod "pod-projected-configmaps-9d945c7d-b279-4bd3-9a18-4e3f8ef93d16": Phase="Running", Reason="", readiness=false. Elapsed: 2.010876742s
Mar  1 13:12:42.739: INFO: Pod "pod-projected-configmaps-9d945c7d-b279-4bd3-9a18-4e3f8ef93d16": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010611101s
STEP: Saw pod success 03/01/23 13:12:42.739
Mar  1 13:12:42.739: INFO: Pod "pod-projected-configmaps-9d945c7d-b279-4bd3-9a18-4e3f8ef93d16" satisfied condition "Succeeded or Failed"
Mar  1 13:12:42.743: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-projected-configmaps-9d945c7d-b279-4bd3-9a18-4e3f8ef93d16 container agnhost-container: <nil>
STEP: delete the pod 03/01/23 13:12:42.752
Mar  1 13:12:42.767: INFO: Waiting for pod pod-projected-configmaps-9d945c7d-b279-4bd3-9a18-4e3f8ef93d16 to disappear
Mar  1 13:12:42.771: INFO: Pod pod-projected-configmaps-9d945c7d-b279-4bd3-9a18-4e3f8ef93d16 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Mar  1 13:12:42.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5828" for this suite. 03/01/23 13:12:42.776
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":322,"skipped":5908,"failed":0}
------------------------------
â€¢ [4.098 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:12:38.685
    Mar  1 13:12:38.685: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename projected 03/01/23 13:12:38.686
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:12:38.707
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:12:38.71
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:46
    STEP: Creating configMap with name projected-configmap-test-volume-84c23c34-850b-49e9-ada9-5704901d6004 03/01/23 13:12:38.712
    STEP: Creating a pod to test consume configMaps 03/01/23 13:12:38.718
    Mar  1 13:12:38.728: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9d945c7d-b279-4bd3-9a18-4e3f8ef93d16" in namespace "projected-5828" to be "Succeeded or Failed"
    Mar  1 13:12:38.733: INFO: Pod "pod-projected-configmaps-9d945c7d-b279-4bd3-9a18-4e3f8ef93d16": Phase="Pending", Reason="", readiness=false. Elapsed: 5.040091ms
    Mar  1 13:12:40.739: INFO: Pod "pod-projected-configmaps-9d945c7d-b279-4bd3-9a18-4e3f8ef93d16": Phase="Running", Reason="", readiness=false. Elapsed: 2.010876742s
    Mar  1 13:12:42.739: INFO: Pod "pod-projected-configmaps-9d945c7d-b279-4bd3-9a18-4e3f8ef93d16": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010611101s
    STEP: Saw pod success 03/01/23 13:12:42.739
    Mar  1 13:12:42.739: INFO: Pod "pod-projected-configmaps-9d945c7d-b279-4bd3-9a18-4e3f8ef93d16" satisfied condition "Succeeded or Failed"
    Mar  1 13:12:42.743: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-projected-configmaps-9d945c7d-b279-4bd3-9a18-4e3f8ef93d16 container agnhost-container: <nil>
    STEP: delete the pod 03/01/23 13:12:42.752
    Mar  1 13:12:42.767: INFO: Waiting for pod pod-projected-configmaps-9d945c7d-b279-4bd3-9a18-4e3f8ef93d16 to disappear
    Mar  1 13:12:42.771: INFO: Pod pod-projected-configmaps-9d945c7d-b279-4bd3-9a18-4e3f8ef93d16 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Mar  1 13:12:42.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5828" for this suite. 03/01/23 13:12:42.776
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:12:42.784
Mar  1 13:12:42.785: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename replicaset 03/01/23 13:12:42.785
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:12:42.807
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:12:42.81
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 03/01/23 13:12:42.812
Mar  1 13:12:42.823: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-2106" to be "running and ready"
Mar  1 13:12:42.827: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 3.894277ms
Mar  1 13:12:42.827: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Mar  1 13:12:44.833: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.010224291s
Mar  1 13:12:44.833: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Mar  1 13:12:44.833: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 03/01/23 13:12:44.837
STEP: Then the orphan pod is adopted 03/01/23 13:12:44.843
STEP: When the matched label of one of its pods change 03/01/23 13:12:45.852
Mar  1 13:12:45.857: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 03/01/23 13:12:45.872
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Mar  1 13:12:46.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2106" for this suite. 03/01/23 13:12:46.889
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","completed":323,"skipped":5919,"failed":0}
------------------------------
â€¢ [4.116 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:12:42.784
    Mar  1 13:12:42.785: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename replicaset 03/01/23 13:12:42.785
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:12:42.807
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:12:42.81
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 03/01/23 13:12:42.812
    Mar  1 13:12:42.823: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-2106" to be "running and ready"
    Mar  1 13:12:42.827: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 3.894277ms
    Mar  1 13:12:42.827: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Mar  1 13:12:44.833: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.010224291s
    Mar  1 13:12:44.833: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Mar  1 13:12:44.833: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 03/01/23 13:12:44.837
    STEP: Then the orphan pod is adopted 03/01/23 13:12:44.843
    STEP: When the matched label of one of its pods change 03/01/23 13:12:45.852
    Mar  1 13:12:45.857: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 03/01/23 13:12:45.872
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Mar  1 13:12:46.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-2106" for this suite. 03/01/23 13:12:46.889
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:12:46.901
Mar  1 13:12:46.901: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename emptydir 03/01/23 13:12:46.902
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:12:46.924
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:12:46.927
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
STEP: Creating a pod to test emptydir 0666 on tmpfs 03/01/23 13:12:46.93
Mar  1 13:12:46.941: INFO: Waiting up to 5m0s for pod "pod-a02e9021-f6ef-41bc-8e72-a49dc2f7bdc5" in namespace "emptydir-4329" to be "Succeeded or Failed"
Mar  1 13:12:46.946: INFO: Pod "pod-a02e9021-f6ef-41bc-8e72-a49dc2f7bdc5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.894103ms
Mar  1 13:12:48.951: INFO: Pod "pod-a02e9021-f6ef-41bc-8e72-a49dc2f7bdc5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009980318s
Mar  1 13:12:50.951: INFO: Pod "pod-a02e9021-f6ef-41bc-8e72-a49dc2f7bdc5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009220169s
STEP: Saw pod success 03/01/23 13:12:50.951
Mar  1 13:12:50.951: INFO: Pod "pod-a02e9021-f6ef-41bc-8e72-a49dc2f7bdc5" satisfied condition "Succeeded or Failed"
Mar  1 13:12:50.955: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-a02e9021-f6ef-41bc-8e72-a49dc2f7bdc5 container test-container: <nil>
STEP: delete the pod 03/01/23 13:12:50.963
Mar  1 13:12:50.979: INFO: Waiting for pod pod-a02e9021-f6ef-41bc-8e72-a49dc2f7bdc5 to disappear
Mar  1 13:12:50.983: INFO: Pod pod-a02e9021-f6ef-41bc-8e72-a49dc2f7bdc5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar  1 13:12:50.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4329" for this suite. 03/01/23 13:12:50.989
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":324,"skipped":5920,"failed":0}
------------------------------
â€¢ [4.097 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:12:46.901
    Mar  1 13:12:46.901: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename emptydir 03/01/23 13:12:46.902
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:12:46.924
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:12:46.927
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:106
    STEP: Creating a pod to test emptydir 0666 on tmpfs 03/01/23 13:12:46.93
    Mar  1 13:12:46.941: INFO: Waiting up to 5m0s for pod "pod-a02e9021-f6ef-41bc-8e72-a49dc2f7bdc5" in namespace "emptydir-4329" to be "Succeeded or Failed"
    Mar  1 13:12:46.946: INFO: Pod "pod-a02e9021-f6ef-41bc-8e72-a49dc2f7bdc5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.894103ms
    Mar  1 13:12:48.951: INFO: Pod "pod-a02e9021-f6ef-41bc-8e72-a49dc2f7bdc5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009980318s
    Mar  1 13:12:50.951: INFO: Pod "pod-a02e9021-f6ef-41bc-8e72-a49dc2f7bdc5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009220169s
    STEP: Saw pod success 03/01/23 13:12:50.951
    Mar  1 13:12:50.951: INFO: Pod "pod-a02e9021-f6ef-41bc-8e72-a49dc2f7bdc5" satisfied condition "Succeeded or Failed"
    Mar  1 13:12:50.955: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-a02e9021-f6ef-41bc-8e72-a49dc2f7bdc5 container test-container: <nil>
    STEP: delete the pod 03/01/23 13:12:50.963
    Mar  1 13:12:50.979: INFO: Waiting for pod pod-a02e9021-f6ef-41bc-8e72-a49dc2f7bdc5 to disappear
    Mar  1 13:12:50.983: INFO: Pod pod-a02e9021-f6ef-41bc-8e72-a49dc2f7bdc5 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar  1 13:12:50.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-4329" for this suite. 03/01/23 13:12:50.989
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:12:51.001
Mar  1 13:12:51.001: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename projected 03/01/23 13:12:51.001
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:12:51.026
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:12:51.031
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
STEP: Creating configMap with name projected-configmap-test-volume-map-44fea070-0869-4aaa-98d6-8eba91584d59 03/01/23 13:12:51.034
STEP: Creating a pod to test consume configMaps 03/01/23 13:12:51.04
Mar  1 13:12:51.055: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5d20ada6-867f-4a90-a7f7-7e2c86ea0991" in namespace "projected-8861" to be "Succeeded or Failed"
Mar  1 13:12:51.061: INFO: Pod "pod-projected-configmaps-5d20ada6-867f-4a90-a7f7-7e2c86ea0991": Phase="Pending", Reason="", readiness=false. Elapsed: 6.646077ms
Mar  1 13:12:53.066: INFO: Pod "pod-projected-configmaps-5d20ada6-867f-4a90-a7f7-7e2c86ea0991": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01154048s
Mar  1 13:12:55.069: INFO: Pod "pod-projected-configmaps-5d20ada6-867f-4a90-a7f7-7e2c86ea0991": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013970447s
STEP: Saw pod success 03/01/23 13:12:55.069
Mar  1 13:12:55.069: INFO: Pod "pod-projected-configmaps-5d20ada6-867f-4a90-a7f7-7e2c86ea0991" satisfied condition "Succeeded or Failed"
Mar  1 13:12:55.073: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-projected-configmaps-5d20ada6-867f-4a90-a7f7-7e2c86ea0991 container agnhost-container: <nil>
STEP: delete the pod 03/01/23 13:12:55.083
Mar  1 13:12:55.099: INFO: Waiting for pod pod-projected-configmaps-5d20ada6-867f-4a90-a7f7-7e2c86ea0991 to disappear
Mar  1 13:12:55.103: INFO: Pod pod-projected-configmaps-5d20ada6-867f-4a90-a7f7-7e2c86ea0991 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Mar  1 13:12:55.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8861" for this suite. 03/01/23 13:12:55.11
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":325,"skipped":5929,"failed":0}
------------------------------
â€¢ [4.120 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:12:51.001
    Mar  1 13:12:51.001: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename projected 03/01/23 13:12:51.001
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:12:51.026
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:12:51.031
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:88
    STEP: Creating configMap with name projected-configmap-test-volume-map-44fea070-0869-4aaa-98d6-8eba91584d59 03/01/23 13:12:51.034
    STEP: Creating a pod to test consume configMaps 03/01/23 13:12:51.04
    Mar  1 13:12:51.055: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5d20ada6-867f-4a90-a7f7-7e2c86ea0991" in namespace "projected-8861" to be "Succeeded or Failed"
    Mar  1 13:12:51.061: INFO: Pod "pod-projected-configmaps-5d20ada6-867f-4a90-a7f7-7e2c86ea0991": Phase="Pending", Reason="", readiness=false. Elapsed: 6.646077ms
    Mar  1 13:12:53.066: INFO: Pod "pod-projected-configmaps-5d20ada6-867f-4a90-a7f7-7e2c86ea0991": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01154048s
    Mar  1 13:12:55.069: INFO: Pod "pod-projected-configmaps-5d20ada6-867f-4a90-a7f7-7e2c86ea0991": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013970447s
    STEP: Saw pod success 03/01/23 13:12:55.069
    Mar  1 13:12:55.069: INFO: Pod "pod-projected-configmaps-5d20ada6-867f-4a90-a7f7-7e2c86ea0991" satisfied condition "Succeeded or Failed"
    Mar  1 13:12:55.073: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-projected-configmaps-5d20ada6-867f-4a90-a7f7-7e2c86ea0991 container agnhost-container: <nil>
    STEP: delete the pod 03/01/23 13:12:55.083
    Mar  1 13:12:55.099: INFO: Waiting for pod pod-projected-configmaps-5d20ada6-867f-4a90-a7f7-7e2c86ea0991 to disappear
    Mar  1 13:12:55.103: INFO: Pod pod-projected-configmaps-5d20ada6-867f-4a90-a7f7-7e2c86ea0991 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Mar  1 13:12:55.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8861" for this suite. 03/01/23 13:12:55.11
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:12:55.122
Mar  1 13:12:55.122: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename proxy 03/01/23 13:12:55.123
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:12:55.158
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:12:55.161
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 03/01/23 13:12:55.185
STEP: creating replication controller proxy-service-xw2dk in namespace proxy-6998 03/01/23 13:12:55.185
I0301 13:12:55.196070      19 runners.go:193] Created replication controller with name: proxy-service-xw2dk, namespace: proxy-6998, replica count: 1
I0301 13:12:56.249351      19 runners.go:193] proxy-service-xw2dk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0301 13:12:57.250039      19 runners.go:193] proxy-service-xw2dk Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0301 13:12:58.250252      19 runners.go:193] proxy-service-xw2dk Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  1 13:12:58.254: INFO: setup took 3.088281008s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 03/01/23 13:12:58.254
Mar  1 13:12:58.264: INFO: (0) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">... (200; 8.862236ms)
Mar  1 13:12:58.266: INFO: (0) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 10.891113ms)
Mar  1 13:12:58.266: INFO: (0) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">test<... (200; 11.368587ms)
Mar  1 13:12:58.270: INFO: (0) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname1/proxy/: foo (200; 15.135193ms)
Mar  1 13:12:58.270: INFO: (0) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 14.889573ms)
Mar  1 13:12:58.270: INFO: (0) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/rewriteme">test</a> (200; 15.247992ms)
Mar  1 13:12:58.270: INFO: (0) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 15.468797ms)
Mar  1 13:12:58.270: INFO: (0) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname2/proxy/: bar (200; 15.798219ms)
Mar  1 13:12:58.271: INFO: (0) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 15.59624ms)
Mar  1 13:12:58.271: INFO: (0) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname1/proxy/: foo (200; 15.722488ms)
Mar  1 13:12:58.271: INFO: (0) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname2/proxy/: bar (200; 15.714862ms)
Mar  1 13:12:58.273: INFO: (0) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/tlsrewritem... (200; 17.779619ms)
Mar  1 13:12:58.273: INFO: (0) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname2/proxy/: tls qux (200; 18.33245ms)
Mar  1 13:12:58.273: INFO: (0) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:462/proxy/: tls qux (200; 18.39353ms)
Mar  1 13:12:58.273: INFO: (0) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname1/proxy/: tls baz (200; 18.575425ms)
Mar  1 13:12:58.273: INFO: (0) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:460/proxy/: tls baz (200; 18.483209ms)
Mar  1 13:12:58.284: INFO: (1) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname1/proxy/: foo (200; 10.071065ms)
Mar  1 13:12:58.284: INFO: (1) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:462/proxy/: tls qux (200; 9.952932ms)
Mar  1 13:12:58.284: INFO: (1) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 10.394353ms)
Mar  1 13:12:58.284: INFO: (1) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 10.440742ms)
Mar  1 13:12:58.284: INFO: (1) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">test<... (200; 10.316554ms)
Mar  1 13:12:58.284: INFO: (1) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 10.849088ms)
Mar  1 13:12:58.285: INFO: (1) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">... (200; 10.955571ms)
Mar  1 13:12:58.286: INFO: (1) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/tlsrewritem... (200; 11.878317ms)
Mar  1 13:12:58.286: INFO: (1) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/rewriteme">test</a> (200; 12.578095ms)
Mar  1 13:12:58.293: INFO: (1) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname1/proxy/: tls baz (200; 19.216284ms)
Mar  1 13:12:58.294: INFO: (1) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname2/proxy/: bar (200; 20.357267ms)
Mar  1 13:12:58.294: INFO: (1) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:460/proxy/: tls baz (200; 19.907466ms)
Mar  1 13:12:58.294: INFO: (1) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname2/proxy/: bar (200; 20.228193ms)
Mar  1 13:12:58.294: INFO: (1) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname2/proxy/: tls qux (200; 20.802019ms)
Mar  1 13:12:58.294: INFO: (1) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 20.440108ms)
Mar  1 13:12:58.295: INFO: (1) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname1/proxy/: foo (200; 20.615935ms)
Mar  1 13:12:58.302: INFO: (2) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">test<... (200; 7.502113ms)
Mar  1 13:12:58.303: INFO: (2) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 7.762718ms)
Mar  1 13:12:58.304: INFO: (2) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname2/proxy/: bar (200; 8.991691ms)
Mar  1 13:12:58.304: INFO: (2) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 9.109522ms)
Mar  1 13:12:58.304: INFO: (2) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 9.610589ms)
Mar  1 13:12:58.305: INFO: (2) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:462/proxy/: tls qux (200; 9.904631ms)
Mar  1 13:12:58.305: INFO: (2) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:460/proxy/: tls baz (200; 10.15064ms)
Mar  1 13:12:58.305: INFO: (2) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname1/proxy/: foo (200; 10.239447ms)
Mar  1 13:12:58.305: INFO: (2) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname1/proxy/: tls baz (200; 10.832599ms)
Mar  1 13:12:58.307: INFO: (2) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname2/proxy/: bar (200; 12.022996ms)
Mar  1 13:12:58.309: INFO: (2) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 14.306065ms)
Mar  1 13:12:58.309: INFO: (2) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/rewriteme">test</a> (200; 14.153931ms)
Mar  1 13:12:58.310: INFO: (2) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/tlsrewritem... (200; 14.718793ms)
Mar  1 13:12:58.311: INFO: (2) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">... (200; 15.749909ms)
Mar  1 13:12:58.311: INFO: (2) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname1/proxy/: foo (200; 16.18578ms)
Mar  1 13:12:58.313: INFO: (2) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname2/proxy/: tls qux (200; 17.945042ms)
Mar  1 13:12:58.319: INFO: (3) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">... (200; 6.365232ms)
Mar  1 13:12:58.321: INFO: (3) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:462/proxy/: tls qux (200; 7.480705ms)
Mar  1 13:12:58.321: INFO: (3) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 7.753993ms)
Mar  1 13:12:58.321: INFO: (3) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:460/proxy/: tls baz (200; 8.355006ms)
Mar  1 13:12:58.322: INFO: (3) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/rewriteme">test</a> (200; 8.953337ms)
Mar  1 13:12:58.322: INFO: (3) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">test<... (200; 9.308513ms)
Mar  1 13:12:58.323: INFO: (3) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/tlsrewritem... (200; 9.448461ms)
Mar  1 13:12:58.324: INFO: (3) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname1/proxy/: foo (200; 10.577511ms)
Mar  1 13:12:58.324: INFO: (3) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname2/proxy/: tls qux (200; 10.929674ms)
Mar  1 13:12:58.325: INFO: (3) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname1/proxy/: foo (200; 11.6343ms)
Mar  1 13:12:58.326: INFO: (3) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 13.346555ms)
Mar  1 13:12:58.327: INFO: (3) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 13.75269ms)
Mar  1 13:12:58.327: INFO: (3) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 13.792934ms)
Mar  1 13:12:58.329: INFO: (3) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname2/proxy/: bar (200; 15.328055ms)
Mar  1 13:12:58.330: INFO: (3) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname1/proxy/: tls baz (200; 17.329829ms)
Mar  1 13:12:58.332: INFO: (3) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname2/proxy/: bar (200; 18.886305ms)
Mar  1 13:12:58.339: INFO: (4) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/tlsrewritem... (200; 6.700406ms)
Mar  1 13:12:58.340: INFO: (4) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 7.552634ms)
Mar  1 13:12:58.341: INFO: (4) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">test<... (200; 8.552381ms)
Mar  1 13:12:58.341: INFO: (4) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:460/proxy/: tls baz (200; 8.489446ms)
Mar  1 13:12:58.341: INFO: (4) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:462/proxy/: tls qux (200; 8.486471ms)
Mar  1 13:12:58.341: INFO: (4) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/rewriteme">test</a> (200; 9.119426ms)
Mar  1 13:12:58.341: INFO: (4) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 9.102728ms)
Mar  1 13:12:58.343: INFO: (4) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 10.7513ms)
Mar  1 13:12:58.344: INFO: (4) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">... (200; 12.044739ms)
Mar  1 13:12:58.345: INFO: (4) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname1/proxy/: foo (200; 12.406425ms)
Mar  1 13:12:58.345: INFO: (4) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 12.475209ms)
Mar  1 13:12:58.345: INFO: (4) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname1/proxy/: tls baz (200; 12.827303ms)
Mar  1 13:12:58.345: INFO: (4) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname2/proxy/: tls qux (200; 12.521821ms)
Mar  1 13:12:58.345: INFO: (4) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname2/proxy/: bar (200; 12.707702ms)
Mar  1 13:12:58.347: INFO: (4) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname1/proxy/: foo (200; 14.36038ms)
Mar  1 13:12:58.347: INFO: (4) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname2/proxy/: bar (200; 15.076185ms)
Mar  1 13:12:58.353: INFO: (5) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/rewriteme">test</a> (200; 5.552814ms)
Mar  1 13:12:58.355: INFO: (5) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 7.008007ms)
Mar  1 13:12:58.356: INFO: (5) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:462/proxy/: tls qux (200; 7.836777ms)
Mar  1 13:12:58.358: INFO: (5) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname1/proxy/: foo (200; 10.957139ms)
Mar  1 13:12:58.358: INFO: (5) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 10.918851ms)
Mar  1 13:12:58.358: INFO: (5) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 10.792096ms)
Mar  1 13:12:58.359: INFO: (5) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">test<... (200; 10.735857ms)
Mar  1 13:12:58.359: INFO: (5) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname1/proxy/: foo (200; 11.655728ms)
Mar  1 13:12:58.359: INFO: (5) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 11.550334ms)
Mar  1 13:12:58.360: INFO: (5) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname2/proxy/: bar (200; 12.716217ms)
Mar  1 13:12:58.361: INFO: (5) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">... (200; 12.991312ms)
Mar  1 13:12:58.361: INFO: (5) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:460/proxy/: tls baz (200; 13.0572ms)
Mar  1 13:12:58.361: INFO: (5) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/tlsrewritem... (200; 13.482916ms)
Mar  1 13:12:58.361: INFO: (5) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname2/proxy/: bar (200; 13.061477ms)
Mar  1 13:12:58.362: INFO: (5) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname2/proxy/: tls qux (200; 14.619279ms)
Mar  1 13:12:58.363: INFO: (5) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname1/proxy/: tls baz (200; 15.536445ms)
Mar  1 13:12:58.369: INFO: (6) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:460/proxy/: tls baz (200; 5.494114ms)
Mar  1 13:12:58.372: INFO: (6) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:462/proxy/: tls qux (200; 8.956846ms)
Mar  1 13:12:58.374: INFO: (6) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 9.7588ms)
Mar  1 13:12:58.374: INFO: (6) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname1/proxy/: tls baz (200; 10.246158ms)
Mar  1 13:12:58.374: INFO: (6) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 10.088805ms)
Mar  1 13:12:58.375: INFO: (6) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/tlsrewritem... (200; 11.480166ms)
Mar  1 13:12:58.375: INFO: (6) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/rewriteme">test</a> (200; 11.311322ms)
Mar  1 13:12:58.377: INFO: (6) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">test<... (200; 13.250981ms)
Mar  1 13:12:58.377: INFO: (6) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname1/proxy/: foo (200; 13.284132ms)
Mar  1 13:12:58.377: INFO: (6) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname2/proxy/: bar (200; 13.581646ms)
Mar  1 13:12:58.378: INFO: (6) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 14.20924ms)
Mar  1 13:12:58.378: INFO: (6) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">... (200; 14.240971ms)
Mar  1 13:12:58.378: INFO: (6) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 14.286017ms)
Mar  1 13:12:58.378: INFO: (6) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname1/proxy/: foo (200; 14.564903ms)
Mar  1 13:12:58.379: INFO: (6) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname2/proxy/: bar (200; 14.74832ms)
Mar  1 13:12:58.379: INFO: (6) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname2/proxy/: tls qux (200; 14.928535ms)
Mar  1 13:12:58.384: INFO: (7) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 5.000457ms)
Mar  1 13:12:58.384: INFO: (7) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 5.470706ms)
Mar  1 13:12:58.385: INFO: (7) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:460/proxy/: tls baz (200; 5.686443ms)
Mar  1 13:12:58.387: INFO: (7) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:462/proxy/: tls qux (200; 7.61331ms)
Mar  1 13:12:58.388: INFO: (7) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 8.527369ms)
Mar  1 13:12:58.388: INFO: (7) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">test<... (200; 8.879015ms)
Mar  1 13:12:58.388: INFO: (7) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 8.804997ms)
Mar  1 13:12:58.388: INFO: (7) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">... (200; 8.877344ms)
Mar  1 13:12:58.388: INFO: (7) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/rewriteme">test</a> (200; 9.028761ms)
Mar  1 13:12:58.389: INFO: (7) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/tlsrewritem... (200; 9.810696ms)
Mar  1 13:12:58.390: INFO: (7) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname1/proxy/: tls baz (200; 10.91879ms)
Mar  1 13:12:58.390: INFO: (7) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname2/proxy/: bar (200; 10.807879ms)
Mar  1 13:12:58.391: INFO: (7) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname2/proxy/: bar (200; 11.509415ms)
Mar  1 13:12:58.391: INFO: (7) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname1/proxy/: foo (200; 12.224619ms)
Mar  1 13:12:58.392: INFO: (7) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname2/proxy/: tls qux (200; 12.6956ms)
Mar  1 13:12:58.392: INFO: (7) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname1/proxy/: foo (200; 13.285476ms)
Mar  1 13:12:58.399: INFO: (8) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">... (200; 6.456829ms)
Mar  1 13:12:58.399: INFO: (8) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">test<... (200; 6.548422ms)
Mar  1 13:12:58.400: INFO: (8) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/tlsrewritem... (200; 7.798494ms)
Mar  1 13:12:58.402: INFO: (8) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 9.337544ms)
Mar  1 13:12:58.402: INFO: (8) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 9.789151ms)
Mar  1 13:12:58.403: INFO: (8) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 10.133358ms)
Mar  1 13:12:58.404: INFO: (8) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:460/proxy/: tls baz (200; 11.035772ms)
Mar  1 13:12:58.404: INFO: (8) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname1/proxy/: tls baz (200; 11.60713ms)
Mar  1 13:12:58.404: INFO: (8) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 11.630521ms)
Mar  1 13:12:58.404: INFO: (8) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname2/proxy/: bar (200; 11.94508ms)
Mar  1 13:12:58.405: INFO: (8) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:462/proxy/: tls qux (200; 12.328738ms)
Mar  1 13:12:58.413: INFO: (8) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname1/proxy/: foo (200; 20.209265ms)
Mar  1 13:12:58.413: INFO: (8) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/rewriteme">test</a> (200; 20.167492ms)
Mar  1 13:12:58.418: INFO: (8) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname1/proxy/: foo (200; 25.087059ms)
Mar  1 13:12:58.418: INFO: (8) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname2/proxy/: tls qux (200; 25.25604ms)
Mar  1 13:12:58.419: INFO: (8) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname2/proxy/: bar (200; 26.706755ms)
Mar  1 13:12:58.446: INFO: (9) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">test<... (200; 26.31228ms)
Mar  1 13:12:58.446: INFO: (9) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/rewriteme">test</a> (200; 26.459951ms)
Mar  1 13:12:58.446: INFO: (9) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 26.887266ms)
Mar  1 13:12:58.452: INFO: (9) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 32.085332ms)
Mar  1 13:12:58.452: INFO: (9) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:462/proxy/: tls qux (200; 32.363664ms)
Mar  1 13:12:58.453: INFO: (9) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">... (200; 33.020891ms)
Mar  1 13:12:58.458: INFO: (9) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname1/proxy/: foo (200; 38.377622ms)
Mar  1 13:12:58.459: INFO: (9) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 39.042527ms)
Mar  1 13:12:58.459: INFO: (9) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname2/proxy/: bar (200; 39.113477ms)
Mar  1 13:12:58.459: INFO: (9) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 39.282727ms)
Mar  1 13:12:58.459: INFO: (9) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname1/proxy/: foo (200; 39.364366ms)
Mar  1 13:12:58.460: INFO: (9) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/tlsrewritem... (200; 40.598735ms)
Mar  1 13:12:58.460: INFO: (9) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname1/proxy/: tls baz (200; 40.859837ms)
Mar  1 13:12:58.462: INFO: (9) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname2/proxy/: tls qux (200; 42.153988ms)
Mar  1 13:12:58.462: INFO: (9) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:460/proxy/: tls baz (200; 42.227992ms)
Mar  1 13:12:58.462: INFO: (9) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname2/proxy/: bar (200; 42.433004ms)
Mar  1 13:12:58.477: INFO: (10) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">test<... (200; 15.390445ms)
Mar  1 13:12:58.487: INFO: (10) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 24.979131ms)
Mar  1 13:12:58.487: INFO: (10) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/rewriteme">test</a> (200; 25.123941ms)
Mar  1 13:12:58.490: INFO: (10) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 27.932004ms)
Mar  1 13:12:58.492: INFO: (10) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname2/proxy/: bar (200; 30.297072ms)
Mar  1 13:12:58.494: INFO: (10) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/tlsrewritem... (200; 31.862162ms)
Mar  1 13:12:58.494: INFO: (10) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 31.864147ms)
Mar  1 13:12:58.496: INFO: (10) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:462/proxy/: tls qux (200; 33.877529ms)
Mar  1 13:12:58.496: INFO: (10) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:460/proxy/: tls baz (200; 33.840584ms)
Mar  1 13:12:58.499: INFO: (10) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">... (200; 36.606454ms)
Mar  1 13:12:58.501: INFO: (10) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 38.881017ms)
Mar  1 13:12:58.503: INFO: (10) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname1/proxy/: foo (200; 40.944537ms)
Mar  1 13:12:58.503: INFO: (10) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname2/proxy/: bar (200; 41.049228ms)
Mar  1 13:12:58.503: INFO: (10) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname1/proxy/: foo (200; 41.051402ms)
Mar  1 13:12:58.503: INFO: (10) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname1/proxy/: tls baz (200; 41.49589ms)
Mar  1 13:12:58.505: INFO: (10) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname2/proxy/: tls qux (200; 42.856456ms)
Mar  1 13:12:58.512: INFO: (11) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 7.104804ms)
Mar  1 13:12:58.513: INFO: (11) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/rewriteme">test</a> (200; 7.662154ms)
Mar  1 13:12:58.516: INFO: (11) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:460/proxy/: tls baz (200; 10.921767ms)
Mar  1 13:12:58.516: INFO: (11) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:462/proxy/: tls qux (200; 10.911925ms)
Mar  1 13:12:58.516: INFO: (11) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">... (200; 10.869675ms)
Mar  1 13:12:58.518: INFO: (11) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 12.80996ms)
Mar  1 13:12:58.520: INFO: (11) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname2/proxy/: bar (200; 14.502633ms)
Mar  1 13:12:58.520: INFO: (11) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 14.650368ms)
Mar  1 13:12:58.521: INFO: (11) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/tlsrewritem... (200; 15.107951ms)
Mar  1 13:12:58.521: INFO: (11) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname1/proxy/: foo (200; 15.883494ms)
Mar  1 13:12:58.522: INFO: (11) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">test<... (200; 15.924294ms)
Mar  1 13:12:58.522: INFO: (11) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 16.199546ms)
Mar  1 13:12:58.522: INFO: (11) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname1/proxy/: tls baz (200; 16.106457ms)
Mar  1 13:12:58.522: INFO: (11) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname1/proxy/: foo (200; 16.351052ms)
Mar  1 13:12:58.522: INFO: (11) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname2/proxy/: bar (200; 16.861919ms)
Mar  1 13:12:58.523: INFO: (11) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname2/proxy/: tls qux (200; 16.977593ms)
Mar  1 13:12:58.529: INFO: (12) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">test<... (200; 6.023411ms)
Mar  1 13:12:58.530: INFO: (12) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 6.733511ms)
Mar  1 13:12:58.530: INFO: (12) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">... (200; 7.130001ms)
Mar  1 13:12:58.531: INFO: (12) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 7.500633ms)
Mar  1 13:12:58.531: INFO: (12) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/tlsrewritem... (200; 7.733059ms)
Mar  1 13:12:58.533: INFO: (12) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:462/proxy/: tls qux (200; 9.432992ms)
Mar  1 13:12:58.533: INFO: (12) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:460/proxy/: tls baz (200; 9.653953ms)
Mar  1 13:12:58.533: INFO: (12) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname2/proxy/: tls qux (200; 9.917898ms)
Mar  1 13:12:58.533: INFO: (12) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/rewriteme">test</a> (200; 10.232809ms)
Mar  1 13:12:58.534: INFO: (12) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname2/proxy/: bar (200; 10.917464ms)
Mar  1 13:12:58.534: INFO: (12) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 11.006379ms)
Mar  1 13:12:58.534: INFO: (12) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 11.120518ms)
Mar  1 13:12:58.535: INFO: (12) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname1/proxy/: tls baz (200; 11.668039ms)
Mar  1 13:12:58.535: INFO: (12) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname1/proxy/: foo (200; 11.60115ms)
Mar  1 13:12:58.536: INFO: (12) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname2/proxy/: bar (200; 13.235994ms)
Mar  1 13:12:58.537: INFO: (12) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname1/proxy/: foo (200; 13.390949ms)
Mar  1 13:12:58.544: INFO: (13) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 5.326129ms)
Mar  1 13:12:58.546: INFO: (13) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:462/proxy/: tls qux (200; 7.316619ms)
Mar  1 13:12:58.546: INFO: (13) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">... (200; 7.485657ms)
Mar  1 13:12:58.547: INFO: (13) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 7.927658ms)
Mar  1 13:12:58.547: INFO: (13) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:460/proxy/: tls baz (200; 8.570846ms)
Mar  1 13:12:58.547: INFO: (13) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 8.627054ms)
Mar  1 13:12:58.548: INFO: (13) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">test<... (200; 9.588353ms)
Mar  1 13:12:58.549: INFO: (13) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/rewriteme">test</a> (200; 9.805459ms)
Mar  1 13:12:58.549: INFO: (13) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname1/proxy/: foo (200; 10.642795ms)
Mar  1 13:12:58.549: INFO: (13) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/tlsrewritem... (200; 10.262504ms)
Mar  1 13:12:58.550: INFO: (13) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 10.826248ms)
Mar  1 13:12:58.550: INFO: (13) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname1/proxy/: tls baz (200; 10.906795ms)
Mar  1 13:12:58.550: INFO: (13) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname2/proxy/: bar (200; 11.668267ms)
Mar  1 13:12:58.551: INFO: (13) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname2/proxy/: tls qux (200; 11.595791ms)
Mar  1 13:12:58.551: INFO: (13) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname2/proxy/: bar (200; 11.955705ms)
Mar  1 13:12:58.552: INFO: (13) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname1/proxy/: foo (200; 13.416989ms)
Mar  1 13:12:58.562: INFO: (14) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/tlsrewritem... (200; 9.898458ms)
Mar  1 13:12:58.562: INFO: (14) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:462/proxy/: tls qux (200; 9.653962ms)
Mar  1 13:12:58.562: INFO: (14) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 9.727902ms)
Mar  1 13:12:58.562: INFO: (14) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">... (200; 9.619049ms)
Mar  1 13:12:58.563: INFO: (14) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">test<... (200; 9.561512ms)
Mar  1 13:12:58.563: INFO: (14) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 9.820636ms)
Mar  1 13:12:58.563: INFO: (14) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:460/proxy/: tls baz (200; 9.805518ms)
Mar  1 13:12:58.563: INFO: (14) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 9.885731ms)
Mar  1 13:12:58.563: INFO: (14) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/rewriteme">test</a> (200; 10.133928ms)
Mar  1 13:12:58.568: INFO: (14) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname1/proxy/: foo (200; 15.126139ms)
Mar  1 13:12:58.568: INFO: (14) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 14.983542ms)
Mar  1 13:12:58.568: INFO: (14) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname2/proxy/: tls qux (200; 15.491181ms)
Mar  1 13:12:58.568: INFO: (14) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname1/proxy/: foo (200; 15.417497ms)
Mar  1 13:12:58.568: INFO: (14) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname1/proxy/: tls baz (200; 15.430127ms)
Mar  1 13:12:58.568: INFO: (14) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname2/proxy/: bar (200; 15.461684ms)
Mar  1 13:12:58.569: INFO: (14) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname2/proxy/: bar (200; 15.665986ms)
Mar  1 13:12:58.574: INFO: (15) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">test<... (200; 5.785349ms)
Mar  1 13:12:58.576: INFO: (15) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 6.954672ms)
Mar  1 13:12:58.577: INFO: (15) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 7.553335ms)
Mar  1 13:12:58.578: INFO: (15) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 9.298317ms)
Mar  1 13:12:58.579: INFO: (15) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/tlsrewritem... (200; 9.826849ms)
Mar  1 13:12:58.579: INFO: (15) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 9.819745ms)
Mar  1 13:12:58.579: INFO: (15) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:462/proxy/: tls qux (200; 9.933504ms)
Mar  1 13:12:58.579: INFO: (15) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">... (200; 10.15942ms)
Mar  1 13:12:58.579: INFO: (15) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname2/proxy/: bar (200; 10.696493ms)
Mar  1 13:12:58.579: INFO: (15) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:460/proxy/: tls baz (200; 10.253565ms)
Mar  1 13:12:58.580: INFO: (15) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/rewriteme">test</a> (200; 10.706483ms)
Mar  1 13:12:58.582: INFO: (15) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname1/proxy/: foo (200; 12.533559ms)
Mar  1 13:12:58.582: INFO: (15) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname2/proxy/: bar (200; 12.679816ms)
Mar  1 13:12:58.583: INFO: (15) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname1/proxy/: tls baz (200; 13.512186ms)
Mar  1 13:12:58.583: INFO: (15) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname2/proxy/: tls qux (200; 14.010458ms)
Mar  1 13:12:58.583: INFO: (15) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname1/proxy/: foo (200; 14.330159ms)
Mar  1 13:12:58.589: INFO: (16) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 5.929166ms)
Mar  1 13:12:58.590: INFO: (16) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:460/proxy/: tls baz (200; 6.624022ms)
Mar  1 13:12:58.591: INFO: (16) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 7.944783ms)
Mar  1 13:12:58.593: INFO: (16) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/rewriteme">test</a> (200; 9.031528ms)
Mar  1 13:12:58.596: INFO: (16) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 12.160449ms)
Mar  1 13:12:58.596: INFO: (16) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">... (200; 12.330457ms)
Mar  1 13:12:58.597: INFO: (16) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname2/proxy/: bar (200; 13.150025ms)
Mar  1 13:12:58.597: INFO: (16) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:462/proxy/: tls qux (200; 13.204672ms)
Mar  1 13:12:58.597: INFO: (16) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">test<... (200; 13.170611ms)
Mar  1 13:12:58.597: INFO: (16) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname1/proxy/: foo (200; 13.278974ms)
Mar  1 13:12:58.597: INFO: (16) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname2/proxy/: tls qux (200; 13.668557ms)
Mar  1 13:12:58.597: INFO: (16) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 13.62733ms)
Mar  1 13:12:58.597: INFO: (16) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname2/proxy/: bar (200; 13.858468ms)
Mar  1 13:12:58.598: INFO: (16) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/tlsrewritem... (200; 14.030461ms)
Mar  1 13:12:58.598: INFO: (16) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname1/proxy/: foo (200; 14.03993ms)
Mar  1 13:12:58.599: INFO: (16) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname1/proxy/: tls baz (200; 15.924745ms)
Mar  1 13:12:58.605: INFO: (17) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:460/proxy/: tls baz (200; 5.210355ms)
Mar  1 13:12:58.605: INFO: (17) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 5.51741ms)
Mar  1 13:12:58.608: INFO: (17) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/tlsrewritem... (200; 7.910746ms)
Mar  1 13:12:58.608: INFO: (17) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 8.292633ms)
Mar  1 13:12:58.608: INFO: (17) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 8.312991ms)
Mar  1 13:12:58.609: INFO: (17) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">test<... (200; 9.216803ms)
Mar  1 13:12:58.609: INFO: (17) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 9.48487ms)
Mar  1 13:12:58.609: INFO: (17) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/rewriteme">test</a> (200; 9.713965ms)
Mar  1 13:12:58.610: INFO: (17) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">... (200; 9.89657ms)
Mar  1 13:12:58.610: INFO: (17) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname1/proxy/: foo (200; 10.400838ms)
Mar  1 13:12:58.610: INFO: (17) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname1/proxy/: tls baz (200; 10.476023ms)
Mar  1 13:12:58.610: INFO: (17) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname2/proxy/: bar (200; 10.43913ms)
Mar  1 13:12:58.611: INFO: (17) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:462/proxy/: tls qux (200; 11.126309ms)
Mar  1 13:12:58.612: INFO: (17) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname2/proxy/: bar (200; 12.331608ms)
Mar  1 13:12:58.612: INFO: (17) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname1/proxy/: foo (200; 12.652323ms)
Mar  1 13:12:58.613: INFO: (17) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname2/proxy/: tls qux (200; 12.887824ms)
Mar  1 13:12:58.618: INFO: (18) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">test<... (200; 5.298675ms)
Mar  1 13:12:58.620: INFO: (18) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 6.987285ms)
Mar  1 13:12:58.620: INFO: (18) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 7.254947ms)
Mar  1 13:12:58.620: INFO: (18) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/rewriteme">test</a> (200; 7.542689ms)
Mar  1 13:12:58.622: INFO: (18) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:462/proxy/: tls qux (200; 8.746745ms)
Mar  1 13:12:58.622: INFO: (18) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:460/proxy/: tls baz (200; 8.732189ms)
Mar  1 13:12:58.623: INFO: (18) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 9.625898ms)
Mar  1 13:12:58.623: INFO: (18) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 9.901599ms)
Mar  1 13:12:58.623: INFO: (18) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">... (200; 10.133877ms)
Mar  1 13:12:58.623: INFO: (18) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/tlsrewritem... (200; 10.66881ms)
Mar  1 13:12:58.624: INFO: (18) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname1/proxy/: tls baz (200; 11.147516ms)
Mar  1 13:12:58.625: INFO: (18) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname2/proxy/: tls qux (200; 11.640961ms)
Mar  1 13:12:58.625: INFO: (18) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname1/proxy/: foo (200; 11.866805ms)
Mar  1 13:12:58.625: INFO: (18) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname2/proxy/: bar (200; 11.762708ms)
Mar  1 13:12:58.626: INFO: (18) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname2/proxy/: bar (200; 12.86988ms)
Mar  1 13:12:58.626: INFO: (18) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname1/proxy/: foo (200; 12.77059ms)
Mar  1 13:12:58.631: INFO: (19) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">test<... (200; 5.287154ms)
Mar  1 13:12:58.631: INFO: (19) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/rewriteme">test</a> (200; 5.349706ms)
Mar  1 13:12:58.632: INFO: (19) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 6.024501ms)
Mar  1 13:12:58.633: INFO: (19) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:462/proxy/: tls qux (200; 7.121419ms)
Mar  1 13:12:58.634: INFO: (19) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">... (200; 7.754885ms)
Mar  1 13:12:58.634: INFO: (19) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/tlsrewritem... (200; 8.086002ms)
Mar  1 13:12:58.635: INFO: (19) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 9.374103ms)
Mar  1 13:12:58.636: INFO: (19) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 10.106254ms)
Mar  1 13:12:58.636: INFO: (19) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 10.192009ms)
Mar  1 13:12:58.636: INFO: (19) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname1/proxy/: tls baz (200; 10.60299ms)
Mar  1 13:12:58.637: INFO: (19) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:460/proxy/: tls baz (200; 11.198697ms)
Mar  1 13:12:58.637: INFO: (19) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname1/proxy/: foo (200; 11.716469ms)
Mar  1 13:12:58.638: INFO: (19) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname2/proxy/: bar (200; 11.824513ms)
Mar  1 13:12:58.638: INFO: (19) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname2/proxy/: tls qux (200; 11.981838ms)
Mar  1 13:12:58.638: INFO: (19) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname1/proxy/: foo (200; 12.182632ms)
Mar  1 13:12:58.639: INFO: (19) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname2/proxy/: bar (200; 13.470024ms)
STEP: deleting ReplicationController proxy-service-xw2dk in namespace proxy-6998, will wait for the garbage collector to delete the pods 03/01/23 13:12:58.64
Mar  1 13:12:58.704: INFO: Deleting ReplicationController proxy-service-xw2dk took: 9.182327ms
Mar  1 13:12:58.805: INFO: Terminating ReplicationController proxy-service-xw2dk pods took: 101.036987ms
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Mar  1 13:13:01.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-6998" for this suite. 03/01/23 13:13:01.614
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","completed":326,"skipped":5951,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.502 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:12:55.122
    Mar  1 13:12:55.122: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename proxy 03/01/23 13:12:55.123
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:12:55.158
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:12:55.161
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 03/01/23 13:12:55.185
    STEP: creating replication controller proxy-service-xw2dk in namespace proxy-6998 03/01/23 13:12:55.185
    I0301 13:12:55.196070      19 runners.go:193] Created replication controller with name: proxy-service-xw2dk, namespace: proxy-6998, replica count: 1
    I0301 13:12:56.249351      19 runners.go:193] proxy-service-xw2dk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0301 13:12:57.250039      19 runners.go:193] proxy-service-xw2dk Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
    I0301 13:12:58.250252      19 runners.go:193] proxy-service-xw2dk Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar  1 13:12:58.254: INFO: setup took 3.088281008s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 03/01/23 13:12:58.254
    Mar  1 13:12:58.264: INFO: (0) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">... (200; 8.862236ms)
    Mar  1 13:12:58.266: INFO: (0) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 10.891113ms)
    Mar  1 13:12:58.266: INFO: (0) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">test<... (200; 11.368587ms)
    Mar  1 13:12:58.270: INFO: (0) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname1/proxy/: foo (200; 15.135193ms)
    Mar  1 13:12:58.270: INFO: (0) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 14.889573ms)
    Mar  1 13:12:58.270: INFO: (0) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/rewriteme">test</a> (200; 15.247992ms)
    Mar  1 13:12:58.270: INFO: (0) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 15.468797ms)
    Mar  1 13:12:58.270: INFO: (0) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname2/proxy/: bar (200; 15.798219ms)
    Mar  1 13:12:58.271: INFO: (0) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 15.59624ms)
    Mar  1 13:12:58.271: INFO: (0) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname1/proxy/: foo (200; 15.722488ms)
    Mar  1 13:12:58.271: INFO: (0) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname2/proxy/: bar (200; 15.714862ms)
    Mar  1 13:12:58.273: INFO: (0) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/tlsrewritem... (200; 17.779619ms)
    Mar  1 13:12:58.273: INFO: (0) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname2/proxy/: tls qux (200; 18.33245ms)
    Mar  1 13:12:58.273: INFO: (0) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:462/proxy/: tls qux (200; 18.39353ms)
    Mar  1 13:12:58.273: INFO: (0) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname1/proxy/: tls baz (200; 18.575425ms)
    Mar  1 13:12:58.273: INFO: (0) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:460/proxy/: tls baz (200; 18.483209ms)
    Mar  1 13:12:58.284: INFO: (1) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname1/proxy/: foo (200; 10.071065ms)
    Mar  1 13:12:58.284: INFO: (1) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:462/proxy/: tls qux (200; 9.952932ms)
    Mar  1 13:12:58.284: INFO: (1) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 10.394353ms)
    Mar  1 13:12:58.284: INFO: (1) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 10.440742ms)
    Mar  1 13:12:58.284: INFO: (1) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">test<... (200; 10.316554ms)
    Mar  1 13:12:58.284: INFO: (1) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 10.849088ms)
    Mar  1 13:12:58.285: INFO: (1) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">... (200; 10.955571ms)
    Mar  1 13:12:58.286: INFO: (1) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/tlsrewritem... (200; 11.878317ms)
    Mar  1 13:12:58.286: INFO: (1) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/rewriteme">test</a> (200; 12.578095ms)
    Mar  1 13:12:58.293: INFO: (1) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname1/proxy/: tls baz (200; 19.216284ms)
    Mar  1 13:12:58.294: INFO: (1) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname2/proxy/: bar (200; 20.357267ms)
    Mar  1 13:12:58.294: INFO: (1) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:460/proxy/: tls baz (200; 19.907466ms)
    Mar  1 13:12:58.294: INFO: (1) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname2/proxy/: bar (200; 20.228193ms)
    Mar  1 13:12:58.294: INFO: (1) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname2/proxy/: tls qux (200; 20.802019ms)
    Mar  1 13:12:58.294: INFO: (1) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 20.440108ms)
    Mar  1 13:12:58.295: INFO: (1) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname1/proxy/: foo (200; 20.615935ms)
    Mar  1 13:12:58.302: INFO: (2) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">test<... (200; 7.502113ms)
    Mar  1 13:12:58.303: INFO: (2) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 7.762718ms)
    Mar  1 13:12:58.304: INFO: (2) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname2/proxy/: bar (200; 8.991691ms)
    Mar  1 13:12:58.304: INFO: (2) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 9.109522ms)
    Mar  1 13:12:58.304: INFO: (2) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 9.610589ms)
    Mar  1 13:12:58.305: INFO: (2) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:462/proxy/: tls qux (200; 9.904631ms)
    Mar  1 13:12:58.305: INFO: (2) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:460/proxy/: tls baz (200; 10.15064ms)
    Mar  1 13:12:58.305: INFO: (2) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname1/proxy/: foo (200; 10.239447ms)
    Mar  1 13:12:58.305: INFO: (2) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname1/proxy/: tls baz (200; 10.832599ms)
    Mar  1 13:12:58.307: INFO: (2) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname2/proxy/: bar (200; 12.022996ms)
    Mar  1 13:12:58.309: INFO: (2) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 14.306065ms)
    Mar  1 13:12:58.309: INFO: (2) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/rewriteme">test</a> (200; 14.153931ms)
    Mar  1 13:12:58.310: INFO: (2) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/tlsrewritem... (200; 14.718793ms)
    Mar  1 13:12:58.311: INFO: (2) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">... (200; 15.749909ms)
    Mar  1 13:12:58.311: INFO: (2) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname1/proxy/: foo (200; 16.18578ms)
    Mar  1 13:12:58.313: INFO: (2) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname2/proxy/: tls qux (200; 17.945042ms)
    Mar  1 13:12:58.319: INFO: (3) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">... (200; 6.365232ms)
    Mar  1 13:12:58.321: INFO: (3) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:462/proxy/: tls qux (200; 7.480705ms)
    Mar  1 13:12:58.321: INFO: (3) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 7.753993ms)
    Mar  1 13:12:58.321: INFO: (3) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:460/proxy/: tls baz (200; 8.355006ms)
    Mar  1 13:12:58.322: INFO: (3) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/rewriteme">test</a> (200; 8.953337ms)
    Mar  1 13:12:58.322: INFO: (3) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">test<... (200; 9.308513ms)
    Mar  1 13:12:58.323: INFO: (3) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/tlsrewritem... (200; 9.448461ms)
    Mar  1 13:12:58.324: INFO: (3) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname1/proxy/: foo (200; 10.577511ms)
    Mar  1 13:12:58.324: INFO: (3) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname2/proxy/: tls qux (200; 10.929674ms)
    Mar  1 13:12:58.325: INFO: (3) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname1/proxy/: foo (200; 11.6343ms)
    Mar  1 13:12:58.326: INFO: (3) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 13.346555ms)
    Mar  1 13:12:58.327: INFO: (3) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 13.75269ms)
    Mar  1 13:12:58.327: INFO: (3) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 13.792934ms)
    Mar  1 13:12:58.329: INFO: (3) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname2/proxy/: bar (200; 15.328055ms)
    Mar  1 13:12:58.330: INFO: (3) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname1/proxy/: tls baz (200; 17.329829ms)
    Mar  1 13:12:58.332: INFO: (3) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname2/proxy/: bar (200; 18.886305ms)
    Mar  1 13:12:58.339: INFO: (4) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/tlsrewritem... (200; 6.700406ms)
    Mar  1 13:12:58.340: INFO: (4) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 7.552634ms)
    Mar  1 13:12:58.341: INFO: (4) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">test<... (200; 8.552381ms)
    Mar  1 13:12:58.341: INFO: (4) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:460/proxy/: tls baz (200; 8.489446ms)
    Mar  1 13:12:58.341: INFO: (4) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:462/proxy/: tls qux (200; 8.486471ms)
    Mar  1 13:12:58.341: INFO: (4) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/rewriteme">test</a> (200; 9.119426ms)
    Mar  1 13:12:58.341: INFO: (4) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 9.102728ms)
    Mar  1 13:12:58.343: INFO: (4) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 10.7513ms)
    Mar  1 13:12:58.344: INFO: (4) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">... (200; 12.044739ms)
    Mar  1 13:12:58.345: INFO: (4) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname1/proxy/: foo (200; 12.406425ms)
    Mar  1 13:12:58.345: INFO: (4) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 12.475209ms)
    Mar  1 13:12:58.345: INFO: (4) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname1/proxy/: tls baz (200; 12.827303ms)
    Mar  1 13:12:58.345: INFO: (4) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname2/proxy/: tls qux (200; 12.521821ms)
    Mar  1 13:12:58.345: INFO: (4) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname2/proxy/: bar (200; 12.707702ms)
    Mar  1 13:12:58.347: INFO: (4) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname1/proxy/: foo (200; 14.36038ms)
    Mar  1 13:12:58.347: INFO: (4) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname2/proxy/: bar (200; 15.076185ms)
    Mar  1 13:12:58.353: INFO: (5) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/rewriteme">test</a> (200; 5.552814ms)
    Mar  1 13:12:58.355: INFO: (5) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 7.008007ms)
    Mar  1 13:12:58.356: INFO: (5) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:462/proxy/: tls qux (200; 7.836777ms)
    Mar  1 13:12:58.358: INFO: (5) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname1/proxy/: foo (200; 10.957139ms)
    Mar  1 13:12:58.358: INFO: (5) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 10.918851ms)
    Mar  1 13:12:58.358: INFO: (5) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 10.792096ms)
    Mar  1 13:12:58.359: INFO: (5) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">test<... (200; 10.735857ms)
    Mar  1 13:12:58.359: INFO: (5) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname1/proxy/: foo (200; 11.655728ms)
    Mar  1 13:12:58.359: INFO: (5) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 11.550334ms)
    Mar  1 13:12:58.360: INFO: (5) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname2/proxy/: bar (200; 12.716217ms)
    Mar  1 13:12:58.361: INFO: (5) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">... (200; 12.991312ms)
    Mar  1 13:12:58.361: INFO: (5) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:460/proxy/: tls baz (200; 13.0572ms)
    Mar  1 13:12:58.361: INFO: (5) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/tlsrewritem... (200; 13.482916ms)
    Mar  1 13:12:58.361: INFO: (5) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname2/proxy/: bar (200; 13.061477ms)
    Mar  1 13:12:58.362: INFO: (5) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname2/proxy/: tls qux (200; 14.619279ms)
    Mar  1 13:12:58.363: INFO: (5) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname1/proxy/: tls baz (200; 15.536445ms)
    Mar  1 13:12:58.369: INFO: (6) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:460/proxy/: tls baz (200; 5.494114ms)
    Mar  1 13:12:58.372: INFO: (6) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:462/proxy/: tls qux (200; 8.956846ms)
    Mar  1 13:12:58.374: INFO: (6) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 9.7588ms)
    Mar  1 13:12:58.374: INFO: (6) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname1/proxy/: tls baz (200; 10.246158ms)
    Mar  1 13:12:58.374: INFO: (6) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 10.088805ms)
    Mar  1 13:12:58.375: INFO: (6) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/tlsrewritem... (200; 11.480166ms)
    Mar  1 13:12:58.375: INFO: (6) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/rewriteme">test</a> (200; 11.311322ms)
    Mar  1 13:12:58.377: INFO: (6) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">test<... (200; 13.250981ms)
    Mar  1 13:12:58.377: INFO: (6) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname1/proxy/: foo (200; 13.284132ms)
    Mar  1 13:12:58.377: INFO: (6) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname2/proxy/: bar (200; 13.581646ms)
    Mar  1 13:12:58.378: INFO: (6) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 14.20924ms)
    Mar  1 13:12:58.378: INFO: (6) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">... (200; 14.240971ms)
    Mar  1 13:12:58.378: INFO: (6) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 14.286017ms)
    Mar  1 13:12:58.378: INFO: (6) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname1/proxy/: foo (200; 14.564903ms)
    Mar  1 13:12:58.379: INFO: (6) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname2/proxy/: bar (200; 14.74832ms)
    Mar  1 13:12:58.379: INFO: (6) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname2/proxy/: tls qux (200; 14.928535ms)
    Mar  1 13:12:58.384: INFO: (7) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 5.000457ms)
    Mar  1 13:12:58.384: INFO: (7) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 5.470706ms)
    Mar  1 13:12:58.385: INFO: (7) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:460/proxy/: tls baz (200; 5.686443ms)
    Mar  1 13:12:58.387: INFO: (7) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:462/proxy/: tls qux (200; 7.61331ms)
    Mar  1 13:12:58.388: INFO: (7) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 8.527369ms)
    Mar  1 13:12:58.388: INFO: (7) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">test<... (200; 8.879015ms)
    Mar  1 13:12:58.388: INFO: (7) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 8.804997ms)
    Mar  1 13:12:58.388: INFO: (7) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">... (200; 8.877344ms)
    Mar  1 13:12:58.388: INFO: (7) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/rewriteme">test</a> (200; 9.028761ms)
    Mar  1 13:12:58.389: INFO: (7) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/tlsrewritem... (200; 9.810696ms)
    Mar  1 13:12:58.390: INFO: (7) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname1/proxy/: tls baz (200; 10.91879ms)
    Mar  1 13:12:58.390: INFO: (7) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname2/proxy/: bar (200; 10.807879ms)
    Mar  1 13:12:58.391: INFO: (7) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname2/proxy/: bar (200; 11.509415ms)
    Mar  1 13:12:58.391: INFO: (7) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname1/proxy/: foo (200; 12.224619ms)
    Mar  1 13:12:58.392: INFO: (7) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname2/proxy/: tls qux (200; 12.6956ms)
    Mar  1 13:12:58.392: INFO: (7) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname1/proxy/: foo (200; 13.285476ms)
    Mar  1 13:12:58.399: INFO: (8) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">... (200; 6.456829ms)
    Mar  1 13:12:58.399: INFO: (8) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">test<... (200; 6.548422ms)
    Mar  1 13:12:58.400: INFO: (8) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/tlsrewritem... (200; 7.798494ms)
    Mar  1 13:12:58.402: INFO: (8) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 9.337544ms)
    Mar  1 13:12:58.402: INFO: (8) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 9.789151ms)
    Mar  1 13:12:58.403: INFO: (8) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 10.133358ms)
    Mar  1 13:12:58.404: INFO: (8) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:460/proxy/: tls baz (200; 11.035772ms)
    Mar  1 13:12:58.404: INFO: (8) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname1/proxy/: tls baz (200; 11.60713ms)
    Mar  1 13:12:58.404: INFO: (8) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 11.630521ms)
    Mar  1 13:12:58.404: INFO: (8) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname2/proxy/: bar (200; 11.94508ms)
    Mar  1 13:12:58.405: INFO: (8) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:462/proxy/: tls qux (200; 12.328738ms)
    Mar  1 13:12:58.413: INFO: (8) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname1/proxy/: foo (200; 20.209265ms)
    Mar  1 13:12:58.413: INFO: (8) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/rewriteme">test</a> (200; 20.167492ms)
    Mar  1 13:12:58.418: INFO: (8) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname1/proxy/: foo (200; 25.087059ms)
    Mar  1 13:12:58.418: INFO: (8) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname2/proxy/: tls qux (200; 25.25604ms)
    Mar  1 13:12:58.419: INFO: (8) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname2/proxy/: bar (200; 26.706755ms)
    Mar  1 13:12:58.446: INFO: (9) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">test<... (200; 26.31228ms)
    Mar  1 13:12:58.446: INFO: (9) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/rewriteme">test</a> (200; 26.459951ms)
    Mar  1 13:12:58.446: INFO: (9) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 26.887266ms)
    Mar  1 13:12:58.452: INFO: (9) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 32.085332ms)
    Mar  1 13:12:58.452: INFO: (9) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:462/proxy/: tls qux (200; 32.363664ms)
    Mar  1 13:12:58.453: INFO: (9) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">... (200; 33.020891ms)
    Mar  1 13:12:58.458: INFO: (9) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname1/proxy/: foo (200; 38.377622ms)
    Mar  1 13:12:58.459: INFO: (9) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 39.042527ms)
    Mar  1 13:12:58.459: INFO: (9) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname2/proxy/: bar (200; 39.113477ms)
    Mar  1 13:12:58.459: INFO: (9) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 39.282727ms)
    Mar  1 13:12:58.459: INFO: (9) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname1/proxy/: foo (200; 39.364366ms)
    Mar  1 13:12:58.460: INFO: (9) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/tlsrewritem... (200; 40.598735ms)
    Mar  1 13:12:58.460: INFO: (9) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname1/proxy/: tls baz (200; 40.859837ms)
    Mar  1 13:12:58.462: INFO: (9) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname2/proxy/: tls qux (200; 42.153988ms)
    Mar  1 13:12:58.462: INFO: (9) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:460/proxy/: tls baz (200; 42.227992ms)
    Mar  1 13:12:58.462: INFO: (9) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname2/proxy/: bar (200; 42.433004ms)
    Mar  1 13:12:58.477: INFO: (10) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">test<... (200; 15.390445ms)
    Mar  1 13:12:58.487: INFO: (10) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 24.979131ms)
    Mar  1 13:12:58.487: INFO: (10) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/rewriteme">test</a> (200; 25.123941ms)
    Mar  1 13:12:58.490: INFO: (10) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 27.932004ms)
    Mar  1 13:12:58.492: INFO: (10) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname2/proxy/: bar (200; 30.297072ms)
    Mar  1 13:12:58.494: INFO: (10) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/tlsrewritem... (200; 31.862162ms)
    Mar  1 13:12:58.494: INFO: (10) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 31.864147ms)
    Mar  1 13:12:58.496: INFO: (10) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:462/proxy/: tls qux (200; 33.877529ms)
    Mar  1 13:12:58.496: INFO: (10) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:460/proxy/: tls baz (200; 33.840584ms)
    Mar  1 13:12:58.499: INFO: (10) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">... (200; 36.606454ms)
    Mar  1 13:12:58.501: INFO: (10) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 38.881017ms)
    Mar  1 13:12:58.503: INFO: (10) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname1/proxy/: foo (200; 40.944537ms)
    Mar  1 13:12:58.503: INFO: (10) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname2/proxy/: bar (200; 41.049228ms)
    Mar  1 13:12:58.503: INFO: (10) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname1/proxy/: foo (200; 41.051402ms)
    Mar  1 13:12:58.503: INFO: (10) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname1/proxy/: tls baz (200; 41.49589ms)
    Mar  1 13:12:58.505: INFO: (10) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname2/proxy/: tls qux (200; 42.856456ms)
    Mar  1 13:12:58.512: INFO: (11) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 7.104804ms)
    Mar  1 13:12:58.513: INFO: (11) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/rewriteme">test</a> (200; 7.662154ms)
    Mar  1 13:12:58.516: INFO: (11) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:460/proxy/: tls baz (200; 10.921767ms)
    Mar  1 13:12:58.516: INFO: (11) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:462/proxy/: tls qux (200; 10.911925ms)
    Mar  1 13:12:58.516: INFO: (11) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">... (200; 10.869675ms)
    Mar  1 13:12:58.518: INFO: (11) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 12.80996ms)
    Mar  1 13:12:58.520: INFO: (11) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname2/proxy/: bar (200; 14.502633ms)
    Mar  1 13:12:58.520: INFO: (11) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 14.650368ms)
    Mar  1 13:12:58.521: INFO: (11) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/tlsrewritem... (200; 15.107951ms)
    Mar  1 13:12:58.521: INFO: (11) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname1/proxy/: foo (200; 15.883494ms)
    Mar  1 13:12:58.522: INFO: (11) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">test<... (200; 15.924294ms)
    Mar  1 13:12:58.522: INFO: (11) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 16.199546ms)
    Mar  1 13:12:58.522: INFO: (11) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname1/proxy/: tls baz (200; 16.106457ms)
    Mar  1 13:12:58.522: INFO: (11) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname1/proxy/: foo (200; 16.351052ms)
    Mar  1 13:12:58.522: INFO: (11) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname2/proxy/: bar (200; 16.861919ms)
    Mar  1 13:12:58.523: INFO: (11) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname2/proxy/: tls qux (200; 16.977593ms)
    Mar  1 13:12:58.529: INFO: (12) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">test<... (200; 6.023411ms)
    Mar  1 13:12:58.530: INFO: (12) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 6.733511ms)
    Mar  1 13:12:58.530: INFO: (12) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">... (200; 7.130001ms)
    Mar  1 13:12:58.531: INFO: (12) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 7.500633ms)
    Mar  1 13:12:58.531: INFO: (12) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/tlsrewritem... (200; 7.733059ms)
    Mar  1 13:12:58.533: INFO: (12) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:462/proxy/: tls qux (200; 9.432992ms)
    Mar  1 13:12:58.533: INFO: (12) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:460/proxy/: tls baz (200; 9.653953ms)
    Mar  1 13:12:58.533: INFO: (12) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname2/proxy/: tls qux (200; 9.917898ms)
    Mar  1 13:12:58.533: INFO: (12) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/rewriteme">test</a> (200; 10.232809ms)
    Mar  1 13:12:58.534: INFO: (12) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname2/proxy/: bar (200; 10.917464ms)
    Mar  1 13:12:58.534: INFO: (12) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 11.006379ms)
    Mar  1 13:12:58.534: INFO: (12) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 11.120518ms)
    Mar  1 13:12:58.535: INFO: (12) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname1/proxy/: tls baz (200; 11.668039ms)
    Mar  1 13:12:58.535: INFO: (12) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname1/proxy/: foo (200; 11.60115ms)
    Mar  1 13:12:58.536: INFO: (12) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname2/proxy/: bar (200; 13.235994ms)
    Mar  1 13:12:58.537: INFO: (12) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname1/proxy/: foo (200; 13.390949ms)
    Mar  1 13:12:58.544: INFO: (13) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 5.326129ms)
    Mar  1 13:12:58.546: INFO: (13) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:462/proxy/: tls qux (200; 7.316619ms)
    Mar  1 13:12:58.546: INFO: (13) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">... (200; 7.485657ms)
    Mar  1 13:12:58.547: INFO: (13) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 7.927658ms)
    Mar  1 13:12:58.547: INFO: (13) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:460/proxy/: tls baz (200; 8.570846ms)
    Mar  1 13:12:58.547: INFO: (13) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 8.627054ms)
    Mar  1 13:12:58.548: INFO: (13) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">test<... (200; 9.588353ms)
    Mar  1 13:12:58.549: INFO: (13) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/rewriteme">test</a> (200; 9.805459ms)
    Mar  1 13:12:58.549: INFO: (13) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname1/proxy/: foo (200; 10.642795ms)
    Mar  1 13:12:58.549: INFO: (13) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/tlsrewritem... (200; 10.262504ms)
    Mar  1 13:12:58.550: INFO: (13) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 10.826248ms)
    Mar  1 13:12:58.550: INFO: (13) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname1/proxy/: tls baz (200; 10.906795ms)
    Mar  1 13:12:58.550: INFO: (13) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname2/proxy/: bar (200; 11.668267ms)
    Mar  1 13:12:58.551: INFO: (13) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname2/proxy/: tls qux (200; 11.595791ms)
    Mar  1 13:12:58.551: INFO: (13) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname2/proxy/: bar (200; 11.955705ms)
    Mar  1 13:12:58.552: INFO: (13) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname1/proxy/: foo (200; 13.416989ms)
    Mar  1 13:12:58.562: INFO: (14) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/tlsrewritem... (200; 9.898458ms)
    Mar  1 13:12:58.562: INFO: (14) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:462/proxy/: tls qux (200; 9.653962ms)
    Mar  1 13:12:58.562: INFO: (14) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 9.727902ms)
    Mar  1 13:12:58.562: INFO: (14) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">... (200; 9.619049ms)
    Mar  1 13:12:58.563: INFO: (14) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">test<... (200; 9.561512ms)
    Mar  1 13:12:58.563: INFO: (14) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 9.820636ms)
    Mar  1 13:12:58.563: INFO: (14) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:460/proxy/: tls baz (200; 9.805518ms)
    Mar  1 13:12:58.563: INFO: (14) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 9.885731ms)
    Mar  1 13:12:58.563: INFO: (14) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/rewriteme">test</a> (200; 10.133928ms)
    Mar  1 13:12:58.568: INFO: (14) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname1/proxy/: foo (200; 15.126139ms)
    Mar  1 13:12:58.568: INFO: (14) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 14.983542ms)
    Mar  1 13:12:58.568: INFO: (14) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname2/proxy/: tls qux (200; 15.491181ms)
    Mar  1 13:12:58.568: INFO: (14) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname1/proxy/: foo (200; 15.417497ms)
    Mar  1 13:12:58.568: INFO: (14) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname1/proxy/: tls baz (200; 15.430127ms)
    Mar  1 13:12:58.568: INFO: (14) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname2/proxy/: bar (200; 15.461684ms)
    Mar  1 13:12:58.569: INFO: (14) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname2/proxy/: bar (200; 15.665986ms)
    Mar  1 13:12:58.574: INFO: (15) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">test<... (200; 5.785349ms)
    Mar  1 13:12:58.576: INFO: (15) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 6.954672ms)
    Mar  1 13:12:58.577: INFO: (15) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 7.553335ms)
    Mar  1 13:12:58.578: INFO: (15) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 9.298317ms)
    Mar  1 13:12:58.579: INFO: (15) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/tlsrewritem... (200; 9.826849ms)
    Mar  1 13:12:58.579: INFO: (15) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 9.819745ms)
    Mar  1 13:12:58.579: INFO: (15) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:462/proxy/: tls qux (200; 9.933504ms)
    Mar  1 13:12:58.579: INFO: (15) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">... (200; 10.15942ms)
    Mar  1 13:12:58.579: INFO: (15) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname2/proxy/: bar (200; 10.696493ms)
    Mar  1 13:12:58.579: INFO: (15) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:460/proxy/: tls baz (200; 10.253565ms)
    Mar  1 13:12:58.580: INFO: (15) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/rewriteme">test</a> (200; 10.706483ms)
    Mar  1 13:12:58.582: INFO: (15) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname1/proxy/: foo (200; 12.533559ms)
    Mar  1 13:12:58.582: INFO: (15) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname2/proxy/: bar (200; 12.679816ms)
    Mar  1 13:12:58.583: INFO: (15) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname1/proxy/: tls baz (200; 13.512186ms)
    Mar  1 13:12:58.583: INFO: (15) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname2/proxy/: tls qux (200; 14.010458ms)
    Mar  1 13:12:58.583: INFO: (15) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname1/proxy/: foo (200; 14.330159ms)
    Mar  1 13:12:58.589: INFO: (16) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 5.929166ms)
    Mar  1 13:12:58.590: INFO: (16) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:460/proxy/: tls baz (200; 6.624022ms)
    Mar  1 13:12:58.591: INFO: (16) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 7.944783ms)
    Mar  1 13:12:58.593: INFO: (16) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/rewriteme">test</a> (200; 9.031528ms)
    Mar  1 13:12:58.596: INFO: (16) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 12.160449ms)
    Mar  1 13:12:58.596: INFO: (16) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">... (200; 12.330457ms)
    Mar  1 13:12:58.597: INFO: (16) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname2/proxy/: bar (200; 13.150025ms)
    Mar  1 13:12:58.597: INFO: (16) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:462/proxy/: tls qux (200; 13.204672ms)
    Mar  1 13:12:58.597: INFO: (16) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">test<... (200; 13.170611ms)
    Mar  1 13:12:58.597: INFO: (16) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname1/proxy/: foo (200; 13.278974ms)
    Mar  1 13:12:58.597: INFO: (16) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname2/proxy/: tls qux (200; 13.668557ms)
    Mar  1 13:12:58.597: INFO: (16) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 13.62733ms)
    Mar  1 13:12:58.597: INFO: (16) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname2/proxy/: bar (200; 13.858468ms)
    Mar  1 13:12:58.598: INFO: (16) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/tlsrewritem... (200; 14.030461ms)
    Mar  1 13:12:58.598: INFO: (16) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname1/proxy/: foo (200; 14.03993ms)
    Mar  1 13:12:58.599: INFO: (16) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname1/proxy/: tls baz (200; 15.924745ms)
    Mar  1 13:12:58.605: INFO: (17) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:460/proxy/: tls baz (200; 5.210355ms)
    Mar  1 13:12:58.605: INFO: (17) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 5.51741ms)
    Mar  1 13:12:58.608: INFO: (17) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/tlsrewritem... (200; 7.910746ms)
    Mar  1 13:12:58.608: INFO: (17) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 8.292633ms)
    Mar  1 13:12:58.608: INFO: (17) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 8.312991ms)
    Mar  1 13:12:58.609: INFO: (17) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">test<... (200; 9.216803ms)
    Mar  1 13:12:58.609: INFO: (17) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 9.48487ms)
    Mar  1 13:12:58.609: INFO: (17) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/rewriteme">test</a> (200; 9.713965ms)
    Mar  1 13:12:58.610: INFO: (17) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">... (200; 9.89657ms)
    Mar  1 13:12:58.610: INFO: (17) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname1/proxy/: foo (200; 10.400838ms)
    Mar  1 13:12:58.610: INFO: (17) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname1/proxy/: tls baz (200; 10.476023ms)
    Mar  1 13:12:58.610: INFO: (17) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname2/proxy/: bar (200; 10.43913ms)
    Mar  1 13:12:58.611: INFO: (17) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:462/proxy/: tls qux (200; 11.126309ms)
    Mar  1 13:12:58.612: INFO: (17) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname2/proxy/: bar (200; 12.331608ms)
    Mar  1 13:12:58.612: INFO: (17) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname1/proxy/: foo (200; 12.652323ms)
    Mar  1 13:12:58.613: INFO: (17) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname2/proxy/: tls qux (200; 12.887824ms)
    Mar  1 13:12:58.618: INFO: (18) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">test<... (200; 5.298675ms)
    Mar  1 13:12:58.620: INFO: (18) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 6.987285ms)
    Mar  1 13:12:58.620: INFO: (18) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 7.254947ms)
    Mar  1 13:12:58.620: INFO: (18) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/rewriteme">test</a> (200; 7.542689ms)
    Mar  1 13:12:58.622: INFO: (18) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:462/proxy/: tls qux (200; 8.746745ms)
    Mar  1 13:12:58.622: INFO: (18) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:460/proxy/: tls baz (200; 8.732189ms)
    Mar  1 13:12:58.623: INFO: (18) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 9.625898ms)
    Mar  1 13:12:58.623: INFO: (18) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 9.901599ms)
    Mar  1 13:12:58.623: INFO: (18) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">... (200; 10.133877ms)
    Mar  1 13:12:58.623: INFO: (18) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/tlsrewritem... (200; 10.66881ms)
    Mar  1 13:12:58.624: INFO: (18) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname1/proxy/: tls baz (200; 11.147516ms)
    Mar  1 13:12:58.625: INFO: (18) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname2/proxy/: tls qux (200; 11.640961ms)
    Mar  1 13:12:58.625: INFO: (18) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname1/proxy/: foo (200; 11.866805ms)
    Mar  1 13:12:58.625: INFO: (18) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname2/proxy/: bar (200; 11.762708ms)
    Mar  1 13:12:58.626: INFO: (18) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname2/proxy/: bar (200; 12.86988ms)
    Mar  1 13:12:58.626: INFO: (18) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname1/proxy/: foo (200; 12.77059ms)
    Mar  1 13:12:58.631: INFO: (19) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">test<... (200; 5.287154ms)
    Mar  1 13:12:58.631: INFO: (19) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt/proxy/rewriteme">test</a> (200; 5.349706ms)
    Mar  1 13:12:58.632: INFO: (19) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 6.024501ms)
    Mar  1 13:12:58.633: INFO: (19) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:462/proxy/: tls qux (200; 7.121419ms)
    Mar  1 13:12:58.634: INFO: (19) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:1080/proxy/rewriteme">... (200; 7.754885ms)
    Mar  1 13:12:58.634: INFO: (19) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/: <a href="/api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:443/proxy/tlsrewritem... (200; 8.086002ms)
    Mar  1 13:12:58.635: INFO: (19) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 9.374103ms)
    Mar  1 13:12:58.636: INFO: (19) /api/v1/namespaces/proxy-6998/pods/proxy-service-xw2dk-v9ggt:160/proxy/: foo (200; 10.106254ms)
    Mar  1 13:12:58.636: INFO: (19) /api/v1/namespaces/proxy-6998/pods/http:proxy-service-xw2dk-v9ggt:162/proxy/: bar (200; 10.192009ms)
    Mar  1 13:12:58.636: INFO: (19) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname1/proxy/: tls baz (200; 10.60299ms)
    Mar  1 13:12:58.637: INFO: (19) /api/v1/namespaces/proxy-6998/pods/https:proxy-service-xw2dk-v9ggt:460/proxy/: tls baz (200; 11.198697ms)
    Mar  1 13:12:58.637: INFO: (19) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname1/proxy/: foo (200; 11.716469ms)
    Mar  1 13:12:58.638: INFO: (19) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname2/proxy/: bar (200; 11.824513ms)
    Mar  1 13:12:58.638: INFO: (19) /api/v1/namespaces/proxy-6998/services/https:proxy-service-xw2dk:tlsportname2/proxy/: tls qux (200; 11.981838ms)
    Mar  1 13:12:58.638: INFO: (19) /api/v1/namespaces/proxy-6998/services/http:proxy-service-xw2dk:portname1/proxy/: foo (200; 12.182632ms)
    Mar  1 13:12:58.639: INFO: (19) /api/v1/namespaces/proxy-6998/services/proxy-service-xw2dk:portname2/proxy/: bar (200; 13.470024ms)
    STEP: deleting ReplicationController proxy-service-xw2dk in namespace proxy-6998, will wait for the garbage collector to delete the pods 03/01/23 13:12:58.64
    Mar  1 13:12:58.704: INFO: Deleting ReplicationController proxy-service-xw2dk took: 9.182327ms
    Mar  1 13:12:58.805: INFO: Terminating ReplicationController proxy-service-xw2dk pods took: 101.036987ms
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Mar  1 13:13:01.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-6998" for this suite. 03/01/23 13:13:01.614
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:13:01.626
Mar  1 13:13:01.627: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename container-runtime 03/01/23 13:13:01.627
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:13:01.65
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:13:01.653
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
STEP: create the container 03/01/23 13:13:01.656
STEP: wait for the container to reach Succeeded 03/01/23 13:13:01.666
STEP: get the container status 03/01/23 13:13:04.686
STEP: the container should be terminated 03/01/23 13:13:04.691
STEP: the termination message should be set 03/01/23 13:13:04.691
Mar  1 13:13:04.691: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 03/01/23 13:13:04.691
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Mar  1 13:13:04.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3677" for this suite. 03/01/23 13:13:04.721
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","completed":327,"skipped":5974,"failed":0}
------------------------------
â€¢ [3.105 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:13:01.626
    Mar  1 13:13:01.627: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename container-runtime 03/01/23 13:13:01.627
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:13:01.65
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:13:01.653
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194
    STEP: create the container 03/01/23 13:13:01.656
    STEP: wait for the container to reach Succeeded 03/01/23 13:13:01.666
    STEP: get the container status 03/01/23 13:13:04.686
    STEP: the container should be terminated 03/01/23 13:13:04.691
    STEP: the termination message should be set 03/01/23 13:13:04.691
    Mar  1 13:13:04.691: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 03/01/23 13:13:04.691
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Mar  1 13:13:04.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-3677" for this suite. 03/01/23 13:13:04.721
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:13:04.738
Mar  1 13:13:04.738: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename pods 03/01/23 13:13:04.738
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:13:04.762
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:13:04.765
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
STEP: creating the pod 03/01/23 13:13:04.767
STEP: submitting the pod to kubernetes 03/01/23 13:13:04.768
Mar  1 13:13:04.779: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-e3863d3c-cbf7-4dcd-b593-41b4706cab28" in namespace "pods-8489" to be "running and ready"
Mar  1 13:13:04.784: INFO: Pod "pod-update-activedeadlineseconds-e3863d3c-cbf7-4dcd-b593-41b4706cab28": Phase="Pending", Reason="", readiness=false. Elapsed: 5.174547ms
Mar  1 13:13:04.784: INFO: The phase of Pod pod-update-activedeadlineseconds-e3863d3c-cbf7-4dcd-b593-41b4706cab28 is Pending, waiting for it to be Running (with Ready = true)
Mar  1 13:13:06.790: INFO: Pod "pod-update-activedeadlineseconds-e3863d3c-cbf7-4dcd-b593-41b4706cab28": Phase="Running", Reason="", readiness=true. Elapsed: 2.011299927s
Mar  1 13:13:06.790: INFO: The phase of Pod pod-update-activedeadlineseconds-e3863d3c-cbf7-4dcd-b593-41b4706cab28 is Running (Ready = true)
Mar  1 13:13:06.790: INFO: Pod "pod-update-activedeadlineseconds-e3863d3c-cbf7-4dcd-b593-41b4706cab28" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 03/01/23 13:13:06.797
STEP: updating the pod 03/01/23 13:13:06.801
Mar  1 13:13:07.322: INFO: Successfully updated pod "pod-update-activedeadlineseconds-e3863d3c-cbf7-4dcd-b593-41b4706cab28"
Mar  1 13:13:07.322: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-e3863d3c-cbf7-4dcd-b593-41b4706cab28" in namespace "pods-8489" to be "terminated with reason DeadlineExceeded"
Mar  1 13:13:07.326: INFO: Pod "pod-update-activedeadlineseconds-e3863d3c-cbf7-4dcd-b593-41b4706cab28": Phase="Running", Reason="", readiness=true. Elapsed: 4.178434ms
Mar  1 13:13:09.332: INFO: Pod "pod-update-activedeadlineseconds-e3863d3c-cbf7-4dcd-b593-41b4706cab28": Phase="Running", Reason="", readiness=true. Elapsed: 2.009921725s
Mar  1 13:13:11.333: INFO: Pod "pod-update-activedeadlineseconds-e3863d3c-cbf7-4dcd-b593-41b4706cab28": Phase="Running", Reason="", readiness=false. Elapsed: 4.011528514s
Mar  1 13:13:13.333: INFO: Pod "pod-update-activedeadlineseconds-e3863d3c-cbf7-4dcd-b593-41b4706cab28": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.011019846s
Mar  1 13:13:13.333: INFO: Pod "pod-update-activedeadlineseconds-e3863d3c-cbf7-4dcd-b593-41b4706cab28" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Mar  1 13:13:13.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8489" for this suite. 03/01/23 13:13:13.339
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","completed":328,"skipped":6023,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.611 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:13:04.738
    Mar  1 13:13:04.738: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename pods 03/01/23 13:13:04.738
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:13:04.762
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:13:04.765
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:397
    STEP: creating the pod 03/01/23 13:13:04.767
    STEP: submitting the pod to kubernetes 03/01/23 13:13:04.768
    Mar  1 13:13:04.779: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-e3863d3c-cbf7-4dcd-b593-41b4706cab28" in namespace "pods-8489" to be "running and ready"
    Mar  1 13:13:04.784: INFO: Pod "pod-update-activedeadlineseconds-e3863d3c-cbf7-4dcd-b593-41b4706cab28": Phase="Pending", Reason="", readiness=false. Elapsed: 5.174547ms
    Mar  1 13:13:04.784: INFO: The phase of Pod pod-update-activedeadlineseconds-e3863d3c-cbf7-4dcd-b593-41b4706cab28 is Pending, waiting for it to be Running (with Ready = true)
    Mar  1 13:13:06.790: INFO: Pod "pod-update-activedeadlineseconds-e3863d3c-cbf7-4dcd-b593-41b4706cab28": Phase="Running", Reason="", readiness=true. Elapsed: 2.011299927s
    Mar  1 13:13:06.790: INFO: The phase of Pod pod-update-activedeadlineseconds-e3863d3c-cbf7-4dcd-b593-41b4706cab28 is Running (Ready = true)
    Mar  1 13:13:06.790: INFO: Pod "pod-update-activedeadlineseconds-e3863d3c-cbf7-4dcd-b593-41b4706cab28" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 03/01/23 13:13:06.797
    STEP: updating the pod 03/01/23 13:13:06.801
    Mar  1 13:13:07.322: INFO: Successfully updated pod "pod-update-activedeadlineseconds-e3863d3c-cbf7-4dcd-b593-41b4706cab28"
    Mar  1 13:13:07.322: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-e3863d3c-cbf7-4dcd-b593-41b4706cab28" in namespace "pods-8489" to be "terminated with reason DeadlineExceeded"
    Mar  1 13:13:07.326: INFO: Pod "pod-update-activedeadlineseconds-e3863d3c-cbf7-4dcd-b593-41b4706cab28": Phase="Running", Reason="", readiness=true. Elapsed: 4.178434ms
    Mar  1 13:13:09.332: INFO: Pod "pod-update-activedeadlineseconds-e3863d3c-cbf7-4dcd-b593-41b4706cab28": Phase="Running", Reason="", readiness=true. Elapsed: 2.009921725s
    Mar  1 13:13:11.333: INFO: Pod "pod-update-activedeadlineseconds-e3863d3c-cbf7-4dcd-b593-41b4706cab28": Phase="Running", Reason="", readiness=false. Elapsed: 4.011528514s
    Mar  1 13:13:13.333: INFO: Pod "pod-update-activedeadlineseconds-e3863d3c-cbf7-4dcd-b593-41b4706cab28": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.011019846s
    Mar  1 13:13:13.333: INFO: Pod "pod-update-activedeadlineseconds-e3863d3c-cbf7-4dcd-b593-41b4706cab28" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Mar  1 13:13:13.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-8489" for this suite. 03/01/23 13:13:13.339
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:13:13.35
Mar  1 13:13:13.350: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename daemonsets 03/01/23 13:13:13.351
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:13:13.371
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:13:13.373
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
STEP: Creating simple DaemonSet "daemon-set" 03/01/23 13:13:13.403
STEP: Check that daemon pods launch on every node of the cluster. 03/01/23 13:13:13.41
Mar  1 13:13:13.418: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 13:13:13.418: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 13:13:13.418: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 13:13:13.422: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  1 13:13:13.422: INFO: Node lab1-k8s-node-1 is running 0 daemon pod, expected 1
Mar  1 13:13:14.428: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 13:13:14.428: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 13:13:14.429: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 13:13:14.433: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar  1 13:13:14.433: INFO: Node lab1-k8s-node-2 is running 0 daemon pod, expected 1
Mar  1 13:13:15.429: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 13:13:15.430: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 13:13:15.430: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 13:13:15.434: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar  1 13:13:15.434: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 03/01/23 13:13:15.438
Mar  1 13:13:15.458: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 13:13:15.459: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 13:13:15.459: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 13:13:15.463: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar  1 13:13:15.463: INFO: Node lab1-k8s-node-2 is running 0 daemon pod, expected 1
Mar  1 13:13:16.469: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 13:13:16.470: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 13:13:16.470: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 13:13:16.475: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar  1 13:13:16.475: INFO: Node lab1-k8s-node-2 is running 0 daemon pod, expected 1
Mar  1 13:13:17.471: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 13:13:17.471: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 13:13:17.472: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 13:13:17.477: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar  1 13:13:17.477: INFO: Node lab1-k8s-node-2 is running 0 daemon pod, expected 1
Mar  1 13:13:18.469: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 13:13:18.470: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 13:13:18.470: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 13:13:18.474: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar  1 13:13:18.474: INFO: Node lab1-k8s-node-2 is running 0 daemon pod, expected 1
Mar  1 13:13:19.470: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 13:13:19.470: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 13:13:19.470: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 13:13:19.474: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar  1 13:13:19.474: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 03/01/23 13:13:19.478
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1512, will wait for the garbage collector to delete the pods 03/01/23 13:13:19.478
Mar  1 13:13:19.544: INFO: Deleting DaemonSet.extensions daemon-set took: 9.015936ms
Mar  1 13:13:19.645: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.240775ms
Mar  1 13:13:21.751: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  1 13:13:21.751: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar  1 13:13:21.756: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"45988"},"items":null}

Mar  1 13:13:21.760: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"45988"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Mar  1 13:13:21.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1512" for this suite. 03/01/23 13:13:21.785
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","completed":329,"skipped":6027,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.443 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:13:13.35
    Mar  1 13:13:13.350: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename daemonsets 03/01/23 13:13:13.351
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:13:13.371
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:13:13.373
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:165
    STEP: Creating simple DaemonSet "daemon-set" 03/01/23 13:13:13.403
    STEP: Check that daemon pods launch on every node of the cluster. 03/01/23 13:13:13.41
    Mar  1 13:13:13.418: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 13:13:13.418: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 13:13:13.418: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 13:13:13.422: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  1 13:13:13.422: INFO: Node lab1-k8s-node-1 is running 0 daemon pod, expected 1
    Mar  1 13:13:14.428: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 13:13:14.428: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 13:13:14.429: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 13:13:14.433: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Mar  1 13:13:14.433: INFO: Node lab1-k8s-node-2 is running 0 daemon pod, expected 1
    Mar  1 13:13:15.429: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 13:13:15.430: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 13:13:15.430: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 13:13:15.434: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar  1 13:13:15.434: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 03/01/23 13:13:15.438
    Mar  1 13:13:15.458: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 13:13:15.459: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 13:13:15.459: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 13:13:15.463: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar  1 13:13:15.463: INFO: Node lab1-k8s-node-2 is running 0 daemon pod, expected 1
    Mar  1 13:13:16.469: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 13:13:16.470: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 13:13:16.470: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 13:13:16.475: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar  1 13:13:16.475: INFO: Node lab1-k8s-node-2 is running 0 daemon pod, expected 1
    Mar  1 13:13:17.471: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 13:13:17.471: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 13:13:17.472: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 13:13:17.477: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar  1 13:13:17.477: INFO: Node lab1-k8s-node-2 is running 0 daemon pod, expected 1
    Mar  1 13:13:18.469: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 13:13:18.470: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 13:13:18.470: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 13:13:18.474: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar  1 13:13:18.474: INFO: Node lab1-k8s-node-2 is running 0 daemon pod, expected 1
    Mar  1 13:13:19.470: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 13:13:19.470: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 13:13:19.470: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 13:13:19.474: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar  1 13:13:19.474: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 03/01/23 13:13:19.478
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1512, will wait for the garbage collector to delete the pods 03/01/23 13:13:19.478
    Mar  1 13:13:19.544: INFO: Deleting DaemonSet.extensions daemon-set took: 9.015936ms
    Mar  1 13:13:19.645: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.240775ms
    Mar  1 13:13:21.751: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  1 13:13:21.751: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Mar  1 13:13:21.756: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"45988"},"items":null}

    Mar  1 13:13:21.760: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"45988"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Mar  1 13:13:21.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-1512" for this suite. 03/01/23 13:13:21.785
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:13:21.798
Mar  1 13:13:21.799: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename projected 03/01/23 13:13:21.799
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:13:21.819
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:13:21.821
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
STEP: Creating a pod to test downward API volume plugin 03/01/23 13:13:21.824
Mar  1 13:13:21.834: INFO: Waiting up to 5m0s for pod "downwardapi-volume-101f8aec-7f86-49c8-bace-887768099773" in namespace "projected-2239" to be "Succeeded or Failed"
Mar  1 13:13:21.839: INFO: Pod "downwardapi-volume-101f8aec-7f86-49c8-bace-887768099773": Phase="Pending", Reason="", readiness=false. Elapsed: 4.905389ms
Mar  1 13:13:23.845: INFO: Pod "downwardapi-volume-101f8aec-7f86-49c8-bace-887768099773": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011267289s
Mar  1 13:13:25.845: INFO: Pod "downwardapi-volume-101f8aec-7f86-49c8-bace-887768099773": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010733046s
STEP: Saw pod success 03/01/23 13:13:25.845
Mar  1 13:13:25.845: INFO: Pod "downwardapi-volume-101f8aec-7f86-49c8-bace-887768099773" satisfied condition "Succeeded or Failed"
Mar  1 13:13:25.850: INFO: Trying to get logs from node lab1-k8s-node-3 pod downwardapi-volume-101f8aec-7f86-49c8-bace-887768099773 container client-container: <nil>
STEP: delete the pod 03/01/23 13:13:25.858
Mar  1 13:13:25.874: INFO: Waiting for pod downwardapi-volume-101f8aec-7f86-49c8-bace-887768099773 to disappear
Mar  1 13:13:25.879: INFO: Pod downwardapi-volume-101f8aec-7f86-49c8-bace-887768099773 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Mar  1 13:13:25.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2239" for this suite. 03/01/23 13:13:25.885
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","completed":330,"skipped":6068,"failed":0}
------------------------------
â€¢ [4.103 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:13:21.798
    Mar  1 13:13:21.799: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename projected 03/01/23 13:13:21.799
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:13:21.819
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:13:21.821
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:52
    STEP: Creating a pod to test downward API volume plugin 03/01/23 13:13:21.824
    Mar  1 13:13:21.834: INFO: Waiting up to 5m0s for pod "downwardapi-volume-101f8aec-7f86-49c8-bace-887768099773" in namespace "projected-2239" to be "Succeeded or Failed"
    Mar  1 13:13:21.839: INFO: Pod "downwardapi-volume-101f8aec-7f86-49c8-bace-887768099773": Phase="Pending", Reason="", readiness=false. Elapsed: 4.905389ms
    Mar  1 13:13:23.845: INFO: Pod "downwardapi-volume-101f8aec-7f86-49c8-bace-887768099773": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011267289s
    Mar  1 13:13:25.845: INFO: Pod "downwardapi-volume-101f8aec-7f86-49c8-bace-887768099773": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010733046s
    STEP: Saw pod success 03/01/23 13:13:25.845
    Mar  1 13:13:25.845: INFO: Pod "downwardapi-volume-101f8aec-7f86-49c8-bace-887768099773" satisfied condition "Succeeded or Failed"
    Mar  1 13:13:25.850: INFO: Trying to get logs from node lab1-k8s-node-3 pod downwardapi-volume-101f8aec-7f86-49c8-bace-887768099773 container client-container: <nil>
    STEP: delete the pod 03/01/23 13:13:25.858
    Mar  1 13:13:25.874: INFO: Waiting for pod downwardapi-volume-101f8aec-7f86-49c8-bace-887768099773 to disappear
    Mar  1 13:13:25.879: INFO: Pod downwardapi-volume-101f8aec-7f86-49c8-bace-887768099773 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Mar  1 13:13:25.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2239" for this suite. 03/01/23 13:13:25.885
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:13:25.904
Mar  1 13:13:25.904: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename job 03/01/23 13:13:25.905
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:13:25.928
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:13:25.931
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
STEP: Creating Indexed job 03/01/23 13:13:25.934
STEP: Ensuring job reaches completions 03/01/23 13:13:25.94
STEP: Ensuring pods with index for job exist 03/01/23 13:13:35.945
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Mar  1 13:13:35.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6269" for this suite. 03/01/23 13:13:35.955
{"msg":"PASSED [sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]","completed":331,"skipped":6092,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.059 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:13:25.904
    Mar  1 13:13:25.904: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename job 03/01/23 13:13:25.905
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:13:25.928
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:13:25.931
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:194
    STEP: Creating Indexed job 03/01/23 13:13:25.934
    STEP: Ensuring job reaches completions 03/01/23 13:13:25.94
    STEP: Ensuring pods with index for job exist 03/01/23 13:13:35.945
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Mar  1 13:13:35.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-6269" for this suite. 03/01/23 13:13:35.955
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:13:35.964
Mar  1 13:13:35.964: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename pods 03/01/23 13:13:35.965
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:13:35.986
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:13:35.988
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
STEP: creating the pod 03/01/23 13:13:35.991
STEP: submitting the pod to kubernetes 03/01/23 13:13:35.991
Mar  1 13:13:36.000: INFO: Waiting up to 5m0s for pod "pod-update-700f2701-69de-436e-9890-e20e855ab43f" in namespace "pods-1089" to be "running and ready"
Mar  1 13:13:36.006: INFO: Pod "pod-update-700f2701-69de-436e-9890-e20e855ab43f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.578044ms
Mar  1 13:13:36.006: INFO: The phase of Pod pod-update-700f2701-69de-436e-9890-e20e855ab43f is Pending, waiting for it to be Running (with Ready = true)
Mar  1 13:13:38.012: INFO: Pod "pod-update-700f2701-69de-436e-9890-e20e855ab43f": Phase="Running", Reason="", readiness=true. Elapsed: 2.011599487s
Mar  1 13:13:38.012: INFO: The phase of Pod pod-update-700f2701-69de-436e-9890-e20e855ab43f is Running (Ready = true)
Mar  1 13:13:38.012: INFO: Pod "pod-update-700f2701-69de-436e-9890-e20e855ab43f" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 03/01/23 13:13:38.016
STEP: updating the pod 03/01/23 13:13:38.021
Mar  1 13:13:38.536: INFO: Successfully updated pod "pod-update-700f2701-69de-436e-9890-e20e855ab43f"
Mar  1 13:13:38.536: INFO: Waiting up to 5m0s for pod "pod-update-700f2701-69de-436e-9890-e20e855ab43f" in namespace "pods-1089" to be "running"
Mar  1 13:13:38.541: INFO: Pod "pod-update-700f2701-69de-436e-9890-e20e855ab43f": Phase="Running", Reason="", readiness=true. Elapsed: 4.786174ms
Mar  1 13:13:38.541: INFO: Pod "pod-update-700f2701-69de-436e-9890-e20e855ab43f" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 03/01/23 13:13:38.541
Mar  1 13:13:38.546: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Mar  1 13:13:38.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1089" for this suite. 03/01/23 13:13:38.552
{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","completed":332,"skipped":6093,"failed":0}
------------------------------
â€¢ [2.596 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:13:35.964
    Mar  1 13:13:35.964: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename pods 03/01/23 13:13:35.965
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:13:35.986
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:13:35.988
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:343
    STEP: creating the pod 03/01/23 13:13:35.991
    STEP: submitting the pod to kubernetes 03/01/23 13:13:35.991
    Mar  1 13:13:36.000: INFO: Waiting up to 5m0s for pod "pod-update-700f2701-69de-436e-9890-e20e855ab43f" in namespace "pods-1089" to be "running and ready"
    Mar  1 13:13:36.006: INFO: Pod "pod-update-700f2701-69de-436e-9890-e20e855ab43f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.578044ms
    Mar  1 13:13:36.006: INFO: The phase of Pod pod-update-700f2701-69de-436e-9890-e20e855ab43f is Pending, waiting for it to be Running (with Ready = true)
    Mar  1 13:13:38.012: INFO: Pod "pod-update-700f2701-69de-436e-9890-e20e855ab43f": Phase="Running", Reason="", readiness=true. Elapsed: 2.011599487s
    Mar  1 13:13:38.012: INFO: The phase of Pod pod-update-700f2701-69de-436e-9890-e20e855ab43f is Running (Ready = true)
    Mar  1 13:13:38.012: INFO: Pod "pod-update-700f2701-69de-436e-9890-e20e855ab43f" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 03/01/23 13:13:38.016
    STEP: updating the pod 03/01/23 13:13:38.021
    Mar  1 13:13:38.536: INFO: Successfully updated pod "pod-update-700f2701-69de-436e-9890-e20e855ab43f"
    Mar  1 13:13:38.536: INFO: Waiting up to 5m0s for pod "pod-update-700f2701-69de-436e-9890-e20e855ab43f" in namespace "pods-1089" to be "running"
    Mar  1 13:13:38.541: INFO: Pod "pod-update-700f2701-69de-436e-9890-e20e855ab43f": Phase="Running", Reason="", readiness=true. Elapsed: 4.786174ms
    Mar  1 13:13:38.541: INFO: Pod "pod-update-700f2701-69de-436e-9890-e20e855ab43f" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 03/01/23 13:13:38.541
    Mar  1 13:13:38.546: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Mar  1 13:13:38.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-1089" for this suite. 03/01/23 13:13:38.552
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:13:38.564
Mar  1 13:13:38.564: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename downward-api 03/01/23 13:13:38.565
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:13:38.589
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:13:38.592
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
STEP: Creating a pod to test downward API volume plugin 03/01/23 13:13:38.594
Mar  1 13:13:38.607: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3ab8e946-2bb1-4d94-b7a1-49b67a3a1fe4" in namespace "downward-api-9371" to be "Succeeded or Failed"
Mar  1 13:13:38.613: INFO: Pod "downwardapi-volume-3ab8e946-2bb1-4d94-b7a1-49b67a3a1fe4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.811808ms
Mar  1 13:13:40.619: INFO: Pod "downwardapi-volume-3ab8e946-2bb1-4d94-b7a1-49b67a3a1fe4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011505941s
Mar  1 13:13:42.619: INFO: Pod "downwardapi-volume-3ab8e946-2bb1-4d94-b7a1-49b67a3a1fe4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011577649s
STEP: Saw pod success 03/01/23 13:13:42.619
Mar  1 13:13:42.619: INFO: Pod "downwardapi-volume-3ab8e946-2bb1-4d94-b7a1-49b67a3a1fe4" satisfied condition "Succeeded or Failed"
Mar  1 13:13:42.624: INFO: Trying to get logs from node lab1-k8s-node-3 pod downwardapi-volume-3ab8e946-2bb1-4d94-b7a1-49b67a3a1fe4 container client-container: <nil>
STEP: delete the pod 03/01/23 13:13:42.634
Mar  1 13:13:42.651: INFO: Waiting for pod downwardapi-volume-3ab8e946-2bb1-4d94-b7a1-49b67a3a1fe4 to disappear
Mar  1 13:13:42.655: INFO: Pod downwardapi-volume-3ab8e946-2bb1-4d94-b7a1-49b67a3a1fe4 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Mar  1 13:13:42.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9371" for this suite. 03/01/23 13:13:42.662
{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":333,"skipped":6116,"failed":0}
------------------------------
â€¢ [4.109 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:13:38.564
    Mar  1 13:13:38.564: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename downward-api 03/01/23 13:13:38.565
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:13:38.589
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:13:38.592
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:67
    STEP: Creating a pod to test downward API volume plugin 03/01/23 13:13:38.594
    Mar  1 13:13:38.607: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3ab8e946-2bb1-4d94-b7a1-49b67a3a1fe4" in namespace "downward-api-9371" to be "Succeeded or Failed"
    Mar  1 13:13:38.613: INFO: Pod "downwardapi-volume-3ab8e946-2bb1-4d94-b7a1-49b67a3a1fe4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.811808ms
    Mar  1 13:13:40.619: INFO: Pod "downwardapi-volume-3ab8e946-2bb1-4d94-b7a1-49b67a3a1fe4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011505941s
    Mar  1 13:13:42.619: INFO: Pod "downwardapi-volume-3ab8e946-2bb1-4d94-b7a1-49b67a3a1fe4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011577649s
    STEP: Saw pod success 03/01/23 13:13:42.619
    Mar  1 13:13:42.619: INFO: Pod "downwardapi-volume-3ab8e946-2bb1-4d94-b7a1-49b67a3a1fe4" satisfied condition "Succeeded or Failed"
    Mar  1 13:13:42.624: INFO: Trying to get logs from node lab1-k8s-node-3 pod downwardapi-volume-3ab8e946-2bb1-4d94-b7a1-49b67a3a1fe4 container client-container: <nil>
    STEP: delete the pod 03/01/23 13:13:42.634
    Mar  1 13:13:42.651: INFO: Waiting for pod downwardapi-volume-3ab8e946-2bb1-4d94-b7a1-49b67a3a1fe4 to disappear
    Mar  1 13:13:42.655: INFO: Pod downwardapi-volume-3ab8e946-2bb1-4d94-b7a1-49b67a3a1fe4 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Mar  1 13:13:42.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-9371" for this suite. 03/01/23 13:13:42.662
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:13:42.675
Mar  1 13:13:42.675: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename var-expansion 03/01/23 13:13:42.676
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:13:42.697
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:13:42.7
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
STEP: Creating a pod to test substitution in container's args 03/01/23 13:13:42.702
Mar  1 13:13:42.712: INFO: Waiting up to 5m0s for pod "var-expansion-2fc7ae14-e11f-4460-8424-737b731a716c" in namespace "var-expansion-184" to be "Succeeded or Failed"
Mar  1 13:13:42.716: INFO: Pod "var-expansion-2fc7ae14-e11f-4460-8424-737b731a716c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.709973ms
Mar  1 13:13:44.721: INFO: Pod "var-expansion-2fc7ae14-e11f-4460-8424-737b731a716c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008459499s
Mar  1 13:13:46.722: INFO: Pod "var-expansion-2fc7ae14-e11f-4460-8424-737b731a716c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009774016s
STEP: Saw pod success 03/01/23 13:13:46.722
Mar  1 13:13:46.722: INFO: Pod "var-expansion-2fc7ae14-e11f-4460-8424-737b731a716c" satisfied condition "Succeeded or Failed"
Mar  1 13:13:46.727: INFO: Trying to get logs from node lab1-k8s-node-3 pod var-expansion-2fc7ae14-e11f-4460-8424-737b731a716c container dapi-container: <nil>
STEP: delete the pod 03/01/23 13:13:46.735
Mar  1 13:13:46.749: INFO: Waiting for pod var-expansion-2fc7ae14-e11f-4460-8424-737b731a716c to disappear
Mar  1 13:13:46.754: INFO: Pod var-expansion-2fc7ae14-e11f-4460-8424-737b731a716c no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Mar  1 13:13:46.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-184" for this suite. 03/01/23 13:13:46.761
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","completed":334,"skipped":6122,"failed":0}
------------------------------
â€¢ [4.093 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:13:42.675
    Mar  1 13:13:42.675: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename var-expansion 03/01/23 13:13:42.676
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:13:42.697
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:13:42.7
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:91
    STEP: Creating a pod to test substitution in container's args 03/01/23 13:13:42.702
    Mar  1 13:13:42.712: INFO: Waiting up to 5m0s for pod "var-expansion-2fc7ae14-e11f-4460-8424-737b731a716c" in namespace "var-expansion-184" to be "Succeeded or Failed"
    Mar  1 13:13:42.716: INFO: Pod "var-expansion-2fc7ae14-e11f-4460-8424-737b731a716c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.709973ms
    Mar  1 13:13:44.721: INFO: Pod "var-expansion-2fc7ae14-e11f-4460-8424-737b731a716c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008459499s
    Mar  1 13:13:46.722: INFO: Pod "var-expansion-2fc7ae14-e11f-4460-8424-737b731a716c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009774016s
    STEP: Saw pod success 03/01/23 13:13:46.722
    Mar  1 13:13:46.722: INFO: Pod "var-expansion-2fc7ae14-e11f-4460-8424-737b731a716c" satisfied condition "Succeeded or Failed"
    Mar  1 13:13:46.727: INFO: Trying to get logs from node lab1-k8s-node-3 pod var-expansion-2fc7ae14-e11f-4460-8424-737b731a716c container dapi-container: <nil>
    STEP: delete the pod 03/01/23 13:13:46.735
    Mar  1 13:13:46.749: INFO: Waiting for pod var-expansion-2fc7ae14-e11f-4460-8424-737b731a716c to disappear
    Mar  1 13:13:46.754: INFO: Pod var-expansion-2fc7ae14-e11f-4460-8424-737b731a716c no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Mar  1 13:13:46.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-184" for this suite. 03/01/23 13:13:46.761
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:13:46.771
Mar  1 13:13:46.771: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename webhook 03/01/23 13:13:46.772
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:13:46.795
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:13:46.797
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/01/23 13:13:46.814
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/01/23 13:13:47.244
STEP: Deploying the webhook pod 03/01/23 13:13:47.255
STEP: Wait for the deployment to be ready 03/01/23 13:13:47.269
Mar  1 13:13:47.293: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/01/23 13:13:49.307
STEP: Verifying the service has paired with the endpoint 03/01/23 13:13:49.32
Mar  1 13:13:50.320: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
STEP: Setting timeout (1s) shorter than webhook latency (5s) 03/01/23 13:13:50.324
STEP: Registering slow webhook via the AdmissionRegistration API 03/01/23 13:13:50.324
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 03/01/23 13:13:50.341
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 03/01/23 13:13:51.353
STEP: Registering slow webhook via the AdmissionRegistration API 03/01/23 13:13:51.353
STEP: Having no error when timeout is longer than webhook latency 03/01/23 13:13:52.389
STEP: Registering slow webhook via the AdmissionRegistration API 03/01/23 13:13:52.389
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 03/01/23 13:13:57.433
STEP: Registering slow webhook via the AdmissionRegistration API 03/01/23 13:13:57.433
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  1 13:14:02.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8981" for this suite. 03/01/23 13:14:02.477
STEP: Destroying namespace "webhook-8981-markers" for this suite. 03/01/23 13:14:02.486
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","completed":335,"skipped":6174,"failed":0}
------------------------------
â€¢ [SLOW TEST] [15.783 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:13:46.771
    Mar  1 13:13:46.771: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename webhook 03/01/23 13:13:46.772
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:13:46.795
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:13:46.797
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/01/23 13:13:46.814
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/01/23 13:13:47.244
    STEP: Deploying the webhook pod 03/01/23 13:13:47.255
    STEP: Wait for the deployment to be ready 03/01/23 13:13:47.269
    Mar  1 13:13:47.293: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/01/23 13:13:49.307
    STEP: Verifying the service has paired with the endpoint 03/01/23 13:13:49.32
    Mar  1 13:13:50.320: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:380
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 03/01/23 13:13:50.324
    STEP: Registering slow webhook via the AdmissionRegistration API 03/01/23 13:13:50.324
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 03/01/23 13:13:50.341
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 03/01/23 13:13:51.353
    STEP: Registering slow webhook via the AdmissionRegistration API 03/01/23 13:13:51.353
    STEP: Having no error when timeout is longer than webhook latency 03/01/23 13:13:52.389
    STEP: Registering slow webhook via the AdmissionRegistration API 03/01/23 13:13:52.389
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 03/01/23 13:13:57.433
    STEP: Registering slow webhook via the AdmissionRegistration API 03/01/23 13:13:57.433
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  1 13:14:02.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-8981" for this suite. 03/01/23 13:14:02.477
    STEP: Destroying namespace "webhook-8981-markers" for this suite. 03/01/23 13:14:02.486
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:14:02.561
Mar  1 13:14:02.561: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename security-context 03/01/23 13:14:02.562
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:14:02.582
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:14:02.586
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 03/01/23 13:14:02.589
Mar  1 13:14:02.600: INFO: Waiting up to 5m0s for pod "security-context-7a12b983-140a-450a-abfb-6d89febae55c" in namespace "security-context-6355" to be "Succeeded or Failed"
Mar  1 13:14:02.606: INFO: Pod "security-context-7a12b983-140a-450a-abfb-6d89febae55c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.210825ms
Mar  1 13:14:04.611: INFO: Pod "security-context-7a12b983-140a-450a-abfb-6d89febae55c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010424497s
Mar  1 13:14:06.613: INFO: Pod "security-context-7a12b983-140a-450a-abfb-6d89febae55c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01217657s
STEP: Saw pod success 03/01/23 13:14:06.613
Mar  1 13:14:06.613: INFO: Pod "security-context-7a12b983-140a-450a-abfb-6d89febae55c" satisfied condition "Succeeded or Failed"
Mar  1 13:14:06.617: INFO: Trying to get logs from node lab1-k8s-node-3 pod security-context-7a12b983-140a-450a-abfb-6d89febae55c container test-container: <nil>
STEP: delete the pod 03/01/23 13:14:06.626
Mar  1 13:14:06.641: INFO: Waiting for pod security-context-7a12b983-140a-450a-abfb-6d89febae55c to disappear
Mar  1 13:14:06.646: INFO: Pod security-context-7a12b983-140a-450a-abfb-6d89febae55c no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Mar  1 13:14:06.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-6355" for this suite. 03/01/23 13:14:06.651
{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":336,"skipped":6191,"failed":0}
------------------------------
â€¢ [4.102 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:14:02.561
    Mar  1 13:14:02.561: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename security-context 03/01/23 13:14:02.562
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:14:02.582
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:14:02.586
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:132
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 03/01/23 13:14:02.589
    Mar  1 13:14:02.600: INFO: Waiting up to 5m0s for pod "security-context-7a12b983-140a-450a-abfb-6d89febae55c" in namespace "security-context-6355" to be "Succeeded or Failed"
    Mar  1 13:14:02.606: INFO: Pod "security-context-7a12b983-140a-450a-abfb-6d89febae55c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.210825ms
    Mar  1 13:14:04.611: INFO: Pod "security-context-7a12b983-140a-450a-abfb-6d89febae55c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010424497s
    Mar  1 13:14:06.613: INFO: Pod "security-context-7a12b983-140a-450a-abfb-6d89febae55c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01217657s
    STEP: Saw pod success 03/01/23 13:14:06.613
    Mar  1 13:14:06.613: INFO: Pod "security-context-7a12b983-140a-450a-abfb-6d89febae55c" satisfied condition "Succeeded or Failed"
    Mar  1 13:14:06.617: INFO: Trying to get logs from node lab1-k8s-node-3 pod security-context-7a12b983-140a-450a-abfb-6d89febae55c container test-container: <nil>
    STEP: delete the pod 03/01/23 13:14:06.626
    Mar  1 13:14:06.641: INFO: Waiting for pod security-context-7a12b983-140a-450a-abfb-6d89febae55c to disappear
    Mar  1 13:14:06.646: INFO: Pod security-context-7a12b983-140a-450a-abfb-6d89febae55c no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Mar  1 13:14:06.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-6355" for this suite. 03/01/23 13:14:06.651
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:14:06.665
Mar  1 13:14:06.665: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename projected 03/01/23 13:14:06.666
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:14:06.687
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:14:06.69
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
STEP: Creating a pod to test downward API volume plugin 03/01/23 13:14:06.692
Mar  1 13:14:06.702: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6345745b-d81a-4cb3-9717-ccc0d3bc348f" in namespace "projected-9903" to be "Succeeded or Failed"
Mar  1 13:14:06.706: INFO: Pod "downwardapi-volume-6345745b-d81a-4cb3-9717-ccc0d3bc348f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.435649ms
Mar  1 13:14:08.712: INFO: Pod "downwardapi-volume-6345745b-d81a-4cb3-9717-ccc0d3bc348f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009451772s
Mar  1 13:14:10.711: INFO: Pod "downwardapi-volume-6345745b-d81a-4cb3-9717-ccc0d3bc348f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008405175s
STEP: Saw pod success 03/01/23 13:14:10.711
Mar  1 13:14:10.711: INFO: Pod "downwardapi-volume-6345745b-d81a-4cb3-9717-ccc0d3bc348f" satisfied condition "Succeeded or Failed"
Mar  1 13:14:10.716: INFO: Trying to get logs from node lab1-k8s-node-3 pod downwardapi-volume-6345745b-d81a-4cb3-9717-ccc0d3bc348f container client-container: <nil>
STEP: delete the pod 03/01/23 13:14:10.723
Mar  1 13:14:10.739: INFO: Waiting for pod downwardapi-volume-6345745b-d81a-4cb3-9717-ccc0d3bc348f to disappear
Mar  1 13:14:10.743: INFO: Pod downwardapi-volume-6345745b-d81a-4cb3-9717-ccc0d3bc348f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Mar  1 13:14:10.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9903" for this suite. 03/01/23 13:14:10.75
{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":337,"skipped":6202,"failed":0}
------------------------------
â€¢ [4.094 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:14:06.665
    Mar  1 13:14:06.665: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename projected 03/01/23 13:14:06.666
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:14:06.687
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:14:06.69
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:67
    STEP: Creating a pod to test downward API volume plugin 03/01/23 13:14:06.692
    Mar  1 13:14:06.702: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6345745b-d81a-4cb3-9717-ccc0d3bc348f" in namespace "projected-9903" to be "Succeeded or Failed"
    Mar  1 13:14:06.706: INFO: Pod "downwardapi-volume-6345745b-d81a-4cb3-9717-ccc0d3bc348f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.435649ms
    Mar  1 13:14:08.712: INFO: Pod "downwardapi-volume-6345745b-d81a-4cb3-9717-ccc0d3bc348f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009451772s
    Mar  1 13:14:10.711: INFO: Pod "downwardapi-volume-6345745b-d81a-4cb3-9717-ccc0d3bc348f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008405175s
    STEP: Saw pod success 03/01/23 13:14:10.711
    Mar  1 13:14:10.711: INFO: Pod "downwardapi-volume-6345745b-d81a-4cb3-9717-ccc0d3bc348f" satisfied condition "Succeeded or Failed"
    Mar  1 13:14:10.716: INFO: Trying to get logs from node lab1-k8s-node-3 pod downwardapi-volume-6345745b-d81a-4cb3-9717-ccc0d3bc348f container client-container: <nil>
    STEP: delete the pod 03/01/23 13:14:10.723
    Mar  1 13:14:10.739: INFO: Waiting for pod downwardapi-volume-6345745b-d81a-4cb3-9717-ccc0d3bc348f to disappear
    Mar  1 13:14:10.743: INFO: Pod downwardapi-volume-6345745b-d81a-4cb3-9717-ccc0d3bc348f no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Mar  1 13:14:10.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9903" for this suite. 03/01/23 13:14:10.75
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:14:10.762
Mar  1 13:14:10.762: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename services 03/01/23 13:14:10.763
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:14:10.782
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:14:10.785
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231
STEP: creating an Endpoint 03/01/23 13:14:10.791
STEP: waiting for available Endpoint 03/01/23 13:14:10.797
STEP: listing all Endpoints 03/01/23 13:14:10.798
STEP: updating the Endpoint 03/01/23 13:14:10.803
STEP: fetching the Endpoint 03/01/23 13:14:10.809
STEP: patching the Endpoint 03/01/23 13:14:10.813
STEP: fetching the Endpoint 03/01/23 13:14:10.823
STEP: deleting the Endpoint by Collection 03/01/23 13:14:10.827
STEP: waiting for Endpoint deletion 03/01/23 13:14:10.837
STEP: fetching the Endpoint 03/01/23 13:14:10.839
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar  1 13:14:10.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2075" for this suite. 03/01/23 13:14:10.847
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","completed":338,"skipped":6208,"failed":0}
------------------------------
â€¢ [0.094 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:14:10.762
    Mar  1 13:14:10.762: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename services 03/01/23 13:14:10.763
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:14:10.782
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:14:10.785
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3231
    STEP: creating an Endpoint 03/01/23 13:14:10.791
    STEP: waiting for available Endpoint 03/01/23 13:14:10.797
    STEP: listing all Endpoints 03/01/23 13:14:10.798
    STEP: updating the Endpoint 03/01/23 13:14:10.803
    STEP: fetching the Endpoint 03/01/23 13:14:10.809
    STEP: patching the Endpoint 03/01/23 13:14:10.813
    STEP: fetching the Endpoint 03/01/23 13:14:10.823
    STEP: deleting the Endpoint by Collection 03/01/23 13:14:10.827
    STEP: waiting for Endpoint deletion 03/01/23 13:14:10.837
    STEP: fetching the Endpoint 03/01/23 13:14:10.839
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar  1 13:14:10.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-2075" for this suite. 03/01/23 13:14:10.847
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:14:10.857
Mar  1 13:14:10.857: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename deployment 03/01/23 13:14:10.858
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:14:10.881
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:14:10.883
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Mar  1 13:14:10.886: INFO: Creating deployment "test-recreate-deployment"
Mar  1 13:14:10.891: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Mar  1 13:14:10.900: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Mar  1 13:14:12.909: INFO: Waiting deployment "test-recreate-deployment" to complete
Mar  1 13:14:12.913: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Mar  1 13:14:12.926: INFO: Updating deployment test-recreate-deployment
Mar  1 13:14:12.926: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar  1 13:14:13.013: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-6082  10d83bac-ec46-4724-ba23-1813672395b0 46610 2 2023-03-01 13:14:10 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-03-01 13:14:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-01 13:14:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0022332c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-03-01 13:14:12 +0000 UTC,LastTransitionTime:2023-03-01 13:14:12 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-9d58999df" is progressing.,LastUpdateTime:2023-03-01 13:14:13 +0000 UTC,LastTransitionTime:2023-03-01 13:14:10 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Mar  1 13:14:13.017: INFO: New ReplicaSet "test-recreate-deployment-9d58999df" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-9d58999df  deployment-6082  f2f1bcc5-d018-40be-8c18-50d501d7e097 46606 1 2023-03-01 13:14:12 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 10d83bac-ec46-4724-ba23-1813672395b0 0xc002b59570 0xc002b59571}] [] [{kube-controller-manager Update apps/v1 2023-03-01 13:14:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"10d83bac-ec46-4724-ba23-1813672395b0\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-01 13:14:12 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 9d58999df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002b59608 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  1 13:14:13.017: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Mar  1 13:14:13.018: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d8b6f647f  deployment-6082  b15af191-869d-4142-8ad7-abd268c29fd9 46598 2 2023-03-01 13:14:10 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 10d83bac-ec46-4724-ba23-1813672395b0 0xc002b59457 0xc002b59458}] [] [{kube-controller-manager Update apps/v1 2023-03-01 13:14:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"10d83bac-ec46-4724-ba23-1813672395b0\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-01 13:14:12 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d8b6f647f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002b59508 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  1 13:14:13.022: INFO: Pod "test-recreate-deployment-9d58999df-7rdm8" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-9d58999df-7rdm8 test-recreate-deployment-9d58999df- deployment-6082  185e3f87-5ac7-44ec-b7f2-eff893b999e4 46609 0 2023-03-01 13:14:12 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [{apps/v1 ReplicaSet test-recreate-deployment-9d58999df f2f1bcc5-d018-40be-8c18-50d501d7e097 0xc002b59a40 0xc002b59a41}] [] [{kube-controller-manager Update v1 2023-03-01 13:14:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f2f1bcc5-d018-40be-8c18-50d501d7e097\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-01 13:14:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m2h28,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m2h28,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:14:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:14:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:14:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:14:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.2.241,PodIP:,StartTime:2023-03-01 13:14:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Mar  1 13:14:13.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6082" for this suite. 03/01/23 13:14:13.028
{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","completed":339,"skipped":6214,"failed":0}
------------------------------
â€¢ [2.182 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:14:10.857
    Mar  1 13:14:10.857: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename deployment 03/01/23 13:14:10.858
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:14:10.881
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:14:10.883
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Mar  1 13:14:10.886: INFO: Creating deployment "test-recreate-deployment"
    Mar  1 13:14:10.891: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Mar  1 13:14:10.900: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
    Mar  1 13:14:12.909: INFO: Waiting deployment "test-recreate-deployment" to complete
    Mar  1 13:14:12.913: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Mar  1 13:14:12.926: INFO: Updating deployment test-recreate-deployment
    Mar  1 13:14:12.926: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar  1 13:14:13.013: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-6082  10d83bac-ec46-4724-ba23-1813672395b0 46610 2 2023-03-01 13:14:10 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-03-01 13:14:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-01 13:14:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0022332c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-03-01 13:14:12 +0000 UTC,LastTransitionTime:2023-03-01 13:14:12 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-9d58999df" is progressing.,LastUpdateTime:2023-03-01 13:14:13 +0000 UTC,LastTransitionTime:2023-03-01 13:14:10 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Mar  1 13:14:13.017: INFO: New ReplicaSet "test-recreate-deployment-9d58999df" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-9d58999df  deployment-6082  f2f1bcc5-d018-40be-8c18-50d501d7e097 46606 1 2023-03-01 13:14:12 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 10d83bac-ec46-4724-ba23-1813672395b0 0xc002b59570 0xc002b59571}] [] [{kube-controller-manager Update apps/v1 2023-03-01 13:14:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"10d83bac-ec46-4724-ba23-1813672395b0\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-01 13:14:12 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 9d58999df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002b59608 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Mar  1 13:14:13.017: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Mar  1 13:14:13.018: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d8b6f647f  deployment-6082  b15af191-869d-4142-8ad7-abd268c29fd9 46598 2 2023-03-01 13:14:10 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 10d83bac-ec46-4724-ba23-1813672395b0 0xc002b59457 0xc002b59458}] [] [{kube-controller-manager Update apps/v1 2023-03-01 13:14:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"10d83bac-ec46-4724-ba23-1813672395b0\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-01 13:14:12 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d8b6f647f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002b59508 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Mar  1 13:14:13.022: INFO: Pod "test-recreate-deployment-9d58999df-7rdm8" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-9d58999df-7rdm8 test-recreate-deployment-9d58999df- deployment-6082  185e3f87-5ac7-44ec-b7f2-eff893b999e4 46609 0 2023-03-01 13:14:12 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [{apps/v1 ReplicaSet test-recreate-deployment-9d58999df f2f1bcc5-d018-40be-8c18-50d501d7e097 0xc002b59a40 0xc002b59a41}] [] [{kube-controller-manager Update v1 2023-03-01 13:14:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f2f1bcc5-d018-40be-8c18-50d501d7e097\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-01 13:14:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m2h28,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m2h28,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:14:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:14:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:14:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:14:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.2.241,PodIP:,StartTime:2023-03-01 13:14:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Mar  1 13:14:13.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-6082" for this suite. 03/01/23 13:14:13.028
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:14:13.039
Mar  1 13:14:13.039: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename projected 03/01/23 13:14:13.04
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:14:13.072
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:14:13.075
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
STEP: Creating configMap with name projected-configmap-test-volume-map-af6c7f4c-6559-418a-90cd-dfa1812cae01 03/01/23 13:14:13.077
STEP: Creating a pod to test consume configMaps 03/01/23 13:14:13.083
Mar  1 13:14:13.094: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0ed25671-3d1b-4fb5-820d-9268fa30ecfb" in namespace "projected-9338" to be "Succeeded or Failed"
Mar  1 13:14:13.101: INFO: Pod "pod-projected-configmaps-0ed25671-3d1b-4fb5-820d-9268fa30ecfb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.83876ms
Mar  1 13:14:15.107: INFO: Pod "pod-projected-configmaps-0ed25671-3d1b-4fb5-820d-9268fa30ecfb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012870947s
Mar  1 13:14:17.107: INFO: Pod "pod-projected-configmaps-0ed25671-3d1b-4fb5-820d-9268fa30ecfb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0135896s
STEP: Saw pod success 03/01/23 13:14:17.108
Mar  1 13:14:17.108: INFO: Pod "pod-projected-configmaps-0ed25671-3d1b-4fb5-820d-9268fa30ecfb" satisfied condition "Succeeded or Failed"
Mar  1 13:14:17.114: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-projected-configmaps-0ed25671-3d1b-4fb5-820d-9268fa30ecfb container agnhost-container: <nil>
STEP: delete the pod 03/01/23 13:14:17.123
Mar  1 13:14:17.139: INFO: Waiting for pod pod-projected-configmaps-0ed25671-3d1b-4fb5-820d-9268fa30ecfb to disappear
Mar  1 13:14:17.145: INFO: Pod pod-projected-configmaps-0ed25671-3d1b-4fb5-820d-9268fa30ecfb no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Mar  1 13:14:17.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9338" for this suite. 03/01/23 13:14:17.153
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":340,"skipped":6224,"failed":0}
------------------------------
â€¢ [4.125 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:14:13.039
    Mar  1 13:14:13.039: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename projected 03/01/23 13:14:13.04
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:14:13.072
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:14:13.075
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:98
    STEP: Creating configMap with name projected-configmap-test-volume-map-af6c7f4c-6559-418a-90cd-dfa1812cae01 03/01/23 13:14:13.077
    STEP: Creating a pod to test consume configMaps 03/01/23 13:14:13.083
    Mar  1 13:14:13.094: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0ed25671-3d1b-4fb5-820d-9268fa30ecfb" in namespace "projected-9338" to be "Succeeded or Failed"
    Mar  1 13:14:13.101: INFO: Pod "pod-projected-configmaps-0ed25671-3d1b-4fb5-820d-9268fa30ecfb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.83876ms
    Mar  1 13:14:15.107: INFO: Pod "pod-projected-configmaps-0ed25671-3d1b-4fb5-820d-9268fa30ecfb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012870947s
    Mar  1 13:14:17.107: INFO: Pod "pod-projected-configmaps-0ed25671-3d1b-4fb5-820d-9268fa30ecfb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0135896s
    STEP: Saw pod success 03/01/23 13:14:17.108
    Mar  1 13:14:17.108: INFO: Pod "pod-projected-configmaps-0ed25671-3d1b-4fb5-820d-9268fa30ecfb" satisfied condition "Succeeded or Failed"
    Mar  1 13:14:17.114: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-projected-configmaps-0ed25671-3d1b-4fb5-820d-9268fa30ecfb container agnhost-container: <nil>
    STEP: delete the pod 03/01/23 13:14:17.123
    Mar  1 13:14:17.139: INFO: Waiting for pod pod-projected-configmaps-0ed25671-3d1b-4fb5-820d-9268fa30ecfb to disappear
    Mar  1 13:14:17.145: INFO: Pod pod-projected-configmaps-0ed25671-3d1b-4fb5-820d-9268fa30ecfb no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Mar  1 13:14:17.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9338" for this suite. 03/01/23 13:14:17.153
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:14:17.167
Mar  1 13:14:17.167: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename crd-publish-openapi 03/01/23 13:14:17.167
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:14:17.188
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:14:17.191
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
STEP: set up a multi version CRD 03/01/23 13:14:17.193
Mar  1 13:14:17.194: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: rename a version 03/01/23 13:14:30.592
STEP: check the new version name is served 03/01/23 13:14:30.61
STEP: check the old version name is removed 03/01/23 13:14:32.737
STEP: check the other version is not changed 03/01/23 13:14:34.141
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  1 13:14:39.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7165" for this suite. 03/01/23 13:14:39.618
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","completed":341,"skipped":6248,"failed":0}
------------------------------
â€¢ [SLOW TEST] [22.465 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:14:17.167
    Mar  1 13:14:17.167: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename crd-publish-openapi 03/01/23 13:14:17.167
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:14:17.188
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:14:17.191
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:390
    STEP: set up a multi version CRD 03/01/23 13:14:17.193
    Mar  1 13:14:17.194: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: rename a version 03/01/23 13:14:30.592
    STEP: check the new version name is served 03/01/23 13:14:30.61
    STEP: check the old version name is removed 03/01/23 13:14:32.737
    STEP: check the other version is not changed 03/01/23 13:14:34.141
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  1 13:14:39.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-7165" for this suite. 03/01/23 13:14:39.618
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:14:39.633
Mar  1 13:14:39.633: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename secrets 03/01/23 13:14:39.634
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:14:39.656
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:14:39.659
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
STEP: creating secret secrets-5929/secret-test-6374978f-680e-4718-8a39-9898d13d6c22 03/01/23 13:14:39.663
STEP: Creating a pod to test consume secrets 03/01/23 13:14:39.669
Mar  1 13:14:39.680: INFO: Waiting up to 5m0s for pod "pod-configmaps-8cdd59dd-7c1c-4a23-81c5-b78bf1439df8" in namespace "secrets-5929" to be "Succeeded or Failed"
Mar  1 13:14:39.687: INFO: Pod "pod-configmaps-8cdd59dd-7c1c-4a23-81c5-b78bf1439df8": Phase="Pending", Reason="", readiness=false. Elapsed: 7.492347ms
Mar  1 13:14:41.696: INFO: Pod "pod-configmaps-8cdd59dd-7c1c-4a23-81c5-b78bf1439df8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01609637s
Mar  1 13:14:43.693: INFO: Pod "pod-configmaps-8cdd59dd-7c1c-4a23-81c5-b78bf1439df8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012764076s
STEP: Saw pod success 03/01/23 13:14:43.693
Mar  1 13:14:43.693: INFO: Pod "pod-configmaps-8cdd59dd-7c1c-4a23-81c5-b78bf1439df8" satisfied condition "Succeeded or Failed"
Mar  1 13:14:43.697: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-configmaps-8cdd59dd-7c1c-4a23-81c5-b78bf1439df8 container env-test: <nil>
STEP: delete the pod 03/01/23 13:14:43.714
Mar  1 13:14:43.733: INFO: Waiting for pod pod-configmaps-8cdd59dd-7c1c-4a23-81c5-b78bf1439df8 to disappear
Mar  1 13:14:43.742: INFO: Pod pod-configmaps-8cdd59dd-7c1c-4a23-81c5-b78bf1439df8 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Mar  1 13:14:43.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5929" for this suite. 03/01/23 13:14:43.751
{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","completed":342,"skipped":6251,"failed":0}
------------------------------
â€¢ [4.128 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:14:39.633
    Mar  1 13:14:39.633: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename secrets 03/01/23 13:14:39.634
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:14:39.656
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:14:39.659
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:94
    STEP: creating secret secrets-5929/secret-test-6374978f-680e-4718-8a39-9898d13d6c22 03/01/23 13:14:39.663
    STEP: Creating a pod to test consume secrets 03/01/23 13:14:39.669
    Mar  1 13:14:39.680: INFO: Waiting up to 5m0s for pod "pod-configmaps-8cdd59dd-7c1c-4a23-81c5-b78bf1439df8" in namespace "secrets-5929" to be "Succeeded or Failed"
    Mar  1 13:14:39.687: INFO: Pod "pod-configmaps-8cdd59dd-7c1c-4a23-81c5-b78bf1439df8": Phase="Pending", Reason="", readiness=false. Elapsed: 7.492347ms
    Mar  1 13:14:41.696: INFO: Pod "pod-configmaps-8cdd59dd-7c1c-4a23-81c5-b78bf1439df8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01609637s
    Mar  1 13:14:43.693: INFO: Pod "pod-configmaps-8cdd59dd-7c1c-4a23-81c5-b78bf1439df8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012764076s
    STEP: Saw pod success 03/01/23 13:14:43.693
    Mar  1 13:14:43.693: INFO: Pod "pod-configmaps-8cdd59dd-7c1c-4a23-81c5-b78bf1439df8" satisfied condition "Succeeded or Failed"
    Mar  1 13:14:43.697: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-configmaps-8cdd59dd-7c1c-4a23-81c5-b78bf1439df8 container env-test: <nil>
    STEP: delete the pod 03/01/23 13:14:43.714
    Mar  1 13:14:43.733: INFO: Waiting for pod pod-configmaps-8cdd59dd-7c1c-4a23-81c5-b78bf1439df8 to disappear
    Mar  1 13:14:43.742: INFO: Pod pod-configmaps-8cdd59dd-7c1c-4a23-81c5-b78bf1439df8 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Mar  1 13:14:43.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-5929" for this suite. 03/01/23 13:14:43.751
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:14:43.762
Mar  1 13:14:43.762: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename security-context 03/01/23 13:14:43.763
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:14:43.783
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:14:43.786
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 03/01/23 13:14:43.789
Mar  1 13:14:43.800: INFO: Waiting up to 5m0s for pod "security-context-db08fc54-b927-4501-9aa6-fb4a40e28ec0" in namespace "security-context-3395" to be "Succeeded or Failed"
Mar  1 13:14:43.805: INFO: Pod "security-context-db08fc54-b927-4501-9aa6-fb4a40e28ec0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.784601ms
Mar  1 13:14:45.811: INFO: Pod "security-context-db08fc54-b927-4501-9aa6-fb4a40e28ec0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010990696s
Mar  1 13:14:47.810: INFO: Pod "security-context-db08fc54-b927-4501-9aa6-fb4a40e28ec0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010096263s
STEP: Saw pod success 03/01/23 13:14:47.81
Mar  1 13:14:47.811: INFO: Pod "security-context-db08fc54-b927-4501-9aa6-fb4a40e28ec0" satisfied condition "Succeeded or Failed"
Mar  1 13:14:47.816: INFO: Trying to get logs from node lab1-k8s-node-3 pod security-context-db08fc54-b927-4501-9aa6-fb4a40e28ec0 container test-container: <nil>
STEP: delete the pod 03/01/23 13:14:47.826
Mar  1 13:14:47.844: INFO: Waiting for pod security-context-db08fc54-b927-4501-9aa6-fb4a40e28ec0 to disappear
Mar  1 13:14:47.848: INFO: Pod security-context-db08fc54-b927-4501-9aa6-fb4a40e28ec0 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Mar  1 13:14:47.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-3395" for this suite. 03/01/23 13:14:47.854
{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":343,"skipped":6274,"failed":0}
------------------------------
â€¢ [4.109 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:14:43.762
    Mar  1 13:14:43.762: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename security-context 03/01/23 13:14:43.763
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:14:43.783
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:14:43.786
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:97
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 03/01/23 13:14:43.789
    Mar  1 13:14:43.800: INFO: Waiting up to 5m0s for pod "security-context-db08fc54-b927-4501-9aa6-fb4a40e28ec0" in namespace "security-context-3395" to be "Succeeded or Failed"
    Mar  1 13:14:43.805: INFO: Pod "security-context-db08fc54-b927-4501-9aa6-fb4a40e28ec0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.784601ms
    Mar  1 13:14:45.811: INFO: Pod "security-context-db08fc54-b927-4501-9aa6-fb4a40e28ec0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010990696s
    Mar  1 13:14:47.810: INFO: Pod "security-context-db08fc54-b927-4501-9aa6-fb4a40e28ec0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010096263s
    STEP: Saw pod success 03/01/23 13:14:47.81
    Mar  1 13:14:47.811: INFO: Pod "security-context-db08fc54-b927-4501-9aa6-fb4a40e28ec0" satisfied condition "Succeeded or Failed"
    Mar  1 13:14:47.816: INFO: Trying to get logs from node lab1-k8s-node-3 pod security-context-db08fc54-b927-4501-9aa6-fb4a40e28ec0 container test-container: <nil>
    STEP: delete the pod 03/01/23 13:14:47.826
    Mar  1 13:14:47.844: INFO: Waiting for pod security-context-db08fc54-b927-4501-9aa6-fb4a40e28ec0 to disappear
    Mar  1 13:14:47.848: INFO: Pod security-context-db08fc54-b927-4501-9aa6-fb4a40e28ec0 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Mar  1 13:14:47.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-3395" for this suite. 03/01/23 13:14:47.854
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:14:47.874
Mar  1 13:14:47.875: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename emptydir 03/01/23 13:14:47.875
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:14:47.9
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:14:47.904
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
STEP: Creating a pod to test emptydir 0777 on node default medium 03/01/23 13:14:47.907
Mar  1 13:14:47.926: INFO: Waiting up to 5m0s for pod "pod-b415a60c-e32f-41ee-b686-a956ffe06143" in namespace "emptydir-8869" to be "Succeeded or Failed"
Mar  1 13:14:47.932: INFO: Pod "pod-b415a60c-e32f-41ee-b686-a956ffe06143": Phase="Pending", Reason="", readiness=false. Elapsed: 5.663558ms
Mar  1 13:14:49.940: INFO: Pod "pod-b415a60c-e32f-41ee-b686-a956ffe06143": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013881307s
Mar  1 13:14:51.938: INFO: Pod "pod-b415a60c-e32f-41ee-b686-a956ffe06143": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011870077s
STEP: Saw pod success 03/01/23 13:14:51.938
Mar  1 13:14:51.938: INFO: Pod "pod-b415a60c-e32f-41ee-b686-a956ffe06143" satisfied condition "Succeeded or Failed"
Mar  1 13:14:51.944: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-b415a60c-e32f-41ee-b686-a956ffe06143 container test-container: <nil>
STEP: delete the pod 03/01/23 13:14:51.951
Mar  1 13:14:51.966: INFO: Waiting for pod pod-b415a60c-e32f-41ee-b686-a956ffe06143 to disappear
Mar  1 13:14:51.970: INFO: Pod pod-b415a60c-e32f-41ee-b686-a956ffe06143 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar  1 13:14:51.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8869" for this suite. 03/01/23 13:14:51.977
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":344,"skipped":6286,"failed":0}
------------------------------
â€¢ [4.112 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:14:47.874
    Mar  1 13:14:47.875: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename emptydir 03/01/23 13:14:47.875
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:14:47.9
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:14:47.904
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:186
    STEP: Creating a pod to test emptydir 0777 on node default medium 03/01/23 13:14:47.907
    Mar  1 13:14:47.926: INFO: Waiting up to 5m0s for pod "pod-b415a60c-e32f-41ee-b686-a956ffe06143" in namespace "emptydir-8869" to be "Succeeded or Failed"
    Mar  1 13:14:47.932: INFO: Pod "pod-b415a60c-e32f-41ee-b686-a956ffe06143": Phase="Pending", Reason="", readiness=false. Elapsed: 5.663558ms
    Mar  1 13:14:49.940: INFO: Pod "pod-b415a60c-e32f-41ee-b686-a956ffe06143": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013881307s
    Mar  1 13:14:51.938: INFO: Pod "pod-b415a60c-e32f-41ee-b686-a956ffe06143": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011870077s
    STEP: Saw pod success 03/01/23 13:14:51.938
    Mar  1 13:14:51.938: INFO: Pod "pod-b415a60c-e32f-41ee-b686-a956ffe06143" satisfied condition "Succeeded or Failed"
    Mar  1 13:14:51.944: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-b415a60c-e32f-41ee-b686-a956ffe06143 container test-container: <nil>
    STEP: delete the pod 03/01/23 13:14:51.951
    Mar  1 13:14:51.966: INFO: Waiting for pod pod-b415a60c-e32f-41ee-b686-a956ffe06143 to disappear
    Mar  1 13:14:51.970: INFO: Pod pod-b415a60c-e32f-41ee-b686-a956ffe06143 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar  1 13:14:51.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-8869" for this suite. 03/01/23 13:14:51.977
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:14:51.987
Mar  1 13:14:51.987: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename projected 03/01/23 13:14:51.988
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:14:52.012
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:14:52.016
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
STEP: Creating projection with secret that has name projected-secret-test-3103f1e5-96b0-4dee-a067-af788d3f4de4 03/01/23 13:14:52.019
STEP: Creating a pod to test consume secrets 03/01/23 13:14:52.026
Mar  1 13:14:52.039: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1b0e219e-52a8-4df4-8bd3-8a3824167c48" in namespace "projected-7058" to be "Succeeded or Failed"
Mar  1 13:14:52.044: INFO: Pod "pod-projected-secrets-1b0e219e-52a8-4df4-8bd3-8a3824167c48": Phase="Pending", Reason="", readiness=false. Elapsed: 4.862598ms
Mar  1 13:14:54.053: INFO: Pod "pod-projected-secrets-1b0e219e-52a8-4df4-8bd3-8a3824167c48": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013124357s
Mar  1 13:14:56.049: INFO: Pod "pod-projected-secrets-1b0e219e-52a8-4df4-8bd3-8a3824167c48": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009872209s
STEP: Saw pod success 03/01/23 13:14:56.049
Mar  1 13:14:56.050: INFO: Pod "pod-projected-secrets-1b0e219e-52a8-4df4-8bd3-8a3824167c48" satisfied condition "Succeeded or Failed"
Mar  1 13:14:56.054: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-projected-secrets-1b0e219e-52a8-4df4-8bd3-8a3824167c48 container projected-secret-volume-test: <nil>
STEP: delete the pod 03/01/23 13:14:56.063
Mar  1 13:14:56.078: INFO: Waiting for pod pod-projected-secrets-1b0e219e-52a8-4df4-8bd3-8a3824167c48 to disappear
Mar  1 13:14:56.082: INFO: Pod pod-projected-secrets-1b0e219e-52a8-4df4-8bd3-8a3824167c48 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Mar  1 13:14:56.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7058" for this suite. 03/01/23 13:14:56.09
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":345,"skipped":6292,"failed":0}
------------------------------
â€¢ [4.112 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:14:51.987
    Mar  1 13:14:51.987: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename projected 03/01/23 13:14:51.988
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:14:52.012
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:14:52.016
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:66
    STEP: Creating projection with secret that has name projected-secret-test-3103f1e5-96b0-4dee-a067-af788d3f4de4 03/01/23 13:14:52.019
    STEP: Creating a pod to test consume secrets 03/01/23 13:14:52.026
    Mar  1 13:14:52.039: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1b0e219e-52a8-4df4-8bd3-8a3824167c48" in namespace "projected-7058" to be "Succeeded or Failed"
    Mar  1 13:14:52.044: INFO: Pod "pod-projected-secrets-1b0e219e-52a8-4df4-8bd3-8a3824167c48": Phase="Pending", Reason="", readiness=false. Elapsed: 4.862598ms
    Mar  1 13:14:54.053: INFO: Pod "pod-projected-secrets-1b0e219e-52a8-4df4-8bd3-8a3824167c48": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013124357s
    Mar  1 13:14:56.049: INFO: Pod "pod-projected-secrets-1b0e219e-52a8-4df4-8bd3-8a3824167c48": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009872209s
    STEP: Saw pod success 03/01/23 13:14:56.049
    Mar  1 13:14:56.050: INFO: Pod "pod-projected-secrets-1b0e219e-52a8-4df4-8bd3-8a3824167c48" satisfied condition "Succeeded or Failed"
    Mar  1 13:14:56.054: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-projected-secrets-1b0e219e-52a8-4df4-8bd3-8a3824167c48 container projected-secret-volume-test: <nil>
    STEP: delete the pod 03/01/23 13:14:56.063
    Mar  1 13:14:56.078: INFO: Waiting for pod pod-projected-secrets-1b0e219e-52a8-4df4-8bd3-8a3824167c48 to disappear
    Mar  1 13:14:56.082: INFO: Pod pod-projected-secrets-1b0e219e-52a8-4df4-8bd3-8a3824167c48 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Mar  1 13:14:56.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7058" for this suite. 03/01/23 13:14:56.09
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:14:56.1
Mar  1 13:14:56.100: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename kubelet-test 03/01/23 13:14:56.101
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:14:56.127
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:14:56.131
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Mar  1 13:15:00.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8652" for this suite. 03/01/23 13:15:00.165
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","completed":346,"skipped":6293,"failed":0}
------------------------------
â€¢ [4.074 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:14:56.1
    Mar  1 13:14:56.100: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename kubelet-test 03/01/23 13:14:56.101
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:14:56.127
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:14:56.131
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Mar  1 13:15:00.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-8652" for this suite. 03/01/23 13:15:00.165
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:15:00.18
Mar  1 13:15:00.180: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename kubectl 03/01/23 13:15:00.181
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:15:00.204
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:15:00.209
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
STEP: validating cluster-info 03/01/23 13:15:00.212
Mar  1 13:15:00.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-2888 cluster-info'
Mar  1 13:15:00.274: INFO: stderr: ""
Mar  1 13:15:00.274: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.233.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar  1 13:15:00.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2888" for this suite. 03/01/23 13:15:00.283
{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","completed":347,"skipped":6320,"failed":0}
------------------------------
â€¢ [0.112 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1242
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:15:00.18
    Mar  1 13:15:00.180: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename kubectl 03/01/23 13:15:00.181
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:15:00.204
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:15:00.209
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1248
    STEP: validating cluster-info 03/01/23 13:15:00.212
    Mar  1 13:15:00.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=kubectl-2888 cluster-info'
    Mar  1 13:15:00.274: INFO: stderr: ""
    Mar  1 13:15:00.274: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.233.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar  1 13:15:00.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-2888" for this suite. 03/01/23 13:15:00.283
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:15:00.297
Mar  1 13:15:00.297: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename deployment 03/01/23 13:15:00.298
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:15:00.317
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:15:00.32
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 03/01/23 13:15:00.329
STEP: waiting for Deployment to be created 03/01/23 13:15:00.336
STEP: waiting for all Replicas to be Ready 03/01/23 13:15:00.338
Mar  1 13:15:00.340: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  1 13:15:00.340: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  1 13:15:00.350: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  1 13:15:00.350: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  1 13:15:00.381: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  1 13:15:00.381: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  1 13:15:00.415: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  1 13:15:00.415: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  1 13:15:01.569: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Mar  1 13:15:01.569: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Mar  1 13:15:01.905: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 03/01/23 13:15:01.905
W0301 13:15:01.913942      19 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Mar  1 13:15:01.916: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 03/01/23 13:15:01.916
Mar  1 13:15:01.918: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 0
Mar  1 13:15:01.918: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 0
Mar  1 13:15:01.918: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 0
Mar  1 13:15:01.918: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 0
Mar  1 13:15:01.918: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 0
Mar  1 13:15:01.918: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 0
Mar  1 13:15:01.918: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 0
Mar  1 13:15:01.918: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 0
Mar  1 13:15:01.919: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 1
Mar  1 13:15:01.919: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 1
Mar  1 13:15:01.919: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 2
Mar  1 13:15:01.919: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 2
Mar  1 13:15:01.919: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 2
Mar  1 13:15:01.919: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 2
Mar  1 13:15:01.933: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 2
Mar  1 13:15:01.933: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 2
Mar  1 13:15:01.955: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 2
Mar  1 13:15:01.955: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 2
Mar  1 13:15:01.975: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 1
Mar  1 13:15:01.975: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 1
Mar  1 13:15:01.989: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 1
Mar  1 13:15:01.989: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 1
Mar  1 13:15:02.921: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 2
Mar  1 13:15:02.921: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 2
Mar  1 13:15:02.957: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 1
STEP: listing Deployments 03/01/23 13:15:02.957
Mar  1 13:15:02.963: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 03/01/23 13:15:02.963
Mar  1 13:15:02.978: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 03/01/23 13:15:02.978
Mar  1 13:15:02.988: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar  1 13:15:02.995: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar  1 13:15:03.013: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar  1 13:15:03.034: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar  1 13:15:03.048: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar  1 13:15:03.938: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Mar  1 13:15:03.962: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Mar  1 13:15:03.975: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Mar  1 13:15:03.991: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Mar  1 13:15:04.010: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Mar  1 13:15:04.893: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 03/01/23 13:15:04.923
STEP: fetching the DeploymentStatus 03/01/23 13:15:04.934
Mar  1 13:15:04.942: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 1
Mar  1 13:15:04.942: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 1
Mar  1 13:15:04.942: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 1
Mar  1 13:15:04.942: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 1
Mar  1 13:15:04.942: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 1
Mar  1 13:15:04.942: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 2
Mar  1 13:15:04.942: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 2
Mar  1 13:15:04.942: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 2
Mar  1 13:15:04.943: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 2
Mar  1 13:15:04.943: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 2
Mar  1 13:15:04.943: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 3
STEP: deleting the Deployment 03/01/23 13:15:04.943
Mar  1 13:15:04.959: INFO: observed event type MODIFIED
Mar  1 13:15:04.959: INFO: observed event type MODIFIED
Mar  1 13:15:04.959: INFO: observed event type MODIFIED
Mar  1 13:15:04.960: INFO: observed event type MODIFIED
Mar  1 13:15:04.960: INFO: observed event type MODIFIED
Mar  1 13:15:04.960: INFO: observed event type MODIFIED
Mar  1 13:15:04.960: INFO: observed event type MODIFIED
Mar  1 13:15:04.960: INFO: observed event type MODIFIED
Mar  1 13:15:04.960: INFO: observed event type MODIFIED
Mar  1 13:15:04.961: INFO: observed event type MODIFIED
Mar  1 13:15:04.961: INFO: observed event type MODIFIED
Mar  1 13:15:04.961: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar  1 13:15:04.966: INFO: Log out all the ReplicaSets if there is no deployment created
Mar  1 13:15:04.972: INFO: ReplicaSet "test-deployment-54cc775c4b":
&ReplicaSet{ObjectMeta:{test-deployment-54cc775c4b  deployment-3317  a31c48c3-8e13-48fe-afb3-4cbf396d3b89 47172 4 2023-03-01 13:15:01 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 6eec8877-2d68-47d7-a1df-29169cde9ef3 0xc004923667 0xc004923668}] [] [{kube-controller-manager Update apps/v1 2023-03-01 13:15:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6eec8877-2d68-47d7-a1df-29169cde9ef3\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-01 13:15:04 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 54cc775c4b,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0049236f0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Mar  1 13:15:04.979: INFO: pod: "test-deployment-54cc775c4b-qhcmg":
&Pod{ObjectMeta:{test-deployment-54cc775c4b-qhcmg test-deployment-54cc775c4b- deployment-3317  8055a9ac-8d36-4423-955d-e1f797c50a4d 47155 0 2023-03-01 13:15:02 +0000 UTC 2023-03-01 13:15:04 +0000 UTC 0xc004923b48 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[cni.projectcalico.org/containerID:46e7adb9ae1cc4a48fe1c96c483a27a5c3a322722a1e728b74bc4d0bc321cffd cni.projectcalico.org/podIP:10.233.95.190/32 cni.projectcalico.org/podIPs:10.233.95.190/32] [{apps/v1 ReplicaSet test-deployment-54cc775c4b a31c48c3-8e13-48fe-afb3-4cbf396d3b89 0xc004923b97 0xc004923b98}] [] [{kube-controller-manager Update v1 2023-03-01 13:15:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a31c48c3-8e13-48fe-afb3-4cbf396d3b89\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-01 13:15:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-01 13:15:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.95.190\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q9vzm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q9vzm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:15:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:15:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:15:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:15:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.178,PodIP:10.233.95.190,StartTime:2023-03-01 13:15:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-01 13:15:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:containerd://1c7649b178a74a456d4c8ddf63f136f416bea60db71257d78a19b3c7d213506a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.95.190,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Mar  1 13:15:04.980: INFO: pod: "test-deployment-54cc775c4b-xzg2w":
&Pod{ObjectMeta:{test-deployment-54cc775c4b-xzg2w test-deployment-54cc775c4b- deployment-3317  a1f04440-7925-44d6-9dc5-d204d2055162 47168 0 2023-03-01 13:15:01 +0000 UTC 2023-03-01 13:15:05 +0000 UTC 0xc004923d80 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[cni.projectcalico.org/containerID:564182f6c1afbb6aedb19b47a8f245c8c5375b96218323d61f4d803f40ec8f4b cni.projectcalico.org/podIP:10.233.74.170/32 cni.projectcalico.org/podIPs:10.233.74.170/32] [{apps/v1 ReplicaSet test-deployment-54cc775c4b a31c48c3-8e13-48fe-afb3-4cbf396d3b89 0xc004923dd7 0xc004923dd8}] [] [{kube-controller-manager Update v1 2023-03-01 13:15:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a31c48c3-8e13-48fe-afb3-4cbf396d3b89\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-01 13:15:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-01 13:15:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.74.170\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sh4qn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sh4qn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:15:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:15:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:15:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:15:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.2.241,PodIP:10.233.74.170,StartTime:2023-03-01 13:15:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-01 13:15:02 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:containerd://7869e7f8be2c213eda833f2b2ddf1bed8f6eb41af54a81b8768bdbe5d340d985,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.74.170,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Mar  1 13:15:04.980: INFO: ReplicaSet "test-deployment-7c7d8d58c8":
&ReplicaSet{ObjectMeta:{test-deployment-7c7d8d58c8  deployment-3317  72d8273c-b551-4929-87f2-26f91f36cd9c 47164 2 2023-03-01 13:15:03 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 6eec8877-2d68-47d7-a1df-29169cde9ef3 0xc004923757 0xc004923758}] [] [{kube-controller-manager Update apps/v1 2023-03-01 13:15:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6eec8877-2d68-47d7-a1df-29169cde9ef3\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-01 13:15:04 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7c7d8d58c8,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0049237e0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Mar  1 13:15:04.987: INFO: pod: "test-deployment-7c7d8d58c8-fprvj":
&Pod{ObjectMeta:{test-deployment-7c7d8d58c8-fprvj test-deployment-7c7d8d58c8- deployment-3317  aceeb4c0-cfbc-4353-931d-8a3314e4d73a 47121 0 2023-03-01 13:15:03 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cni.projectcalico.org/containerID:8b8040b50bbc1d36fd3969a4c7047081231a25438b9e3d07c6a64cb576f3ad5e cni.projectcalico.org/podIP:10.233.74.159/32 cni.projectcalico.org/podIPs:10.233.74.159/32] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 72d8273c-b551-4929-87f2-26f91f36cd9c 0xc0048ee597 0xc0048ee598}] [] [{calico Update v1 2023-03-01 13:15:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-01 13:15:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"72d8273c-b551-4929-87f2-26f91f36cd9c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-01 13:15:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.74.159\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6ck6l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6ck6l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:15:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:15:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:15:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:15:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.2.241,PodIP:10.233.74.159,StartTime:2023-03-01 13:15:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-01 13:15:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://b710d462f0eb7f76c587d7a5c802c7084ded70e232da99d43771d524de3e7ba2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.74.159,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Mar  1 13:15:04.987: INFO: pod: "test-deployment-7c7d8d58c8-g2b7j":
&Pod{ObjectMeta:{test-deployment-7c7d8d58c8-g2b7j test-deployment-7c7d8d58c8- deployment-3317  bbd5f4a9-09bb-4c47-b496-0e4d46117b3c 47163 0 2023-03-01 13:15:03 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cni.projectcalico.org/containerID:2b8dfc822d91696260783af6e68f857059be083ac637055e963c3ec4e329b417 cni.projectcalico.org/podIP:10.233.64.145/32 cni.projectcalico.org/podIPs:10.233.64.145/32] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 72d8273c-b551-4929-87f2-26f91f36cd9c 0xc0048ee7c7 0xc0048ee7c8}] [] [{kube-controller-manager Update v1 2023-03-01 13:15:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"72d8273c-b551-4929-87f2-26f91f36cd9c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-01 13:15:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-01 13:15:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.145\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ltrh7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ltrh7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:15:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:15:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:15:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:15:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.76,PodIP:10.233.64.145,StartTime:2023-03-01 13:15:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-01 13:15:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://c8167e9219b861860aba0efa2661d655808dd9292862aafffcdae78351f60bcc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.145,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Mar  1 13:15:04.987: INFO: ReplicaSet "test-deployment-8594bb6fdd":
&ReplicaSet{ObjectMeta:{test-deployment-8594bb6fdd  deployment-3317  9dd6d353-c169-43d8-8c36-fc3368fb2cf7 47080 3 2023-03-01 13:15:00 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 6eec8877-2d68-47d7-a1df-29169cde9ef3 0xc004923847 0xc004923848}] [] [{kube-controller-manager Update apps/v1 2023-03-01 13:15:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6eec8877-2d68-47d7-a1df-29169cde9ef3\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-01 13:15:02 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 8594bb6fdd,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0049238d0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Mar  1 13:15:04.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3317" for this suite. 03/01/23 13:15:05.003
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","completed":348,"skipped":6421,"failed":0}
------------------------------
â€¢ [4.715 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:15:00.297
    Mar  1 13:15:00.297: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename deployment 03/01/23 13:15:00.298
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:15:00.317
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:15:00.32
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 03/01/23 13:15:00.329
    STEP: waiting for Deployment to be created 03/01/23 13:15:00.336
    STEP: waiting for all Replicas to be Ready 03/01/23 13:15:00.338
    Mar  1 13:15:00.340: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar  1 13:15:00.340: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar  1 13:15:00.350: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar  1 13:15:00.350: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar  1 13:15:00.381: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar  1 13:15:00.381: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar  1 13:15:00.415: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar  1 13:15:00.415: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar  1 13:15:01.569: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Mar  1 13:15:01.569: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Mar  1 13:15:01.905: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 03/01/23 13:15:01.905
    W0301 13:15:01.913942      19 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Mar  1 13:15:01.916: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 03/01/23 13:15:01.916
    Mar  1 13:15:01.918: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 0
    Mar  1 13:15:01.918: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 0
    Mar  1 13:15:01.918: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 0
    Mar  1 13:15:01.918: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 0
    Mar  1 13:15:01.918: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 0
    Mar  1 13:15:01.918: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 0
    Mar  1 13:15:01.918: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 0
    Mar  1 13:15:01.918: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 0
    Mar  1 13:15:01.919: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 1
    Mar  1 13:15:01.919: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 1
    Mar  1 13:15:01.919: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 2
    Mar  1 13:15:01.919: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 2
    Mar  1 13:15:01.919: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 2
    Mar  1 13:15:01.919: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 2
    Mar  1 13:15:01.933: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 2
    Mar  1 13:15:01.933: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 2
    Mar  1 13:15:01.955: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 2
    Mar  1 13:15:01.955: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 2
    Mar  1 13:15:01.975: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 1
    Mar  1 13:15:01.975: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 1
    Mar  1 13:15:01.989: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 1
    Mar  1 13:15:01.989: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 1
    Mar  1 13:15:02.921: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 2
    Mar  1 13:15:02.921: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 2
    Mar  1 13:15:02.957: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 1
    STEP: listing Deployments 03/01/23 13:15:02.957
    Mar  1 13:15:02.963: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 03/01/23 13:15:02.963
    Mar  1 13:15:02.978: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 03/01/23 13:15:02.978
    Mar  1 13:15:02.988: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Mar  1 13:15:02.995: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Mar  1 13:15:03.013: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Mar  1 13:15:03.034: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Mar  1 13:15:03.048: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Mar  1 13:15:03.938: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Mar  1 13:15:03.962: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Mar  1 13:15:03.975: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Mar  1 13:15:03.991: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Mar  1 13:15:04.010: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Mar  1 13:15:04.893: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 03/01/23 13:15:04.923
    STEP: fetching the DeploymentStatus 03/01/23 13:15:04.934
    Mar  1 13:15:04.942: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 1
    Mar  1 13:15:04.942: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 1
    Mar  1 13:15:04.942: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 1
    Mar  1 13:15:04.942: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 1
    Mar  1 13:15:04.942: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 1
    Mar  1 13:15:04.942: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 2
    Mar  1 13:15:04.942: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 2
    Mar  1 13:15:04.942: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 2
    Mar  1 13:15:04.943: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 2
    Mar  1 13:15:04.943: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 2
    Mar  1 13:15:04.943: INFO: observed Deployment test-deployment in namespace deployment-3317 with ReadyReplicas 3
    STEP: deleting the Deployment 03/01/23 13:15:04.943
    Mar  1 13:15:04.959: INFO: observed event type MODIFIED
    Mar  1 13:15:04.959: INFO: observed event type MODIFIED
    Mar  1 13:15:04.959: INFO: observed event type MODIFIED
    Mar  1 13:15:04.960: INFO: observed event type MODIFIED
    Mar  1 13:15:04.960: INFO: observed event type MODIFIED
    Mar  1 13:15:04.960: INFO: observed event type MODIFIED
    Mar  1 13:15:04.960: INFO: observed event type MODIFIED
    Mar  1 13:15:04.960: INFO: observed event type MODIFIED
    Mar  1 13:15:04.960: INFO: observed event type MODIFIED
    Mar  1 13:15:04.961: INFO: observed event type MODIFIED
    Mar  1 13:15:04.961: INFO: observed event type MODIFIED
    Mar  1 13:15:04.961: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar  1 13:15:04.966: INFO: Log out all the ReplicaSets if there is no deployment created
    Mar  1 13:15:04.972: INFO: ReplicaSet "test-deployment-54cc775c4b":
    &ReplicaSet{ObjectMeta:{test-deployment-54cc775c4b  deployment-3317  a31c48c3-8e13-48fe-afb3-4cbf396d3b89 47172 4 2023-03-01 13:15:01 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 6eec8877-2d68-47d7-a1df-29169cde9ef3 0xc004923667 0xc004923668}] [] [{kube-controller-manager Update apps/v1 2023-03-01 13:15:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6eec8877-2d68-47d7-a1df-29169cde9ef3\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-01 13:15:04 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 54cc775c4b,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0049236f0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    Mar  1 13:15:04.979: INFO: pod: "test-deployment-54cc775c4b-qhcmg":
    &Pod{ObjectMeta:{test-deployment-54cc775c4b-qhcmg test-deployment-54cc775c4b- deployment-3317  8055a9ac-8d36-4423-955d-e1f797c50a4d 47155 0 2023-03-01 13:15:02 +0000 UTC 2023-03-01 13:15:04 +0000 UTC 0xc004923b48 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[cni.projectcalico.org/containerID:46e7adb9ae1cc4a48fe1c96c483a27a5c3a322722a1e728b74bc4d0bc321cffd cni.projectcalico.org/podIP:10.233.95.190/32 cni.projectcalico.org/podIPs:10.233.95.190/32] [{apps/v1 ReplicaSet test-deployment-54cc775c4b a31c48c3-8e13-48fe-afb3-4cbf396d3b89 0xc004923b97 0xc004923b98}] [] [{kube-controller-manager Update v1 2023-03-01 13:15:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a31c48c3-8e13-48fe-afb3-4cbf396d3b89\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-01 13:15:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-01 13:15:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.95.190\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q9vzm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q9vzm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:15:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:15:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:15:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:15:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.178,PodIP:10.233.95.190,StartTime:2023-03-01 13:15:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-01 13:15:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:containerd://1c7649b178a74a456d4c8ddf63f136f416bea60db71257d78a19b3c7d213506a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.95.190,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Mar  1 13:15:04.980: INFO: pod: "test-deployment-54cc775c4b-xzg2w":
    &Pod{ObjectMeta:{test-deployment-54cc775c4b-xzg2w test-deployment-54cc775c4b- deployment-3317  a1f04440-7925-44d6-9dc5-d204d2055162 47168 0 2023-03-01 13:15:01 +0000 UTC 2023-03-01 13:15:05 +0000 UTC 0xc004923d80 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[cni.projectcalico.org/containerID:564182f6c1afbb6aedb19b47a8f245c8c5375b96218323d61f4d803f40ec8f4b cni.projectcalico.org/podIP:10.233.74.170/32 cni.projectcalico.org/podIPs:10.233.74.170/32] [{apps/v1 ReplicaSet test-deployment-54cc775c4b a31c48c3-8e13-48fe-afb3-4cbf396d3b89 0xc004923dd7 0xc004923dd8}] [] [{kube-controller-manager Update v1 2023-03-01 13:15:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a31c48c3-8e13-48fe-afb3-4cbf396d3b89\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-01 13:15:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-01 13:15:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.74.170\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sh4qn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sh4qn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:15:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:15:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:15:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:15:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.2.241,PodIP:10.233.74.170,StartTime:2023-03-01 13:15:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-01 13:15:02 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:containerd://7869e7f8be2c213eda833f2b2ddf1bed8f6eb41af54a81b8768bdbe5d340d985,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.74.170,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Mar  1 13:15:04.980: INFO: ReplicaSet "test-deployment-7c7d8d58c8":
    &ReplicaSet{ObjectMeta:{test-deployment-7c7d8d58c8  deployment-3317  72d8273c-b551-4929-87f2-26f91f36cd9c 47164 2 2023-03-01 13:15:03 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 6eec8877-2d68-47d7-a1df-29169cde9ef3 0xc004923757 0xc004923758}] [] [{kube-controller-manager Update apps/v1 2023-03-01 13:15:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6eec8877-2d68-47d7-a1df-29169cde9ef3\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-01 13:15:04 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7c7d8d58c8,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0049237e0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

    Mar  1 13:15:04.987: INFO: pod: "test-deployment-7c7d8d58c8-fprvj":
    &Pod{ObjectMeta:{test-deployment-7c7d8d58c8-fprvj test-deployment-7c7d8d58c8- deployment-3317  aceeb4c0-cfbc-4353-931d-8a3314e4d73a 47121 0 2023-03-01 13:15:03 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cni.projectcalico.org/containerID:8b8040b50bbc1d36fd3969a4c7047081231a25438b9e3d07c6a64cb576f3ad5e cni.projectcalico.org/podIP:10.233.74.159/32 cni.projectcalico.org/podIPs:10.233.74.159/32] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 72d8273c-b551-4929-87f2-26f91f36cd9c 0xc0048ee597 0xc0048ee598}] [] [{calico Update v1 2023-03-01 13:15:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-03-01 13:15:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"72d8273c-b551-4929-87f2-26f91f36cd9c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-01 13:15:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.74.159\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6ck6l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6ck6l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:15:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:15:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:15:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:15:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.2.241,PodIP:10.233.74.159,StartTime:2023-03-01 13:15:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-01 13:15:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://b710d462f0eb7f76c587d7a5c802c7084ded70e232da99d43771d524de3e7ba2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.74.159,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Mar  1 13:15:04.987: INFO: pod: "test-deployment-7c7d8d58c8-g2b7j":
    &Pod{ObjectMeta:{test-deployment-7c7d8d58c8-g2b7j test-deployment-7c7d8d58c8- deployment-3317  bbd5f4a9-09bb-4c47-b496-0e4d46117b3c 47163 0 2023-03-01 13:15:03 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cni.projectcalico.org/containerID:2b8dfc822d91696260783af6e68f857059be083ac637055e963c3ec4e329b417 cni.projectcalico.org/podIP:10.233.64.145/32 cni.projectcalico.org/podIPs:10.233.64.145/32] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 72d8273c-b551-4929-87f2-26f91f36cd9c 0xc0048ee7c7 0xc0048ee7c8}] [] [{kube-controller-manager Update v1 2023-03-01 13:15:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"72d8273c-b551-4929-87f2-26f91f36cd9c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-01 13:15:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-01 13:15:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.145\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ltrh7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ltrh7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:15:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:15:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:15:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-01 13:15:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.76,PodIP:10.233.64.145,StartTime:2023-03-01 13:15:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-01 13:15:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://c8167e9219b861860aba0efa2661d655808dd9292862aafffcdae78351f60bcc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.145,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Mar  1 13:15:04.987: INFO: ReplicaSet "test-deployment-8594bb6fdd":
    &ReplicaSet{ObjectMeta:{test-deployment-8594bb6fdd  deployment-3317  9dd6d353-c169-43d8-8c36-fc3368fb2cf7 47080 3 2023-03-01 13:15:00 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 6eec8877-2d68-47d7-a1df-29169cde9ef3 0xc004923847 0xc004923848}] [] [{kube-controller-manager Update apps/v1 2023-03-01 13:15:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6eec8877-2d68-47d7-a1df-29169cde9ef3\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-01 13:15:02 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 8594bb6fdd,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0049238d0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Mar  1 13:15:04.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-3317" for this suite. 03/01/23 13:15:05.003
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:15:05.019
Mar  1 13:15:05.020: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename var-expansion 03/01/23 13:15:05.021
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:15:05.043
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:15:05.047
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
STEP: Creating a pod to test env composition 03/01/23 13:15:05.051
Mar  1 13:15:05.073: INFO: Waiting up to 5m0s for pod "var-expansion-dc61ccb8-9a75-4b07-bdc0-4d1e8b65f5d7" in namespace "var-expansion-7887" to be "Succeeded or Failed"
Mar  1 13:15:05.079: INFO: Pod "var-expansion-dc61ccb8-9a75-4b07-bdc0-4d1e8b65f5d7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.222453ms
Mar  1 13:15:07.084: INFO: Pod "var-expansion-dc61ccb8-9a75-4b07-bdc0-4d1e8b65f5d7": Phase="Running", Reason="", readiness=true. Elapsed: 2.01073511s
Mar  1 13:15:09.084: INFO: Pod "var-expansion-dc61ccb8-9a75-4b07-bdc0-4d1e8b65f5d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010373466s
STEP: Saw pod success 03/01/23 13:15:09.084
Mar  1 13:15:09.084: INFO: Pod "var-expansion-dc61ccb8-9a75-4b07-bdc0-4d1e8b65f5d7" satisfied condition "Succeeded or Failed"
Mar  1 13:15:09.089: INFO: Trying to get logs from node lab1-k8s-node-3 pod var-expansion-dc61ccb8-9a75-4b07-bdc0-4d1e8b65f5d7 container dapi-container: <nil>
STEP: delete the pod 03/01/23 13:15:09.098
Mar  1 13:15:09.115: INFO: Waiting for pod var-expansion-dc61ccb8-9a75-4b07-bdc0-4d1e8b65f5d7 to disappear
Mar  1 13:15:09.120: INFO: Pod var-expansion-dc61ccb8-9a75-4b07-bdc0-4d1e8b65f5d7 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Mar  1 13:15:09.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7887" for this suite. 03/01/23 13:15:09.127
{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","completed":349,"skipped":6466,"failed":0}
------------------------------
â€¢ [4.116 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:15:05.019
    Mar  1 13:15:05.020: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename var-expansion 03/01/23 13:15:05.021
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:15:05.043
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:15:05.047
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:43
    STEP: Creating a pod to test env composition 03/01/23 13:15:05.051
    Mar  1 13:15:05.073: INFO: Waiting up to 5m0s for pod "var-expansion-dc61ccb8-9a75-4b07-bdc0-4d1e8b65f5d7" in namespace "var-expansion-7887" to be "Succeeded or Failed"
    Mar  1 13:15:05.079: INFO: Pod "var-expansion-dc61ccb8-9a75-4b07-bdc0-4d1e8b65f5d7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.222453ms
    Mar  1 13:15:07.084: INFO: Pod "var-expansion-dc61ccb8-9a75-4b07-bdc0-4d1e8b65f5d7": Phase="Running", Reason="", readiness=true. Elapsed: 2.01073511s
    Mar  1 13:15:09.084: INFO: Pod "var-expansion-dc61ccb8-9a75-4b07-bdc0-4d1e8b65f5d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010373466s
    STEP: Saw pod success 03/01/23 13:15:09.084
    Mar  1 13:15:09.084: INFO: Pod "var-expansion-dc61ccb8-9a75-4b07-bdc0-4d1e8b65f5d7" satisfied condition "Succeeded or Failed"
    Mar  1 13:15:09.089: INFO: Trying to get logs from node lab1-k8s-node-3 pod var-expansion-dc61ccb8-9a75-4b07-bdc0-4d1e8b65f5d7 container dapi-container: <nil>
    STEP: delete the pod 03/01/23 13:15:09.098
    Mar  1 13:15:09.115: INFO: Waiting for pod var-expansion-dc61ccb8-9a75-4b07-bdc0-4d1e8b65f5d7 to disappear
    Mar  1 13:15:09.120: INFO: Pod var-expansion-dc61ccb8-9a75-4b07-bdc0-4d1e8b65f5d7 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Mar  1 13:15:09.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-7887" for this suite. 03/01/23 13:15:09.127
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:15:09.137
Mar  1 13:15:09.138: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename container-runtime 03/01/23 13:15:09.138
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:15:09.162
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:15:09.165
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
STEP: create the container 03/01/23 13:15:09.168
STEP: wait for the container to reach Succeeded 03/01/23 13:15:09.18
STEP: get the container status 03/01/23 13:15:13.216
STEP: the container should be terminated 03/01/23 13:15:13.22
STEP: the termination message should be set 03/01/23 13:15:13.22
Mar  1 13:15:13.220: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 03/01/23 13:15:13.22
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Mar  1 13:15:13.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5793" for this suite. 03/01/23 13:15:13.253
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":350,"skipped":6466,"failed":0}
------------------------------
â€¢ [4.125 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:15:09.137
    Mar  1 13:15:09.138: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename container-runtime 03/01/23 13:15:09.138
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:15:09.162
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:15:09.165
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231
    STEP: create the container 03/01/23 13:15:09.168
    STEP: wait for the container to reach Succeeded 03/01/23 13:15:09.18
    STEP: get the container status 03/01/23 13:15:13.216
    STEP: the container should be terminated 03/01/23 13:15:13.22
    STEP: the termination message should be set 03/01/23 13:15:13.22
    Mar  1 13:15:13.220: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 03/01/23 13:15:13.22
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Mar  1 13:15:13.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-5793" for this suite. 03/01/23 13:15:13.253
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:15:13.268
Mar  1 13:15:13.268: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename daemonsets 03/01/23 13:15:13.269
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:15:13.292
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:15:13.295
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
STEP: Creating a simple DaemonSet "daemon-set" 03/01/23 13:15:13.326
STEP: Check that daemon pods launch on every node of the cluster. 03/01/23 13:15:13.333
Mar  1 13:15:13.341: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 13:15:13.341: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 13:15:13.341: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 13:15:13.347: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  1 13:15:13.347: INFO: Node lab1-k8s-node-1 is running 0 daemon pod, expected 1
Mar  1 13:15:14.356: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 13:15:14.356: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 13:15:14.356: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 13:15:14.361: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  1 13:15:14.361: INFO: Node lab1-k8s-node-1 is running 0 daemon pod, expected 1
Mar  1 13:15:15.357: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 13:15:15.357: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 13:15:15.357: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 13:15:15.363: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar  1 13:15:15.363: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 03/01/23 13:15:15.368
Mar  1 13:15:15.391: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 13:15:15.391: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 13:15:15.391: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 13:15:15.398: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar  1 13:15:15.398: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 03/01/23 13:15:15.398
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 03/01/23 13:15:16.414
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9325, will wait for the garbage collector to delete the pods 03/01/23 13:15:16.414
Mar  1 13:15:16.481: INFO: Deleting DaemonSet.extensions daemon-set took: 11.277511ms
Mar  1 13:15:16.581: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.554082ms
Mar  1 13:15:18.986: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  1 13:15:18.986: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar  1 13:15:18.991: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"47484"},"items":null}

Mar  1 13:15:18.995: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"47484"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Mar  1 13:15:19.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9325" for this suite. 03/01/23 13:15:19.023
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","completed":351,"skipped":6528,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.765 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:15:13.268
    Mar  1 13:15:13.268: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename daemonsets 03/01/23 13:15:13.269
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:15:13.292
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:15:13.295
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:293
    STEP: Creating a simple DaemonSet "daemon-set" 03/01/23 13:15:13.326
    STEP: Check that daemon pods launch on every node of the cluster. 03/01/23 13:15:13.333
    Mar  1 13:15:13.341: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 13:15:13.341: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 13:15:13.341: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 13:15:13.347: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  1 13:15:13.347: INFO: Node lab1-k8s-node-1 is running 0 daemon pod, expected 1
    Mar  1 13:15:14.356: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 13:15:14.356: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 13:15:14.356: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 13:15:14.361: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  1 13:15:14.361: INFO: Node lab1-k8s-node-1 is running 0 daemon pod, expected 1
    Mar  1 13:15:15.357: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 13:15:15.357: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 13:15:15.357: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 13:15:15.363: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar  1 13:15:15.363: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 03/01/23 13:15:15.368
    Mar  1 13:15:15.391: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 13:15:15.391: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 13:15:15.391: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar  1 13:15:15.398: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar  1 13:15:15.398: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 03/01/23 13:15:15.398
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 03/01/23 13:15:16.414
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9325, will wait for the garbage collector to delete the pods 03/01/23 13:15:16.414
    Mar  1 13:15:16.481: INFO: Deleting DaemonSet.extensions daemon-set took: 11.277511ms
    Mar  1 13:15:16.581: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.554082ms
    Mar  1 13:15:18.986: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  1 13:15:18.986: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Mar  1 13:15:18.991: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"47484"},"items":null}

    Mar  1 13:15:18.995: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"47484"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Mar  1 13:15:19.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-9325" for this suite. 03/01/23 13:15:19.023
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:15:19.033
Mar  1 13:15:19.033: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename webhook 03/01/23 13:15:19.034
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:15:19.056
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:15:19.06
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/01/23 13:15:19.08
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/01/23 13:15:19.471
STEP: Deploying the webhook pod 03/01/23 13:15:19.482
STEP: Wait for the deployment to be ready 03/01/23 13:15:19.498
Mar  1 13:15:19.510: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/01/23 13:15:21.526
STEP: Verifying the service has paired with the endpoint 03/01/23 13:15:21.541
Mar  1 13:15:22.542: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 03/01/23 13:15:22.547
STEP: create a configmap that should be updated by the webhook 03/01/23 13:15:22.566
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  1 13:15:22.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4288" for this suite. 03/01/23 13:15:22.596
STEP: Destroying namespace "webhook-4288-markers" for this suite. 03/01/23 13:15:22.605
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","completed":352,"skipped":6528,"failed":0}
------------------------------
â€¢ [3.658 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:15:19.033
    Mar  1 13:15:19.033: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename webhook 03/01/23 13:15:19.034
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:15:19.056
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:15:19.06
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/01/23 13:15:19.08
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/01/23 13:15:19.471
    STEP: Deploying the webhook pod 03/01/23 13:15:19.482
    STEP: Wait for the deployment to be ready 03/01/23 13:15:19.498
    Mar  1 13:15:19.510: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/01/23 13:15:21.526
    STEP: Verifying the service has paired with the endpoint 03/01/23 13:15:21.541
    Mar  1 13:15:22.542: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:251
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 03/01/23 13:15:22.547
    STEP: create a configmap that should be updated by the webhook 03/01/23 13:15:22.566
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  1 13:15:22.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-4288" for this suite. 03/01/23 13:15:22.596
    STEP: Destroying namespace "webhook-4288-markers" for this suite. 03/01/23 13:15:22.605
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:15:22.695
Mar  1 13:15:22.695: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename disruption 03/01/23 13:15:22.696
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:15:22.721
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:15:22.724
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
STEP: Creating a pdb that targets all three pods in a test replica set 03/01/23 13:15:22.727
STEP: Waiting for the pdb to be processed 03/01/23 13:15:22.733
STEP: First trying to evict a pod which shouldn't be evictable 03/01/23 13:15:24.754
STEP: Waiting for all pods to be running 03/01/23 13:15:24.754
Mar  1 13:15:24.759: INFO: pods: 0 < 3
STEP: locating a running pod 03/01/23 13:15:26.765
STEP: Updating the pdb to allow a pod to be evicted 03/01/23 13:15:26.78
STEP: Waiting for the pdb to be processed 03/01/23 13:15:26.792
STEP: Trying to evict the same pod we tried earlier which should now be evictable 03/01/23 13:15:28.804
STEP: Waiting for all pods to be running 03/01/23 13:15:28.804
STEP: Waiting for the pdb to observed all healthy pods 03/01/23 13:15:28.81
STEP: Patching the pdb to disallow a pod to be evicted 03/01/23 13:15:28.841
STEP: Waiting for the pdb to be processed 03/01/23 13:15:28.873
STEP: Waiting for all pods to be running 03/01/23 13:15:30.888
STEP: locating a running pod 03/01/23 13:15:30.894
STEP: Deleting the pdb to allow a pod to be evicted 03/01/23 13:15:30.909
STEP: Waiting for the pdb to be deleted 03/01/23 13:15:30.92
STEP: Trying to evict the same pod we tried earlier which should now be evictable 03/01/23 13:15:30.925
STEP: Waiting for all pods to be running 03/01/23 13:15:30.925
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Mar  1 13:15:30.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-8495" for this suite. 03/01/23 13:15:30.963
{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","completed":353,"skipped":6543,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.285 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:15:22.695
    Mar  1 13:15:22.695: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename disruption 03/01/23 13:15:22.696
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:15:22.721
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:15:22.724
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:346
    STEP: Creating a pdb that targets all three pods in a test replica set 03/01/23 13:15:22.727
    STEP: Waiting for the pdb to be processed 03/01/23 13:15:22.733
    STEP: First trying to evict a pod which shouldn't be evictable 03/01/23 13:15:24.754
    STEP: Waiting for all pods to be running 03/01/23 13:15:24.754
    Mar  1 13:15:24.759: INFO: pods: 0 < 3
    STEP: locating a running pod 03/01/23 13:15:26.765
    STEP: Updating the pdb to allow a pod to be evicted 03/01/23 13:15:26.78
    STEP: Waiting for the pdb to be processed 03/01/23 13:15:26.792
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 03/01/23 13:15:28.804
    STEP: Waiting for all pods to be running 03/01/23 13:15:28.804
    STEP: Waiting for the pdb to observed all healthy pods 03/01/23 13:15:28.81
    STEP: Patching the pdb to disallow a pod to be evicted 03/01/23 13:15:28.841
    STEP: Waiting for the pdb to be processed 03/01/23 13:15:28.873
    STEP: Waiting for all pods to be running 03/01/23 13:15:30.888
    STEP: locating a running pod 03/01/23 13:15:30.894
    STEP: Deleting the pdb to allow a pod to be evicted 03/01/23 13:15:30.909
    STEP: Waiting for the pdb to be deleted 03/01/23 13:15:30.92
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 03/01/23 13:15:30.925
    STEP: Waiting for all pods to be running 03/01/23 13:15:30.925
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Mar  1 13:15:30.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-8495" for this suite. 03/01/23 13:15:30.963
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:15:30.983
Mar  1 13:15:30.983: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename projected 03/01/23 13:15:30.986
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:15:31.009
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:15:31.013
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
STEP: Creating the pod 03/01/23 13:15:31.017
Mar  1 13:15:31.028: INFO: Waiting up to 5m0s for pod "labelsupdate2dc63909-f0b5-4109-b088-420fa0071470" in namespace "projected-7934" to be "running and ready"
Mar  1 13:15:31.037: INFO: Pod "labelsupdate2dc63909-f0b5-4109-b088-420fa0071470": Phase="Pending", Reason="", readiness=false. Elapsed: 8.374714ms
Mar  1 13:15:31.037: INFO: The phase of Pod labelsupdate2dc63909-f0b5-4109-b088-420fa0071470 is Pending, waiting for it to be Running (with Ready = true)
Mar  1 13:15:33.042: INFO: Pod "labelsupdate2dc63909-f0b5-4109-b088-420fa0071470": Phase="Running", Reason="", readiness=true. Elapsed: 2.013959116s
Mar  1 13:15:33.042: INFO: The phase of Pod labelsupdate2dc63909-f0b5-4109-b088-420fa0071470 is Running (Ready = true)
Mar  1 13:15:33.042: INFO: Pod "labelsupdate2dc63909-f0b5-4109-b088-420fa0071470" satisfied condition "running and ready"
Mar  1 13:15:33.576: INFO: Successfully updated pod "labelsupdate2dc63909-f0b5-4109-b088-420fa0071470"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Mar  1 13:15:37.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7934" for this suite. 03/01/23 13:15:37.618
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","completed":354,"skipped":6568,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.644 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:15:30.983
    Mar  1 13:15:30.983: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename projected 03/01/23 13:15:30.986
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:15:31.009
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:15:31.013
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:129
    STEP: Creating the pod 03/01/23 13:15:31.017
    Mar  1 13:15:31.028: INFO: Waiting up to 5m0s for pod "labelsupdate2dc63909-f0b5-4109-b088-420fa0071470" in namespace "projected-7934" to be "running and ready"
    Mar  1 13:15:31.037: INFO: Pod "labelsupdate2dc63909-f0b5-4109-b088-420fa0071470": Phase="Pending", Reason="", readiness=false. Elapsed: 8.374714ms
    Mar  1 13:15:31.037: INFO: The phase of Pod labelsupdate2dc63909-f0b5-4109-b088-420fa0071470 is Pending, waiting for it to be Running (with Ready = true)
    Mar  1 13:15:33.042: INFO: Pod "labelsupdate2dc63909-f0b5-4109-b088-420fa0071470": Phase="Running", Reason="", readiness=true. Elapsed: 2.013959116s
    Mar  1 13:15:33.042: INFO: The phase of Pod labelsupdate2dc63909-f0b5-4109-b088-420fa0071470 is Running (Ready = true)
    Mar  1 13:15:33.042: INFO: Pod "labelsupdate2dc63909-f0b5-4109-b088-420fa0071470" satisfied condition "running and ready"
    Mar  1 13:15:33.576: INFO: Successfully updated pod "labelsupdate2dc63909-f0b5-4109-b088-420fa0071470"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Mar  1 13:15:37.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7934" for this suite. 03/01/23 13:15:37.618
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:15:37.628
Mar  1 13:15:37.628: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename services 03/01/23 13:15:37.628
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:15:37.65
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:15:37.653
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206
STEP: fetching services 03/01/23 13:15:37.656
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar  1 13:15:37.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-273" for this suite. 03/01/23 13:15:37.665
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","completed":355,"skipped":6569,"failed":0}
------------------------------
â€¢ [0.048 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:15:37.628
    Mar  1 13:15:37.628: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename services 03/01/23 13:15:37.628
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:15:37.65
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:15:37.653
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3206
    STEP: fetching services 03/01/23 13:15:37.656
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar  1 13:15:37.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-273" for this suite. 03/01/23 13:15:37.665
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:15:37.678
Mar  1 13:15:37.678: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename var-expansion 03/01/23 13:15:37.679
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:15:37.702
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:15:37.706
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
STEP: creating the pod 03/01/23 13:15:37.709
STEP: waiting for pod running 03/01/23 13:15:37.721
Mar  1 13:15:37.722: INFO: Waiting up to 2m0s for pod "var-expansion-fb9fefb8-3ff7-491c-81fc-09b016de8203" in namespace "var-expansion-5316" to be "running"
Mar  1 13:15:37.733: INFO: Pod "var-expansion-fb9fefb8-3ff7-491c-81fc-09b016de8203": Phase="Pending", Reason="", readiness=false. Elapsed: 11.562212ms
Mar  1 13:15:39.740: INFO: Pod "var-expansion-fb9fefb8-3ff7-491c-81fc-09b016de8203": Phase="Running", Reason="", readiness=true. Elapsed: 2.017953125s
Mar  1 13:15:39.740: INFO: Pod "var-expansion-fb9fefb8-3ff7-491c-81fc-09b016de8203" satisfied condition "running"
STEP: creating a file in subpath 03/01/23 13:15:39.74
Mar  1 13:15:39.747: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-5316 PodName:var-expansion-fb9fefb8-3ff7-491c-81fc-09b016de8203 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  1 13:15:39.747: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
Mar  1 13:15:39.748: INFO: ExecWithOptions: Clientset creation
Mar  1 13:15:39.748: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-5316/pods/var-expansion-fb9fefb8-3ff7-491c-81fc-09b016de8203/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 03/01/23 13:15:39.835
Mar  1 13:15:39.841: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-5316 PodName:var-expansion-fb9fefb8-3ff7-491c-81fc-09b016de8203 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  1 13:15:39.841: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
Mar  1 13:15:39.841: INFO: ExecWithOptions: Clientset creation
Mar  1 13:15:39.842: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-5316/pods/var-expansion-fb9fefb8-3ff7-491c-81fc-09b016de8203/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 03/01/23 13:15:39.915
Mar  1 13:15:40.432: INFO: Successfully updated pod "var-expansion-fb9fefb8-3ff7-491c-81fc-09b016de8203"
STEP: waiting for annotated pod running 03/01/23 13:15:40.432
Mar  1 13:15:40.432: INFO: Waiting up to 2m0s for pod "var-expansion-fb9fefb8-3ff7-491c-81fc-09b016de8203" in namespace "var-expansion-5316" to be "running"
Mar  1 13:15:40.437: INFO: Pod "var-expansion-fb9fefb8-3ff7-491c-81fc-09b016de8203": Phase="Running", Reason="", readiness=true. Elapsed: 5.238437ms
Mar  1 13:15:40.437: INFO: Pod "var-expansion-fb9fefb8-3ff7-491c-81fc-09b016de8203" satisfied condition "running"
STEP: deleting the pod gracefully 03/01/23 13:15:40.437
Mar  1 13:15:40.438: INFO: Deleting pod "var-expansion-fb9fefb8-3ff7-491c-81fc-09b016de8203" in namespace "var-expansion-5316"
Mar  1 13:15:40.448: INFO: Wait up to 5m0s for pod "var-expansion-fb9fefb8-3ff7-491c-81fc-09b016de8203" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Mar  1 13:16:14.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5316" for this suite. 03/01/23 13:16:14.469
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","completed":356,"skipped":6588,"failed":0}
------------------------------
â€¢ [SLOW TEST] [36.801 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:15:37.678
    Mar  1 13:15:37.678: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename var-expansion 03/01/23 13:15:37.679
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:15:37.702
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:15:37.706
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:296
    STEP: creating the pod 03/01/23 13:15:37.709
    STEP: waiting for pod running 03/01/23 13:15:37.721
    Mar  1 13:15:37.722: INFO: Waiting up to 2m0s for pod "var-expansion-fb9fefb8-3ff7-491c-81fc-09b016de8203" in namespace "var-expansion-5316" to be "running"
    Mar  1 13:15:37.733: INFO: Pod "var-expansion-fb9fefb8-3ff7-491c-81fc-09b016de8203": Phase="Pending", Reason="", readiness=false. Elapsed: 11.562212ms
    Mar  1 13:15:39.740: INFO: Pod "var-expansion-fb9fefb8-3ff7-491c-81fc-09b016de8203": Phase="Running", Reason="", readiness=true. Elapsed: 2.017953125s
    Mar  1 13:15:39.740: INFO: Pod "var-expansion-fb9fefb8-3ff7-491c-81fc-09b016de8203" satisfied condition "running"
    STEP: creating a file in subpath 03/01/23 13:15:39.74
    Mar  1 13:15:39.747: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-5316 PodName:var-expansion-fb9fefb8-3ff7-491c-81fc-09b016de8203 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  1 13:15:39.747: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    Mar  1 13:15:39.748: INFO: ExecWithOptions: Clientset creation
    Mar  1 13:15:39.748: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-5316/pods/var-expansion-fb9fefb8-3ff7-491c-81fc-09b016de8203/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 03/01/23 13:15:39.835
    Mar  1 13:15:39.841: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-5316 PodName:var-expansion-fb9fefb8-3ff7-491c-81fc-09b016de8203 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  1 13:15:39.841: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    Mar  1 13:15:39.841: INFO: ExecWithOptions: Clientset creation
    Mar  1 13:15:39.842: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-5316/pods/var-expansion-fb9fefb8-3ff7-491c-81fc-09b016de8203/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 03/01/23 13:15:39.915
    Mar  1 13:15:40.432: INFO: Successfully updated pod "var-expansion-fb9fefb8-3ff7-491c-81fc-09b016de8203"
    STEP: waiting for annotated pod running 03/01/23 13:15:40.432
    Mar  1 13:15:40.432: INFO: Waiting up to 2m0s for pod "var-expansion-fb9fefb8-3ff7-491c-81fc-09b016de8203" in namespace "var-expansion-5316" to be "running"
    Mar  1 13:15:40.437: INFO: Pod "var-expansion-fb9fefb8-3ff7-491c-81fc-09b016de8203": Phase="Running", Reason="", readiness=true. Elapsed: 5.238437ms
    Mar  1 13:15:40.437: INFO: Pod "var-expansion-fb9fefb8-3ff7-491c-81fc-09b016de8203" satisfied condition "running"
    STEP: deleting the pod gracefully 03/01/23 13:15:40.437
    Mar  1 13:15:40.438: INFO: Deleting pod "var-expansion-fb9fefb8-3ff7-491c-81fc-09b016de8203" in namespace "var-expansion-5316"
    Mar  1 13:15:40.448: INFO: Wait up to 5m0s for pod "var-expansion-fb9fefb8-3ff7-491c-81fc-09b016de8203" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Mar  1 13:16:14.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-5316" for this suite. 03/01/23 13:16:14.469
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:16:14.481
Mar  1 13:16:14.481: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename emptydir 03/01/23 13:16:14.483
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:16:14.515
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:16:14.519
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
STEP: Creating a pod to test emptydir 0777 on node default medium 03/01/23 13:16:14.522
Mar  1 13:16:14.533: INFO: Waiting up to 5m0s for pod "pod-0c560b3e-5b89-423b-b67d-9bfe706ed206" in namespace "emptydir-3127" to be "Succeeded or Failed"
Mar  1 13:16:14.558: INFO: Pod "pod-0c560b3e-5b89-423b-b67d-9bfe706ed206": Phase="Pending", Reason="", readiness=false. Elapsed: 24.74275ms
Mar  1 13:16:16.563: INFO: Pod "pod-0c560b3e-5b89-423b-b67d-9bfe706ed206": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030150773s
Mar  1 13:16:18.564: INFO: Pod "pod-0c560b3e-5b89-423b-b67d-9bfe706ed206": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030170264s
STEP: Saw pod success 03/01/23 13:16:18.564
Mar  1 13:16:18.564: INFO: Pod "pod-0c560b3e-5b89-423b-b67d-9bfe706ed206" satisfied condition "Succeeded or Failed"
Mar  1 13:16:18.569: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-0c560b3e-5b89-423b-b67d-9bfe706ed206 container test-container: <nil>
STEP: delete the pod 03/01/23 13:16:18.577
Mar  1 13:16:18.593: INFO: Waiting for pod pod-0c560b3e-5b89-423b-b67d-9bfe706ed206 to disappear
Mar  1 13:16:18.597: INFO: Pod pod-0c560b3e-5b89-423b-b67d-9bfe706ed206 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar  1 13:16:18.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3127" for this suite. 03/01/23 13:16:18.607
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":357,"skipped":6618,"failed":0}
------------------------------
â€¢ [4.135 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:16:14.481
    Mar  1 13:16:14.481: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename emptydir 03/01/23 13:16:14.483
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:16:14.515
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:16:14.519
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:216
    STEP: Creating a pod to test emptydir 0777 on node default medium 03/01/23 13:16:14.522
    Mar  1 13:16:14.533: INFO: Waiting up to 5m0s for pod "pod-0c560b3e-5b89-423b-b67d-9bfe706ed206" in namespace "emptydir-3127" to be "Succeeded or Failed"
    Mar  1 13:16:14.558: INFO: Pod "pod-0c560b3e-5b89-423b-b67d-9bfe706ed206": Phase="Pending", Reason="", readiness=false. Elapsed: 24.74275ms
    Mar  1 13:16:16.563: INFO: Pod "pod-0c560b3e-5b89-423b-b67d-9bfe706ed206": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030150773s
    Mar  1 13:16:18.564: INFO: Pod "pod-0c560b3e-5b89-423b-b67d-9bfe706ed206": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030170264s
    STEP: Saw pod success 03/01/23 13:16:18.564
    Mar  1 13:16:18.564: INFO: Pod "pod-0c560b3e-5b89-423b-b67d-9bfe706ed206" satisfied condition "Succeeded or Failed"
    Mar  1 13:16:18.569: INFO: Trying to get logs from node lab1-k8s-node-3 pod pod-0c560b3e-5b89-423b-b67d-9bfe706ed206 container test-container: <nil>
    STEP: delete the pod 03/01/23 13:16:18.577
    Mar  1 13:16:18.593: INFO: Waiting for pod pod-0c560b3e-5b89-423b-b67d-9bfe706ed206 to disappear
    Mar  1 13:16:18.597: INFO: Pod pod-0c560b3e-5b89-423b-b67d-9bfe706ed206 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar  1 13:16:18.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-3127" for this suite. 03/01/23 13:16:18.607
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:16:18.617
Mar  1 13:16:18.617: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename containers 03/01/23 13:16:18.618
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:16:18.642
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:16:18.646
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
STEP: Creating a pod to test override all 03/01/23 13:16:18.65
Mar  1 13:16:18.661: INFO: Waiting up to 5m0s for pod "client-containers-6d2ce38a-224b-4124-a911-e43f9bca0fea" in namespace "containers-8216" to be "Succeeded or Failed"
Mar  1 13:16:18.668: INFO: Pod "client-containers-6d2ce38a-224b-4124-a911-e43f9bca0fea": Phase="Pending", Reason="", readiness=false. Elapsed: 6.43129ms
Mar  1 13:16:20.674: INFO: Pod "client-containers-6d2ce38a-224b-4124-a911-e43f9bca0fea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012716185s
Mar  1 13:16:22.673: INFO: Pod "client-containers-6d2ce38a-224b-4124-a911-e43f9bca0fea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01201758s
STEP: Saw pod success 03/01/23 13:16:22.673
Mar  1 13:16:22.673: INFO: Pod "client-containers-6d2ce38a-224b-4124-a911-e43f9bca0fea" satisfied condition "Succeeded or Failed"
Mar  1 13:16:22.678: INFO: Trying to get logs from node lab1-k8s-node-3 pod client-containers-6d2ce38a-224b-4124-a911-e43f9bca0fea container agnhost-container: <nil>
STEP: delete the pod 03/01/23 13:16:22.687
Mar  1 13:16:22.701: INFO: Waiting for pod client-containers-6d2ce38a-224b-4124-a911-e43f9bca0fea to disappear
Mar  1 13:16:22.706: INFO: Pod client-containers-6d2ce38a-224b-4124-a911-e43f9bca0fea no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Mar  1 13:16:22.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8216" for this suite. 03/01/23 13:16:22.712
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","completed":358,"skipped":6636,"failed":0}
------------------------------
â€¢ [4.104 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:16:18.617
    Mar  1 13:16:18.617: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename containers 03/01/23 13:16:18.618
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:16:18.642
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:16:18.646
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:86
    STEP: Creating a pod to test override all 03/01/23 13:16:18.65
    Mar  1 13:16:18.661: INFO: Waiting up to 5m0s for pod "client-containers-6d2ce38a-224b-4124-a911-e43f9bca0fea" in namespace "containers-8216" to be "Succeeded or Failed"
    Mar  1 13:16:18.668: INFO: Pod "client-containers-6d2ce38a-224b-4124-a911-e43f9bca0fea": Phase="Pending", Reason="", readiness=false. Elapsed: 6.43129ms
    Mar  1 13:16:20.674: INFO: Pod "client-containers-6d2ce38a-224b-4124-a911-e43f9bca0fea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012716185s
    Mar  1 13:16:22.673: INFO: Pod "client-containers-6d2ce38a-224b-4124-a911-e43f9bca0fea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01201758s
    STEP: Saw pod success 03/01/23 13:16:22.673
    Mar  1 13:16:22.673: INFO: Pod "client-containers-6d2ce38a-224b-4124-a911-e43f9bca0fea" satisfied condition "Succeeded or Failed"
    Mar  1 13:16:22.678: INFO: Trying to get logs from node lab1-k8s-node-3 pod client-containers-6d2ce38a-224b-4124-a911-e43f9bca0fea container agnhost-container: <nil>
    STEP: delete the pod 03/01/23 13:16:22.687
    Mar  1 13:16:22.701: INFO: Waiting for pod client-containers-6d2ce38a-224b-4124-a911-e43f9bca0fea to disappear
    Mar  1 13:16:22.706: INFO: Pod client-containers-6d2ce38a-224b-4124-a911-e43f9bca0fea no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Mar  1 13:16:22.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-8216" for this suite. 03/01/23 13:16:22.712
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:16:22.724
Mar  1 13:16:22.724: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename crd-publish-openapi 03/01/23 13:16:22.725
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:16:22.75
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:16:22.753
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
Mar  1 13:16:22.756: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 03/01/23 13:16:30.58
Mar  1 13:16:30.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=crd-publish-openapi-6547 --namespace=crd-publish-openapi-6547 create -f -'
Mar  1 13:16:31.181: INFO: stderr: ""
Mar  1 13:16:31.181: INFO: stdout: "e2e-test-crd-publish-openapi-873-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Mar  1 13:16:31.181: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=crd-publish-openapi-6547 --namespace=crd-publish-openapi-6547 delete e2e-test-crd-publish-openapi-873-crds test-cr'
Mar  1 13:16:31.253: INFO: stderr: ""
Mar  1 13:16:31.253: INFO: stdout: "e2e-test-crd-publish-openapi-873-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Mar  1 13:16:31.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=crd-publish-openapi-6547 --namespace=crd-publish-openapi-6547 apply -f -'
Mar  1 13:16:31.805: INFO: stderr: ""
Mar  1 13:16:31.805: INFO: stdout: "e2e-test-crd-publish-openapi-873-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Mar  1 13:16:31.805: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=crd-publish-openapi-6547 --namespace=crd-publish-openapi-6547 delete e2e-test-crd-publish-openapi-873-crds test-cr'
Mar  1 13:16:31.872: INFO: stderr: ""
Mar  1 13:16:31.872: INFO: stdout: "e2e-test-crd-publish-openapi-873-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 03/01/23 13:16:31.872
Mar  1 13:16:31.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=crd-publish-openapi-6547 explain e2e-test-crd-publish-openapi-873-crds'
Mar  1 13:16:32.073: INFO: stderr: ""
Mar  1 13:16:32.073: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-873-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  1 13:16:34.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6547" for this suite. 03/01/23 13:16:34.598
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","completed":359,"skipped":6643,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.883 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:16:22.724
    Mar  1 13:16:22.724: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename crd-publish-openapi 03/01/23 13:16:22.725
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:16:22.75
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:16:22.753
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:193
    Mar  1 13:16:22.756: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 03/01/23 13:16:30.58
    Mar  1 13:16:30.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=crd-publish-openapi-6547 --namespace=crd-publish-openapi-6547 create -f -'
    Mar  1 13:16:31.181: INFO: stderr: ""
    Mar  1 13:16:31.181: INFO: stdout: "e2e-test-crd-publish-openapi-873-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Mar  1 13:16:31.181: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=crd-publish-openapi-6547 --namespace=crd-publish-openapi-6547 delete e2e-test-crd-publish-openapi-873-crds test-cr'
    Mar  1 13:16:31.253: INFO: stderr: ""
    Mar  1 13:16:31.253: INFO: stdout: "e2e-test-crd-publish-openapi-873-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Mar  1 13:16:31.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=crd-publish-openapi-6547 --namespace=crd-publish-openapi-6547 apply -f -'
    Mar  1 13:16:31.805: INFO: stderr: ""
    Mar  1 13:16:31.805: INFO: stdout: "e2e-test-crd-publish-openapi-873-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Mar  1 13:16:31.805: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=crd-publish-openapi-6547 --namespace=crd-publish-openapi-6547 delete e2e-test-crd-publish-openapi-873-crds test-cr'
    Mar  1 13:16:31.872: INFO: stderr: ""
    Mar  1 13:16:31.872: INFO: stdout: "e2e-test-crd-publish-openapi-873-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 03/01/23 13:16:31.872
    Mar  1 13:16:31.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3765612722 --namespace=crd-publish-openapi-6547 explain e2e-test-crd-publish-openapi-873-crds'
    Mar  1 13:16:32.073: INFO: stderr: ""
    Mar  1 13:16:32.073: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-873-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  1 13:16:34.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-6547" for this suite. 03/01/23 13:16:34.598
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:16:34.613
Mar  1 13:16:34.613: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename container-lifecycle-hook 03/01/23 13:16:34.613
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:16:34.633
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:16:34.636
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 03/01/23 13:16:34.646
Mar  1 13:16:34.658: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-7241" to be "running and ready"
Mar  1 13:16:34.663: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.73825ms
Mar  1 13:16:34.663: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar  1 13:16:36.670: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.011807792s
Mar  1 13:16:36.670: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Mar  1 13:16:36.670: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
STEP: create the pod with lifecycle hook 03/01/23 13:16:36.676
Mar  1 13:16:36.683: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-7241" to be "running and ready"
Mar  1 13:16:36.690: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 6.579584ms
Mar  1 13:16:36.690: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Mar  1 13:16:38.697: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.014033675s
Mar  1 13:16:38.698: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Mar  1 13:16:38.698: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 03/01/23 13:16:38.702
STEP: delete the pod with lifecycle hook 03/01/23 13:16:38.721
Mar  1 13:16:38.732: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  1 13:16:38.740: INFO: Pod pod-with-poststart-http-hook still exists
Mar  1 13:16:40.741: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  1 13:16:40.746: INFO: Pod pod-with-poststart-http-hook still exists
Mar  1 13:16:42.741: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  1 13:16:42.747: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Mar  1 13:16:42.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7241" for this suite. 03/01/23 13:16:42.754
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","completed":360,"skipped":6683,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.151 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:16:34.613
    Mar  1 13:16:34.613: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename container-lifecycle-hook 03/01/23 13:16:34.613
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:16:34.633
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:16:34.636
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 03/01/23 13:16:34.646
    Mar  1 13:16:34.658: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-7241" to be "running and ready"
    Mar  1 13:16:34.663: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.73825ms
    Mar  1 13:16:34.663: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Mar  1 13:16:36.670: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.011807792s
    Mar  1 13:16:36.670: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Mar  1 13:16:36.670: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:130
    STEP: create the pod with lifecycle hook 03/01/23 13:16:36.676
    Mar  1 13:16:36.683: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-7241" to be "running and ready"
    Mar  1 13:16:36.690: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 6.579584ms
    Mar  1 13:16:36.690: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Mar  1 13:16:38.697: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.014033675s
    Mar  1 13:16:38.698: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Mar  1 13:16:38.698: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 03/01/23 13:16:38.702
    STEP: delete the pod with lifecycle hook 03/01/23 13:16:38.721
    Mar  1 13:16:38.732: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Mar  1 13:16:38.740: INFO: Pod pod-with-poststart-http-hook still exists
    Mar  1 13:16:40.741: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Mar  1 13:16:40.746: INFO: Pod pod-with-poststart-http-hook still exists
    Mar  1 13:16:42.741: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Mar  1 13:16:42.747: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Mar  1 13:16:42.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-7241" for this suite. 03/01/23 13:16:42.754
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:16:42.763
Mar  1 13:16:42.764: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename pod-network-test 03/01/23 13:16:42.764
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:16:42.786
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:16:42.79
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-8747 03/01/23 13:16:42.793
STEP: creating a selector 03/01/23 13:16:42.793
STEP: Creating the service pods in kubernetes 03/01/23 13:16:42.793
Mar  1 13:16:42.793: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar  1 13:16:42.830: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-8747" to be "running and ready"
Mar  1 13:16:42.838: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.30444ms
Mar  1 13:16:42.838: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar  1 13:16:44.844: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.013411588s
Mar  1 13:16:44.844: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  1 13:16:46.844: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.014023588s
Mar  1 13:16:46.845: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  1 13:16:48.844: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.013292124s
Mar  1 13:16:48.844: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  1 13:16:50.846: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.015255086s
Mar  1 13:16:50.846: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  1 13:16:52.844: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.013335837s
Mar  1 13:16:52.844: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  1 13:16:54.845: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.014327633s
Mar  1 13:16:54.845: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Mar  1 13:16:54.845: INFO: Pod "netserver-0" satisfied condition "running and ready"
Mar  1 13:16:54.850: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-8747" to be "running and ready"
Mar  1 13:16:54.856: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 5.765324ms
Mar  1 13:16:54.856: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Mar  1 13:16:54.856: INFO: Pod "netserver-1" satisfied condition "running and ready"
Mar  1 13:16:54.865: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-8747" to be "running and ready"
Mar  1 13:16:54.871: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 6.54525ms
Mar  1 13:16:54.871: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Mar  1 13:16:54.871: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 03/01/23 13:16:54.879
Mar  1 13:16:54.897: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-8747" to be "running"
Mar  1 13:16:54.908: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 11.037435ms
Mar  1 13:16:56.914: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.017732664s
Mar  1 13:16:56.915: INFO: Pod "test-container-pod" satisfied condition "running"
Mar  1 13:16:56.921: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-8747" to be "running"
Mar  1 13:16:56.926: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.565234ms
Mar  1 13:16:56.926: INFO: Pod "host-test-container-pod" satisfied condition "running"
Mar  1 13:16:56.931: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Mar  1 13:16:56.931: INFO: Going to poll 10.233.95.195 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Mar  1 13:16:56.937: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.95.195:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8747 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  1 13:16:56.937: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
Mar  1 13:16:56.937: INFO: ExecWithOptions: Clientset creation
Mar  1 13:16:56.938: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-8747/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.95.195%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar  1 13:16:57.007: INFO: Found all 1 expected endpoints: [netserver-0]
Mar  1 13:16:57.008: INFO: Going to poll 10.233.64.149 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Mar  1 13:16:57.013: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.64.149:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8747 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  1 13:16:57.013: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
Mar  1 13:16:57.014: INFO: ExecWithOptions: Clientset creation
Mar  1 13:16:57.014: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-8747/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.64.149%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar  1 13:16:57.085: INFO: Found all 1 expected endpoints: [netserver-1]
Mar  1 13:16:57.085: INFO: Going to poll 10.233.74.184 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Mar  1 13:16:57.091: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.74.184:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8747 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  1 13:16:57.091: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
Mar  1 13:16:57.092: INFO: ExecWithOptions: Clientset creation
Mar  1 13:16:57.092: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-8747/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.74.184%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar  1 13:16:57.157: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Mar  1 13:16:57.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8747" for this suite. 03/01/23 13:16:57.166
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","completed":361,"skipped":6690,"failed":0}
------------------------------
â€¢ [SLOW TEST] [14.413 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:16:42.763
    Mar  1 13:16:42.764: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename pod-network-test 03/01/23 13:16:42.764
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:16:42.786
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:16:42.79
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-8747 03/01/23 13:16:42.793
    STEP: creating a selector 03/01/23 13:16:42.793
    STEP: Creating the service pods in kubernetes 03/01/23 13:16:42.793
    Mar  1 13:16:42.793: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Mar  1 13:16:42.830: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-8747" to be "running and ready"
    Mar  1 13:16:42.838: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.30444ms
    Mar  1 13:16:42.838: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Mar  1 13:16:44.844: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.013411588s
    Mar  1 13:16:44.844: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  1 13:16:46.844: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.014023588s
    Mar  1 13:16:46.845: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  1 13:16:48.844: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.013292124s
    Mar  1 13:16:48.844: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  1 13:16:50.846: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.015255086s
    Mar  1 13:16:50.846: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  1 13:16:52.844: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.013335837s
    Mar  1 13:16:52.844: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  1 13:16:54.845: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.014327633s
    Mar  1 13:16:54.845: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Mar  1 13:16:54.845: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Mar  1 13:16:54.850: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-8747" to be "running and ready"
    Mar  1 13:16:54.856: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 5.765324ms
    Mar  1 13:16:54.856: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Mar  1 13:16:54.856: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Mar  1 13:16:54.865: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-8747" to be "running and ready"
    Mar  1 13:16:54.871: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 6.54525ms
    Mar  1 13:16:54.871: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Mar  1 13:16:54.871: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 03/01/23 13:16:54.879
    Mar  1 13:16:54.897: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-8747" to be "running"
    Mar  1 13:16:54.908: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 11.037435ms
    Mar  1 13:16:56.914: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.017732664s
    Mar  1 13:16:56.915: INFO: Pod "test-container-pod" satisfied condition "running"
    Mar  1 13:16:56.921: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-8747" to be "running"
    Mar  1 13:16:56.926: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.565234ms
    Mar  1 13:16:56.926: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Mar  1 13:16:56.931: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Mar  1 13:16:56.931: INFO: Going to poll 10.233.95.195 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Mar  1 13:16:56.937: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.95.195:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8747 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  1 13:16:56.937: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    Mar  1 13:16:56.937: INFO: ExecWithOptions: Clientset creation
    Mar  1 13:16:56.938: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-8747/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.95.195%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar  1 13:16:57.007: INFO: Found all 1 expected endpoints: [netserver-0]
    Mar  1 13:16:57.008: INFO: Going to poll 10.233.64.149 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Mar  1 13:16:57.013: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.64.149:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8747 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  1 13:16:57.013: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    Mar  1 13:16:57.014: INFO: ExecWithOptions: Clientset creation
    Mar  1 13:16:57.014: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-8747/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.64.149%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar  1 13:16:57.085: INFO: Found all 1 expected endpoints: [netserver-1]
    Mar  1 13:16:57.085: INFO: Going to poll 10.233.74.184 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Mar  1 13:16:57.091: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.74.184:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8747 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  1 13:16:57.091: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    Mar  1 13:16:57.092: INFO: ExecWithOptions: Clientset creation
    Mar  1 13:16:57.092: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-8747/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.74.184%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar  1 13:16:57.157: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Mar  1 13:16:57.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-8747" for this suite. 03/01/23 13:16:57.166
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/01/23 13:16:57.179
Mar  1 13:16:57.179: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
STEP: Building a namespace api object, basename pods 03/01/23 13:16:57.18
STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:16:57.198
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:16:57.202
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
Mar  1 13:16:57.215: INFO: Waiting up to 5m0s for pod "server-envvars-643d6a7f-fc6f-450c-8abf-68f6e174ea56" in namespace "pods-890" to be "running and ready"
Mar  1 13:16:57.222: INFO: Pod "server-envvars-643d6a7f-fc6f-450c-8abf-68f6e174ea56": Phase="Pending", Reason="", readiness=false. Elapsed: 6.799778ms
Mar  1 13:16:57.222: INFO: The phase of Pod server-envvars-643d6a7f-fc6f-450c-8abf-68f6e174ea56 is Pending, waiting for it to be Running (with Ready = true)
Mar  1 13:16:59.229: INFO: Pod "server-envvars-643d6a7f-fc6f-450c-8abf-68f6e174ea56": Phase="Running", Reason="", readiness=true. Elapsed: 2.013597111s
Mar  1 13:16:59.229: INFO: The phase of Pod server-envvars-643d6a7f-fc6f-450c-8abf-68f6e174ea56 is Running (Ready = true)
Mar  1 13:16:59.229: INFO: Pod "server-envvars-643d6a7f-fc6f-450c-8abf-68f6e174ea56" satisfied condition "running and ready"
Mar  1 13:16:59.258: INFO: Waiting up to 5m0s for pod "client-envvars-6d8c35ce-b0be-4213-b55a-5606facdb7ca" in namespace "pods-890" to be "Succeeded or Failed"
Mar  1 13:16:59.265: INFO: Pod "client-envvars-6d8c35ce-b0be-4213-b55a-5606facdb7ca": Phase="Pending", Reason="", readiness=false. Elapsed: 6.604902ms
Mar  1 13:17:01.274: INFO: Pod "client-envvars-6d8c35ce-b0be-4213-b55a-5606facdb7ca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015578021s
Mar  1 13:17:03.270: INFO: Pod "client-envvars-6d8c35ce-b0be-4213-b55a-5606facdb7ca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011665942s
STEP: Saw pod success 03/01/23 13:17:03.27
Mar  1 13:17:03.270: INFO: Pod "client-envvars-6d8c35ce-b0be-4213-b55a-5606facdb7ca" satisfied condition "Succeeded or Failed"
Mar  1 13:17:03.276: INFO: Trying to get logs from node lab1-k8s-node-3 pod client-envvars-6d8c35ce-b0be-4213-b55a-5606facdb7ca container env3cont: <nil>
STEP: delete the pod 03/01/23 13:17:03.296
Mar  1 13:17:03.314: INFO: Waiting for pod client-envvars-6d8c35ce-b0be-4213-b55a-5606facdb7ca to disappear
Mar  1 13:17:03.318: INFO: Pod client-envvars-6d8c35ce-b0be-4213-b55a-5606facdb7ca no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Mar  1 13:17:03.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-890" for this suite. 03/01/23 13:17:03.325
{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","completed":362,"skipped":6692,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.154 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/01/23 13:16:57.179
    Mar  1 13:16:57.179: INFO: >>> kubeConfig: /tmp/kubeconfig-3765612722
    STEP: Building a namespace api object, basename pods 03/01/23 13:16:57.18
    STEP: Waiting for a default service account to be provisioned in namespace 03/01/23 13:16:57.198
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/01/23 13:16:57.202
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:443
    Mar  1 13:16:57.215: INFO: Waiting up to 5m0s for pod "server-envvars-643d6a7f-fc6f-450c-8abf-68f6e174ea56" in namespace "pods-890" to be "running and ready"
    Mar  1 13:16:57.222: INFO: Pod "server-envvars-643d6a7f-fc6f-450c-8abf-68f6e174ea56": Phase="Pending", Reason="", readiness=false. Elapsed: 6.799778ms
    Mar  1 13:16:57.222: INFO: The phase of Pod server-envvars-643d6a7f-fc6f-450c-8abf-68f6e174ea56 is Pending, waiting for it to be Running (with Ready = true)
    Mar  1 13:16:59.229: INFO: Pod "server-envvars-643d6a7f-fc6f-450c-8abf-68f6e174ea56": Phase="Running", Reason="", readiness=true. Elapsed: 2.013597111s
    Mar  1 13:16:59.229: INFO: The phase of Pod server-envvars-643d6a7f-fc6f-450c-8abf-68f6e174ea56 is Running (Ready = true)
    Mar  1 13:16:59.229: INFO: Pod "server-envvars-643d6a7f-fc6f-450c-8abf-68f6e174ea56" satisfied condition "running and ready"
    Mar  1 13:16:59.258: INFO: Waiting up to 5m0s for pod "client-envvars-6d8c35ce-b0be-4213-b55a-5606facdb7ca" in namespace "pods-890" to be "Succeeded or Failed"
    Mar  1 13:16:59.265: INFO: Pod "client-envvars-6d8c35ce-b0be-4213-b55a-5606facdb7ca": Phase="Pending", Reason="", readiness=false. Elapsed: 6.604902ms
    Mar  1 13:17:01.274: INFO: Pod "client-envvars-6d8c35ce-b0be-4213-b55a-5606facdb7ca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015578021s
    Mar  1 13:17:03.270: INFO: Pod "client-envvars-6d8c35ce-b0be-4213-b55a-5606facdb7ca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011665942s
    STEP: Saw pod success 03/01/23 13:17:03.27
    Mar  1 13:17:03.270: INFO: Pod "client-envvars-6d8c35ce-b0be-4213-b55a-5606facdb7ca" satisfied condition "Succeeded or Failed"
    Mar  1 13:17:03.276: INFO: Trying to get logs from node lab1-k8s-node-3 pod client-envvars-6d8c35ce-b0be-4213-b55a-5606facdb7ca container env3cont: <nil>
    STEP: delete the pod 03/01/23 13:17:03.296
    Mar  1 13:17:03.314: INFO: Waiting for pod client-envvars-6d8c35ce-b0be-4213-b55a-5606facdb7ca to disappear
    Mar  1 13:17:03.318: INFO: Pod client-envvars-6d8c35ce-b0be-4213-b55a-5606facdb7ca no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Mar  1 13:17:03.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-890" for this suite. 03/01/23 13:17:03.325
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
{"msg":"Test Suite completed","completed":362,"skipped":6704,"failed":0}
Mar  1 13:17:03.335: INFO: Running AfterSuite actions on all nodes
Mar  1 13:17:03.335: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
Mar  1 13:17:03.335: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
Mar  1 13:17:03.335: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
Mar  1 13:17:03.335: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
Mar  1 13:17:03.335: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
Mar  1 13:17:03.335: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
Mar  1 13:17:03.335: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
Mar  1 13:17:03.335: INFO: Running AfterSuite actions on node 1
Mar  1 13:17:03.335: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.000 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    Mar  1 13:17:03.335: INFO: Running AfterSuite actions on all nodes
    Mar  1 13:17:03.335: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
    Mar  1 13:17:03.335: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
    Mar  1 13:17:03.335: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
    Mar  1 13:17:03.335: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
    Mar  1 13:17:03.335: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
    Mar  1 13:17:03.335: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
    Mar  1 13:17:03.335: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    Mar  1 13:17:03.335: INFO: Running AfterSuite actions on node 1
    Mar  1 13:17:03.335: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:146
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:146
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:559
------------------------------
[ReportAfterSuite] PASSED [0.046 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:559
  << End Captured GinkgoWriter Output
------------------------------

Ran 362 of 7066 Specs in 5681.049 seconds
SUCCESS! -- 362 Passed | 0 Failed | 0 Pending | 6704 Skipped
PASS

Ginkgo ran 1 suite in 1h34m41.303374084s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.1.6[0m

